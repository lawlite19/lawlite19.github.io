<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Lawlite的博客</title>
  <icon>https://www.gravatar.com/avatar/af9133ab432a8ab359eccaf9cdcbfa55</icon>
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://lawlite.me/"/>
  <updated>2018-03-03T07:07:34.555Z</updated>
  <id>http://lawlite.me/</id>
  
  <author>
    <name>Lawlite</name>
    <email>lawlitewang@gmail.com</email>
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>风格迁移 Style transfer</title>
    <link href="http://lawlite.me/2018/02/28/%E9%A3%8E%E6%A0%BC%E8%BF%81%E7%A7%BBStyle-transfer/"/>
    <id>http://lawlite.me/2018/02/28/风格迁移Style-transfer/</id>
    <published>2018-02-28T11:30:05.000Z</published>
    <updated>2018-03-03T07:07:34.555Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一、介绍"><a href="#一、介绍" class="headerlink" title="一、介绍"></a>一、介绍</h2><ul><li>将一张图片的艺术风格应用在另外一张图片上</li></ul><p><img src="/assets/blog_images/Style_transfer/01_style-transfer.png" alt="风格迁移结果" title="01_style-transfer"></p><ul><li>使用深度卷积网络CNN提取一张图片的<strong>内容</strong>和提取一张图片的<strong>风格</strong>， 然后将两者结合起来得到最后的结果</li></ul><a id="more"></a><h2 id="二、-方法"><a href="#二、-方法" class="headerlink" title="二、 方法"></a>二、 方法</h2><p><img src="/assets/blog_images/Style_transfer/03_representation.png" alt="提取图片内容和风格并重构" title="02_representation"></p><ul><li>我们知道 <code>CNN</code> 可以捕捉图像的高层次特征，如上图所示，内容图片经过<code>CNN</code>可以得到对应的图像表述(<code>representation</code>, 就是经过卷积操作的<code>feature map</code>)，然后经过重构可以得到近似原图的效果<ul><li>特别是前面几层经过重构得到的结果和原图更接近，也说明前几层保留的图片细节会更多，因为后面还有<code>pooling</code>层，自然会丢弃调一些信息</li><li>这里的网络使用的是<code>VGG-16</code> (如下图)，包含 <code>13</code> 个卷积层，<code>3</code> 个全连接层<br><img src="/assets/blog_images/Style_transfer/04_vgg16.png" alt="vgg-16结构" title="04_vgg16"></li></ul></li></ul><h3 id="1、内容损失"><a href="#1、内容损失" class="headerlink" title="1、内容损失"></a>1、内容损失</h3><ul><li>假设一个卷积层包含 ${N_l}$ 个过滤器 <code>filters</code>，则可以得到 ${N_l}$ 个<code>feature maps</code>，假设 <code>feature map</code>的大小是 $M_l$ (长乘宽)，则可以通过一个矩阵来存储 <code>l</code> 层的数据 $$F^l \in R^{N_l \times M_l} $$<ul><li>$F^l_{i,j}$ 表示第 <code>l</code> 层的第 <code>i</code> 个<code>filter</code> 在 <code>j</code> 位置上的激活值</li></ul></li><li>所以现在一张内容图片$\overrightarrow p$，一张生成图片$\overrightarrow x$(初始值为高斯分布), 经过一层<strong>卷积层l</strong>可以得到其对应的特征表示：$P^l$ 和 $F_l$, 则对应的损失采用<strong>均方误差</strong>: $$L_{content}(\overrightarrow p, \overrightarrow x, l) = {1 \over 2} \sum_{ij}(F^l_{ij}-P^l_{ij})^2$$<ul><li>$F$ 和 $P$是两个矩阵，大小是$N_l \times M_l$，即<code>l</code>层过滤器的个数 和 <code>feature map</code> 的长乘宽的值</li></ul></li></ul><h3 id="2、风格损失"><a href="#2、风格损失" class="headerlink" title="2、风格损失"></a>2、风格损失</h3><ul><li><strong>风格的表示</strong>这里采用<strong>格拉姆矩阵</strong>(<code>Gram Matrix</code>): $G^l \in R^{N_l \times N_l}$  $$G^l_{ij} = {\sum_k F^l_{ik}F^l_{jk}}$$<ul><li>格拉姆矩阵计算的是<strong>两两特征的相关性</strong> , 即哪两个特征是同时出现的，哪两个特征是此消彼长的等，能够<strong>保留图像的风格</strong></li><li>( 比如一幅画中有人和树，它们可以出现在任意位置，格拉姆矩阵可以衡量它们之间的关系，可以认为是这幅画的风格信息 )</li></ul></li><li><p>假设$\overrightarrow a$是风格图像，$\overrightarrow x$是生成图像，$A^l$ 和 $G^l$ 表示在 $l$ 层的格拉姆矩阵，则这一层的损失为：$$E_l = {1 \over 4N^2_lM^2_l}{\sum_{i,j} (G^l_{ij}-A^l_{ij})^2}$$</p></li><li><p>提取风格信息是我们会使用多个卷积层的输出，所以总损失为：$$L_{style}(\overrightarrow a, \overrightarrow x) = {\sum^L_lw_lE_l}$$</p><ul><li>这里$w_l$是每一层损失的权重</li></ul></li></ul><h3 id="3、总损失函数"><a href="#3、总损失函数" class="headerlink" title="3、总损失函数"></a>3、总损失函数</h3><ul><li>通过<strong>白噪声初始化</strong>(就是高斯分布)一个输出的结果，然后通过网络对这个结果进行风格和内容两方面的约束进行修正<br>$$L_{total}(\overrightarrow p,\overrightarrow a,\overrightarrow x)=\alpha L_{content}(\overrightarrow p, \overrightarrow x) +\beta L_{style}(\overrightarrow a, \overrightarrow x)$$</li></ul><h2 id="三、代码实现"><a href="#三、代码实现" class="headerlink" title="三、代码实现"></a>三、代码实现</h2><h3 id="1、说明"><a href="#1、说明" class="headerlink" title="1、说明"></a>1、说明</h3><ul><li>全部代码：<a href="https://github.com/lawlite19/Blog-Back-Up/blob/master/code/style-transfer/style_transfer.py" target="_blank" rel="external">点击查看</a></li><li>图像使用一张建筑图和梵高的星空<br><img src="/assets/blog_images/Style_transfer/05_buildings.jpg" alt="建筑" title="05_buildings"><br><img src="/assets/blog_images/Style_transfer/06_starry-sky.jpg" alt="星空" title="06_starry-sky"></li></ul><h3 id="2、加载并预处理图片和初始化输出图片"><a href="#2、加载并预处理图片和初始化输出图片" class="headerlink" title="2、加载并预处理图片和初始化输出图片"></a>2、加载并预处理图片和初始化输出图片</h3><ul><li>输出图片采用高斯分布初始化</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> backend <span class="keyword">as</span> K</div><div class="line"><span class="keyword">from</span> keras.applications.vgg16 <span class="keyword">import</span> preprocess_input</div><div class="line"><span class="keyword">from</span> keras.preprocessing.image <span class="keyword">import</span> load_img, img_to_array</div><div class="line"></div><div class="line"><span class="keyword">from</span> keras.applications <span class="keyword">import</span> VGG16</div><div class="line"><span class="keyword">from</span> scipy.optimize <span class="keyword">import</span> fmin_l_bfgs_b</div><div class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</div><div class="line"></div><div class="line"><span class="string">'''图片路径'''</span></div><div class="line">content_image_path = <span class="string">'./data/buildings.jpg'</span></div><div class="line">style_image_path = <span class="string">'./data/starry-sky.jpg'</span></div><div class="line">generate_image_path = <span class="string">'./data/output.jpg'</span></div><div class="line"></div><div class="line"></div><div class="line"><span class="string">'''加载图片并初始化输出图片'''</span></div><div class="line">target_height = <span class="number">512</span></div><div class="line">target_width = <span class="number">512</span></div><div class="line">target_size = (target_height, target_width)</div><div class="line"></div><div class="line">content_image = load_img(content_image_path, target_size=target_size)</div><div class="line">content_image_array = img_to_array(content_image)</div><div class="line">content_image_array = K.variable(preprocess_input(np.expand_dims(content_image_array, <span class="number">0</span>)), dtype=<span class="string">'float32'</span>)</div><div class="line"></div><div class="line">style_image = load_img(style_image_path, target_size=target_size)</div><div class="line">style_image_array = img_to_array(style_image)</div><div class="line">style_image_array = K.variable(preprocess_input(np.expand_dims(style_image_array, <span class="number">0</span>)), dtype=<span class="string">'float32'</span>)</div><div class="line"></div><div class="line">generate_image = np.random.randint(<span class="number">256</span>, size=(target_width, target_height, <span class="number">3</span>)).astype(<span class="string">'float64'</span>)</div><div class="line">generate_image = preprocess_input(np.expand_dims(generate_image, <span class="number">0</span>))</div><div class="line">generate_image_placeholder = K.placeholder(shape=(<span class="number">1</span>, target_width, target_height, <span class="number">3</span>))</div></pre></td></tr></table></figure><h3 id="3、获取网络中对应层的输出"><a href="#3、获取网络中对应层的输出" class="headerlink" title="3、获取网络中对应层的输出"></a>3、获取网络中对应层的输出</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_feature_represent</span><span class="params">(x, layer_names, model)</span>:</span></div><div class="line">    <span class="string">'''图片的特征图表示</span></div><div class="line">    </div><div class="line">    参数</div><div class="line">    ----------------------------------------------</div><div class="line">    x : 输入，</div><div class="line">        这里并没有使用，可以看作一个输入的标识</div><div class="line">    layer_names : list</div><div class="line">        CNN网络层的名字</div><div class="line">    model : CNN模型</div><div class="line">    </div><div class="line">    返回值</div><div class="line">    ----------------------------------------------</div><div class="line">    feature_matrices : list</div><div class="line">        经过CNN卷积层的特征表示，这里大小是(filter个数, feature map的长*宽)</div><div class="line">    </div><div class="line">    '''</div><div class="line">    feature_matrices = []</div><div class="line">    <span class="keyword">for</span> ln <span class="keyword">in</span> layer_names:</div><div class="line">        select_layer = model.get_layer(ln)</div><div class="line">        feature_raw = select_layer.output</div><div class="line">        feature_raw_shape = K.shape(feature_raw).eval(session=tf_session)</div><div class="line">        N_l = feature_raw_shape[<span class="number">-1</span>]</div><div class="line">        M_l = feature_raw_shape[<span class="number">1</span>]*feature_raw_shape[<span class="number">2</span>]</div><div class="line">        feature_matrix = K.reshape(feature_raw, (M_l, N_l))</div><div class="line">        feature_matrix = K.transpose(feature_matrix)</div><div class="line">        feature_matrices.append(feature_matrix)</div><div class="line">    <span class="keyword">return</span> feature_matrices</div></pre></td></tr></table></figure><h3 id="4、内容损失函数"><a href="#4、内容损失函数" class="headerlink" title="4、内容损失函数"></a>4、内容损失函数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_content_loss</span><span class="params">(F, P)</span>:</span></div><div class="line">    <span class="string">'''计算内容损失</span></div><div class="line">    </div><div class="line">    参数</div><div class="line">    ---------------------------------------</div><div class="line">    F : tensor, float32</div><div class="line">        生成图片特征图矩阵</div><div class="line">    P : tensor, float32</div><div class="line">        内容图片特征图矩阵</div><div class="line">    </div><div class="line">    返回值</div><div class="line">    ---------------------------------------</div><div class="line">    content_loss : tensor, float32</div><div class="line">        内容损失</div><div class="line">    '''</div><div class="line">    content_loss = <span class="number">0.5</span>*K.sum(K.square(F-P))</div><div class="line">    <span class="keyword">return</span> content_loss</div></pre></td></tr></table></figure><h3 id="5、Gram矩阵和风格损失"><a href="#5、Gram矩阵和风格损失" class="headerlink" title="5、Gram矩阵和风格损失"></a>5、Gram矩阵和风格损失</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_gram_matrix</span><span class="params">(F)</span>:</span></div><div class="line">    <span class="string">'''计算gram矩阵'''</span></div><div class="line">    G = K.dot(F, K.transpose(F))</div><div class="line">    <span class="keyword">return</span> G</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_style_loss</span><span class="params">(ws, Gs, As)</span>:</span></div><div class="line">    <span class="string">'''计算风格损失</span></div><div class="line">    </div><div class="line">    参数</div><div class="line">    ---------------------------------------</div><div class="line">    ws : array</div><div class="line">         每一层layer的权重</div><div class="line">    Gs : list</div><div class="line">         生成图片每一层得到的特征表示组成的list</div><div class="line">    As : list</div><div class="line">         风格图片每一层得到的特征表示组成的list</div><div class="line">    '''</div><div class="line">    style_loss = K.variable(<span class="number">0.</span>)</div><div class="line">    <span class="keyword">for</span> w, G, A <span class="keyword">in</span> zip(ws, Gs, As):</div><div class="line">        M_l = K.int_shape(G)[<span class="number">1</span>]</div><div class="line">        N_l = K.int_shape(G)[<span class="number">0</span>]</div><div class="line">        G_gram = get_gram_matrix(G)</div><div class="line">        A_gram = get_gram_matrix(A)</div><div class="line">        style_loss += w*<span class="number">0.25</span>*K.sum(K.square(G_gram-A_gram))/(N_l**<span class="number">2</span>*M_l**<span class="number">2</span>)</div><div class="line">    <span class="keyword">return</span> style_loss</div></pre></td></tr></table></figure><h3 id="6、总损失"><a href="#6、总损失" class="headerlink" title="6、总损失"></a>6、总损失</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_total_loss</span><span class="params">(generate_image_placeholder, alpha=<span class="number">1.0</span>, beta=<span class="number">10000.0</span>)</span>:</span></div><div class="line">    <span class="string">'''总损失</span></div><div class="line">    '''</div><div class="line">    F = get_feature_represent(generate_image_placeholder, layer_names=[content_layer_name], model=gModel)[<span class="number">0</span>]</div><div class="line">    Gs = get_feature_represent(generate_image_placeholder, layer_names=style_layer_names, model=gModel)</div><div class="line">    content_loss = get_content_loss(F, P)</div><div class="line">    style_loss = get_style_loss(ws, Gs, As)</div><div class="line">    total_loss = alpha*content_loss + beta*style_loss</div><div class="line">    <span class="keyword">return</span> total_loss</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">calculate_loss</span><span class="params">(gen_image_array)</span>:</span></div><div class="line">    <span class="string">'''调用总损失函数，计算得到总损失数值'''</span></div><div class="line">    <span class="keyword">if</span> gen_image_array != (<span class="number">1</span>, target_width, target_height, <span class="number">3</span>):</div><div class="line">        gen_image_array = gen_image_array.reshape((<span class="number">1</span>, target_width, target_height, <span class="number">3</span>))</div><div class="line">    loss_fn = K.function(inputs=[gModel.input], outputs=[get_total_loss(gModel.input)])</div><div class="line">    <span class="keyword">return</span> loss_fn([gen_image_array])[<span class="number">0</span>].astype(<span class="string">'float64'</span>)</div></pre></td></tr></table></figure><h3 id="7、损失函数梯度"><a href="#7、损失函数梯度" class="headerlink" title="7、损失函数梯度"></a>7、损失函数梯度</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_grad</span><span class="params">(gen_image_array)</span>:</span></div><div class="line">    <span class="string">'''计算损失函数的梯度'''</span></div><div class="line">    <span class="keyword">if</span> gen_image_array != (<span class="number">1</span>, target_width, target_height, <span class="number">3</span>):</div><div class="line">        gen_image_array = gen_image_array.reshape((<span class="number">1</span>, target_width, target_height, <span class="number">3</span>))</div><div class="line">    grad_fn = K.function([gModel.input], K.gradients(get_total_loss(gModel.input), [gModel.input]))</div><div class="line">    grad = grad_fn([gen_image_array])[<span class="number">0</span>].flatten().astype(<span class="string">'float64'</span>)</div><div class="line">    <span class="keyword">return</span> grad</div></pre></td></tr></table></figure><h3 id="8、生成结果后处理"><a href="#8、生成结果后处理" class="headerlink" title="8、生成结果后处理"></a>8、生成结果后处理</h3><ul><li>因为之<code>前preprocess_input</code>函数中做了处理，这里进行逆处理还原<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">postprocess_array</span><span class="params">(x)</span>:</span></div><div class="line">    <span class="string">'''生成图片后处理，因为之前preprocess_input函数中做了处理，这里进行逆处理还原</span></div><div class="line">    </div><div class="line">    '''</div><div class="line">    <span class="keyword">if</span> x.shape != (target_width, target_height, <span class="number">3</span>):</div><div class="line">        x = x.reshape((target_width, target_height, <span class="number">3</span>))</div><div class="line">    x[..., <span class="number">0</span>] += <span class="number">103.939</span></div><div class="line">    x[..., <span class="number">1</span>] += <span class="number">116.779</span></div><div class="line">    x[..., <span class="number">2</span>] += <span class="number">123.68</span></div><div class="line">    </div><div class="line">    x = x[..., ::<span class="number">-1</span>]  <span class="comment"># BGR--&gt;RGB</span></div><div class="line">    x = np.clip(x, <span class="number">0</span>, <span class="number">255</span>)</div><div class="line">    x = x.astype(<span class="string">'uint8'</span>)</div><div class="line">    <span class="keyword">return</span> x</div></pre></td></tr></table></figure></li></ul><h3 id="9、定义模型并优化"><a href="#9、定义模型并优化" class="headerlink" title="9、定义模型并优化"></a>9、定义模型并优化</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line"><span class="string">'''定义VGG模型'''</span></div><div class="line">tf_session = K.get_session()</div><div class="line">cModel = VGG16(include_top=<span class="keyword">False</span>, input_tensor=content_image_array)</div><div class="line">sModel = VGG16(include_top=<span class="keyword">False</span>, input_tensor=style_image_array)</div><div class="line">gModel = VGG16(include_top=<span class="keyword">False</span>, input_tensor=generate_image_placeholder)</div><div class="line">content_layer_name = <span class="string">'block4_conv2'</span></div><div class="line">style_layer_names = [</div><div class="line">    <span class="string">'block1_conv1'</span>,</div><div class="line">    <span class="string">'block2_conv1'</span>,</div><div class="line">    <span class="string">'block3_conv1'</span>,</div><div class="line">    <span class="string">'block4_conv1'</span></div><div class="line">]</div><div class="line"></div><div class="line"><span class="string">'''得到对应的representation矩阵'''</span></div><div class="line">P = get_feature_represent(x=content_image_array, layer_names=[content_layer_name], model=cModel)[<span class="number">0</span>]</div><div class="line">As = get_feature_represent(x=style_image_array, layer_names=style_layer_names, model=sModel)</div><div class="line">ws = np.ones(len(style_layer_names))/float(len(style_layer_names))</div><div class="line"></div><div class="line"><span class="string">'''使用fmin_l_bfgs_b进行损失函数优化'''</span></div><div class="line">iterations = <span class="number">600</span></div><div class="line">x_val = generate_image.flatten()</div><div class="line">xopt, f_val, info = fmin_l_bfgs_b(func=calculate_loss, x0=x_val, fprime=get_grad, maxiter=iterations, disp=<span class="keyword">True</span>)</div><div class="line"></div><div class="line">x_out = postprocess_array(xopt)</div></pre></td></tr></table></figure><h3 id="10、输出结果"><a href="#10、输出结果" class="headerlink" title="10、输出结果"></a>10、输出结果</h3><ul><li>初始化输出图片</li></ul><p><img src="/assets/blog_images/Style_transfer/07_initialize.jpg" alt="初始化输出图片" title="07_initialize"></p><ul><li>迭代200次，${\beta \over \alpha} = 10^3$<br><img src="/assets/blog_images/Style_transfer/09_final-result100.jpg" alt="迭代200轮结果" title="09_final-result100"></li></ul><ul><li>迭代<code>500</code>轮，${\beta \over \alpha} = 10^4$<br><img src="/assets/blog_images/Style_transfer/08_final-result.jpg" alt="迭代500轮结果" title="08_final-result"></li></ul><h2 id="四、总结"><a href="#四、总结" class="headerlink" title="四、总结"></a>四、总结</h2><ul><li><code>style tranfer</code>通过<strong>白噪声初始化</strong>(就是高斯分布)一个输出的结果，然后通过优化损失对这个结果进行<strong>风格</strong>和<strong>内容</strong>两方面的约束修正</li><li>图片的风格信息使用的是 <strong>Gram矩阵</strong>来表示</li><li>其中超参数风格损失的权重<code>ws</code>、内容损失和风格损失的权重$\alpha$, $\beta$可以进行调整查看结果<ul><li>论文给出的${\beta \over \alpha} = 10^3或10^4$结果较好，可以自己适当增加看看最后的结果</li></ul></li></ul><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li>Paper: <a href="https://arxiv.org/pdf/1508.06576.pdf" target="_blank" rel="external">https://arxiv.org/pdf/1508.06576.pdf</a></li><li><a href="https://www.cs.toronto.edu/~frossard/post/vgg16/" target="_blank" rel="external">https://www.cs.toronto.edu/~frossard/post/vgg16/</a></li><li><a href="https://zhuanlan.zhihu.com/p/33910138" target="_blank" rel="external">https://zhuanlan.zhihu.com/p/33910138</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;一、介绍&quot;&gt;&lt;a href=&quot;#一、介绍&quot; class=&quot;headerlink&quot; title=&quot;一、介绍&quot;&gt;&lt;/a&gt;一、介绍&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;将一张图片的艺术风格应用在另外一张图片上&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&quot;/assets/blog_images/Style_transfer/01_style-transfer.png&quot; alt=&quot;风格迁移结果&quot; title=&quot;01_style-transfer&quot;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;使用深度卷积网络CNN提取一张图片的&lt;strong&gt;内容&lt;/strong&gt;和提取一张图片的&lt;strong&gt;风格&lt;/strong&gt;， 然后将两者结合起来得到最后的结果&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="Style transfer" scheme="http://lawlite.me/tags/Style-transfer/"/>
    
      <category term="Keras" scheme="http://lawlite.me/tags/Keras/"/>
    
  </entry>
  
  <entry>
    <title>阿里云GPU服务器上Torch安装与使用</title>
    <link href="http://lawlite.me/2017/12/25/%E9%98%BF%E9%87%8C%E4%BA%91GPU%E6%9C%8D%E5%8A%A1%E5%99%A8%E4%B8%8ATorch%E5%AE%89%E8%A3%85%E4%B8%8E%E4%BD%BF%E7%94%A8/"/>
    <id>http://lawlite.me/2017/12/25/阿里云GPU服务器上Torch安装与使用/</id>
    <published>2017-12-25T01:35:30.000Z</published>
    <updated>2017-12-28T09:12:19.248Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一、介绍"><a href="#一、介绍" class="headerlink" title="一、介绍"></a>一、介绍</h2><ul><li>阿里云的<code>GPU</code>也有了竞价服务，每小时大概1块多，还是可以接受的</li><li>主要想跑<code>github</code>上的一个<a href="https://github.com/ShuangLI59/Person-Search-with-Natural-Language-Description" target="_blank" rel="external">论文代码</a>，使用的<code>GPU</code>,(奈何实验室没有<code>GPU</code>)， 本来我已经改成<code>CPU</code>版本的了，但是他训练好的模型是基于<code>GPU</code>的，所以还需要重新训练，结果非常的慢…</li><li>包含以下内容：<ul><li>购买竞价<code>GPU</code></li><li>通过<code>SSH</code>连接云服务器</li><li>安装<code>Torch、hdf5、cjson、loadcaffe</code></li><li>安装<code>cuda、cudnn、cunn</code></li></ul></li></ul><a id="more"></a><h2 id="二、购买GPU服务器"><a href="#二、购买GPU服务器" class="headerlink" title="二、购买GPU服务器"></a>二、购买GPU服务器</h2><ul><li>进入阿里云<code>GPU</code>介绍页，<a href="https://www.aliyun.com/product/ecs/gpu?spm=5176.8142029.388261.231.6a1d38418YDpK6" target="_blank" rel="external">点击访问</a>，界面如下，我选择的是<code>GN5(P100)</code></li></ul><p><img src="/assets/blog_images/GPU/0_GPU%E4%BB%8B%E7%BB%8D%E9%A1%B5.png" alt="GPU介绍页" title="0_GPU介绍页"></p><ul><li>选择竞价实例</li></ul><p><img src="/assets/blog_images/GPU/1_%E7%AB%9E%E4%BB%B7%E5%AE%9E%E4%BE%8B.png" alt="选择竞价实例" title="1_竞价实例"></p><ul><li>选择<code>GPU</code></li></ul><p><img src="/assets/blog_images/GPU/2_%E9%80%89%E6%8B%A9GPU.jpg" alt="选择GPU" title="2_选择GPU"></p><ul><li>选择<code>Ubuntu</code>版本和带宽<ul><li>这里按使用流量，所以带宽设置大点没有影响</li></ul></li></ul><p><img src="/assets/blog_images/GPU/3_%E9%80%89%E6%8B%A9Ubuntu%E7%89%88%E6%9C%AC%E5%92%8C%E5%B8%A6%E5%AE%BD.png" alt="选择系统和带宽" title="3_选择Ubuntu版本和带宽"></p><ul><li>在控制台可以看到服务器信息，下面需要使用公网<code>IP</code>连接</li></ul><p><img src="/assets/blog_images/GPU/13_%E6%8E%A7%E5%88%B6%E5%8F%B0.png" alt="控制台" title="13_控制台"></p><h2 id="三、连接GPU服务器以及软件的安装"><a href="#三、连接GPU服务器以及软件的安装" class="headerlink" title="三、连接GPU服务器以及软件的安装"></a>三、连接GPU服务器以及软件的安装</h2><h3 id="1、使用SecureCRT连接服务器"><a href="#1、使用SecureCRT连接服务器" class="headerlink" title="1、使用SecureCRT连接服务器"></a>1、使用<code>SecureCRT</code>连接服务器</h3><p><img src="/assets/blog_images/GPU/4_secureCRT.png" alt="连接服务器" title="4_secureCRT"></p><h3 id="2、安装前准备工作"><a href="#2、安装前准备工作" class="headerlink" title="2、安装前准备工作"></a>2、安装前准备工作</h3><ul><li><code>apt clean</code></li><li><code>apt update</code></li><li>安装<code>git</code>命令行：<code>apt install git</code></li><li>生成<code>ssh-key</code> : <code>ssh-keygen -t rsa -C &quot;youremail@example.com&quot;</code><ul><li>将<code>/root/.ssh/id_rsa.pub</code>中内容加入到<code>github</code></li></ul></li></ul><h3 id="3、安装Torch"><a href="#3、安装Torch" class="headerlink" title="3、安装Torch"></a>3、安装<code>Torch</code></h3><ul><li>网址：<a href="http://torch.ch/docs/getting-started.html" target="_blank" rel="external">http://torch.ch/docs/getting-started.html</a></li><li><code>git clone https://github.com/torch/distro.git ~/torch --recursive</code></li><li><code>cd ~/torch</code></li><li><code>bash install-deps</code></li><li><code>./install.sh</code></li><li><code>source ~/.bashrc</code></li><li>输入<code>th</code>查看安装是否成功</li></ul><p><img src="/assets/blog_images/GPU/9_torch.png" alt="torch" title="9_torch"></p><h3 id="4、-安装hdf5"><a href="#4、-安装hdf5" class="headerlink" title="4、 安装hdf5"></a>4、 安装<code>hdf5</code></h3><ul><li>地址: <a href="https://github.com/deepmind/torch-hdf5/blob/master/doc/usage.md" target="_blank" rel="external">https://github.com/deepmind/torch-hdf5/blob/master/doc/usage.md</a></li><li><code>apt-get install libhdf5-serial-dev hdf5-tools</code></li><li><code>git clone https://github.com/deepmind/torch-hdf5</code></li><li><code>cd torch-hdf5</code></li><li><code>luarocks make hdf5-0-0.rockspec LIBHDF5_LIBDIR=&quot;/usr/lib/x86_64-linux-gnu/&quot;</code><ul><li>注意这里 <code>luarocks</code> 是 <code>Torch</code> 里的，在 <code>/root/torch/install/bin</code> 目录下</li></ul></li></ul><h3 id="5、-安装-cjson-和-loadcaffe"><a href="#5、-安装-cjson-和-loadcaffe" class="headerlink" title="5、 安装 cjson 和 loadcaffe"></a>5、 安装 <code>cjson</code> 和 <code>loadcaffe</code></h3><ul><li><code>luarocks install lua-cjson</code></li><li><code>apt-get install libprotobuf-dev protobuf-compiler</code></li><li><code>luarocks install loadcaffe</code></li></ul><h3 id="6、安装Cuda"><a href="#6、安装Cuda" class="headerlink" title="6、安装Cuda"></a>6、安装<code>Cuda</code></h3><ul><li>网址：<a href="https://developer.nvidia.com/cuda-downloads?target_os=Linux&amp;target_arch=x86_64&amp;target_distro=Ubuntu&amp;target_version=1604&amp;target_type=deblocal" target="_blank" rel="external">点击查看</a></li><li>选择对应的<code>cuda</code>版本</li></ul><p><img src="/assets/blog_images/GPU/5_cuda%E9%80%89%E6%8B%A9.png" alt="cuda" title="5_cuda选择"></p><ul><li><code>sudo dpkg -i cuda-repo-ubuntu1604-9-1-local_9.1.85-1_amd64.deb</code></li><li><code>sudo apt-key add /var/cuda-repo-&lt;version&gt;/7fa2af80.pub</code></li><li><code>sudo apt-get update</code></li><li><code>sudo apt-get install cuda</code></li><li>安装完成后会在<code>/usr/local/</code>目录下出现<code>cuda-9.1</code>的目录</li><li>加入到环境变量<ul><li><code>echo &quot;export PATH=/usr/local/cuda-9.1/bin/:\$PATH; export LD_LIBRARY_PATH=/usr/local/cuda-9.1/lib64/:\$LD_LIBRARY_PATH; &quot; &gt;&gt;~/.bashrc &amp;&amp; source ~/.bashrc</code></li></ul></li><li>此时<code>cuda</code>已经安装成功，可以通过<code>nvcc -V</code>测试是否安装成功<ul><li><code>nvidia-smi</code>命令查看<code>GPU</code>使用情况</li></ul></li></ul><p><img src="/assets/blog_images/GPU/6_cuda%E5%AE%89%E8%A3%85%E6%B5%8B%E8%AF%95.png" alt="cuda安装测试" title="6_cuda安装测试"></p><ul><li>有时可能需要重启一下</li></ul><h3 id="7、安装cudnn"><a href="#7、安装cudnn" class="headerlink" title="7、安装cudnn"></a>7、安装<code>cudnn</code></h3><ul><li>网址1：<a href="https://github.com/facebookarchive/fbcunn/blob/master/INSTALL.md#install-cuda" target="_blank" rel="external">点击查看</a></li><li>网址2：<a href="https://developer.nvidia.com/rdp/cudnn-download" target="_blank" rel="external">下载cudnn</a><ul><li>需要先注册登录才能下载</li></ul></li><li>注意这里下载的版本，我这里使用的是<code>5.1</code>版本（尝试了最新的7.x版本，有问题）</li></ul><p><img src="/assets/blog_images/GPU/7_cudnn%E4%B8%8B%E8%BD%BD.png" alt="cudnn版本" title="7_cudnn下载"></p><ul><li>直接<code>luarocks install cudnn</code>是可以成功安装的，但是有问题</li><li>下载的是<strong>压缩包</strong>,里面有两个文件夹</li></ul><p><img src="/assets/blog_images/GPU/8_cudnn%E5%8E%8B%E7%BC%A9%E5%8C%85.png" alt="cudnn压缩包" title="8_cudnn压缩包"></p><ul><li>将<code>include</code>下的<code>cudnn.h</code>文件拷贝到<code>/usr/local/cuda-9.1/include/</code>文件夹下</li><li>将<code>lib64</code>下的<code>libcudnn.so.5.1.10</code>文件拷贝到<code>/usr/local/cuda-9.1/lib64/</code>文件夹下<ul><li>并且创建软连接: <code>ln -s libcudnn.so.5.1.10 libcudnn.so.5</code></li></ul></li><li>添加环境变量：<code>export CUDNN_PATH=&quot;/usr/local/cuda-9.1/lib64/libcudnn.so.5&quot;</code></li></ul><p><img src="/assets/blog_images/GPU/12_cudnn.png" alt="cudnn5.x" title="12_cudnn"></p><h2 id="四、测试"><a href="#四、测试" class="headerlink" title="四、测试"></a>四、测试</h2><ul><li>下面是我跑的一个程序</li></ul><p><img src="/assets/blog_images/GPU/10_GPU%E4%BD%BF%E7%94%A8%E6%83%85%E5%86%B5.png" alt="GPU使用情况" title="10_GPU使用情况"></p><h2 id="五、其他一些说明"><a href="#五、其他一些说明" class="headerlink" title="五、其他一些说明"></a>五、其他一些说明</h2><h3 id="1、rz-sz文件传输"><a href="#1、rz-sz文件传输" class="headerlink" title="1、rz/sz文件传输"></a>1、<code>rz/sz</code>文件传输</h3><ul><li><code>wget https://raw.githubusercontent.com/lawlite19/LinuxSoftware/master/rz-sz/lrzsz-0.12.20.tar.gz</code></li><li><code>tar zxvf lrzsz-0.12.20.tar.gz</code></li><li><code>cd lrzsz-0.12.20</code></li><li><code>./configure &amp;&amp; make &amp;&amp; make install</code></li><li><code>cd /usr/local/bin</code></li><li><code>ln -s lrz rz</code></li><li><code>ln -s lsz sz</code><h3 id="2、使用xftp等工具传输文件"><a href="#2、使用xftp等工具传输文件" class="headerlink" title="2、使用xftp等工具传输文件"></a>2、使用<code>xftp</code>等工具传输文件</h3></li><li>服务器上需要安装<code>ftp</code>服务<ul><li><code>apt install vsftpd</code></li></ul></li><li>配置文件；<code>vim /etc/vstfpd.conf</code></li></ul><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">write_enable=YES</div><div class="line">userlist_deny=NO</div><div class="line">userlist_enable=YES</div><div class="line">userlist_file=/etc/vsftpd<span class="selector-class">.user_list</span></div><div class="line">seccomp_sandbox=NO</div></pre></td></tr></table></figure><ul><li>加入允许连接的用户 <code>vim /etc/vstfpd.user_list</code></li></ul><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">root</div><div class="line">lawlite</div></pre></td></tr></table></figure><ul><li>删除禁止连接的用户：<code>vim /etc/ftpusers</code></li><li>使用<code>xftp、FileZilla</code>等工具连接即可<ul><li>注意端口<code>22</code></li><li>协议<code>sftp</code></li></ul></li></ul><h3 id="3、wget-下载百度云盘文件"><a href="#3、wget-下载百度云盘文件" class="headerlink" title="3、wget 下载百度云盘文件"></a>3、<code>wget</code> 下载百度云盘文件</h3><ul><li><code>wget -c ----referer=百度云盘分享地址 -O 要保存的文件名 &quot;百度云文件真实地址&quot;</code></li><li>文件的真实地址获取<ul><li>浏览器按<code>F12</code>, 点击下载找到<code>download?</code>的信息</li><li><code>dlink</code>为真实地址，注意去除转义字符<code>\</code></li></ul></li></ul><p><img src="/assets/blog_images/GPU/11_%E8%8E%B7%E5%8F%96%E7%99%BE%E5%BA%A6%E4%BA%91%E5%9C%B0%E5%9D%80.png" alt="获取百度云真实地址" title="11_获取百度云地址"></p><ul><li>比如： <code>wget -c --referer=https://pan.baidu.com/s/1kV7Xo7H -O lstm1_rnn512_bestACC.zip &quot;https://d.pcs.baidu.com/file/4e4cd12ad77d7ac60d2cfcb8e009bf1c?fid=3174489928-250528-212189063946307&amp;time=1514127189&amp;rt=pr&amp;sign=FDTAERVCY-DCb740ccc5511e5e8fedcff06b081203-LWe3VIBsW3foAEVnTUqSROJQ46s%3D&amp;expires=8h&amp;chkv=1&amp;chkbd=1&amp;chkpc=et&amp;dp-logid=8301954057401711855&amp;dp-callid=0&amp;r=884079691&quot;</code></li></ul><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li>Cuda: <ul><li><a href="https://developer.nvidia.com/cuda-downloads?target_os=Linux&amp;target_arch=x86_64&amp;target_distro=Ubuntu&amp;target_version=1604&amp;target_type=deblocal" target="_blank" rel="external">https://developer.nvidia.com/cuda-downloads?target_os=Linux&amp;target_arch=x86_64&amp;target_distro=Ubuntu&amp;target_version=1604&amp;target_type=deblocal</a></li><li><a href="http://blog.csdn.net/u012235003/article/details/54575758" target="_blank" rel="external">http://blog.csdn.net/u012235003/article/details/54575758</a></li><li><a href="http://blog.csdn.net/hungryof/article/details/51557666" target="_blank" rel="external">http://blog.csdn.net/hungryof/article/details/51557666</a></li><li><a href="https://github.com/facebookarchive/fbcunn/blob/master/INSTALL.md#install-cuda" target="_blank" rel="external">https://github.com/facebookarchive/fbcunn/blob/master/INSTALL.md#install-cuda</a></li></ul></li><li><code>Wget</code>下载百度云：<ul><li><a href="http://blog.csdn.net/zhongdajiajiao/article/details/51917886" target="_blank" rel="external">http://blog.csdn.net/zhongdajiajiao/article/details/51917886</a></li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;一、介绍&quot;&gt;&lt;a href=&quot;#一、介绍&quot; class=&quot;headerlink&quot; title=&quot;一、介绍&quot;&gt;&lt;/a&gt;一、介绍&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;阿里云的&lt;code&gt;GPU&lt;/code&gt;也有了竞价服务，每小时大概1块多，还是可以接受的&lt;/li&gt;
&lt;li&gt;主要想跑&lt;code&gt;github&lt;/code&gt;上的一个&lt;a href=&quot;https://github.com/ShuangLI59/Person-Search-with-Natural-Language-Description&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;论文代码&lt;/a&gt;，使用的&lt;code&gt;GPU&lt;/code&gt;,(奈何实验室没有&lt;code&gt;GPU&lt;/code&gt;)， 本来我已经改成&lt;code&gt;CPU&lt;/code&gt;版本的了，但是他训练好的模型是基于&lt;code&gt;GPU&lt;/code&gt;的，所以还需要重新训练，结果非常的慢…&lt;/li&gt;
&lt;li&gt;包含以下内容：&lt;ul&gt;
&lt;li&gt;购买竞价&lt;code&gt;GPU&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;通过&lt;code&gt;SSH&lt;/code&gt;连接云服务器&lt;/li&gt;
&lt;li&gt;安装&lt;code&gt;Torch、hdf5、cjson、loadcaffe&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;安装&lt;code&gt;cuda、cudnn、cunn&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="GPU" scheme="http://lawlite.me/tags/GPU/"/>
    
      <category term="Torch" scheme="http://lawlite.me/tags/Torch/"/>
    
      <category term="Cudnn" scheme="http://lawlite.me/tags/Cudnn/"/>
    
      <category term="阿里云" scheme="http://lawlite.me/tags/%E9%98%BF%E9%87%8C%E4%BA%91/"/>
    
  </entry>
  
  <entry>
    <title>论文记录_U-Net:Convolutional Networks for Biomedical Image Segmentation</title>
    <link href="http://lawlite.me/2017/10/18/%E8%AE%BA%E6%96%87%E8%AE%B0%E5%BD%95-U-Net-Convolutional-Networks-for-Biomedical-Image-Segmentation/"/>
    <id>http://lawlite.me/2017/10/18/论文记录-U-Net-Convolutional-Networks-for-Biomedical-Image-Segmentation/</id>
    <published>2017-10-18T11:30:05.000Z</published>
    <updated>2017-10-19T09:28:32.721Z</updated>
    
    <content type="html"><![CDATA[<ul><li><code>Keras</code> 中的实现（可基于<code>Theano</code>和<code>Tensorflow</code>）：<a href="https://github.com/ZFTurbo/ZF_UNET_224_Pretrained_Model" target="_blank" rel="external">点击查看</a></li><li><code>Tensorflow</code> 中的实现：<a href="https://github.com/jakeret/tf_unet" target="_blank" rel="external">点击查看</a></li></ul><h2 id="1、概述"><a href="#1、概述" class="headerlink" title="1、概述"></a>1、概述</h2><ul><li><code>2015</code>年提出的用于<strong>生物图像分割</strong>的网络，并在当时取得了冠军<ul><li>[实际也可用于<strong>其他领域</strong>，比如一些<code>Kaggle</code>图像分割比赛中也有使用，并且取得了很好的成绩]</li></ul></li><li>一般的 <code>CNN</code> 是预测一张图片的类别，而图像分割需要<strong>预测每个像素属于的类别</strong>，使用是<strong>全卷积网络</strong> <code>FCN</code> (<code>fully convolutional network</code>)</li><li>网络结构包括<strong>收缩</strong> [就是 <code>CNN</code> 的卷积操作]和对应的<strong>扩展</strong> [可以进行<strong>反卷积</strong>，这里采用<strong>上采样</strong>(<code>upsample</code>)]<ul><li>收缩主要用于<strong>捕捉上下文特征</strong></li><li>扩展用于<strong>定位</strong></li></ul></li><li>需要很少的训练集即可完成训练，很容易收敛</li></ul><a id="more"></a><h2 id="2、网络结构"><a href="#2、网络结构" class="headerlink" title="2、网络结构"></a>2、网络结构</h2><p><img src="/assets/blog_images/Image_Segmentation/01-U-net_architecture.png" alt="U-net architecture" title="U-net_architecture"></p><ul><li>左半部分收缩就是平常的<code>CNN</code>卷积操作<ul><li>经过卷积、<code>Relu</code> 和 <code>2x2 max pooling</code></li></ul></li><li><p>右半部分扩展采用的上采样（<code>upsampling</code>）操作，<strong>同时拼接上对应的左半部分</strong>的<code>feature maps</code></p><ul><li>因为前面几层的卷积层<strong>分辨率比较高</strong>，<strong>定位</strong>比较准确</li><li>后面的几层卷积层<strong>分辨率比较低</strong>，<strong>分类</strong>比较准确</li><li>所以把低分辨率和高分辨率的<code>feature map</code> 拼接起来，得到更好的结果</li></ul></li><li><p>如果预测时输入图片大小有问题，可以使用镜像拼接方式，同时也可以调整输入的大小是偶数，方便进行 <code>2x2 max pooling</code></p><ul><li>图中黄色部分待预测，需要蓝色部分作为输入，对称的方式生成周围的部分。</li></ul></li></ul><p><img src="/assets/blog_images/Image_Segmentation/02-Overlap-tile-strategy.png" alt="输入调整" title="02-Overlap-tile-strategy"></p><h2 id="3、训练"><a href="#3、训练" class="headerlink" title="3、训练"></a>3、训练</h2><ul><li>损失函数使用<strong>逐像素</strong>的 <code>softmax</code> 函数和<strong>交叉熵损失函数</strong>的结合</li><li><code>Softmax</code>函数：$${p_k(x)} =  {e^{a_k(x)} \over \sum_{k’=1}^K e^{a_{k’(x)}}}$$<ul><li>$a_k(x)$表示在<code>feature maps</code>中的的<code>channel=k</code>的<code>feature map</code>像素位置为<code>x</code>的激活值</li><li>$K$是类别数</li><li>[就是单个<code>feature map</code>上每个像素的类别概率]</li></ul></li><li>训练需要标注对应的<code>mask</code>,就是类别的区域标记</li></ul><p><img src="/assets/blog_images/Image_Segmentation//03_train.png" alt="example" title="03_train"></p><ul><li>因为使用了 <code>Relu</code> 激励函数，对应的权重初始化方法为<strong>标准差</strong>为$\sqrt{2/N}$的高斯分布，具体关于不同激励函数对应的输出花方法可以<a href="http://lawlite.me/2017/01/09/%E8%AE%BA%E6%96%87%E8%AE%B0%E5%BD%95-Relu%E6%BF%80%E5%8A%B1%E5%87%BD%E6%95%B0%E6%9D%83%E9%87%8D%E5%88%9D%E5%A7%8B%E5%8C%96/">看这里</a></li><li>训练集小的话可以做<strong>数据增强</strong></li></ul><h2 id="4、Keras-中的实现"><a href="#4、Keras-中的实现" class="headerlink" title="4、Keras 中的实现"></a>4、<code>Keras</code> 中的实现</h2><ul><li>借鉴<code>github</code>上实现好的：<a href="https://github.com/ZFTurbo/ZF_UNET_224_Pretrained_Model" target="_blank" rel="external">点击查看</a>，版本：<code>Keras (2.0.8)</code>，<code>tensorflow (1.3.0)</code></li><li><p>两层卷积操作函数</p><ul><li>判断是使用<code>theano</code>还是<code>tensorflow</code>作为backend， 因为他们对应的数据维度不同</li><li>可以使用<code>BN</code>和<code>Dropout</code>操作</li><li>两层卷积也就对应了上面<code>U-net</code>结构图的两个卷积操作<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">def double_conv_layer(x, size, dropout, batch_norm):</div><div class="line">    if K.image_dim_ordering() == &apos;th&apos;:</div><div class="line">        axis = 1</div><div class="line">    else:</div><div class="line">        axis = 3</div><div class="line">    conv = Conv2D(size, (3, 3), padding=&apos;same&apos;)(x)</div><div class="line">    if batch_norm is True:</div><div class="line">        conv = BatchNormalization(axis=axis)(conv)</div><div class="line">    conv = Activation(&apos;relu&apos;)(conv)</div><div class="line">    conv = Conv2D(size, (3, 3), padding=&apos;same&apos;)(conv)</div><div class="line">    if batch_norm is True:</div><div class="line">        conv = BatchNormalization(axis=axis)(conv)</div><div class="line">    conv = Activation(&apos;relu&apos;)(conv)</div><div class="line">    if dropout &gt; 0:</div><div class="line">        conv = Dropout(dropout)(conv)</div><div class="line">    return conv</div></pre></td></tr></table></figure></li></ul></li><li><p>构建网络</p><ul><li>最后是<code>1x1</code>的卷积，使用的<code>Sigmoid</code>函数作为最后的输出概率<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div></pre></td><td class="code"><pre><div class="line">def ZF_UNET_224(dropout_val=0.0, batch_norm=True):</div><div class="line">    print(&quot;con&quot;)</div><div class="line">    if K.image_dim_ordering() == &apos;th&apos;:</div><div class="line">        inputs = Input((INPUT_CHANNELS, 224, 224))</div><div class="line">        axis = 1</div><div class="line">    else:</div><div class="line">        inputs = Input((224, 224, INPUT_CHANNELS))</div><div class="line">        axis = 3</div><div class="line">    filters = 32</div><div class="line"></div><div class="line">    conv_224 = double_conv_layer(inputs, filters, dropout_val, batch_norm)</div><div class="line">    pool_112 = MaxPooling2D(pool_size=(2, 2))(conv_224)</div><div class="line"></div><div class="line">    conv_112 = double_conv_layer(pool_112, 2*filters, dropout_val, batch_norm)</div><div class="line">    pool_56 = MaxPooling2D(pool_size=(2, 2))(conv_112)</div><div class="line"></div><div class="line">    conv_56 = double_conv_layer(pool_56, 4*filters, dropout_val, batch_norm)</div><div class="line">    pool_28 = MaxPooling2D(pool_size=(2, 2))(conv_56)</div><div class="line"></div><div class="line">    conv_28 = double_conv_layer(pool_28, 8*filters, dropout_val, batch_norm)</div><div class="line">    pool_14 = MaxPooling2D(pool_size=(2, 2))(conv_28)</div><div class="line"></div><div class="line">    conv_14 = double_conv_layer(pool_14, 16*filters, dropout_val, batch_norm)</div><div class="line">    pool_7 = MaxPooling2D(pool_size=(2, 2))(conv_14)</div><div class="line"></div><div class="line">    conv_7 = double_conv_layer(pool_7, 32*filters, dropout_val, batch_norm)</div><div class="line"></div><div class="line">    up_14 = concatenate([UpSampling2D(size=(2, 2))(conv_7), conv_14], axis=axis)</div><div class="line">    up_conv_14 = double_conv_layer(up_14, 16*filters, dropout_val, batch_norm)</div><div class="line"></div><div class="line">    up_28 = concatenate([UpSampling2D(size=(2, 2))(up_conv_14), conv_28], axis=axis)</div><div class="line">    up_conv_28 = double_conv_layer(up_28, 8*filters, dropout_val, batch_norm)</div><div class="line"></div><div class="line">    up_56 = concatenate([UpSampling2D(size=(2, 2))(up_conv_28), conv_56], axis=axis)</div><div class="line">    up_conv_56 = double_conv_layer(up_56, 4*filters, dropout_val, batch_norm)</div><div class="line"></div><div class="line">    up_112 = concatenate([UpSampling2D(size=(2, 2))(up_conv_56), conv_112], axis=axis)</div><div class="line">    up_conv_112 = double_conv_layer(up_112, 2*filters, dropout_val, batch_norm)</div><div class="line"></div><div class="line">    up_224 = concatenate([UpSampling2D(size=(2, 2))(up_conv_112), conv_224], axis=axis)</div><div class="line">    up_conv_224 = double_conv_layer(up_224, filters, 0, batch_norm)</div><div class="line"></div><div class="line">    conv_final = Conv2D(OUTPUT_MASK_CHANNELS, (1, 1))(up_conv_224)</div><div class="line">    conv_final = BatchNormalization(axis=axis)(conv_final)</div><div class="line">    conv_final = Activation(&apos;sigmoid&apos;)(conv_final)</div><div class="line"></div><div class="line">    model = Model(inputs, conv_final, name=&quot;ZF_UNET_224&quot;)</div><div class="line">    return model</div></pre></td></tr></table></figure></li></ul></li></ul><h2 id="5、总结"><a href="#5、总结" class="headerlink" title="5、总结"></a>5、总结</h2><ul><li>图像分割能够获得很好的结果，要求的训练集比较小</li><li>包括了收缩和扩展两部分，扩展部分<strong>拼接</strong>了对应的收缩部分的<code>feature maps</code></li></ul><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li><a href="https://arxiv.org/abs/1505.04597#" target="_blank" rel="external">https://arxiv.org/abs/1505.04597#</a></li><li><a href="https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/" target="_blank" rel="external">https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/</a></li><li><a href="https://github.com/ZFTurbo/ZF_UNET_224_Pretrained_Model" target="_blank" rel="external">https://github.com/ZFTurbo/ZF_UNET_224_Pretrained_Model</a></li><li><a href="https://github.com/jakeret/tf_unet" target="_blank" rel="external">https://github.com/jakeret/tf_unet</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;ul&gt;
&lt;li&gt;&lt;code&gt;Keras&lt;/code&gt; 中的实现（可基于&lt;code&gt;Theano&lt;/code&gt;和&lt;code&gt;Tensorflow&lt;/code&gt;）：&lt;a href=&quot;https://github.com/ZFTurbo/ZF_UNET_224_Pretrained_Model&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;点击查看&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Tensorflow&lt;/code&gt; 中的实现：&lt;a href=&quot;https://github.com/jakeret/tf_unet&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;点击查看&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;1、概述&quot;&gt;&lt;a href=&quot;#1、概述&quot; class=&quot;headerlink&quot; title=&quot;1、概述&quot;&gt;&lt;/a&gt;1、概述&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;code&gt;2015&lt;/code&gt;年提出的用于&lt;strong&gt;生物图像分割&lt;/strong&gt;的网络，并在当时取得了冠军&lt;ul&gt;
&lt;li&gt;[实际也可用于&lt;strong&gt;其他领域&lt;/strong&gt;，比如一些&lt;code&gt;Kaggle&lt;/code&gt;图像分割比赛中也有使用，并且取得了很好的成绩]&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;一般的 &lt;code&gt;CNN&lt;/code&gt; 是预测一张图片的类别，而图像分割需要&lt;strong&gt;预测每个像素属于的类别&lt;/strong&gt;，使用是&lt;strong&gt;全卷积网络&lt;/strong&gt; &lt;code&gt;FCN&lt;/code&gt; (&lt;code&gt;fully convolutional network&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;网络结构包括&lt;strong&gt;收缩&lt;/strong&gt; [就是 &lt;code&gt;CNN&lt;/code&gt; 的卷积操作]和对应的&lt;strong&gt;扩展&lt;/strong&gt; [可以进行&lt;strong&gt;反卷积&lt;/strong&gt;，这里采用&lt;strong&gt;上采样&lt;/strong&gt;(&lt;code&gt;upsample&lt;/code&gt;)]&lt;ul&gt;
&lt;li&gt;收缩主要用于&lt;strong&gt;捕捉上下文特征&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;扩展用于&lt;strong&gt;定位&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;需要很少的训练集即可完成训练，很容易收敛&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="论文记录" scheme="http://lawlite.me/tags/%E8%AE%BA%E6%96%87%E8%AE%B0%E5%BD%95/"/>
    
      <category term="图像分割" scheme="http://lawlite.me/tags/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/"/>
    
  </entry>
  
  <entry>
    <title>论文记录_MobileNets Efficient Convolutional Neural Networks for Mobile Vision Application</title>
    <link href="http://lawlite.me/2017/09/12/%E8%AE%BA%E6%96%87%E8%AE%B0%E5%BD%95-MobileNets-Efficient-Convolutional-Neural-Networks-for-Mobile-Vision-Application/"/>
    <id>http://lawlite.me/2017/09/12/论文记录-MobileNets-Efficient-Convolutional-Neural-Networks-for-Mobile-Vision-Application/</id>
    <published>2017-09-12T03:34:17.000Z</published>
    <updated>2017-09-13T11:50:19.036Z</updated>
    
    <content type="html"><![CDATA[<ul><li><code>Tensorflow</code> 中的实现：<a href="https://github.com/tensorflow/models/blob/master/slim/nets/mobilenet_v1.md" target="_blank" rel="external">点击查看</a></li><li><code>Caffe</code> 中的实现：<a href="https://github.com/shicai/MobileNet-Caffe" target="_blank" rel="external">点击查看</a></li></ul><h2 id="1、概述"><a href="#1、概述" class="headerlink" title="1、概述"></a>1、概述</h2><ul><li><code>Google</code>在<code>2017</code>年提出的适用于手机端的神经网络模型</li><li>主要使用了<strong>深度可分离卷积</strong><code>Depthwise Separable Convolution</code> 将卷积核进行分解计算来减少计算量</li><li>引入了<strong>两个超参数</strong>减少参数量和计算量<ul><li>宽度乘数（<code>Width Multiplier</code>）: [减少输入和输出的 <code>channels</code> ]</li><li>分辨率乘数（<code>Resolution Multiplier</code>）：[减少输入输出的 <code>feature maps</code> 的大小]</li></ul></li></ul><a id="more"></a><h2 id="2、深度可分离卷积（Depthwise-Separable-Convolution）"><a href="#2、深度可分离卷积（Depthwise-Separable-Convolution）" class="headerlink" title="2、深度可分离卷积（Depthwise Separable Convolution）"></a>2、深度可分离卷积（<code>Depthwise Separable Convolution</code>）</h2><ul><li>可以将一个标准卷积核<strong>分成</strong>一个深度卷积<code>depthwise convolution</code> 和 一个<code>1X1</code>的卷积（叫作逐点卷积<code>pointwise convolution</code>）。如下图所示</li></ul><p><img src="/assets/blog_images/ModelCompression/13_depthwise-separable-convolution.png" alt="depthwise separable convolution" title="13_depthwise-separable-convolution"></p><h3 id="2-1-标准卷积"><a href="#2-1-标准卷积" class="headerlink" title="2.1 标准卷积"></a>2.1 标准卷积</h3><ul><li>标准的卷积层是将维度为$D_F \times D_F \times M$的输入层转化为维度为$D_G \times D_G \times N$ [ <a href="http://lawlite.me/2017/09/11/%E8%AE%BA%E6%96%87%E8%AE%B0%E5%BD%95-PruningFiltersForEfficientConvNets/#2-1-%E5%9F%BA%E7%A1%80%EF%BC%88CNN%E7%9B%B8%E5%85%B3%E5%86%85%E5%AE%B9%EF%BC%89">上篇论文</a>中也有提到]<ul><li>$D_F$ 是输入<code>feature map</code>的长和宽，<strong>M</strong> 是输入的通道数（<code>channels</code>）</li><li>$D_G$ 是输出<code>feature map</code>的长和宽，<strong>N</strong> 是输出的通道数</li></ul></li><li>假设卷积核<code>filter</code>的大小是$D_k \times D_k$，则标准卷积的计算量是$$D_k \cdot D_k \cdot M \cdot N \cdot D_F \cdot D_F$$<ul><li>引用上篇论文中的图, 只看<code>kernel matrix</code> 部分，$D_k \cdot D_k$就是一个方格的大小，然后乘上输入和输出的<code>channels</code>个数，然后作用在<strong>input feature maps</strong><br><img src="/assets/blog_images/ModelCompression/07_feature-map.png" alt="kernel matrix" title="07_feature-map"></li></ul></li><li>标准卷积是这样的, 即不管当前<code>pixel</code>有多少<code>channels</code>，卷积之后就是一个<code>channel</code></li></ul><p><img src="/assets/blog_images/ModelCompression/14_RegularConvolution.png" alt="regular conv" title="14_RegularConvolution"></p><h3 id="2-2-Depthwise-Separable-Convolution"><a href="#2-2-Depthwise-Separable-Convolution" class="headerlink" title="2.2 Depthwise Separable Convolution"></a>2.2 Depthwise Separable Convolution</h3><ul><li>分为两个步骤<ul><li>第一步<strong>深度卷积</strong>：卷积核的大小是$D_k \times D_k \times 1 \times M$，所以总的计算量是：$$D_k \cdot D_k \cdot M \cdot D_F \cdot D_F$$</li><li>第二步<strong>逐点卷积</strong>：卷积核大小是$1 \times 1 \times M \times N$，所以总的计算量是：$$M \cdot N \cdot D_F \cdot D_F$$</li></ul></li><li>所以和标准的卷积相比<strong>计算量比率</strong>为：  $${D_k \cdot D_k \cdot M \cdot D_F \cdot D_F + M \cdot N \cdot D_F \cdot D_F \over D_k \cdot D_k \cdot M \cdot N \cdot D_F \cdot D_F} = {1 \over N} + {1 \over D_k^2}$$<ul><li><code>MobileNet</code>使用的是<code>3x3</code>的卷积核，所以计算量可以减少<strong>8-9倍</strong> (因为比率是<code>1/N+1/9</code>)</li></ul></li><li>第一步深度卷积操作是在每一个<code>channel</code>上进行的卷积操作</li></ul><p><img src="/assets/blog_images/ModelCompression/15_DepthwiseConvolution.png" alt="dethwise conv" title="15_DepthwiseConvolution"></p><ul><li>第二步逐点卷积才是结合起来</li></ul><p><img src="/assets/blog_images/ModelCompression/16_PointwiseConvolution.png" alt="PointwiseConvolution" title="16_PointwiseConvolution"></p><h2 id="3-神经网络结构"><a href="#3-神经网络结构" class="headerlink" title="3. 神经网络结构"></a>3. 神经网络结构</h2><ul><li><code>MobileNet</code>共有<code>28</code>层（深度卷积和逐点卷积分开来算）</li><li>之前标准的结构是卷积层之后跟上<code>Batch Normalization</code>层和<code>Relu</code>激活函数，这里引入<code>Depthwise separable convolution</code>之后的结构如下图<ul><li>每一层都跟上了<strong>BN层</strong>和激活函数</li></ul></li><li>总的结构</li></ul><p><img src="/assets/blog_images/ModelCompression/18_structure.png" alt="mobileNet structure" title="18_structure"></p><p><img src="/assets/blog_images/ModelCompression/17_structure_of_mobileNet.png" alt="structure of mobileNet" title="17_structure_of_mobileNet"></p><h2 id="4-宽度乘数（Width-Multiplier）"><a href="#4-宽度乘数（Width-Multiplier）" class="headerlink" title="4. 宽度乘数（Width Multiplier）"></a>4. 宽度乘数（Width Multiplier）</h2><ul><li>引入<strong>超参数</strong>$\alpha$, 目的是使模型<strong>变瘦</strong>,</li><li>即输入层的<code>channels</code>个数<strong>M</strong>，变成$\alpha M$，输出层的<code>channels</code>个数<strong>N</strong>变成了$\alpha N$</li><li>所以引入宽度乘数后的总的计算量是$$D_k \cdot D_k \cdot \alpha M \cdot D_F \cdot D_F + \alpha M \cdot \alpha N \cdot D_F \cdot D_F$$<ul><li>一般$\alpha \in (0,1]$，常取的值是<code>1, 0.75, 0.5, 0.25,</code></li><li><strong>大约</strong>可以减少参数量和计算量的$\alpha ^2$</li></ul></li></ul><h2 id="5-分辨率乘数-（Resolution-Multiplier）"><a href="#5-分辨率乘数-（Resolution-Multiplier）" class="headerlink" title="5. 分辨率乘数 （Resolution Multiplier）"></a>5. 分辨率乘数 （Resolution Multiplier）</h2><ul><li>引入<strong>超参数</strong>$\rho$，目的是降低图片的分辨率</li><li>即作用在输入的<code>feature map</code>上</li><li>所以再引入分辨率乘数后总的计算量是：$$D_k \cdot D_k \cdot \alpha M \cdot  \rho D_F \cdot \rho D_F + \alpha M \cdot \alpha N \cdot \rho D_F \cdot \rho D_F$$<ul><li>一般输入图片的分辨率是<code>224, 192, 160 or 128</code></li><li>大约可以减少计算量的$\rho ^2$</li></ul></li></ul><h2 id="6-实验结果"><a href="#6-实验结果" class="headerlink" title="6. 实验结果"></a>6. 实验结果</h2><ul><li>关于超参数的选择，下图可以看出<strong>准确度和参数量和参数运算量的关系</strong>，之间有个<code>trade off</code>，合理选择参数即可</li></ul><p><img src="/assets/blog_images/ModelCompression/19_accuracy_with_computation.png" alt="准确度和参数运算量的关系" title="19_accuracy_with_computation"></p><p><img src="/assets/blog_images/ModelCompression/20_accuracy_with_parameters.png" alt="准确度和参数量的关系" title="20_accuracy_with_parameters"></p><ul><li>还在<strong>细粒度的识别</strong>，<strong>大规模地理位置识别</strong>，<strong>人脸属性提取</strong>，<strong>目标检测</strong>和<strong>人脸识别</strong>等任务上进行了测试，效果也很好</li></ul><h2 id="7-总结"><a href="#7-总结" class="headerlink" title="7. 总结"></a>7. 总结</h2><ul><li>主要是基于<strong>depthwise separable convolution</strong></li><li>引入了两个超参数<ul><li>[ 第一个宽度乘数就是<strong>减少</strong><code>feature map</code>，以此来降低模型厚度 ]</li><li>[ 第二个分辨率乘数就是<strong>缩小</strong><code>feature map</code>的大小，来减少计算量]</li></ul></li><li>[ 超参数的选择是有个<code>trade off</code>的 ]</li></ul><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li><a href="https://arxiv.org/abs/1704.04861" target="_blank" rel="external">https://arxiv.org/abs/1704.04861</a></li><li><a href="https://github.com/tensorflow/models/blob/master/slim/nets/mobilenet_v1.md" target="_blank" rel="external">https://github.com/tensorflow/models/blob/master/slim/nets/mobilenet_v1.md</a></li><li><a href="https://github.com/shicai/MobileNet-Caffe" target="_blank" rel="external">https://github.com/shicai/MobileNet-Caffe</a></li><li><a href="http://machinethink.net/blog/googles-mobile-net-architecture-on-iphone/" target="_blank" rel="external">http://machinethink.net/blog/googles-mobile-net-architecture-on-iphone/</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;ul&gt;
&lt;li&gt;&lt;code&gt;Tensorflow&lt;/code&gt; 中的实现：&lt;a href=&quot;https://github.com/tensorflow/models/blob/master/slim/nets/mobilenet_v1.md&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;点击查看&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Caffe&lt;/code&gt; 中的实现：&lt;a href=&quot;https://github.com/shicai/MobileNet-Caffe&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;点击查看&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;1、概述&quot;&gt;&lt;a href=&quot;#1、概述&quot; class=&quot;headerlink&quot; title=&quot;1、概述&quot;&gt;&lt;/a&gt;1、概述&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;code&gt;Google&lt;/code&gt;在&lt;code&gt;2017&lt;/code&gt;年提出的适用于手机端的神经网络模型&lt;/li&gt;
&lt;li&gt;主要使用了&lt;strong&gt;深度可分离卷积&lt;/strong&gt;&lt;code&gt;Depthwise Separable Convolution&lt;/code&gt; 将卷积核进行分解计算来减少计算量&lt;/li&gt;
&lt;li&gt;引入了&lt;strong&gt;两个超参数&lt;/strong&gt;减少参数量和计算量&lt;ul&gt;
&lt;li&gt;宽度乘数（&lt;code&gt;Width Multiplier&lt;/code&gt;）: [减少输入和输出的 &lt;code&gt;channels&lt;/code&gt; ]&lt;/li&gt;
&lt;li&gt;分辨率乘数（&lt;code&gt;Resolution Multiplier&lt;/code&gt;）：[减少输入输出的 &lt;code&gt;feature maps&lt;/code&gt; 的大小]&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="Paper阅读记录" scheme="http://lawlite.me/tags/Paper%E9%98%85%E8%AF%BB%E8%AE%B0%E5%BD%95/"/>
    
      <category term="ModelCompression" scheme="http://lawlite.me/tags/ModelCompression/"/>
    
  </entry>
  
  <entry>
    <title>论文记录-Pruning Filters For Efficient ConvNets</title>
    <link href="http://lawlite.me/2017/09/10/%E8%AE%BA%E6%96%87%E8%AE%B0%E5%BD%95-PruningFiltersForEfficientConvNets/"/>
    <id>http://lawlite.me/2017/09/10/论文记录-PruningFiltersForEfficientConvNets/</id>
    <published>2017-09-10T02:32:12.000Z</published>
    <updated>2017-09-12T03:38:29.616Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1、概述"><a href="#1、概述" class="headerlink" title="1、概述"></a>1、概述</h2><ul><li>一些剪枝的操作主要是<strong>减少了全连接层的参数</strong>，全连接层的参数量占比最多（比如<code>VGG-16</code>中全连接层操作占了<code>90%</code>，计算量只占了不到<code>1%</code>）, 但是主要的<strong>计算量集中在卷层操作</strong></li><li>论文就是提出了<strong>对卷积层进行剪枝操作</strong>，然后进行<code>retrain</code>，不会造成<strong>稀疏连接</strong>（像<a href="http://lawlite.me/2017/09/07/%E8%AE%BA%E6%96%87%E8%AE%B0%E5%BD%95-DeepCompression-CompressingDeepNeuralNetworksWithPruning-TrainedQuantizationAndHuffmanCoding/">上篇论文</a>一样，稀疏矩阵操作需要特殊的库等来处理）</li><li>全连接层可以使用<strong>平均池化层</strong>来代替以减少参数量</li></ul><a id="more"></a><h2 id="2、对Filters进行剪枝，以及Feature-maps"><a href="#2、对Filters进行剪枝，以及Feature-maps" class="headerlink" title="2、对Filters进行剪枝，以及Feature maps"></a>2、对<code>Filters</code>进行剪枝，以及<code>Feature maps</code></h2><h3 id="2-1-基础（CNN相关内容）"><a href="#2-1-基础（CNN相关内容）" class="headerlink" title="2.1 基础（CNN相关内容）"></a>2.1 基础（<code>CNN</code>相关内容）</h3><ul><li>设第 <code>i</code> 层的卷积层的输入 <code>channel</code> 有 $n_i$ , $h_i$ 和 $w_i$ 表示输入的特征图<code>feature map</code>的高和宽</li><li>使用$n_{i+1}$ 个<code>3D filters</code> $F_{i,j} \in R^{n_i \times k \times k}$， 则卷积操作可以将输入的<code>feature maps</code> $x_i \in R^{n_i \times h_i \times w_i}$ 转化为 $x_{i+1} \in R^{n_{i+1} \times h_{i+1} \times w_{i+1}}$<ul><li>关于<code>CNN</code>的基础不了解的可以<a href="http://blog.csdn.net/u013082989/article/details/53673602" target="_blank" rel="external">查看这里</a></li></ul></li><li>卷积操作的运算数量是：$n_{i+1}n_ik^2h_{i+1}w_{i+1}$ (对应到下图的<code>kernel matrix</code>)</li><li>所以如下图所示，取出一个<code>feature map</code>可以直接减少$n_ik^2h_{i+1}w_{i+1}$个运算<ul><li>同时接下来的<code>feature map</code>也就没有了，附加移除$n_{i+2}k^2h_{i+2}w_{i+2}$个运算</li></ul></li><li>所以减少<code>m</code>个 <code>featuremaps</code> 可以减少 $m/n_{i+1}$ 的计算量<ul><li>下图的<code>kernel matrix</code>，一个 <code>feature map</code> 对应一列，所以是$m/n_{i+1}$<br><img src="/assets/blog_images/ModelCompression/07_feature-map.png" alt="filters" title="07_feature-map"></li></ul></li></ul><h3 id="2-2-去除哪些filters-在单层中"><a href="#2-2-去除哪些filters-在单层中" class="headerlink" title="2.2 去除哪些filters (在单层中)"></a>2.2 去除哪些<code>filters</code> (在单层中)</h3><ul><li>向<a href="http://lawlite.me/2017/09/07/%E8%AE%BA%E6%96%87%E8%AE%B0%E5%BD%95-DeepCompression-CompressingDeepNeuralNetworksWithPruning-TrainedQuantizationAndHuffmanCoding/">之前的论文</a>介绍的，权重的绝对值越小，则权重的作用也就越小<ul><li>[ 假设权重值都在<code>0</code>附近，进行乘积得到的值很小，所以对结果造成的影响也很小 ]</li><li>[ 删除一些冗余的值还有可能防止过拟合 ]</li></ul></li><li>本文使用的是<code>filter</code>的绝对值的和来衡量这个<code>filter</code>的作用，即 $\sum |F_{i,j}|$ , ($l_1$范数)<ul><li>选择前<code>m</code>个<strong>最小的</strong>绝对值删除</li><li>文章和<strong>随机选择</strong>相同数量<code>的filters</code>和选择<strong>最大值</strong>的结果比较，此方法最好</li></ul></li><li><code>VGG-16</code>在<code>Cifar-10</code>数据集上训练得到的卷积层的权重分布情况，可以看出每一卷积层的分布变化还是很大的</li></ul><p><img src="/assets/blog_images/ModelCompression/08_conv_abs_distribution.png" alt="each layer distribution of abs value " title="08_conv_abs_distribution"></p><h3 id="2-3-剪枝的敏感度-Sensitivity"><a href="#2-3-剪枝的敏感度-Sensitivity" class="headerlink" title="2.3 剪枝的敏感度(Sensitivity)"></a>2.3 剪枝的敏感度(Sensitivity)</h3><ul><li>就是每一卷积层进行<strong>单独剪枝</strong>，查看在<code>validation set</code>上准确度的变化</li><li>对于<code>VGG-16</code>, 一些卷积层的<code>filter</code>数量是一样的，所以对于差不多 <code>Sensitivity</code> 的卷积层，使用相同的比例进行剪枝，而对于 <code>Sensitivity</code> 比较大的，选择最小的比例进行剪枝或者不进行剪枝</li></ul><p><img src="/assets/blog_images/ModelCompression/09_accuracy_of_prune.png" alt="accuracy" title="09_accuracy_of_prune"></p><h3 id="2-4-多层剪枝的策略"><a href="#2-4-多层剪枝的策略" class="headerlink" title="2.4 多层剪枝的策略"></a>2.4 多层剪枝的策略</h3><ul><li>之前的一些剪枝策略是逐层剪枝，然后进行<code>retraining</code>，但是这样是非常耗时的</li><li>两种策略<ul><li>独立剪枝：就是每一层是独立的，然后进行剪枝</li><li>贪心剪枝：就是考虑到上一层被剪掉的情况</li></ul></li><li>如下图，第一种方法就是不考虑已经前面已经移除的<code>filters</code>（蓝色的），就是黄色的<code>kernel</code>仍然参与计算<ul><li>而对于贪心剪枝就不用计算黄色的<code>kernel</code>了</li></ul></li></ul><p><img src="/assets/blog_images/ModelCompression/10_two_strategies_of_pruning.png" alt="two strategies of pruning" title="10_two_strategies_of_pruning"></p><h2 id="3、-Retraining"><a href="#3、-Retraining" class="headerlink" title="3、 Retraining"></a>3、 Retraining</h2><ul><li>剪枝之后，应该<code>retraining</code>，（和迁移学习很像，有些<code>fine-tune</code>的意思）</li><li>也是两种策略：<ul><li>一次性剪枝然后 <code>retrain</code></li><li>逐层剪枝进行 <code>retrain</code></li></ul></li><li>第二种策略结果可能会更好，但是需要更多的<code>epochs</code></li></ul><h2 id="4、实验结果"><a href="#4、实验结果" class="headerlink" title="4、实验结果"></a>4、实验结果</h2><ul><li>剪枝之后进行<code>retrain</code>，在原来的基础之上得到的结果要比完全重新训练得到的结果好</li></ul><p><img src="/assets/blog_images/ModelCompression/11_experiment_result.png" alt="result" title="11_experiment_result"></p><ul><li>和随机剪枝、减去最大值$l_1$范数的<code>filters</code>的结果比较</li></ul><p><img src="/assets/blog_images/ModelCompression/12_comparison.png" alt="comparison" title="12_comparison"></p><h2 id="5、结论"><a href="#5、结论" class="headerlink" title="5、结论"></a>5、结论</h2><ul><li>剪枝<code>filters</code>，减少计算量</li><li>注意有<code>Batch Normalization</code>层的对应剪枝后，<code>BN</code>层也要对应删除</li><li>[其实感觉方法挺简单的]</li></ul><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li><a href="https://arxiv.org/abs/1608.08710" target="_blank" rel="external">https://arxiv.org/abs/1608.08710</a></li><li><a href="http://lawlite.me/2017/09/07/%E8%AE%BA%E6%96%87%E8%AE%B0%E5%BD%95-DeepCompression-CompressingDeepNeuralNetworksWithPruning-TrainedQuantizationAndHuffmanCoding/">http://lawlite.me/2017/09/07/%E8%AE%BA%E6%96%87%E8%AE%B0%E5%BD%95-DeepCompression-CompressingDeepNeuralNetworksWithPruning-TrainedQuantizationAndHuffmanCoding/</a></li><li><a href="http://blog.csdn.net/u013082989/article/details/53673602" target="_blank" rel="external">http://blog.csdn.net/u013082989/article/details/53673602</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;1、概述&quot;&gt;&lt;a href=&quot;#1、概述&quot; class=&quot;headerlink&quot; title=&quot;1、概述&quot;&gt;&lt;/a&gt;1、概述&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;一些剪枝的操作主要是&lt;strong&gt;减少了全连接层的参数&lt;/strong&gt;，全连接层的参数量占比最多（比如&lt;code&gt;VGG-16&lt;/code&gt;中全连接层操作占了&lt;code&gt;90%&lt;/code&gt;，计算量只占了不到&lt;code&gt;1%&lt;/code&gt;）, 但是主要的&lt;strong&gt;计算量集中在卷层操作&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;论文就是提出了&lt;strong&gt;对卷积层进行剪枝操作&lt;/strong&gt;，然后进行&lt;code&gt;retrain&lt;/code&gt;，不会造成&lt;strong&gt;稀疏连接&lt;/strong&gt;（像&lt;a href=&quot;http://lawlite.me/2017/09/07/%E8%AE%BA%E6%96%87%E8%AE%B0%E5%BD%95-DeepCompression-CompressingDeepNeuralNetworksWithPruning-TrainedQuantizationAndHuffmanCoding/&quot;&gt;上篇论文&lt;/a&gt;一样，稀疏矩阵操作需要特殊的库等来处理）&lt;/li&gt;
&lt;li&gt;全连接层可以使用&lt;strong&gt;平均池化层&lt;/strong&gt;来代替以减少参数量&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="Paper阅读记录" scheme="http://lawlite.me/tags/Paper%E9%98%85%E8%AF%BB%E8%AE%B0%E5%BD%95/"/>
    
      <category term="ModelCompression" scheme="http://lawlite.me/tags/ModelCompression/"/>
    
  </entry>
  
  <entry>
    <title>论文记录-Deep Compression:Compressing DeepNeural Networks With Pruning, Trained Quantization And Huffman Coding</title>
    <link href="http://lawlite.me/2017/09/07/%E8%AE%BA%E6%96%87%E8%AE%B0%E5%BD%95-DeepCompression-CompressingDeepNeuralNetworksWithPruning-TrainedQuantizationAndHuffmanCoding/"/>
    <id>http://lawlite.me/2017/09/07/论文记录-DeepCompression-CompressingDeepNeuralNetworksWithPruning-TrainedQuantizationAndHuffmanCoding/</id>
    <published>2017-09-07T02:43:52.000Z</published>
    <updated>2017-09-11T06:40:04.598Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1、概述"><a href="#1、概述" class="headerlink" title="1、概述"></a>1、概述</h2><ul><li>压缩主要分为<strong>三个阶段</strong>：剪枝(<code>pruning</code>)、训练分层(<code>trained quantization</code>)以及 哈夫曼编码(<code>Huffman coding</code>)</li><li>可以压缩<code>35</code>到<code>49</code>倍，并且不影响精度</li><li>[模型压缩的主要用于还是能够用于小型的设备上，例如手机端等，比如Google的<code>Mobile Net</code>, 但是准确度肯定要比正常的电脑端训练的大网络低一些，在所难免]</li><li>[一般的训练好的神经网络模型文件有<strong>几百兆</strong>的大小，比如<a href="https://github.com/tensorflow/models/tree/master/slim#pre-trained-models" target="_blank" rel="external">Google预训练好的模型</a>，计算量也很大，在手机端运行不太现实]</li></ul><a id="more"></a><h2 id="2、Pipline"><a href="#2、Pipline" class="headerlink" title="2、Pipline"></a>2、Pipline</h2><ul><li>剪枝可以压缩<strong>10倍</strong>左右，加上分层可以达到<strong>27到31倍</strong>，再加上哈夫曼编码可以达到<strong>35到49倍</strong><ul><li>剪枝：去除多余的连接，比如权重非常小的连接</li><li>分层：我感觉像是聚类，<strong>多个连接共享一个权重</strong></li></ul></li></ul><p><img src="/assets/blog_images/ModelCompression/01_pipline.png" alt="Pipline" title="01_pipline"></p><h2 id="3、剪枝"><a href="#3、剪枝" class="headerlink" title="3、剪枝"></a>3、剪枝</h2><ul><li>主要是删去权重值<code>weight</code>比较小的，(设置为0)，可以设置一个阈值(<code>threshold</code>)</li><li>所以<strong>权重矩阵</strong>变的比较稀疏，可以采用压缩行存储（<code>Compressed Row Storage(CRS)</code>）或列存储来存储稀疏矩阵<ul><li>主要包括<code>3</code>个数组，浮点值数组<code>val</code>，两个整形数组<code>col_index</code>, <code>row_ptr</code></li><li><code>val(k) = a(i,j), col_index(k) = j</code></li><li><code>row_ptr</code>是每行数据第一个<strong>非0</strong>元素在<code>val</code>中的索引，<strong>最后加上一位非0元素的个数</strong>，即<code>row_ptr(n+1) = a+1</code><ul><li>比如<br><img src="/assets/blog_images/ModelCompression/02_compressed-sparse-row.png" alt="矩阵" title="02_compressed-sparse-row"></li></ul></li></ul></li></ul><table><thead><tr><th>val</th><th>10</th><th>-2</th><th>3</th><th>9</th><th>3</th><th>7</th><th>8</th><th>7</th><th>3 … 9</th><th>13</th><th>4</th><th>2</th><th>-1</th></tr></thead><tbody><tr><td>col_index</td><td>1</td><td>5</td><td>1</td><td>2</td><td>6</td><td>2</td><td>3</td><td>4</td><td>1 … 5</td><td>6</td><td>2</td><td>5</td><td>6</td></tr></tbody></table><table><thead><tr><th>row_ptr</th><th>1</th><th>3</th><th>6</th><th>9</th><th>13</th><th>17</th><th>20</th></tr></thead><tbody><tr><td></td></tr></tbody></table><ul><li>所以总共需要的大小为：<code>2a+n+1</code><ul><li><code>a</code>为矩阵非零元素的个数</li><li><code>n</code>为行数</li></ul></li></ul><h2 id="4、训练分层量化"><a href="#4、训练分层量化" class="headerlink" title="4、训练分层量化"></a>4、训练分层量化</h2><ul><li>比如所有的权重聚成<code>4</code>类，<code>cluster index</code>表示每个权重对应的类别</li><li>梯度采用同一类别内进行<strong>累加</strong>，然后进行<strong>微调更新</strong></li></ul><p><img src="/assets/blog_images/ModelCompression/03_trained-quantization.png" alt="trained quantization" title="03_trained-quantization"></p><ul><li>假设有<code>n</code>个连接，每个连接的用<code>b</code> <code>bits</code>来表示，并假设有<code>k</code>个<code>cluster</code>, 只需要$log_2(k)$<code>bits</code>去表示索引，则压缩率可以为：$$r = {nb \over nlog_2(k)+kb}$$<ul><li><code>nb</code>即为没有聚类前总共需要的<code>bits</code></li><li>$nlog_2(k)+kb$就是聚类索引的<code>bits</code>加上聚类后连接需要的<code>bits</code></li><li>比如上面的例子为：${16<em>32 \over 16</em>2+4*32} = 3.2$</li></ul></li></ul><h3 id="4-1-权值共享"><a href="#4-1-权值共享" class="headerlink" title="4.1 权值共享"></a>4.1 权值共享</h3><ul><li>使用<code>k-means</code>算法进行聚类，确定<strong>每一层共享的权重</strong>，在一个<code>cluster</code>中的权重共享，注意这里<strong>没有跨层</strong></li><li>将$W={\{w_1, w_2, … ,w_n\}}$聚为$C={\{c_1,c_2, … ,c_k\}}$类, 其中<code>n&gt;&gt;k</code><ul><li>优化函数为：$$\mathop {\arg \min }\limits_c \sum\limits_{i=1}^k \sum\limits_{w \in c_i} |w-c_i|^2$$ <h3 id="4-2-共享权重的初始化方法（三种）"><a href="#4-2-共享权重的初始化方法（三种）" class="headerlink" title="4.2 共享权重的初始化方法（三种）"></a>4.2 <strong>共享权重的初始化方法</strong>（三种）</h3></li><li><code>Forgy</code>: 就是随机初始化方法初始化聚类的中心，如下图，因为权重分布有两个峰值，初始化的值都在峰值附近</li><li>基于密度的初始化方法：如下图，先是根据<strong>累积分布函数</strong>(<code>CDF</code>)<strong>线性等分y轴</strong>，然后根据<code>CDF</code>找到对应的<code>x</code>轴的坐标，即为聚类的中心。（也是在峰值附近，和<code>Forgy</code>方法相比更分散一些）</li><li>线性：就是根据权重的最小值和最大值等分，分散性最大</li></ul></li></ul><p><img src="/assets/blog_images/ModelCompression/04_centroids-initialization.png" alt="centroids initialization" title="04_centroids-initialization"></p><ul><li>神经网络中一般<strong>权重值越大，它的作用也就越大</strong>，所以对于前两种初始化方法都是在<strong>峰值附近</strong>，也就意味着值少的地方很小的概率会被初始化，所以不太好，实验中线性初始化的效果最好（但是<strong>大权重值的是很少的</strong>）</li></ul><h3 id="4-3-前向和反向传播"><a href="#4-3-前向和反向传播" class="headerlink" title="4.3 前向和反向传播"></a>4.3 前向和反向传播</h3><ul><li>计算时查表就可以了</li><li><strong>反向传播用于更新聚类中心的权重值</strong><br>$${\partial L \over \partial C_k} = \sum\limits_{ij}{\partial L \over \partial W_{ij}} {\partial W_{ij} \over \partial C_k} = \sum\limits_{ij} {\partial L \over\partial W_{ij}}\Gamma (I_{ij}=k)$$<ul><li>其中<code>L</code>是损失函数，$C_k$是第<code>k</code>个聚类的中心</li><li>$I_{ij}$为聚类中心的索引，如下图，就是<strong>同一类别梯度求和</strong><br><img src="/assets/blog_images/ModelCompression/03_trained-quantization.png" alt="权重" title="03_trained-quantization"></li></ul></li></ul><h2 id="5、哈夫曼编码"><a href="#5、哈夫曼编码" class="headerlink" title="5、哈夫曼编码"></a>5、哈夫曼编码</h2><ul><li>就是按照聚类中心的出现的概率从大到小排序进行<code>Huffman</code>编码</li><li>根据上面的结果，权重大都分布在两个峰值附近，所以利于<code>huffman</code>编码</li></ul><h2 id="6、结果及讨论"><a href="#6、结果及讨论" class="headerlink" title="6、结果及讨论"></a>6、结果及讨论</h2><ul><li>没有准确度损失<br><img src="/assets/blog_images/ModelCompression/05_compressed-result.png" alt="result" title="05_compressed-result"></li><li><code>pruning</code> 和 <code>quantization</code> 结合使用效果最好<br><img src="/assets/blog_images/ModelCompression/06_combination-of-pruning-and-uqantizaiton.png" alt="pruning and quantization" title="06_combination-of-pruning-and-uqantizaiton"></li><li>和之前别人的工作的比较<ul><li><code>SVD</code> 压缩了模型但是精度损失较大</li></ul></li></ul><p><img src="/assets/blog_images/ModelCompression/07_comparison.png" alt="和之前的工作比较" title="07_comparison"></p><ul><li>缺点就是在运行时现有的<code>GPU</code>不能进行间接的矩阵输入查找，以及相对索引 <code>CSC</code> 或 <code>CSR</code>, 还有剪枝的操作主要<strong>是在全连接层</strong>，但是计算量大的卷积层较少，虽然可以通过<code>BLAS libraries</code>或是<code>specialized hardware</code>进行加速，但是也是受限的（下篇<code>Paper</code>中也有提到）</li><li>[我觉得剪枝和权值共享其实是能够防止过拟合的，所以准确度没有损失]</li><li>[权值共享时是当前层的权值共享，不是整个网络的权值共享]</li></ul><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li><a href="https://arxiv.org/abs/1510.00149" target="_blank" rel="external">https://arxiv.org/abs/1510.00149</a></li><li><a href="http://blog.csdn.net/bigpiglet_zju/article/details/20791881" target="_blank" rel="external">http://blog.csdn.net/bigpiglet_zju/article/details/20791881</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;1、概述&quot;&gt;&lt;a href=&quot;#1、概述&quot; class=&quot;headerlink&quot; title=&quot;1、概述&quot;&gt;&lt;/a&gt;1、概述&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;压缩主要分为&lt;strong&gt;三个阶段&lt;/strong&gt;：剪枝(&lt;code&gt;pruning&lt;/code&gt;)、训练分层(&lt;code&gt;trained quantization&lt;/code&gt;)以及 哈夫曼编码(&lt;code&gt;Huffman coding&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;可以压缩&lt;code&gt;35&lt;/code&gt;到&lt;code&gt;49&lt;/code&gt;倍，并且不影响精度&lt;/li&gt;
&lt;li&gt;[模型压缩的主要用于还是能够用于小型的设备上，例如手机端等，比如Google的&lt;code&gt;Mobile Net&lt;/code&gt;, 但是准确度肯定要比正常的电脑端训练的大网络低一些，在所难免]&lt;/li&gt;
&lt;li&gt;[一般的训练好的神经网络模型文件有&lt;strong&gt;几百兆&lt;/strong&gt;的大小，比如&lt;a href=&quot;https://github.com/tensorflow/models/tree/master/slim#pre-trained-models&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Google预训练好的模型&lt;/a&gt;，计算量也很大，在手机端运行不太现实]&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="Paper阅读记录" scheme="http://lawlite.me/tags/Paper%E9%98%85%E8%AF%BB%E8%AE%B0%E5%BD%95/"/>
    
      <category term="ModelCompression" scheme="http://lawlite.me/tags/ModelCompression/"/>
    
  </entry>
  
  <entry>
    <title>R语言学习</title>
    <link href="http://lawlite.me/2017/06/30/R%E8%AF%AD%E8%A8%80%E5%AD%A6%E4%B9%A0/"/>
    <id>http://lawlite.me/2017/06/30/R语言学习/</id>
    <published>2017-06-30T08:48:15.000Z</published>
    <updated>2017-07-01T12:24:11.835Z</updated>
    
    <content type="html"><![CDATA[<ul><li>基础内容来自<code>W3C</code>，直接<a href="https://w3cschool.cn/r/" target="_blank" rel="external">查看这里</a>即可，这里只是个人学习的记录</li><li>只是最近感觉有必要学习一下<code>R</code>，哈哈</li></ul><h1 id="一、基础"><a href="#一、基础" class="headerlink" title="一、基础"></a>一、基础</h1><h2 id="1、概述"><a href="#1、概述" class="headerlink" title="1、概述"></a>1、概述</h2><ul><li>R语言是用于统计分析，图形表示和报告的编程语言和软件环境。</li><li>解释型语言<h2 id="2、Windows上安装"><a href="#2、Windows上安装" class="headerlink" title="2、Windows上安装"></a>2、<code>Windows</code>上安装</h2></li><li>下载地址：<a href="https://cran.r-project.org/bin/windows/base/" target="_blank" rel="external">R-3.4.0</a>，直接下载安装即可</li><li><code>IDE</code>使用<code>RStudio</code>: <a href="https://www.rstudio.com/products/rstudio/download/" target="_blank" rel="external">点击下载</a></li></ul><a id="more"></a><h2 id="3、数据类型"><a href="#3、数据类型" class="headerlink" title="3、数据类型"></a>3、数据类型</h2><ul><li>使用<code>&lt;-</code>进行赋值（<code>RStudio</code>中的快捷键是<code>Alt+-</code>）</li><li>变量分配有<strong>R对象</strong>，R对象的数据类型变为变量的数据类型。</li><li>使用<code>class()</code>来查看类型<h3 id="1-Vector向量"><a href="#1-Vector向量" class="headerlink" title="(1) Vector向量"></a>(1) <code>Vector</code>向量</h3></li><li>创建使用<code>c()</code>函数</li></ul><figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">&gt; apple = c(<span class="string">'apple'</span>, <span class="string">'banana'</span>)</div><div class="line">&gt; print(apple)</div><div class="line">[<span class="number">1</span>] <span class="string">"apple"</span>  <span class="string">"banana"</span></div></pre></td></tr></table></figure><h3 id="2-List列表"><a href="#2-List列表" class="headerlink" title="(2) List列表"></a>(2) <code>List</code>列表</h3><ul><li>列表是一个<strong>R对象</strong>，它可以在其中包含许多不同类型的元素</li></ul><figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">&gt; list &lt;- list(c(<span class="string">'apple'</span>,<span class="string">'banana'</span>), <span class="number">1</span>, <span class="number">2.0</span>)</div><div class="line">&gt; print(list)</div><div class="line">[[<span class="number">1</span>]]</div><div class="line">[<span class="number">4</span>] <span class="string">"apple"</span>  <span class="string">"banana"</span></div><div class="line"></div><div class="line">[[<span class="number">2</span>]]</div><div class="line">[<span class="number">5</span>] <span class="number">1</span></div><div class="line"></div><div class="line">[[<span class="number">3</span>]]</div><div class="line">[<span class="number">6</span>] <span class="number">2</span></div></pre></td></tr></table></figure><h3 id="3-Matrix矩阵"><a href="#3-Matrix矩阵" class="headerlink" title="(3) Matrix矩阵"></a>(3) <code>Matrix</code>矩阵</h3><ul><li>矩阵被限制为二维<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">&gt; M = matrix( c(<span class="string">'a'</span>,<span class="string">'a'</span>,<span class="string">'b'</span>,<span class="string">'c'</span>,<span class="string">'b'</span>,<span class="string">'a'</span>), nrow = <span class="number">2</span>, ncol = <span class="number">3</span>, byrow = <span class="literal">TRUE</span>)</div><div class="line">&gt; print(M)</div><div class="line">     [,<span class="number">1</span>] [,<span class="number">2</span>] [,<span class="number">3</span>]</div><div class="line">[<span class="number">1</span>,] <span class="string">"a"</span>  <span class="string">"a"</span>  <span class="string">"b"</span> </div><div class="line">[<span class="number">2</span>,] <span class="string">"c"</span>  <span class="string">"b"</span>  <span class="string">"a"</span></div></pre></td></tr></table></figure></li></ul><h3 id="4-Array数组"><a href="#4-Array数组" class="headerlink" title="(4) Array数组"></a>(4) <code>Array</code>数组</h3><ul><li><code>dim</code>指定维度，这里穿创建<code>3x3x2</code>的三维矩阵，输入是向量（一致循环创建）</li></ul><figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">&gt; a &lt;- array(c(<span class="string">'green'</span>,<span class="string">'yellow'</span>),dim = c(<span class="number">3</span>,<span class="number">3</span>,<span class="number">2</span>))</div><div class="line">&gt; print(a)</div><div class="line">, , <span class="number">1</span></div><div class="line"></div><div class="line">     [,<span class="number">1</span>]     [,<span class="number">2</span>]     [,<span class="number">3</span>]    </div><div class="line">[<span class="number">1</span>,] <span class="string">"green"</span>  <span class="string">"yellow"</span> <span class="string">"green"</span> </div><div class="line">[<span class="number">2</span>,] <span class="string">"yellow"</span> <span class="string">"green"</span>  <span class="string">"yellow"</span></div><div class="line">[<span class="number">3</span>,] <span class="string">"green"</span>  <span class="string">"yellow"</span> <span class="string">"green"</span> </div><div class="line"></div><div class="line">, , <span class="number">2</span></div><div class="line"></div><div class="line">     [,<span class="number">1</span>]     [,<span class="number">2</span>]     [,<span class="number">3</span>]    </div><div class="line">[<span class="number">1</span>,] <span class="string">"yellow"</span> <span class="string">"green"</span>  <span class="string">"yellow"</span></div><div class="line">[<span class="number">2</span>,] <span class="string">"green"</span>  <span class="string">"yellow"</span> <span class="string">"green"</span> </div><div class="line">[<span class="number">3</span>,] <span class="string">"yellow"</span> <span class="string">"green"</span>  <span class="string">"yellow"</span></div></pre></td></tr></table></figure><h3 id="5-Factor因子"><a href="#5-Factor因子" class="headerlink" title="(5) Factor因子"></a>(5) <code>Factor</code>因子</h3><ul><li>因子是使用<strong>向量创建的r对象</strong>。</li><li><code>nlevels</code>函数可以得到因子中不重复值的个数<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">&gt; factor_content &lt;- factor(c(<span class="string">'green'</span>, <span class="string">'red'</span>, <span class="string">'green'</span>, <span class="string">'yellow'</span>, <span class="string">'red'</span>, <span class="string">'green'</span>))</div><div class="line">&gt; print(factor_content)</div><div class="line">[<span class="number">1</span>] green  red    green  yellow red    green </div><div class="line">Levels: green red yellow</div><div class="line">&gt; print(nlevels(factor_content))</div><div class="line">[<span class="number">1</span>] <span class="number">3</span></div></pre></td></tr></table></figure></li></ul><h3 id="6-Data-Frame-数据帧"><a href="#6-Data-Frame-数据帧" class="headerlink" title="(6) Data Frame 数据帧"></a>(6) <code>Data Frame</code> 数据帧</h3><ul><li><p>数据表</p><figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">&gt; BMI &lt;- data.frame(</div><div class="line">+     gender = c(<span class="string">"Male"</span>, <span class="string">"Male"</span>,<span class="string">"Female"</span>), </div><div class="line">+     height = c(<span class="number">152</span>, <span class="number">171.5</span>, <span class="number">165</span>), </div><div class="line">+     weight = c(<span class="number">81</span>,<span class="number">93</span>, <span class="number">78</span>),</div><div class="line">+     Age = c(<span class="number">42</span>,<span class="number">38</span>,<span class="number">26</span>)</div><div class="line">+ )</div><div class="line">&gt; print(BMI)</div><div class="line">  gender height weight Age</div><div class="line"><span class="number">1</span>   Male  <span class="number">152.0</span>     <span class="number">81</span>  <span class="number">42</span></div><div class="line"><span class="number">2</span>   Male  <span class="number">171.5</span>     <span class="number">93</span>  <span class="number">38</span></div><div class="line"><span class="number">3</span> Female  <span class="number">165.0</span>     <span class="number">78</span>  <span class="number">26</span></div></pre></td></tr></table></figure></li><li><p><code>names(data.frame对象)</code> 查看列名</p></li><li><code>row.names</code> 查看行名</li></ul><h2 id="4、变量"><a href="#4、变量" class="headerlink" title="4、变量"></a>4、变量</h2><h3 id="1-命名"><a href="#1-命名" class="headerlink" title="(1) 命名"></a>(1) 命名</h3><ul><li>有效的变量名称由<strong>字母，数字和点</strong>或<strong>下划线</strong>字符组成</li><li>列如：<code>var_name2.</code>，<code>.var_name</code> <code>var.name</code><h3 id="2-赋值"><a href="#2-赋值" class="headerlink" title="(2) 赋值"></a>(2) 赋值</h3></li><li>可以使用<code>=, &lt;-, -&gt;</code><ul><li><code>=</code>和<code>&lt;-</code>的区别是一个是传值，一个是赋值，一般使用<code>&lt;-</code><figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">&gt; var.1 = c(<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>)</div><div class="line">&gt; var.2 &lt;- c(<span class="string">"learn"</span>,<span class="string">"R"</span>)</div><div class="line">&gt; c(<span class="literal">TRUE</span>,<span class="number">1</span>) -&gt; var.3  </div><div class="line">&gt; print(var.1)</div><div class="line">[<span class="number">1</span>] <span class="number">0</span> <span class="number">1</span> <span class="number">2</span> <span class="number">3</span></div><div class="line">&gt; print(var.2)</div><div class="line">[<span class="number">1</span>] <span class="string">"learn"</span> <span class="string">"R"</span>    </div><div class="line">&gt; print(var.3)</div><div class="line">[<span class="number">1</span>] <span class="number">1</span> <span class="number">1</span></div></pre></td></tr></table></figure></li></ul></li></ul><h3 id="3-查找变量"><a href="#3-查找变量" class="headerlink" title="(3) 查找变量"></a>(3) 查找变量</h3><ul><li>使用<code>ls()</code>函数</li></ul><figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">&gt; print(ls())</div><div class="line"> [<span class="number">1</span>] <span class="string">"a"</span>              <span class="string">"aisles"</span>         <span class="string">"apple"</span>          <span class="string">"BMI"</span>            <span class="string">"data"</span>          </div><div class="line"> [<span class="number">6</span>] <span class="string">"departments"</span>    <span class="string">"factor_content"</span> <span class="string">"list"</span>           <span class="string">"M"</span>              <span class="string">"orderp"</span>        </div><div class="line">[<span class="number">11</span>] <span class="string">"orders"</span>         <span class="string">"ordert"</span>         <span class="string">"path"</span>           <span class="string">"products"</span>       <span class="string">"s"</span>             </div><div class="line">[<span class="number">16</span>] <span class="string">"var.1"</span>          <span class="string">"var.2"</span>          <span class="string">"var.3"</span></div></pre></td></tr></table></figure><ul><li>通过<strong>模式匹配</strong>查找</li></ul><figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">&gt; print(ls(pattern = <span class="string">"var"</span>))</div><div class="line">[<span class="number">1</span>] <span class="string">"var.1"</span> <span class="string">"var.2"</span> <span class="string">"var.3"</span></div></pre></td></tr></table></figure><ul><li>以点<code>.</code>开头的变量名字会被隐藏，可以<code>print(ls(all.name = TRUE))</code>列出所有的</li></ul><h3 id="4-删除变量"><a href="#4-删除变量" class="headerlink" title="(4) 删除变量"></a>(4) 删除变量</h3><ul><li>单个变量通过<code>rm(变量名)</code>删除</li><li>删除所有变量：<code>rm(list = ls())</code></li></ul><h2 id="5、运算符"><a href="#5、运算符" class="headerlink" title="5、运算符"></a>5、运算符</h2><h3 id="1-算数运算符"><a href="#1-算数运算符" class="headerlink" title="(1) 算数运算符"></a>(1) 算数运算符</h3><ul><li><code>%%</code> : 求余</li><li><code>%/%</code> : 相除求商</li><li><code>^</code> : 指数<h3 id="2-关系运算符"><a href="#2-关系运算符" class="headerlink" title="(2) 关系运算符"></a>(2) 关系运算符</h3></li><li>和<code>c</code>等语言一样<h3 id="3-逻辑运算符"><a href="#3-逻辑运算符" class="headerlink" title="(3) 逻辑运算符"></a>(3) 逻辑运算符</h3></li><li><code>&amp;</code> : 与<code>and</code>，两个<code>&amp;&amp;</code>只比较第一个（比如向量只比较第一个元素）</li><li><code>|</code> : 或<code>or</code>， 两个<code>||</code>一样</li></ul><h3 id="4-其他"><a href="#4-其他" class="headerlink" title="(4) 其他"></a>(4) 其他</h3><ul><li><code>:</code> </li></ul><figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">&gt; <span class="number">2</span>:<span class="number">8</span></div><div class="line">[<span class="number">1</span>] <span class="number">2</span> <span class="number">3</span> <span class="number">4</span> <span class="number">5</span> <span class="number">6</span> <span class="number">7</span> <span class="number">8</span></div></pre></td></tr></table></figure><ul><li><code>%in%</code> : 元素是否在一个向量中</li></ul><h2 id="6、包"><a href="#6、包" class="headerlink" title="6、包"></a>6、包</h2><h3 id="1-介绍"><a href="#1-介绍" class="headerlink" title="(1) 介绍"></a>(1) 介绍</h3><ul><li><code>R</code>语言的包是<strong>R函数</strong>，编译代码和样本数据的集合。</li><li>存储在<code>R</code>语言环境中名为<code>“library”</code>的目录下</li><li>可用的<code>R</code>语言的包：<a href="https://cran.r-project.org/web/packages/available_packages_by_name.html" target="_blank" rel="external">点击查看</a><h3 id="2-常用命令"><a href="#2-常用命令" class="headerlink" title="(2) 常用命令"></a>(2) 常用命令</h3></li><li>查看库的位置：<code>.libPaths()</code></li><li>查看已安装所有软件包：<code>library()</code></li><li>当前环境加载的所有包：<code>search()</code></li><li>安装包：<code>install.packages(&quot;Package Name&quot;)</code></li><li>加载到当前<code>R</code>语言环境中：<code>library(&quot;package Name&quot;, lib.loc = &quot;path to library&quot;)</code><h3 id="3-手动安装包"><a href="#3-手动安装包" class="headerlink" title="(3) 手动安装包"></a>(3) 手动安装包</h3></li><li>将包作为<code>.zip</code>文件保存在本地系统中的适当位置。</li><li>安装：<code>install.packages(&quot;E:/XML_3.98-1.3.zip&quot;, repos = NULL, type = &quot;source&quot;)</code></li></ul><h2 id="7、数据重塑Dataframe"><a href="#7、数据重塑Dataframe" class="headerlink" title="7、数据重塑Dataframe"></a>7、数据重塑Dataframe</h2><h3 id="1-数据帧中加入行和列"><a href="#1-数据帧中加入行和列" class="headerlink" title="(1) 数据帧中加入行和列"></a>(1) 数据帧中加入行和列</h3><ul><li><code>cbind()</code>连接多个向量</li></ul><figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">&gt; city &lt;- c(<span class="string">"Tampa"</span>,<span class="string">"Seattle"</span>,<span class="string">"Hartford"</span>,<span class="string">"Denver"</span>)</div><div class="line">&gt; state &lt;- c(<span class="string">"FL"</span>,<span class="string">"WA"</span>,<span class="string">"CT"</span>,<span class="string">"CO"</span>)</div><div class="line">&gt; zipcode &lt;- c(<span class="number">33602</span>,<span class="number">98104</span>,<span class="number">06161</span>,<span class="number">80294</span>)</div><div class="line">&gt; addresses &lt;- cbind(city,state,zipcode)</div><div class="line">&gt; print(addresses)</div><div class="line">     city       state zipcode</div><div class="line">[<span class="number">1</span>,] <span class="string">"Tampa"</span>    <span class="string">"FL"</span>  <span class="string">"33602"</span></div><div class="line">[<span class="number">2</span>,] <span class="string">"Seattle"</span>  <span class="string">"WA"</span>  <span class="string">"98104"</span></div><div class="line">[<span class="number">3</span>,] <span class="string">"Hartford"</span> <span class="string">"CT"</span>  <span class="string">"6161"</span> </div><div class="line">[<span class="number">4</span>,] <span class="string">"Denver"</span>   <span class="string">"CO"</span>  <span class="string">"80294"</span></div></pre></td></tr></table></figure><ul><li><code>rbind()</code>按行拼接<h3 id="2-合并数据帧"><a href="#2-合并数据帧" class="headerlink" title="(2) 合并数据帧"></a>(2) 合并数据帧</h3></li><li>使用<code>merge()</code>函数合并两个数据帧。 数据帧必须具有相同的列名称，在其上进行合并。<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line">&gt; <span class="keyword">library</span>(MASS)</div><div class="line">&gt; merged.Pima &lt;- merge(x = Pima.te, y = Pima.tr,</div><div class="line">+                      by.x = c(<span class="string">"bp"</span>, <span class="string">"bmi"</span>),</div><div class="line">+                      by.y = c(<span class="string">"bp"</span>, <span class="string">"bmi"</span>)</div><div class="line">+ )</div><div class="line">&gt; print(merged.Pima)</div><div class="line">   bp  bmi npreg.x glu.x skin.x ped.x age.x type.x npreg.y glu.y skin.y ped.y age.y type.y</div><div class="line"><span class="number">1</span>  <span class="number">60</span> <span class="number">33.8</span>       <span class="number">1</span>   <span class="number">117</span>     <span class="number">23</span> <span class="number">0.466</span>    <span class="number">27</span>     No       <span class="number">2</span>   <span class="number">125</span>     <span class="number">20</span> <span class="number">0.088</span>    <span class="number">31</span>     No</div><div class="line"><span class="number">2</span>  <span class="number">64</span> <span class="number">29.7</span>       <span class="number">2</span>    <span class="number">75</span>     <span class="number">24</span> <span class="number">0.370</span>    <span class="number">33</span>     No       <span class="number">2</span>   <span class="number">100</span>     <span class="number">23</span> <span class="number">0.368</span>    <span class="number">21</span>     No</div><div class="line"><span class="number">3</span>  <span class="number">64</span> <span class="number">31.2</span>       <span class="number">5</span>   <span class="number">189</span>     <span class="number">33</span> <span class="number">0.583</span>    <span class="number">29</span>    Yes       <span class="number">3</span>   <span class="number">158</span>     <span class="number">13</span> <span class="number">0.295</span>    <span class="number">24</span>     No</div><div class="line"><span class="number">4</span>  <span class="number">64</span> <span class="number">33.2</span>       <span class="number">4</span>   <span class="number">117</span>     <span class="number">27</span> <span class="number">0.230</span>    <span class="number">24</span>     No       <span class="number">1</span>    <span class="number">96</span>     <span class="number">27</span> <span class="number">0.289</span>    <span class="number">21</span>     No</div><div class="line"><span class="number">5</span>  <span class="number">66</span> <span class="number">38.1</span>       <span class="number">3</span>   <span class="number">115</span>     <span class="number">39</span> <span class="number">0.150</span>    <span class="number">28</span>     No       <span class="number">1</span>   <span class="number">114</span>     <span class="number">36</span> <span class="number">0.289</span>    <span class="number">21</span>     No</div><div class="line"><span class="number">6</span>  <span class="number">68</span> <span class="number">38.5</span>       <span class="number">2</span>   <span class="number">100</span>     <span class="number">25</span> <span class="number">0.324</span>    <span class="number">26</span>     No       <span class="number">7</span>   <span class="number">129</span>     <span class="number">49</span> <span class="number">0.439</span>    <span class="number">43</span>    Yes</div><div class="line"><span class="number">7</span>  <span class="number">70</span> <span class="number">27.4</span>       <span class="number">1</span>   <span class="number">116</span>     <span class="number">28</span> <span class="number">0.204</span>    <span class="number">21</span>     No       <span class="number">0</span>   <span class="number">124</span>     <span class="number">20</span> <span class="number">0.254</span>    <span class="number">36</span>    Yes</div><div class="line"><span class="number">8</span>  <span class="number">70</span> <span class="number">33.1</span>       <span class="number">4</span>    <span class="number">91</span>     <span class="number">32</span> <span class="number">0.446</span>    <span class="number">22</span>     No       <span class="number">9</span>   <span class="number">123</span>     <span class="number">44</span> <span class="number">0.374</span>    <span class="number">40</span>     No</div><div class="line"><span class="number">9</span>  <span class="number">70</span> <span class="number">35.4</span>       <span class="number">9</span>   <span class="number">124</span>     <span class="number">33</span> <span class="number">0.282</span>    <span class="number">34</span>     No       <span class="number">6</span>   <span class="number">134</span>     <span class="number">23</span> <span class="number">0.542</span>    <span class="number">29</span>    Yes</div><div class="line"><span class="number">10</span> <span class="number">72</span> <span class="number">25.6</span>       <span class="number">1</span>   <span class="number">157</span>     <span class="number">21</span> <span class="number">0.123</span>    <span class="number">24</span>     No       <span class="number">4</span>    <span class="number">99</span>     <span class="number">17</span> <span class="number">0.294</span>    <span class="number">28</span>     No</div><div class="line"><span class="number">11</span> <span class="number">72</span> <span class="number">37.7</span>       <span class="number">5</span>    <span class="number">95</span>     <span class="number">33</span> <span class="number">0.370</span>    <span class="number">27</span>     No       <span class="number">6</span>   <span class="number">103</span>     <span class="number">32</span> <span class="number">0.324</span>    <span class="number">55</span>     No</div><div class="line"><span class="number">12</span> <span class="number">74</span> <span class="number">25.9</span>       <span class="number">9</span>   <span class="number">134</span>     <span class="number">33</span> <span class="number">0.460</span>    <span class="number">81</span>     No       <span class="number">8</span>   <span class="number">126</span>     <span class="number">38</span> <span class="number">0.162</span>    <span class="number">39</span>     No</div><div class="line"><span class="number">13</span> <span class="number">74</span> <span class="number">25.9</span>       <span class="number">1</span>    <span class="number">95</span>     <span class="number">21</span> <span class="number">0.673</span>    <span class="number">36</span>     No       <span class="number">8</span>   <span class="number">126</span>     <span class="number">38</span> <span class="number">0.162</span>    <span class="number">39</span>     No</div><div class="line"><span class="number">14</span> <span class="number">78</span> <span class="number">27.6</span>       <span class="number">5</span>    <span class="number">88</span>     <span class="number">30</span> <span class="number">0.258</span>    <span class="number">37</span>     No       <span class="number">6</span>   <span class="number">125</span>     <span class="number">31</span> <span class="number">0.565</span>    <span class="number">49</span>    Yes</div><div class="line"><span class="number">15</span> <span class="number">78</span> <span class="number">27.6</span>      <span class="number">10</span>   <span class="number">122</span>     <span class="number">31</span> <span class="number">0.512</span>    <span class="number">45</span>     No       <span class="number">6</span>   <span class="number">125</span>     <span class="number">31</span> <span class="number">0.565</span>    <span class="number">49</span>    Yes</div><div class="line"><span class="number">16</span> <span class="number">78</span> <span class="number">39.4</span>       <span class="number">2</span>   <span class="number">112</span>     <span class="number">50</span> <span class="number">0.175</span>    <span class="number">24</span>     No       <span class="number">4</span>   <span class="number">112</span>     <span class="number">40</span> <span class="number">0.236</span>    <span class="number">38</span>     No</div><div class="line"><span class="number">17</span> <span class="number">88</span> <span class="number">34.5</span>       <span class="number">1</span>   <span class="number">117</span>     <span class="number">24</span> <span class="number">0.403</span>    <span class="number">40</span>    Yes       <span class="number">4</span>   <span class="number">127</span>     <span class="number">11</span> <span class="number">0.598</span>    <span class="number">28</span>     No</div><div class="line">&gt; nrow(merged.Pima)</div><div class="line">[<span class="number">1</span>] <span class="number">17</span></div></pre></td></tr></table></figure></li></ul><h3 id="3-拆分和重组"><a href="#3-拆分和重组" class="headerlink" title="(3) 拆分和重组"></a>(3) 拆分和重组</h3><ul><li><p><code>melt()</code>拆分数据 (以船舶的数据集为例)</p><ul><li>需要安装<code>reshape</code>包</li><li><code>install.packages(&quot;reshape&quot;)</code><figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div></pre></td><td class="code"><pre><div class="line">&gt; <span class="keyword">library</span>(MASS)</div><div class="line">&gt; print(ships)</div><div class="line">   type year period service incidents</div><div class="line"><span class="number">1</span>     A   <span class="number">60</span>     <span class="number">60</span>     <span class="number">127</span>         <span class="number">0</span></div><div class="line"><span class="number">2</span>     A   <span class="number">60</span>     <span class="number">75</span>      <span class="number">63</span>         <span class="number">0</span></div><div class="line"><span class="number">3</span>     A   <span class="number">65</span>     <span class="number">60</span>    <span class="number">1095</span>         <span class="number">3</span></div><div class="line"><span class="number">4</span>     A   <span class="number">65</span>     <span class="number">75</span>    <span class="number">1095</span>         <span class="number">4</span></div><div class="line"><span class="number">5</span>     A   <span class="number">70</span>     <span class="number">60</span>    <span class="number">1512</span>         <span class="number">6</span></div><div class="line"><span class="number">6</span>     A   <span class="number">70</span>     <span class="number">75</span>    <span class="number">3353</span>        <span class="number">18</span></div><div class="line"><span class="number">7</span>     A   <span class="number">75</span>     <span class="number">60</span>       <span class="number">0</span>         <span class="number">0</span></div><div class="line"><span class="number">8</span>     A   <span class="number">75</span>     <span class="number">75</span>    <span class="number">2244</span>        <span class="number">11</span></div><div class="line"><span class="number">9</span>     B   <span class="number">60</span>     <span class="number">60</span>   <span class="number">44882</span>        <span class="number">39</span></div><div class="line"><span class="number">10</span>    B   <span class="number">60</span>     <span class="number">75</span>   <span class="number">17176</span>        <span class="number">29</span></div><div class="line"><span class="number">11</span>    B   <span class="number">65</span>     <span class="number">60</span>   <span class="number">28609</span>        <span class="number">58</span></div><div class="line"><span class="number">12</span>    B   <span class="number">65</span>     <span class="number">75</span>   <span class="number">20370</span>        <span class="number">53</span></div><div class="line"><span class="number">13</span>    B   <span class="number">70</span>     <span class="number">60</span>    <span class="number">7064</span>        <span class="number">12</span></div><div class="line"><span class="number">14</span>    B   <span class="number">70</span>     <span class="number">75</span>   <span class="number">13099</span>        <span class="number">44</span></div><div class="line"><span class="number">15</span>    B   <span class="number">75</span>     <span class="number">60</span>       <span class="number">0</span>         <span class="number">0</span></div><div class="line"><span class="number">16</span>    B   <span class="number">75</span>     <span class="number">75</span>    <span class="number">7117</span>        <span class="number">18</span></div><div class="line"><span class="number">17</span>    C   <span class="number">60</span>     <span class="number">60</span>    <span class="number">1179</span>         <span class="number">1</span></div><div class="line"><span class="number">18</span>    C   <span class="number">60</span>     <span class="number">75</span>     <span class="number">552</span>         <span class="number">1</span></div><div class="line"><span class="number">19</span>    C   <span class="number">65</span>     <span class="number">60</span>     <span class="number">781</span>         <span class="number">0</span></div><div class="line"><span class="number">20</span>    C   <span class="number">65</span>     <span class="number">75</span>     <span class="number">676</span>         <span class="number">1</span></div><div class="line"><span class="number">21</span>    C   <span class="number">70</span>     <span class="number">60</span>     <span class="number">783</span>         <span class="number">6</span></div><div class="line"><span class="number">22</span>    C   <span class="number">70</span>     <span class="number">75</span>    <span class="number">1948</span>         <span class="number">2</span></div><div class="line"><span class="number">23</span>    C   <span class="number">75</span>     <span class="number">60</span>       <span class="number">0</span>         <span class="number">0</span></div><div class="line"><span class="number">24</span>    C   <span class="number">75</span>     <span class="number">75</span>     <span class="number">274</span>         <span class="number">1</span></div><div class="line"><span class="number">25</span>    D   <span class="number">60</span>     <span class="number">60</span>     <span class="number">251</span>         <span class="number">0</span></div><div class="line"><span class="number">26</span>    D   <span class="number">60</span>     <span class="number">75</span>     <span class="number">105</span>         <span class="number">0</span></div><div class="line"><span class="number">27</span>    D   <span class="number">65</span>     <span class="number">60</span>     <span class="number">288</span>         <span class="number">0</span></div><div class="line"><span class="number">28</span>    D   <span class="number">65</span>     <span class="number">75</span>     <span class="number">192</span>         <span class="number">0</span></div><div class="line"><span class="number">29</span>    D   <span class="number">70</span>     <span class="number">60</span>     <span class="number">349</span>         <span class="number">2</span></div><div class="line"><span class="number">30</span>    D   <span class="number">70</span>     <span class="number">75</span>    <span class="number">1208</span>        <span class="number">11</span></div><div class="line"><span class="number">31</span>    D   <span class="number">75</span>     <span class="number">60</span>       <span class="number">0</span>         <span class="number">0</span></div><div class="line"><span class="number">32</span>    D   <span class="number">75</span>     <span class="number">75</span>    <span class="number">2051</span>         <span class="number">4</span></div><div class="line"><span class="number">33</span>    E   <span class="number">60</span>     <span class="number">60</span>      <span class="number">45</span>         <span class="number">0</span></div><div class="line"><span class="number">34</span>    E   <span class="number">60</span>     <span class="number">75</span>       <span class="number">0</span>         <span class="number">0</span></div><div class="line"><span class="number">35</span>    E   <span class="number">65</span>     <span class="number">60</span>     <span class="number">789</span>         <span class="number">7</span></div><div class="line"><span class="number">36</span>    E   <span class="number">65</span>     <span class="number">75</span>     <span class="number">437</span>         <span class="number">7</span></div><div class="line"><span class="number">37</span>    E   <span class="number">70</span>     <span class="number">60</span>    <span class="number">1157</span>         <span class="number">5</span></div><div class="line"><span class="number">38</span>    E   <span class="number">70</span>     <span class="number">75</span>    <span class="number">2161</span>        <span class="number">12</span></div><div class="line"><span class="number">39</span>    E   <span class="number">75</span>     <span class="number">60</span>       <span class="number">0</span>         <span class="number">0</span></div><div class="line"><span class="number">40</span>    E   <span class="number">75</span>     <span class="number">75</span>     <span class="number">542</span>         <span class="number">1</span></div></pre></td></tr></table></figure></li></ul></li><li><p>拆分</p><ul><li><code>melt()</code>将<strong>类型和年份以外</strong>的所有列转换为多行展示<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">library</span>(reshape)</div><div class="line">molten.ships &lt;- melt(ships, id = c(<span class="string">"type"</span>,<span class="string">"year"</span>))</div><div class="line">print(molten.ships)</div><div class="line"></div><div class="line">      type year  variable  value</div><div class="line"><span class="number">1</span>      A   <span class="number">60</span>    period      <span class="number">60</span></div><div class="line"><span class="number">2</span>      A   <span class="number">60</span>    period      <span class="number">75</span></div><div class="line"><span class="number">3</span>      A   <span class="number">65</span>    period      <span class="number">60</span></div><div class="line"><span class="number">4</span>      A   <span class="number">65</span>    period      <span class="number">75</span></div><div class="line">............</div><div class="line">............</div><div class="line"><span class="number">9</span>      B   <span class="number">60</span>    period      <span class="number">60</span></div><div class="line"><span class="number">10</span>     B   <span class="number">60</span>    period      <span class="number">75</span></div><div class="line"><span class="number">11</span>     B   <span class="number">65</span>    period      <span class="number">60</span></div><div class="line"><span class="number">12</span>     B   <span class="number">65</span>    period      <span class="number">75</span></div><div class="line"><span class="number">13</span>     B   <span class="number">70</span>    period      <span class="number">60</span></div><div class="line">...........</div><div class="line">...........</div><div class="line"><span class="number">41</span>     A   <span class="number">60</span>    service    <span class="number">127</span></div><div class="line"><span class="number">42</span>     A   <span class="number">60</span>    service     <span class="number">63</span></div><div class="line"><span class="number">43</span>     A   <span class="number">65</span>    service   <span class="number">1095</span></div><div class="line">...........</div><div class="line">...........</div><div class="line"><span class="number">70</span>     D   <span class="number">70</span>    service   <span class="number">1208</span></div><div class="line"><span class="number">71</span>     D   <span class="number">75</span>    service      <span class="number">0</span></div><div class="line"><span class="number">72</span>     D   <span class="number">75</span>    service   <span class="number">2051</span></div><div class="line"><span class="number">73</span>     E   <span class="number">60</span>    service     <span class="number">45</span></div><div class="line"><span class="number">74</span>     E   <span class="number">60</span>    service      <span class="number">0</span></div><div class="line"><span class="number">75</span>     E   <span class="number">65</span>    service    <span class="number">789</span></div><div class="line">...........</div><div class="line">...........</div><div class="line"><span class="number">101</span>    C   <span class="number">70</span>    incidents    <span class="number">6</span></div><div class="line"><span class="number">102</span>    C   <span class="number">70</span>    incidents    <span class="number">2</span></div><div class="line"><span class="number">103</span>    C   <span class="number">75</span>    incidents    <span class="number">0</span></div><div class="line"><span class="number">104</span>    C   <span class="number">75</span>    incidents    <span class="number">1</span></div><div class="line"><span class="number">105</span>    D   <span class="number">60</span>    incidents    <span class="number">0</span></div><div class="line"><span class="number">106</span>    D   <span class="number">60</span>    incidents    <span class="number">0</span></div><div class="line">...........</div><div class="line">...........</div></pre></td></tr></table></figure></li></ul></li><li><p><code>cast()</code>重构数据</p><ul><li>每年每种类型的船的总和<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line">&gt; recasted.ship &lt;- cast(molten.ships, type+year~variable,sum)</div><div class="line">&gt; print(recasted.ship)</div><div class="line">   type year period service incidents</div><div class="line"><span class="number">1</span>     A   <span class="number">60</span>    <span class="number">135</span>     <span class="number">190</span>         <span class="number">0</span></div><div class="line"><span class="number">2</span>     A   <span class="number">65</span>    <span class="number">135</span>    <span class="number">2190</span>         <span class="number">7</span></div><div class="line"><span class="number">3</span>     A   <span class="number">70</span>    <span class="number">135</span>    <span class="number">4865</span>        <span class="number">24</span></div><div class="line"><span class="number">4</span>     A   <span class="number">75</span>    <span class="number">135</span>    <span class="number">2244</span>        <span class="number">11</span></div><div class="line"><span class="number">5</span>     B   <span class="number">60</span>    <span class="number">135</span>   <span class="number">62058</span>        <span class="number">68</span></div><div class="line"><span class="number">6</span>     B   <span class="number">65</span>    <span class="number">135</span>   <span class="number">48979</span>       <span class="number">111</span></div><div class="line"><span class="number">7</span>     B   <span class="number">70</span>    <span class="number">135</span>   <span class="number">20163</span>        <span class="number">56</span></div><div class="line"><span class="number">8</span>     B   <span class="number">75</span>    <span class="number">135</span>    <span class="number">7117</span>        <span class="number">18</span></div><div class="line"><span class="number">9</span>     C   <span class="number">60</span>    <span class="number">135</span>    <span class="number">1731</span>         <span class="number">2</span></div><div class="line"><span class="number">10</span>    C   <span class="number">65</span>    <span class="number">135</span>    <span class="number">1457</span>         <span class="number">1</span></div><div class="line"><span class="number">11</span>    C   <span class="number">70</span>    <span class="number">135</span>    <span class="number">2731</span>         <span class="number">8</span></div><div class="line"><span class="number">12</span>    C   <span class="number">75</span>    <span class="number">135</span>     <span class="number">274</span>         <span class="number">1</span></div><div class="line"><span class="number">13</span>    D   <span class="number">60</span>    <span class="number">135</span>     <span class="number">356</span>         <span class="number">0</span></div><div class="line"><span class="number">14</span>    D   <span class="number">65</span>    <span class="number">135</span>     <span class="number">480</span>         <span class="number">0</span></div><div class="line"><span class="number">15</span>    D   <span class="number">70</span>    <span class="number">135</span>    <span class="number">1557</span>        <span class="number">13</span></div><div class="line"><span class="number">16</span>    D   <span class="number">75</span>    <span class="number">135</span>    <span class="number">2051</span>         <span class="number">4</span></div><div class="line"><span class="number">17</span>    E   <span class="number">60</span>    <span class="number">135</span>      <span class="number">45</span>         <span class="number">0</span></div><div class="line"><span class="number">18</span>    E   <span class="number">65</span>    <span class="number">135</span>    <span class="number">1226</span>        <span class="number">14</span></div><div class="line"><span class="number">19</span>    E   <span class="number">70</span>    <span class="number">135</span>    <span class="number">3318</span>        <span class="number">17</span></div><div class="line"><span class="number">20</span>    E   <span class="number">75</span>    <span class="number">135</span>     <span class="number">542</span>         <span class="number">1</span></div></pre></td></tr></table></figure></li></ul></li></ul><h2 id="8、函数"><a href="#8、函数" class="headerlink" title="8、函数"></a>8、函数</h2><h3 id="1-定义"><a href="#1-定义" class="headerlink" title="(1) 定义"></a>(1) 定义</h3><figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">function_name &lt;- <span class="keyword">function</span>(arg_1, arg_2, <span class="keyword">...</span>) &#123;</div><div class="line">   Function body </div><div class="line">&#125;</div></pre></td></tr></table></figure><h3 id="2-自定义函数和调用"><a href="#2-自定义函数和调用" class="headerlink" title="(2) 自定义函数和调用"></a>(2) 自定义函数和调用</h3><figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">&gt; new.function &lt;- <span class="keyword">function</span>(a) &#123;</div><div class="line">+     <span class="keyword">for</span>(i <span class="keyword">in</span> <span class="number">1</span>:a) &#123;</div><div class="line">+         b &lt;- i^<span class="number">2</span></div><div class="line">+         print(b)</div><div class="line">+     &#125;</div><div class="line">+ &#125;</div><div class="line">&gt; new.function(<span class="number">3</span>)</div><div class="line">[<span class="number">1</span>] <span class="number">1</span></div><div class="line">[<span class="number">1</span>] <span class="number">4</span></div><div class="line">[<span class="number">1</span>] <span class="number">9</span></div></pre></td></tr></table></figure><h2 id="9、字符串"><a href="#9、字符串" class="headerlink" title="9、字符串"></a>9、字符串</h2><h3 id="1-定义-1"><a href="#1-定义-1" class="headerlink" title="(1) 定义"></a>(1) 定义</h3><ul><li>可以使用<strong>单引号或双引号</strong></li></ul><figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">&gt; s &lt;- <span class="string">'hello world!'</span></div><div class="line">&gt; print(s)</div><div class="line">[<span class="number">1</span>] <span class="string">"hello world!"</span></div></pre></td></tr></table></figure><h3 id="2-字符串操作"><a href="#2-字符串操作" class="headerlink" title="(2) 字符串操作"></a>(2) 字符串操作</h3><ul><li>连接字符串：<code>paste(..., sep = &quot; &quot;, collapse = NULL)</code><ul><li><code>...</code>表示要组合的任意数量的自变量。</li><li><code>sep</code>表示参数之间的任何<strong>分隔符</strong>。 它是可选的。</li><li><code>collapse</code>用于消除<strong>两个字符串之间的空格</strong>。<strong>但</strong>不是一个字符串的两个字内的空间。</li></ul></li><li>格式化字符串：<code>format(x, digits, nsmall, scientific, width, justify = c(&quot;left&quot;, &quot;right&quot;, &quot;centre&quot;, &quot;none&quot;))</code><ul><li><code>x</code>是向量输入。</li><li><code>digits</code>是显示的总位数。</li><li><code>nsmall</code>是小数点右边的最小位数。</li><li>科学设置为<code>TRUE</code>以显示科学记数法。</li><li><code>width</code>指示通过在开始处填充空白来显示的最小宽度。</li><li><code>justify</code>是字符串向左，右或中心的显示。</li></ul></li><li>字符数：<code>nchar()</code></li><li>大小写：<code>toupper()</code> 和 <code>tolower()</code></li><li>截取：<code>substring(x,first,last)</code></li></ul><h2 id="10、向量"><a href="#10、向量" class="headerlink" title="10、向量"></a>10、向量</h2><ul><li>向量是<strong>最基本的R语言数据对象</strong>，有六种类型的原子向量。 它们是逻辑，整数，双精度，复杂，字符和原始。<h3 id="1-序列运算符seq"><a href="#1-序列运算符seq" class="headerlink" title="(1) 序列运算符seq"></a>(1) 序列运算符<code>seq</code></h3></li></ul><figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">&gt; print(seq(<span class="number">1</span>, <span class="number">10</span>, by=<span class="number">2</span>))</div><div class="line">[<span class="number">1</span>] <span class="number">1</span> <span class="number">3</span> <span class="number">5</span> <span class="number">7</span> <span class="number">9</span></div></pre></td></tr></table></figure><h3 id="2-访问向量元素"><a href="#2-访问向量元素" class="headerlink" title="(2) 访问向量元素"></a>(2) 访问向量元素</h3><ul><li>使用索引访问向量的元素。 <code>[]</code>括号用于建立索引。 索引从位置<code>1</code>开始。</li><li><p>位置索引</p><ul><li>注意位置是从<code>1</code>开始的<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">t &lt;- c(<span class="string">"Sun"</span>,<span class="string">"Mon"</span>,<span class="string">"Tue"</span>,<span class="string">"Wed"</span>,<span class="string">"Thurs"</span>,<span class="string">"Fri"</span>,<span class="string">"Sat"</span>)</div><div class="line">u &lt;- t[c(<span class="number">2</span>,<span class="number">3</span>,<span class="number">6</span>)]</div><div class="line">print(u)</div><div class="line">[<span class="number">1</span>] <span class="string">"Mon"</span> <span class="string">"Tue"</span> <span class="string">"Fri"</span></div></pre></td></tr></table></figure></li></ul></li><li><p>负数索引</p><ul><li>使用负数是<strong>丢弃</strong>掉对应正数的索引<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">x &lt;- t[c(-<span class="number">2</span>,-<span class="number">5</span>)]</div><div class="line">print(x)</div><div class="line">[<span class="number">1</span>] <span class="string">"Sun"</span> <span class="string">"Tue"</span> <span class="string">"Wed"</span> <span class="string">"Fri"</span> <span class="string">"Sat"</span></div></pre></td></tr></table></figure></li></ul></li><li><p>排序</p></li></ul><figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">&gt; v &lt;- c(<span class="number">3</span>,<span class="number">8</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">0</span>,<span class="number">11</span>, -<span class="number">9</span>, <span class="number">304</span>)</div><div class="line">&gt; v.result &lt;- sort(v)</div><div class="line">&gt; print(v.result)</div><div class="line">[<span class="number">1</span>]  -<span class="number">9</span>   <span class="number">0</span>   <span class="number">3</span>   <span class="number">4</span>   <span class="number">5</span>   <span class="number">8</span>  <span class="number">11</span> <span class="number">304</span></div></pre></td></tr></table></figure><h2 id="11、列表"><a href="#11、列表" class="headerlink" title="11、列表"></a>11、列表</h2><ul><li><p>可以通过索引和名字访问</p><figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line">&gt; list_data &lt;- list(c(<span class="string">"Jan"</span>,<span class="string">"Feb"</span>,<span class="string">"Mar"</span>), matrix(c(<span class="number">3</span>,<span class="number">9</span>,<span class="number">5</span>,<span class="number">1</span>,-<span class="number">2</span>,<span class="number">8</span>), nrow = <span class="number">2</span>),</div><div class="line">+                   list(<span class="string">"green"</span>,<span class="number">12.3</span>))</div><div class="line">&gt; print(list_data)</div><div class="line">[[<span class="number">1</span>]]</div><div class="line">[<span class="number">1</span>] <span class="string">"Jan"</span> <span class="string">"Feb"</span> <span class="string">"Mar"</span></div><div class="line"></div><div class="line">[[<span class="number">2</span>]]</div><div class="line">     [,<span class="number">1</span>] [,<span class="number">2</span>] [,<span class="number">3</span>]</div><div class="line">[<span class="number">1</span>,]    <span class="number">3</span>    <span class="number">5</span>   -<span class="number">2</span></div><div class="line">[<span class="number">2</span>,]    <span class="number">9</span>    <span class="number">1</span>    <span class="number">8</span></div><div class="line"></div><div class="line">[[<span class="number">3</span>]]</div><div class="line">[[<span class="number">3</span>]][[<span class="number">1</span>]]</div><div class="line">[<span class="number">1</span>] <span class="string">"green"</span></div><div class="line"></div><div class="line">[[<span class="number">3</span>]][[<span class="number">2</span>]]</div><div class="line">[<span class="number">1</span>] <span class="number">12.3</span></div><div class="line"></div><div class="line"></div><div class="line">&gt; names(list_data) &lt;- c(<span class="string">"1st Quarter"</span>, <span class="string">"A_Matrix"</span>, <span class="string">"A Inner list"</span>)</div><div class="line">&gt; print(list_data[<span class="number">1</span>])</div><div class="line">$`1st Quarter`</div><div class="line">[<span class="number">1</span>] <span class="string">"Jan"</span> <span class="string">"Feb"</span> <span class="string">"Mar"</span></div><div class="line"></div><div class="line">&gt; print(list_data$A_Matrix)</div><div class="line">     [,<span class="number">1</span>] [,<span class="number">2</span>] [,<span class="number">3</span>]</div><div class="line">[<span class="number">1</span>,]    <span class="number">3</span>    <span class="number">5</span>   -<span class="number">2</span></div><div class="line">[<span class="number">2</span>,]    <span class="number">9</span>    <span class="number">1</span>    <span class="number">8</span></div></pre></td></tr></table></figure></li><li><p>合并列表：<code>merged.list &lt;- c(list1,list2)</code></p></li><li>列表转向量：<code>v1 &lt;- unlist(list1)</code></li></ul><h2 id="12、矩阵"><a href="#12、矩阵" class="headerlink" title="12、矩阵"></a>12、矩阵</h2><ul><li>矩阵是其中元素以<strong>二维矩形</strong>布局布置的R对象。</li><li>包含相同原子类型的元素。</li><li><code>matrix(data, nrow, ncol, byrow, dimnames)</code></li><li>data数据是成为矩阵的数据元素的输入向量。</li><li>nrow是要创建的行数。</li><li>ncol是要创建的列数。</li><li>byrow是一个逻辑线索。如果为TRUE，则输入向量元素按行排列。</li><li>dimname是分配给行和列的名称。</li></ul><h2 id="13、数组"><a href="#13、数组" class="headerlink" title="13、数组"></a>13、数组</h2><h2 id="14、因子"><a href="#14、因子" class="headerlink" title="14、因子"></a>14、因子</h2><ul><li>因子是用于对数据进行分类并将其存储为级别的数据对象。 它们可以存储字符串和整数。 </li><li>它们在具有有限数量的唯一值的列中很有用。像“男性”，“女性”和True，False等。它们在统计建模的数据分析中很有用。</li></ul><figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">&gt; height &lt;- c(<span class="number">132</span>,<span class="number">151</span>,<span class="number">162</span>,<span class="number">139</span>,<span class="number">166</span>,<span class="number">147</span>,<span class="number">122</span>)</div><div class="line">&gt; weight &lt;- c(<span class="number">48</span>,<span class="number">49</span>,<span class="number">66</span>,<span class="number">53</span>,<span class="number">67</span>,<span class="number">52</span>,<span class="number">40</span>)</div><div class="line">&gt; gender &lt;- c(<span class="string">"male"</span>,<span class="string">"male"</span>,<span class="string">"female"</span>,<span class="string">"female"</span>,<span class="string">"male"</span>,<span class="string">"female"</span>,<span class="string">"male"</span>)</div><div class="line">&gt; </div><div class="line">&gt; input_data &lt;- data.frame(height,weight,gender)</div><div class="line">&gt; print(input_data)</div><div class="line">  height weight gender</div><div class="line"><span class="number">1</span>    <span class="number">132</span>     <span class="number">48</span>   male</div><div class="line"><span class="number">2</span>    <span class="number">151</span>     <span class="number">49</span>   male</div><div class="line"><span class="number">3</span>    <span class="number">162</span>     <span class="number">66</span> female</div><div class="line"><span class="number">4</span>    <span class="number">139</span>     <span class="number">53</span> female</div><div class="line"><span class="number">5</span>    <span class="number">166</span>     <span class="number">67</span>   male</div><div class="line"><span class="number">6</span>    <span class="number">147</span>     <span class="number">52</span> female</div><div class="line"><span class="number">7</span>    <span class="number">122</span>     <span class="number">40</span>   male</div><div class="line">&gt; print(input_data$height)</div><div class="line">[<span class="number">1</span>] <span class="number">132</span> <span class="number">151</span> <span class="number">162</span> <span class="number">139</span> <span class="number">166</span> <span class="number">147</span> <span class="number">122</span></div></pre></td></tr></table></figure><h2 id="15、数据帧"><a href="#15、数据帧" class="headerlink" title="15、数据帧"></a>15、数据帧</h2><ul><li><code>summary()</code></li></ul><figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line">&gt; emp.data &lt;- data.frame(</div><div class="line">+     emp_id = c (<span class="number">1</span>:<span class="number">5</span>), </div><div class="line">+     emp_name = c(<span class="string">"Rick"</span>,<span class="string">"Dan"</span>,<span class="string">"Michelle"</span>,<span class="string">"Ryan"</span>,<span class="string">"Gary"</span>),</div><div class="line">+     salary = c(<span class="number">623.3</span>,<span class="number">515.2</span>,<span class="number">611.0</span>,<span class="number">729.0</span>,<span class="number">843.25</span>), </div><div class="line">+     </div><div class="line">+     start_date = as.Date(c(<span class="string">"2012-01-01"</span>, <span class="string">"2013-09-23"</span>, <span class="string">"2014-11-15"</span>, <span class="string">"2014-05-11"</span>,</div><div class="line">+                            <span class="string">"2015-03-27"</span>)),</div><div class="line">+     stringsAsFactors = <span class="literal">FALSE</span></div><div class="line">+ )</div><div class="line">&gt; <span class="comment"># Print the summary.</span></div><div class="line">&gt; print(summary(emp.data)) </div><div class="line">     emp_id    emp_name             salary        start_date        </div><div class="line"> Min.   :<span class="number">1</span>   Length:<span class="number">5</span>           Min.   :<span class="number">515.2</span>   Min.   :<span class="number">2012</span>-<span class="number">01</span>-<span class="number">01</span>  </div><div class="line"> 1st Qu.:<span class="number">2</span>   Class :character   1st Qu.:<span class="number">611.0</span>   1st Qu.:<span class="number">2013</span>-<span class="number">09</span>-<span class="number">23</span>  </div><div class="line"> Median :<span class="number">3</span>   Mode  :character   Median :<span class="number">623.3</span>   Median :<span class="number">2014</span>-<span class="number">05</span>-<span class="number">11</span>  </div><div class="line"> Mean   :<span class="number">3</span>                      Mean   :<span class="number">664.4</span>   Mean   :<span class="number">2014</span>-<span class="number">01</span>-<span class="number">14</span>  </div><div class="line"> 3rd Qu.:<span class="number">4</span>                      3rd Qu.:<span class="number">729.0</span>   3rd Qu.:<span class="number">2014</span>-<span class="number">11</span>-<span class="number">15</span>  </div><div class="line"> Max.   :<span class="number">5</span>                      Max.   :<span class="number">843.2</span>   Max.   :<span class="number">2015</span>-<span class="number">03</span>-<span class="number">27</span>  </div><div class="line">&gt; print(emp.data)</div><div class="line">  emp_id emp_name salary start_date</div><div class="line"><span class="number">1</span>      <span class="number">1</span>     Rick <span class="number">623.30</span> <span class="number">2012</span>-<span class="number">01</span>-<span class="number">01</span></div><div class="line"><span class="number">2</span>      <span class="number">2</span>      Dan <span class="number">515.20</span> <span class="number">2013</span>-<span class="number">09</span>-<span class="number">23</span></div><div class="line"><span class="number">3</span>      <span class="number">3</span> Michelle <span class="number">611.00</span> <span class="number">2014</span>-<span class="number">11</span>-<span class="number">15</span></div><div class="line"><span class="number">4</span>      <span class="number">4</span>     Ryan <span class="number">729.00</span> <span class="number">2014</span>-<span class="number">05</span>-<span class="number">11</span></div><div class="line"><span class="number">5</span>      <span class="number">5</span>     Gary <span class="number">843.25</span> <span class="number">2015</span>-<span class="number">03</span>-<span class="number">27</span></div></pre></td></tr></table></figure><ul><li>取数据前两行<code>result &lt;- emp.data[1:2,]</code></li><li><code>result &lt;- emp.data[c(3,5),c(2,4)]</code></li><li>扩展数据<code>emp.data$dept &lt;- c(&quot;IT&quot;,&quot;Operations&quot;,&quot;IT&quot;,&quot;HR&quot;,&quot;Finance&quot;)</code></li><li>添加行：<code>emp.finaldata &lt;- rbind(emp.data,emp.newdata)</code></li></ul><h1 id="二、dplyr包"><a href="#二、dplyr包" class="headerlink" title="二、dplyr包"></a>二、<code>dplyr</code>包</h1><h2 id="1、基本操作"><a href="#1、基本操作" class="headerlink" title="1、基本操作"></a>1、基本操作</h2><ul><li>安装：<code>install.packages(&quot;dplyr&quot;)</code><h3 id="1-筛选"><a href="#1-筛选" class="headerlink" title="(1) 筛选"></a>(1) 筛选</h3></li><li><p>筛选行：<code>filter()</code></p><ul><li>前面是<code>dataframe</code>数据，后面是筛选的条件，这里是筛选列名为<code>eval_set</code>的数据</li><li>也可以使用<strong>管道</strong>的方式：datafame %&gt;% filter(eval_set == “prior”)<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">filter(dataframe, eval_set == <span class="string">"prior"</span>)</div></pre></td></tr></table></figure></li></ul></li><li><p>筛选列：<code>select()</code></p><ul><li>筛选列名为<code>user_id, product_id</code>的列，使用<strong>负号</strong>表示<strong>去除</strong>对应的列</li><li>可以使用<strong>管道</strong>的方式：<code>ordert %&gt;% select(user_id, product_id, reordered)</code><figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">select(ordert, user_id, product_id)</div></pre></td></tr></table></figure></li></ul></li></ul><h3 id="2-排序arrange"><a href="#2-排序arrange" class="headerlink" title="(2) 排序arrange()"></a>(2) 排序<code>arrange()</code></h3><ul><li><p>按给定的列名依次进行排序（默认升序）</p><ul><li><strong>管道</strong>的方式：<code>orders_products %&gt;% arrange(user_id, order_number, product_id)</code><figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">arrange(df, user_id, order_number, product_id)</div></pre></td></tr></table></figure></li></ul></li><li><p>降序排列</p></li></ul><figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">arrange(df, desc(ArrDelay))</div></pre></td></tr></table></figure><h3 id="3-变形mutate"><a href="#3-变形mutate" class="headerlink" title="(3) 变形mutate()"></a>(3) 变形<code>mutate()</code></h3><ul><li>对已有列进行数据运算并添加为新列</li></ul><figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">mutate(hflights_df, </div><div class="line">  gain = ArrDelay - DepDelay, </div><div class="line">  speed = Distance / AirTime * <span class="number">60</span>)</div></pre></td></tr></table></figure><h3 id="4-汇总"><a href="#4-汇总" class="headerlink" title="(4) 汇总"></a>(4) 汇总</h3><figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">prd &lt;- orders_products %&gt;%</div><div class="line">    arrange(user_id, order_number, product_id) %&gt;%</div><div class="line">    group_by(user_id, product_id) %&gt;%</div><div class="line">    mutate(product_time = row_number()) %&gt;%</div><div class="line">    ungroup() %&gt;%</div><div class="line">    group_by(product_id) %&gt;%</div><div class="line">    summarise(</div><div class="line">        prod_orders = n(),</div><div class="line">        prod_reorders = sum(reordered),</div><div class="line">        prod_first_orders = sum(product_time == <span class="number">1</span>),</div><div class="line">        prod_second_orders = sum(product_time == <span class="number">2</span>)</div><div class="line">    )</div></pre></td></tr></table></figure><h2 id="2、分组"><a href="#2、分组" class="headerlink" title="2、分组"></a>2、分组</h2><ul><li>当对数据集通过 <code>group_by()</code> 添加了分组信息后,<code>mutate(), arrange() 和 summarise()</code> 函数会自动对这些 <code>tbl</code> 类数据执行分组操作 (R语言泛型函数的优势).</li></ul><figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">prd &lt;- orders_products %&gt;%</div><div class="line">    arrange(user_id, order_number, product_id) %&gt;%</div><div class="line">    group_by(user_id, product_id) %&gt;%</div><div class="line">    mutate(product_time = row_number()) %&gt;%</div><div class="line">    ungroup() %&gt;%</div><div class="line">    group_by(product_id) %&gt;%</div><div class="line">    summarise(</div><div class="line">        prod_orders = n(),</div><div class="line">        prod_reorders = sum(reordered),</div><div class="line">        prod_first_orders = sum(product_time == <span class="number">1</span>),</div><div class="line">        prod_second_orders = sum(product_time == <span class="number">2</span>)</div><div class="line">    )</div></pre></td></tr></table></figure><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul><li><a href="https://w3cschool.cn/r/" target="_blank" rel="external">https://w3cschool.cn/r/</a></li><li><a href="https://cran.rstudio.com/web/packages/dplyr/dplyr.pdf" target="_blank" rel="external">https://cran.rstudio.com/web/packages/dplyr/dplyr.pdf</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;ul&gt;
&lt;li&gt;基础内容来自&lt;code&gt;W3C&lt;/code&gt;，直接&lt;a href=&quot;https://w3cschool.cn/r/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;查看这里&lt;/a&gt;即可，这里只是个人学习的记录&lt;/li&gt;
&lt;li&gt;只是最近感觉有必要学习一下&lt;code&gt;R&lt;/code&gt;，哈哈&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&quot;一、基础&quot;&gt;&lt;a href=&quot;#一、基础&quot; class=&quot;headerlink&quot; title=&quot;一、基础&quot;&gt;&lt;/a&gt;一、基础&lt;/h1&gt;&lt;h2 id=&quot;1、概述&quot;&gt;&lt;a href=&quot;#1、概述&quot; class=&quot;headerlink&quot; title=&quot;1、概述&quot;&gt;&lt;/a&gt;1、概述&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;R语言是用于统计分析，图形表示和报告的编程语言和软件环境。&lt;/li&gt;
&lt;li&gt;解释型语言&lt;h2 id=&quot;2、Windows上安装&quot;&gt;&lt;a href=&quot;#2、Windows上安装&quot; class=&quot;headerlink&quot; title=&quot;2、Windows上安装&quot;&gt;&lt;/a&gt;2、&lt;code&gt;Windows&lt;/code&gt;上安装&lt;/h2&gt;&lt;/li&gt;
&lt;li&gt;下载地址：&lt;a href=&quot;https://cran.r-project.org/bin/windows/base/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;R-3.4.0&lt;/a&gt;，直接下载安装即可&lt;/li&gt;
&lt;li&gt;&lt;code&gt;IDE&lt;/code&gt;使用&lt;code&gt;RStudio&lt;/code&gt;: &lt;a href=&quot;https://www.rstudio.com/products/rstudio/download/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;点击下载&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="R" scheme="http://lawlite.me/tags/R/"/>
    
  </entry>
  
  <entry>
    <title>Tensorflow学习-工具相关</title>
    <link href="http://lawlite.me/2017/06/24/Tensorflow%E5%AD%A6%E4%B9%A0-%E5%B7%A5%E5%85%B7%E7%9B%B8%E5%85%B3/"/>
    <id>http://lawlite.me/2017/06/24/Tensorflow学习-工具相关/</id>
    <published>2017-06-24T11:30:05.000Z</published>
    <updated>2017-06-27T12:35:59.407Z</updated>
    
    <content type="html"><![CDATA[<ul><li><code>Tensorflow</code>版本(<code># 2017-06-24</code>)：<code>1.2.0</code>  </li><li><code>Python</code>版本：<code>3.5.3</code></li><li>包括：<ul><li><code>Tensorboard</code><strong>可视化</strong></li><li><code>tfdbg</code><strong>调试</strong></li><li>常用的<strong>高级函数</strong></li></ul></li></ul><a id="more"></a><h1 id="一、TensorBoard-可视化"><a href="#一、TensorBoard-可视化" class="headerlink" title="一、TensorBoard 可视化"></a>一、<code>TensorBoard</code> 可视化</h1><h2 id="1、可视化计算图"><a href="#1、可视化计算图" class="headerlink" title="1、可视化计算图"></a>1、可视化计算图</h2><ul><li>全部代码：<a href="https://github.com/lawlite19/Blog-Back-Up/blob/master/code/tensorflow-tools/tensorflow_graph.py" target="_blank" rel="external">点击查看</a></li><li>数据集使用<code>MNIST</code>手写数字</li><li>加载数据</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="string">'''加载数据'''</span></div><div class="line">data = input_data.read_data_sets(<span class="string">'MNIST_data'</span>, one_hot=<span class="keyword">True</span>)</div><div class="line">print(<span class="string">"Size of:"</span>)</div><div class="line">print(<span class="string">"\t\t training set:\t\t&#123;&#125;"</span>.format(len(data.train.labels)))</div><div class="line">print(<span class="string">"\t\t test set: \t\t\t&#123;&#125;"</span>.format(len(data.test.labels)))</div><div class="line">print(<span class="string">"\t\t validation set:\t&#123;&#125;"</span>.format(len(data.validation.labels)))</div></pre></td></tr></table></figure><h3 id="1-全连接网络"><a href="#1-全连接网络" class="headerlink" title="(1) 全连接网络"></a>(1) 全连接网络</h3><ul><li>超参数</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="string">'''超参数'''</span></div><div class="line">img_size = <span class="number">28</span></div><div class="line">img_flatten_size = img_size ** <span class="number">2</span></div><div class="line">img_shape = (img_size, img_size)</div><div class="line">num_classes = <span class="number">10</span></div><div class="line">learning_rate = <span class="number">1e-4</span></div></pre></td></tr></table></figure><ul><li><p>定义添加一层的函数</p><ul><li><code>num_layer</code>指定是第几层</li><li><code>activation</code>指定激励函数，若不指定跳过<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="string">'''定义添加一层'''</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">add_fully_layer</span><span class="params">(inputs, input_size, output_size, num_layer, activation=None)</span>:</span></div><div class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">'layer_'</span>+num_layer):</div><div class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">'Weights'</span>):</div><div class="line">            W = tf.Variable(initial_value=tf.random_normal(shape=[input_size, output_size]), name=<span class="string">'W'</span>)</div><div class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">'biases'</span>):</div><div class="line">            b = tf.Variable(initial_value=tf.zeros(shape=[<span class="number">1</span>, output_size]) + <span class="number">0.1</span>, name=<span class="string">'b'</span>)</div><div class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">'Wx_plus_b'</span>):</div><div class="line">            Wx_plus_b = tf.matmul(inputs, W) + b</div><div class="line">        <span class="keyword">if</span> activation <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</div><div class="line">            outputs = activation(Wx_plus_b)</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            outputs = Wx_plus_b</div><div class="line">        <span class="keyword">return</span> outputs</div></pre></td></tr></table></figure></li></ul></li><li><p>定义输入，计算图结构，loss和优化器</p></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="string">'''placehoder'''</span></div><div class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'inputs'</span>):</div><div class="line">    x = tf.placeholder(tf.float32, shape=[<span class="keyword">None</span>, img_flatten_size], name=<span class="string">'x'</span>)</div><div class="line">    y = tf.placeholder(tf.float32, shape=[<span class="keyword">None</span>, num_classes], name=<span class="string">'y'</span>)</div><div class="line"></div><div class="line"><span class="string">'''结构'''</span></div><div class="line">hidden_layer1 = add_fully_layer(x, img_flatten_size, <span class="number">20</span>, <span class="string">'1'</span>, activation=tf.nn.relu)</div><div class="line">logits = add_fully_layer(hidden_layer1, <span class="number">20</span>, num_classes, <span class="string">'2'</span>)</div><div class="line">predictions = tf.nn.softmax(logits)</div><div class="line"></div><div class="line"><span class="string">'''loss'''</span></div><div class="line"><span class="comment">#cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=y,logits=logits)</span></div><div class="line">cross_entropy = -tf.reduce_sum(y*tf.log(predictions), reduction_indices=[<span class="number">1</span>])</div><div class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'losses'</span>):</div><div class="line">    losses = tf.reduce_mean(cross_entropy)</div><div class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'train'</span>):</div><div class="line">    train_step = tf.train.AdamOptimizer(learning_rate).minimize(losses)</div></pre></td></tr></table></figure><ul><li>定义<code>Session</code>和<code>tf.summary.FileWriter</code></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="string">'''session'''</span></div><div class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</div><div class="line">    sess.run(tf.global_variables_initializer())</div><div class="line">    writer = tf.summary.FileWriter(<span class="string">'logs'</span>, sess.graph)  <span class="comment"># 将计算图写入文件</span></div></pre></td></tr></table></figure><ul><li><p>最后在<code>logs</code>的上级目录打开命令行输入：<code>tensorboard --logdir=logs/</code>，浏览器中输入网址：<code>http://localhost:6006</code>即可查看</p></li><li><p>结果</p><ul><li>自定义的<code>cross_entropy = -tf.reduce_sum(y*tf.log(predictions), reduction_indices=[1])</code><br><img src="/assets/blog_images/Tensorflow-tool/01_fully_connected_graph.png" alt="全连接网络结构，cross_entropy自定义" title="01_fully_connected_graph"></li><li>使用<code>tensorflow</code>中自带的<code>cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=y,logits=predictions)</code><br><img src="/assets/blog_images/Tensorflow-tool/02_fully_connected_graph.png" alt="全连接网络结构，自带的cross_entropy" title="02_fully_connected_graph"></li></ul></li><li>可以看出<code>tf.name_scope</code>定义的名字就是其中的方框，点击里面的可以查看里面对应的内容</li></ul><h3 id="2-CNN卷积神经网络"><a href="#2-CNN卷积神经网络" class="headerlink" title="(2) CNN卷积神经网络"></a>(2) <code>CNN</code>卷积神经网络</h3><ul><li><p>添加一层卷积层和<code>pooling</code>层</p><ul><li>这里默认<code>pooling</code>使用<code>maxpooling</code>, 大小为<code>2</code><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="string">'''CNN 定义添加一层卷积层，包括pooling(使用maxpooling, size=2)'''</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">add_conv_layer</span><span class="params">(inputs, filter_size, input_channels, output_channels, num_layer, activation=tf.nn.relu)</span>:</span></div><div class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">'conv_layer_'</span>+num_layer):</div><div class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">'Weights'</span>):</div><div class="line">            Weights = tf.Variable(tf.truncated_normal(stddev=<span class="number">0.1</span>, shape=[filter_size, filter_size, input_channels, output_channels]), name=<span class="string">'W'</span>)</div><div class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">'biases'</span>):</div><div class="line">            b = tf.Variable(tf.constant(<span class="number">0.1</span>, shape=[output_channels]))</div><div class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">'conv2d'</span>):</div><div class="line">            conv2d_plus_b = tf.nn.conv2d(inputs, Weights, strides=[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>], padding=<span class="string">'SAME'</span>, name=<span class="string">'conv'</span>) + b</div><div class="line">            activation_conv_outputs = activation(conv2d_plus_b)</div><div class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">'max_pool'</span>):</div><div class="line">            max_pool_outputs = tf.nn.max_pool(activation_conv_outputs, ksize=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>], strides=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>], padding=<span class="string">'SAME'</span>)</div><div class="line">        <span class="keyword">return</span> max_pool_outputs</div></pre></td></tr></table></figure></li></ul></li><li><p>将卷积层展开</p><ul><li>返回展开层和数量（因为全连接会用到）<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="string">'''将卷积层展开'''</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">flatten_layer</span><span class="params">(layer)</span>:</span></div><div class="line">    <span class="string">'''</span></div><div class="line">    @param layer: the conv layer</div><div class="line">    '''</div><div class="line">    layer_shape = layer.get_shape() <span class="comment"># 获取形状(layer_shape == [num_images, img_height, img_width, num_channels])</span></div><div class="line">    num_features = layer_shape[<span class="number">1</span>:<span class="number">4</span>].num_elements()  <span class="comment"># [1:4] 是最后3个维度，就是展开的长度</span></div><div class="line">    layer_flat = tf.reshape(layer, [<span class="number">-1</span>, num_features])   <span class="comment"># 展开</span></div><div class="line">    <span class="keyword">return</span> layer_flat, num_features</div></pre></td></tr></table></figure></li></ul></li><li><p>定义输入</p><ul><li>需要将<code>x</code>转成图片矩阵的形式<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="string">'''placehoder'''</span></div><div class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'inputs'</span>):</div><div class="line">    x = tf.placeholder(tf.float32, shape=[<span class="keyword">None</span>, img_flatten_size], name=<span class="string">'x'</span>)</div><div class="line">    y = tf.placeholder(tf.float32, shape=[<span class="keyword">None</span>, num_classes], name=<span class="string">'y'</span>)</div><div class="line">    x_image = tf.reshape(x, shape = [<span class="number">-1</span>, img_size, img_size, n_channels], name=<span class="string">'x_images'</span>)</div></pre></td></tr></table></figure></li></ul></li><li><p>定义计算图结构</p></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="string">'''CNN卷积网络结构'''</span></div><div class="line">conv_layer1 = add_conv_layer(x_image, filter_size=<span class="number">5</span>, input_channels=<span class="number">1</span>, </div><div class="line">                            output_channels=<span class="number">32</span>, </div><div class="line">                            num_layer=<span class="string">'1'</span>)</div><div class="line">conv_layer2 = add_conv_layer(conv_layer1, filter_size=<span class="number">5</span>, input_channels=<span class="number">32</span>, </div><div class="line">                            output_channels=<span class="number">64</span>, </div><div class="line">                            num_layer=<span class="string">'2'</span>)</div><div class="line"></div><div class="line"><span class="string">'''全连接层'''</span></div><div class="line">conv_layer2_flat, num_features = flatten_layer(conv_layer2)   <span class="comment"># 将最后操作的数据展开</span></div><div class="line">hidden_layer1 = add_fully_layer(conv_layer2_flat, num_features, <span class="number">1000</span>, num_layer=<span class="string">'1'</span>, activation=tf.nn.relu)</div><div class="line">logits = add_fully_layer(hidden_layer1, <span class="number">1000</span>, num_classes, num_layer=<span class="string">'2'</span>)</div><div class="line">predictions = tf.nn.softmax(logits)</div></pre></td></tr></table></figure><ul><li>结果<ul><li><code>CNN</code>总结构<br><img src="/assets/blog_images/Tensorflow-tool/03_conv_graph.png" alt="CNN结构" title="03_conv_graph"></li><li>第一层卷积和<code>pooling</code>内部结构<br><img src="/assets/blog_images/Tensorflow-tool/04_conv_graph.png" alt="卷积的内容结构" title="04_conv_graph"></li></ul></li></ul><h3 id="3-RNN-LSTM循环神经网络"><a href="#3-RNN-LSTM循环神经网络" class="headerlink" title="(3) RNN_LSTM循环神经网络"></a>(3) <code>RNN_LSTM</code>循环神经网络</h3><ul><li><p>声明<code>placeholder</code></p><ul><li>图片中每一行当做当前的输入，共有<code>n_steps=28</code>步遍历完一张图片，所以输入<code>x</code>的<code>shape=(batch_size, n_steps, n_inputs)</code></li><li><code>n_inputs</code>就是<strong>一行</strong>的像素值<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="string">'''placehoder'''</span></div><div class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'inputs'</span>):</div><div class="line">    <span class="string">'''RNN'''</span></div><div class="line">    x = tf.placeholder(tf.float32, shape=[batch_size, n_steps, n_inputs], name=<span class="string">'x'</span>)</div><div class="line">    y = tf.placeholder(tf.float32, shape=[batch_size, num_classes], name=<span class="string">'y'</span>)</div></pre></td></tr></table></figure></li></ul></li><li><p>添加一层<code>cell</code></p><ul><li>我们最后只需要遍历<code>n_steps</code>之后的输出即可（<strong>遍历完一张图然后分类</strong>），所以对应的是<code>final_state[1]</code>（有两个<code>state</code>, 一个是<code>c state</code>,一个是<code>h state</code>， 输出是<code>h state</code>）</li><li><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="string">'''RNN 添加一层cell'''</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">add_RNN_Cell</span><span class="params">(inputs)</span>:</span></div><div class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">'RNN_LSTM_Cell'</span>):</div><div class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">'weights'</span>):</div><div class="line">            weights = tf.Variable(tf.random_normal(shape=[state_size, num_classes]), name=<span class="string">'W'</span>)</div><div class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">'biases'</span>):</div><div class="line">            biases = tf.Variable(tf.constant(<span class="number">0.1</span>, shape=[num_classes,]), name=<span class="string">'b'</span>)</div><div class="line">        cell = tf.nn.rnn_cell.BasicLSTMCell(num_units=state_size)</div><div class="line">        init_state = cell.zero_state(batch_size, dtype=tf.float32)</div><div class="line">        rnn_outputs, final_state = tf.nn.dynamic_rnn(cell=cell, inputs=x, </div><div class="line">                                                     initial_state=init_state)</div><div class="line">        logits = tf.matmul(final_state[<span class="number">1</span>], weights) + biases</div><div class="line">        <span class="keyword">return</span> logits</div></pre></td></tr></table></figure></li></ul></li><li><p>网络结果和<code>loss</code></p></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="string">'''RNN网络结构'''</span></div><div class="line">logits = add_RNN_Cell(inputs=x)</div><div class="line"></div><div class="line">predictions = tf.nn.softmax(logits)    </div><div class="line"><span class="string">'''loss'''</span></div><div class="line"><span class="comment">#cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=y,logits=predictions)</span></div><div class="line">cross_entropy = -tf.reduce_sum(y*tf.log(predictions), reduction_indices=[<span class="number">1</span>])</div><div class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'losses'</span>):</div><div class="line">    losses = tf.reduce_mean(cross_entropy)</div></pre></td></tr></table></figure><ul><li>结果<ul><li>总体结构<br><img src="/assets/blog_images/Tensorflow-tool/05_rnn_graph.png" alt="RNN LSTM结构" title="05_rnn_graph"></li><li><code>RNN</code>内部结构<br><img src="/assets/blog_images/Tensorflow-tool/06_rnn_graph.png" alt="内部结构" title="06_rnn_graph"></li></ul></li></ul><h2 id="2、可视化训练过程"><a href="#2、可视化训练过程" class="headerlink" title="2、可视化训练过程"></a>2、可视化训练过程</h2><ul><li>全部代码：<a href="https://github.com/lawlite19/Blog-Back-Up/blob/master/code/tensorflow-tools/tensorflow_train_process.py" target="_blank" rel="external">点击查看</a><h3 id="1-权重weights，偏置biases，损失值loss"><a href="#1-权重weights，偏置biases，损失值loss" class="headerlink" title="(1) 权重weights，偏置biases，损失值loss"></a>(1) 权重<code>weights</code>，偏置<code>biases</code>，损失值<code>loss</code></h3></li><li><p>加入一层全连接层的函数变成这样</p><ul><li>加入<code>tf.summary.histogram(name=layer_name+&#39;/Weights&#39;, values=W)</code>即可<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line"><span class="string">'''定义添加一层全连接层'''</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">add_fully_layer</span><span class="params">(inputs, input_size, output_size, num_layer, activation=None)</span>:</span></div><div class="line">    layer_name = <span class="string">'layer_'</span> + num_layer</div><div class="line">    <span class="keyword">with</span> tf.name_scope(layer_name):</div><div class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">'Weights'</span>):</div><div class="line">            low = <span class="number">-4</span>*np.sqrt(<span class="number">6.0</span>/(input_size + output_size)) <span class="comment"># use 4 for sigmoid, 1 for tanh activation </span></div><div class="line">            high = <span class="number">4</span>*np.sqrt(<span class="number">6.0</span>/(input_size + output_size))</div><div class="line">            <span class="comment">#'''xavier方法初始化'''</span></div><div class="line">            <span class="comment">##sigmoid</span></div><div class="line">            <span class="comment">#Weights = tf.Variable(tf.random_uniform(shape=[input_size, output_size], minval=low, maxval=high, dtype=tf.float32), name='W')</span></div><div class="line">            <span class="comment">##relu</span></div><div class="line">            W = tf.Variable(initial_value=tf.random_uniform(shape=[input_size, output_size], minval=low, maxval=high, dtype=tf.float32)/<span class="number">2</span>, name=<span class="string">'W'</span>)</div><div class="line">            tf.summary.histogram(name=layer_name+<span class="string">'/Weights'</span>, values=W)  <span class="comment"># summary.histogram</span></div><div class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">'biases'</span>):</div><div class="line">            b = tf.Variable(initial_value=tf.zeros(shape=[<span class="number">1</span>, output_size]) + <span class="number">0.1</span>, name=<span class="string">'b'</span>)</div><div class="line">            tf.summary.histogram(name=layer_name+<span class="string">'/biases'</span>, values=b)    <span class="comment"># summary.histogram</span></div><div class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">'Wx_plus_b'</span>):</div><div class="line">            Wx_plus_b = tf.matmul(inputs, W) + b</div><div class="line">        <span class="keyword">if</span> activation <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</div><div class="line">            outputs = activation(Wx_plus_b)</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            outputs = Wx_plus_b</div><div class="line">        tf.summary.histogram(name=layer_name+<span class="string">'/outputs'</span>, values=outputs)   <span class="comment"># summary.histogram</span></div><div class="line">    <span class="keyword">return</span> outputs</div></pre></td></tr></table></figure></li></ul></li><li><p>损失</p><ul><li>损失因为是个具体的数，所以使用<code>scalar</code> （上面的权重和偏置都是矩阵，向量）<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">tf.summary.scalar(name=<span class="string">'loss_value'</span>, tensor=losses)</div></pre></td></tr></table></figure></li></ul></li><li><p>训练时<code>merge</code></p><ul><li><code>merged = tf.summary.merge_all()</code>合并所有的<code>summary</code></li><li>对于<code>loss</code>, 训练时执行<code>merge</code>, 然后随步数不断加入<ul><li><code>merged_result = sess.run(merged, feed_dict=feed_dict_train)   # 执行merged</code></li><li><code>writer.add_summary(summary=merged_result, global_step=i)</code><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="string">'''训练'''</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">optimize</span><span class="params">(n_epochs)</span>:</span></div><div class="line">    <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</div><div class="line">        merged = tf.summary.merge_all()</div><div class="line">        writer = tf.summary.FileWriter(<span class="string">'logs'</span>, sess.graph)  <span class="comment"># 将计算图写入文件</span></div><div class="line">        sess.run(tf.global_variables_initializer())</div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(n_epochs):</div><div class="line">            batch_x, batch_y = data.train.next_batch(batch_size)</div><div class="line">            feed_dict_train = &#123;x: batch_x, y: batch_y&#125;</div><div class="line">            sess.run(train_step, feed_dict=feed_dict_train)</div><div class="line">            <span class="keyword">if</span> i % <span class="number">20</span> == <span class="number">0</span>:</div><div class="line">                print(<span class="string">"epoch:&#123;0&#125;, accuracy:&#123;1&#125;"</span>.format(i, sess.run(accuracy, feed_dict=feed_dict_train)))</div><div class="line">                merged_result = sess.run(merged, feed_dict=feed_dict_train)   <span class="comment"># 执行merged</span></div><div class="line">                writer.add_summary(summary=merged_result, global_step=i)      <span class="comment"># 加入到writer</span></div><div class="line">optimize(<span class="number">1001</span>)</div></pre></td></tr></table></figure></li></ul></li></ul></li><li><p>结果</p><ul><li><code>loss</code><br><img src="/assets/blog_images/Tensorflow-tool/07_loss.png" alt="loss value" title="07_loss"></li><li>权重和偏置的数据分布<br><img src="/assets/blog_images/Tensorflow-tool/08_weights_biases.png" alt="权重和偏置" title="08_weights_biases"></li></ul></li></ul><h1 id="二、Tensorflow-调试tfdbg"><a href="#二、Tensorflow-调试tfdbg" class="headerlink" title="二、Tensorflow 调试tfdbg"></a>二、<code>Tensorflow</code> 调试<code>tfdbg</code></h1><ul><li>官网教程：<a href="https://www.tensorflow.org/programmers_guide/debugger" target="_blank" rel="external">点击查看</a></li><li><code>Youtube</code>视频简短教程：<a href="https://www.youtube.com/watch?v=CA7fjRfduOI&amp;t=29s" target="_blank" rel="external">点击查看</a></li><li><code>Tensorflow Debugger (tfdbg)</code>是专业的调试工具，可以查看计算图中内容部的数据等<h2 id="1、本地调试"><a href="#1、本地调试" class="headerlink" title="1、本地调试"></a>1、本地调试</h2></li></ul><h3 id="1-加入代码"><a href="#1-加入代码" class="headerlink" title="(1) 加入代码"></a>(1) 加入代码</h3><ul><li>导入调试的包：<code>from tensorflow.python import debug as tf_debug</code></li><li><code>Wrapper Session</code>和添加<code>filter</code>: <ul><li><code>filter</code>也可以自己定义<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</div><div class="line">    sess = tf_debug.LocalCLIDebugWrapperSession(sess)</div><div class="line">    sess.add_tensor_filter(filter_name=<span class="string">'inf or nan'</span>, tensor_filter=tf_debug.has_inf_or_nan)</div></pre></td></tr></table></figure></li></ul></li></ul><h3 id="2-运行"><a href="#2-运行" class="headerlink" title="(2) 运行"></a>(2) 运行</h3><ul><li>命令行中执行：<code>python xxx.py --debug</code>即可进入调试<ul><li>支持鼠标点击的可以直接点击查看变量的信息</li></ul></li><li><code>run</code>或者<code>r</code>可以查看所有的<code>tensor</code>的名字等信息<ul><li>第一次<code>run</code>还没有初始化变量，<code>pt tenser_name</code>打印<code>tensor</code>的信息<br><img src="/assets/blog_images/Tensorflow-tool/09_debug.png" alt="第一次run" title="09_debug"></li></ul></li><li>再执行一次就是初始化变量<br><img src="/assets/blog_images/Tensorflow-tool/10_debug.png" alt="再次执行run" title="10_debug"><ul><li>可以进行<code>slice</code><br><img src="/assets/blog_images/Tensorflow-tool/12_debug.png" alt="slice" title="12_debug"></li></ul></li><li><a href="https://www.tensorflow.org/programmers_guide/debugger#tfdbg_cli_frequently-used_commands" target="_blank" rel="external">更多命令行</a><br><img src="/assets/blog_images/Tensorflow-tool/11_debug.png" alt="命令行" title="11_debug"></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;ul&gt;
&lt;li&gt;&lt;code&gt;Tensorflow&lt;/code&gt;版本(&lt;code&gt;# 2017-06-24&lt;/code&gt;)：&lt;code&gt;1.2.0&lt;/code&gt;  &lt;/li&gt;
&lt;li&gt;&lt;code&gt;Python&lt;/code&gt;版本：&lt;code&gt;3.5.3&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;包括：&lt;ul&gt;
&lt;li&gt;&lt;code&gt;Tensorboard&lt;/code&gt;&lt;strong&gt;可视化&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;tfdbg&lt;/code&gt;&lt;strong&gt;调试&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;常用的&lt;strong&gt;高级函数&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="Tensorflow" scheme="http://lawlite.me/tags/Tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>（Unfinished）RNN-循环神经网络之LSTM和GRU-04介绍及推导</title>
    <link href="http://lawlite.me/2017/06/21/RNN-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B9%8BLSTM%E5%92%8CGRU-04%E4%BB%8B%E7%BB%8D%E5%8F%8A%E6%8E%A8%E5%AF%BC/"/>
    <id>http://lawlite.me/2017/06/21/RNN-循环神经网络之LSTM和GRU-04介绍及推导/</id>
    <published>2017-06-21T14:45:44.000Z</published>
    <updated>2017-06-25T08:49:42.562Z</updated>
    
    <content type="html"><![CDATA[<h1 id="（Unfinished）尚未完成"><a href="#（Unfinished）尚未完成" class="headerlink" title="（Unfinished）尚未完成"></a>（Unfinished）尚未完成</h1><h1 id="一、说明"><a href="#一、说明" class="headerlink" title="一、说明"></a>一、说明</h1><ul><li>关于<code>LSTM</code>的<code>cell</code>结构和一些计算在之前已经介绍了，可以<a href="http://lawlite.me/2017/06/14/RNN-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8CLSTM-01%E5%9F%BA%E7%A1%80/#%E5%9B%9B%E3%80%81Long-Short-Term-Memory-LSTM%EF%BC%8C%E9%95%BF%E7%9F%AD%E6%97%B6%E8%AE%B0%E5%BF%86%E7%BD%91%E7%BB%9C">点击这里查看</a></li><li>本篇博客主要涉及一下内容：<ul><li><code>LSTM</code>前向计算说明(之前的博客中<a href="http://lawlite.me/2017/06/14/RNN-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8CLSTM-01%E5%9F%BA%E7%A1%80/#%E5%9B%9B%E3%80%81Long-Short-Term-Memory-LSTM%EF%BC%8C%E9%95%BF%E7%9F%AD%E6%97%B6%E8%AE%B0%E5%BF%86%E7%BD%91%E7%BB%9C">LSTM部分</a>实际已经提到过，这里结合图更详细说明)</li></ul></li></ul><a id="more"></a><h1 id="二、LSTM前向计算step-by-step"><a href="#二、LSTM前向计算step-by-step" class="headerlink" title="二、LSTM前向计算step by step"></a>二、<code>LSTM</code>前向计算<code>step by step</code></h1><h2 id="1、结构review"><a href="#1、结构review" class="headerlink" title="1、结构review"></a>1、结构<code>review</code></h2><ul><li>我们知道<code>RNN</code>的结构如下图<ul><li>注意<code>cell</code>中的<strong>神经元可以有多个</strong><br><img src="/assets/blog_images/RNN/RNN_LSTM_19.png" alt="RNN 结构 -w150" title="RNN_LSTM_19"></li></ul></li><li><code>LSTM</code>就是对<code>cell</code>结构的改进<br><img src="/assets/blog_images/RNN/RNN_LSTM_20.png" alt="LSTM结构" title="RNN_LSTM_20"></li><li>符号说明<br><img src="/assets/blog_images/RNN/RNN_LSTM_21.png" alt="符号说明" title="RNN_LSTM_21"></li><li><code>LSTM</code>的关键就是<code>state</code>,就是对应上面的<strong>主线数据</strong>的传递<br><img src="/assets/blog_images/RNN/RNN_LSTM_22.png" alt="LSTM state传递" title="RNN_LSTM_22"></li></ul><h2 id="2、前向计算step-by-step"><a href="#2、前向计算step-by-step" class="headerlink" title="2、前向计算step by step"></a>2、前向计算<code>step by step</code></h2><h3 id="1-决定抛弃的信息"><a href="#1-决定抛弃的信息" class="headerlink" title="(1) 决定抛弃的信息"></a>(1) 决定抛弃的信息</h3><ul><li>遗忘门 (<code>forget gate layer</code>)</li><li>$\sigma$是<code>Sigmoid</code>激励函数，因为它的值域是<code>(0,1)</code>，<code>0</code>代表遗忘所有信息，<code>1</code>代表保留所有信息</li></ul><p><img src="/assets/blog_images/RNN/RNN_LSTM_23.png" alt="遗忘门 forget gate layer" title="RNN_LSTM_23"></p><h3 id="2-决定存储的新信息"><a href="#2-决定存储的新信息" class="headerlink" title="(2) 决定存储的新信息"></a>(2) 决定存储的新信息</h3><ul><li>包括两个部分<ul><li>第一个是输入门 (<code>input gate layer</code>)，对应的是<code>Sigmoid</code>函数</li><li>第二个是经过<code>tanh</code>激励函数</li></ul></li></ul><p><img src="/assets/blog_images/RNN/RNN_LSTM_24.png" alt="决定存储的新信息" title="RNN_LSTM_24"></p><h3 id="3-更新state-C-t-1-成-C-t"><a href="#3-更新state-C-t-1-成-C-t" class="headerlink" title="(3) 更新state$C_{t-1}$成$C_t$"></a>(3) 更新<code>state</code>$C_{t-1}$成$C_t$</h3><ul><li>$f_t$是经过<code>Sigmoid</code>函数的，所以值域在<code>(0,1)</code>之间，$C_{t-1}$点乘<code>0-1</code>之间的数实际就是对$C_{t-1}$的一种<strong>缩放</strong>，（可以认为是记住之前信息的程度）</li><li>然后加入进来的新的信息<br><img src="/assets/blog_images/RNN/RNN_LSTM_25.png" alt="更新新的state" title="RNN_LSTM_25"></li></ul><h3 id="4-最后计算输出"><a href="#4-最后计算输出" class="headerlink" title="(4) 最后计算输出"></a>(4) 最后计算输出</h3><ul><li>输出门(<code>output gate layer</code>)</li></ul><p><img src="/assets/blog_images/RNN/RNN_LSTM_26.png" alt="计算输出" title="RNN_LSTM_26"></p><ul><li>最后再放一下之前的图, 数据流向可能更清晰</li></ul><p><img src="/assets/blog_images/RNN/RNN_LSTM_08.png" alt="LSTM cell" title="RNN_LSTM_08"></p><h1 id="三、GRU-Gated-Recurrent-Unit"><a href="#三、GRU-Gated-Recurrent-Unit" class="headerlink" title="三、GRU (Gated Recurrent Unit)"></a>三、GRU (Gated Recurrent Unit)</h1><h2 id="1、结构和前向计算"><a href="#1、结构和前向计算" class="headerlink" title="1、结构和前向计算"></a>1、结构和前向计算</h2><ul><li>如下图所示<ul><li>相比<code>LSTM</code>，<code>GRU</code>结合了遗忘门和输入门</li><li>同样也合并了<code>cell state</code>和<code>hidden state</code> （就是<code>LSTM</code>中的<code>c</code>和<code>h</code>）</li><li><code>GRU</code>比<code>LSTM</code>更加简单</li></ul></li></ul><p><img src="./images/RNN_GRU_27.png" alt="GRU cell结构" title="RNN_GRU_27"></p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul><li><a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank" rel="external">https://colah.github.io/posts/2015-08-Understanding-LSTMs/</a></li><li><a href="https://r2rt.com/written-memories-understanding-deriving-and-extending-the-lstm.html#dealing-with-vanishing-and-exploding-gradients" target="_blank" rel="external">https://r2rt.com/written-memories-understanding-deriving-and-extending-the-lstm.html#dealing-with-vanishing-and-exploding-gradients</a></li><li><a href="http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf" target="_blank" rel="external">http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf</a></li><li><a href="http://lawlite.me/2016/12/20/%E8%AE%BA%E6%96%87%E8%AE%B0%E5%BD%95-UnderstandingTheDifficultyOfTrainingDeepFeedforwardNeuralNetworks/">http://lawlite.me/2016/12/20/%E8%AE%BA%E6%96%87%E8%AE%B0%E5%BD%95-UnderstandingTheDifficultyOfTrainingDeepFeedforwardNeuralNetworks/</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;（Unfinished）尚未完成&quot;&gt;&lt;a href=&quot;#（Unfinished）尚未完成&quot; class=&quot;headerlink&quot; title=&quot;（Unfinished）尚未完成&quot;&gt;&lt;/a&gt;（Unfinished）尚未完成&lt;/h1&gt;&lt;h1 id=&quot;一、说明&quot;&gt;&lt;a href=&quot;#一、说明&quot; class=&quot;headerlink&quot; title=&quot;一、说明&quot;&gt;&lt;/a&gt;一、说明&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;关于&lt;code&gt;LSTM&lt;/code&gt;的&lt;code&gt;cell&lt;/code&gt;结构和一些计算在之前已经介绍了，可以&lt;a href=&quot;http://lawlite.me/2017/06/14/RNN-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8CLSTM-01%E5%9F%BA%E7%A1%80/#%E5%9B%9B%E3%80%81Long-Short-Term-Memory-LSTM%EF%BC%8C%E9%95%BF%E7%9F%AD%E6%97%B6%E8%AE%B0%E5%BF%86%E7%BD%91%E7%BB%9C&quot;&gt;点击这里查看&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;本篇博客主要涉及一下内容：&lt;ul&gt;
&lt;li&gt;&lt;code&gt;LSTM&lt;/code&gt;前向计算说明(之前的博客中&lt;a href=&quot;http://lawlite.me/2017/06/14/RNN-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8CLSTM-01%E5%9F%BA%E7%A1%80/#%E5%9B%9B%E3%80%81Long-Short-Term-Memory-LSTM%EF%BC%8C%E9%95%BF%E7%9F%AD%E6%97%B6%E8%AE%B0%E5%BF%86%E7%BD%91%E7%BB%9C&quot;&gt;LSTM部分&lt;/a&gt;实际已经提到过，这里结合图更详细说明)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="Python" scheme="http://lawlite.me/tags/Python/"/>
    
      <category term="DeepLearning" scheme="http://lawlite.me/tags/DeepLearning/"/>
    
      <category term="RNN" scheme="http://lawlite.me/tags/RNN/"/>
    
      <category term="LSTM" scheme="http://lawlite.me/tags/LSTM/"/>
    
      <category term="GRU" scheme="http://lawlite.me/tags/GRU/"/>
    
  </entry>
  
  <entry>
    <title>RNN-LSTM循环神经网络-03Tensorflow进阶实现</title>
    <link href="http://lawlite.me/2017/06/21/RNN-LSTM%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-03Tensorflow%E8%BF%9B%E9%98%B6%E5%AE%9E%E7%8E%B0/"/>
    <id>http://lawlite.me/2017/06/21/RNN-LSTM循环神经网络-03Tensorflow进阶实现/</id>
    <published>2017-06-21T08:54:28.000Z</published>
    <updated>2017-06-25T08:49:18.594Z</updated>
    
    <content type="html"><![CDATA[<ul><li>全部代码：<a href="https://github.com/lawlite19/Blog-Back-Up/tree/master/code/rnn/rnn_tensorflow" target="_blank" rel="external">点击这里查看</a></li><li>关于<code>Tensorflow</code>实现一个简单的<strong>二元序列的例子</strong>可以<a href="http://lawlite.me/2017/06/16/RNN-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-02Tensorflow%E4%B8%AD%E7%9A%84%E5%AE%9E%E7%8E%B0/">点击这里查看</a></li><li>关于<code>RNN</code>和<code>LSTM</code>的基础可以<a href="http://lawlite.me/2017/06/14/RNN-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8CLSTM-01%E5%9F%BA%E7%A1%80/">查看这里</a></li><li>这篇博客主要包含以下内容<ul><li>训练一个<code>RNN</code>模型<strong>逐字符生成文本数据</strong>(最后的部分)</li><li>使用<code>Tensorflow</code>的<code>scan</code>函数实现<code>dynamic_rnn</code><strong>动态创建</strong>的效果</li><li>使用<code>multiple RNN</code>创建<strong>多层</strong>的<code>RNN</code></li><li>实现<code>Dropout</code>和<code>Layer Normalization</code>的功能</li></ul></li></ul><a id="more"></a><h1 id="一、模型说明和数据处理"><a href="#一、模型说明和数据处理" class="headerlink" title="一、模型说明和数据处理"></a>一、模型说明和数据处理</h1><h2 id="1、模型说明"><a href="#1、模型说明" class="headerlink" title="1、模型说明"></a>1、模型说明</h2><ul><li>我们要使用<code>RNN</code>学习一个语言模型(<code>language model</code>)去生成字符序列</li><li><code>githbub</code>上有别人实现好的<ul><li><code>Torch</code>中的实现：<a href="https://github.com/karpathy/char-rnn" target="_blank" rel="external">https://github.com/karpathy/char-rnn</a></li><li><code>Tensorflow</code>中的实现：<a href="https://github.com/sherjilozair/char-rnn-tensorflow" target="_blank" rel="external">https://github.com/sherjilozair/char-rnn-tensorflow</a></li></ul></li><li>接下来我们来看如何实现<h2 id="2、数据处理"><a href="#2、数据处理" class="headerlink" title="2、数据处理"></a>2、数据处理</h2></li><li>数据集使用<em>莎士比亚</em>的一段文集，<a href="https://raw.githubusercontent.com/jcjohnson/torch-rnn/master/data/tiny-shakespeare.txt" target="_blank" rel="external">点击这里查看</a>, 实际也可以使用别的</li><li><strong>大小写字符</strong>视为<strong>不同</strong>的字符</li><li>下载并读取数据</li></ul><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="string">''</span><span class="string">'下载数据并读取数据'</span><span class="string">''</span></div><div class="line">file_url = <span class="string">'https://raw.githubusercontent.com/jcjohnson/torch-rnn/master/data/tiny-shakespeare.txt'</span></div><div class="line">file_name = <span class="string">'tinyshakespeare.txt'</span></div><div class="line"><span class="keyword">if</span> not os<span class="selector-class">.path</span><span class="selector-class">.exists</span>(file_name):</div><div class="line">    urllib<span class="selector-class">.request</span><span class="selector-class">.urlretrieve</span>(file_url, filename=file_name)</div><div class="line">with open(file_name, <span class="string">'r'</span>) as f:</div><div class="line">    raw_data = f.read()</div><div class="line">    print(<span class="string">"数据长度"</span>, len(raw_data))</div></pre></td></tr></table></figure><ul><li><p>处理字符数据，转换为数字</p><ul><li>使用<code>set</code>去重，得到所有的唯一字符</li><li>然后一个字符对应一个数字（使用字典）</li><li>然后遍历原始数据，得到所有字符对应的数字<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">'''处理字符数据，转换为数字'''</div><div class="line">vocab = set(raw_data)                    # 使用set去重，这里就是去除重复的字母(大小写是区分的)</div><div class="line">vocab_size = len(vocab)      </div><div class="line">idx_to_vocab = dict(enumerate(vocab))    # 这里将set转为了字典，每个字符对应了一个数字0,1,2,3..........(vocab_size-1)</div><div class="line">vocab_to_idx = dict(zip(idx_to_vocab.values(), idx_to_vocab.keys())) # 这里将字典的(key, value)转换成(value, key)</div><div class="line"></div><div class="line">data = [vocab_to_idx[c] for c in raw_data]   # 处理raw_data, 根据字符，得到对应的value,就是数字</div><div class="line">del raw_data</div></pre></td></tr></table></figure></li></ul></li><li><p>生成<code>batch</code>数据</p><ul><li><code>Tensorflow models</code>给出的<strong>PTB模型</strong>：<a href="https://github.com/tensorflow/models/tree/master/tutorials/rnn/ptb" target="_blank" rel="external">https://github.com/tensorflow/models/tree/master/tutorials/rnn/ptb</a></li></ul></li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">'''超参数'''</div><div class="line">num_steps=200             # 学习的步数</div><div class="line">batch_size=32</div><div class="line">state_size=100            # cell的size</div><div class="line">num_classes = vocab_size</div><div class="line">learning_rate = 1e-4</div><div class="line"></div><div class="line">def gen_epochs(num_epochs, num_steps, batch_size):</div><div class="line">    for i in range(num_epochs):</div><div class="line">        yield reader.ptb_iterator_oldversion(data, batch_size, num_steps)</div></pre></td></tr></table></figure><ul><li><ul><li>ptb_iterator函数实现：<ul><li>返回数据<code>X,Y</code>的<code>shape=[batch_size, num_steps]</code><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div></pre></td><td class="code"><pre><div class="line">def ptb_iterator_oldversion(raw_data, batch_size, num_steps):</div><div class="line">  """Iterate on the raw PTB data.</div><div class="line">  This generates batch_size pointers into the raw PTB data, and allows</div><div class="line">  minibatch iteration along these pointers.</div><div class="line">  Args:</div><div class="line">  raw_data: one of the raw data outputs from ptb_raw_data.</div><div class="line">  batch_size: int, the batch size.</div><div class="line">  num_steps: int, the number of unrolls.</div><div class="line">  Yields:</div><div class="line">  Pairs of the batched data, each a matrix of shape [batch_size, num_steps].</div><div class="line">  The second element of the tuple is the same data time-shifted to the</div><div class="line">  right by one.</div><div class="line">  Raises:</div><div class="line">  ValueError: if batch_size or num_steps are too high.</div><div class="line">  """</div><div class="line">  raw_data = np.array(raw_data, dtype=np.int32)</div><div class="line">  </div><div class="line">  data_len = len(raw_data)</div><div class="line">  batch_len = data_len // batch_size</div><div class="line">  data = np.zeros([batch_size, batch_len], dtype=np.int32)</div><div class="line">  for i in range(batch_size):</div><div class="line">    data[i] = raw_data[batch_len * i:batch_len * (i + 1)]</div><div class="line">  </div><div class="line">  epoch_size = (batch_len - 1) // num_steps</div><div class="line">  </div><div class="line">  if epoch_size == 0:</div><div class="line">    raise ValueError("epoch_size == 0, decrease batch_size or num_steps")</div><div class="line">  </div><div class="line">  for i in range(epoch_size):</div><div class="line">    x = data[:, i*num_steps:(i+1)*num_steps]</div><div class="line">    y = data[:, i*num_steps+1:(i+1)*num_steps+1]</div><div class="line">    yield (x, y)</div></pre></td></tr></table></figure></li></ul></li></ul></li></ul><h1 id="二、使用tf-scan函数和dynamic-rnn"><a href="#二、使用tf-scan函数和dynamic-rnn" class="headerlink" title="二、使用tf.scan函数和dynamic_rnn"></a>二、使用<code>tf.scan</code>函数和<code>dynamic_rnn</code></h1><h2 id="1、为什么使用tf-scan和dynamic-rnn"><a href="#1、为什么使用tf-scan和dynamic-rnn" class="headerlink" title="1、为什么使用tf.scan和dynamic_rnn"></a>1、为什么使用<code>tf.scan</code>和<code>dynamic_rnn</code></h2><ul><li>之前我们<a href="http://lawlite.me/2017/06/16/RNN-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-02Tensorflow%E4%B8%AD%E7%9A%84%E5%AE%9E%E7%8E%B0/#1-实现过程">实现的第一个例子</a>中没有用<code>dynamic_rnn</code>的部分是将输入的三维数据<code>[batch_size,num_steps, state_size]</code>按<code>num_steps</code>维度进行拆分，然后每计算一步都存到<code>list</code>列表中，如下图</li></ul><p><img src="/assets/blog_images/RNN/RNN_list_tensor19.png" alt="计算之后存到list中" title="RNN_list_tensor19"></p><ul><li>这种构建方式<strong>很耗时</strong>，在我们例子中没有体现出来，但是如果我们要<strong>学习的步数</strong>很大(<code>num_steps</code>，也可以说要学习的依赖关系很长），如果再使用<strong>深层的RNN</strong>，这种就不合适了</li><li><p>为了方便比较和<code>dynamic_rnn</code>的运行耗时，下面还是给出使用<code>list</code></p><h2 id="2、使用list的方式-static-rnn"><a href="#2、使用list的方式-static-rnn" class="headerlink" title="2、使用list的方式(static_rnn)"></a>2、使用<code>list</code>的方式(<code>static_rnn</code>)</h2></li><li><p>构建计算图</p><ul><li>我这里<code>tensorflow</code>的版本是<code>1.2.0</code>，与<code>1.0</code> 些许不一样</li><li>和之前的例子差不多，这里不再累述<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div></pre></td><td class="code"><pre><div class="line">'''使用list的方式'''</div><div class="line">def build_basic_rnn_graph_with_list(</div><div class="line">    state_size = state_size,</div><div class="line">    num_classes = num_classes,</div><div class="line">    batch_size = batch_size,</div><div class="line">    num_steps = num_steps,</div><div class="line">    num_layers = 3,</div><div class="line">    learning_rate = learning_rate):</div><div class="line"></div><div class="line">    reset_graph()</div><div class="line"></div><div class="line">    x = tf.placeholder(tf.int32, [batch_size, num_steps], name='x')</div><div class="line">    y = tf.placeholder(tf.int32, [batch_size, num_steps], name='y')</div><div class="line"></div><div class="line">    x_one_hot = tf.one_hot(x, num_classes)   # (batch_size, num_steps, num_classes)</div><div class="line">    '''这里按第二维拆开num_steps*(batch_size, num_classes)'''</div><div class="line">    rnn_inputs = [tf.squeeze(i,squeeze_dims=[1]) for i in tf.split(x_one_hot, num_steps, 1)]</div><div class="line"></div><div class="line">    cell = tf.nn.rnn_cell.BasicRNNCell(state_size)</div><div class="line">    init_state = cell.zero_state(batch_size, tf.float32)</div><div class="line">    '''使用static_rnn方式'''</div><div class="line">    rnn_outputs, final_state = tf.contrib.rnn.static_rnn(cell=cell, inputs=rnn_inputs, </div><div class="line">                                                        initial_state=init_state)</div><div class="line">    #rnn_outputs, final_state = tf.nn.rnn(cell, rnn_inputs, initial_state=init_state) # tensorflow 1.0的方式</div><div class="line">    with tf.variable_scope('softmax'):</div><div class="line">        W = tf.get_variable('W', [state_size, num_classes])</div><div class="line">        b = tf.get_variable('b', [num_classes], initializer=tf.constant_initializer(0.0))</div><div class="line">    logits = [tf.matmul(rnn_output, W) + b for rnn_output in rnn_outputs]</div><div class="line"></div><div class="line">    y_as_list = [tf.squeeze(i, squeeze_dims=[1]) for i in tf.split(y, num_steps, 1)]</div><div class="line"></div><div class="line">    #loss_weights = [tf.ones([batch_size]) for i in range(num_steps)]</div><div class="line">    losses = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y_as_list, </div><div class="line">                                                  logits=logits)</div><div class="line">    #losses = tf.nn.seq2seq.sequence_loss_by_example(logits, y_as_list, loss_weights)  # tensorflow 1.0的方式</div><div class="line">    total_loss = tf.reduce_mean(losses)</div><div class="line">    train_step = tf.train.AdamOptimizer(learning_rate).minimize(total_loss)</div><div class="line"></div><div class="line">    return dict(</div><div class="line">        x = x,</div><div class="line">        y = y,</div><div class="line">        init_state = init_state,</div><div class="line">        final_state = final_state,</div><div class="line">        total_loss = total_loss,</div><div class="line">        train_step = train_step</div><div class="line">    )</div></pre></td></tr></table></figure></li></ul></li><li><p>训练神经网络函数</p><ul><li>和之前例子类似<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line">'''训练rnn网络的函数'''</div><div class="line">def train_rnn(g, num_epochs, num_steps=num_steps, batch_size=batch_size, verbose=True, save=False):</div><div class="line">    tf.set_random_seed(2345)</div><div class="line">    with tf.Session() as sess:</div><div class="line">        sess.run(tf.initialize_all_variables())</div><div class="line">        training_losses = []</div><div class="line">        for idx, epoch in enumerate(gen_epochs(num_epochs, num_steps, batch_size)):</div><div class="line">            training_loss = 0</div><div class="line">            steps = 0</div><div class="line">            training_state = None</div><div class="line">            for X, Y in epoch:</div><div class="line">                steps += 1</div><div class="line">                feed_dict = &#123;g['x']: X, g['y']: Y&#125;</div><div class="line">                if training_state is not None:</div><div class="line">                    feed_dict[g['init_state']] = training_state</div><div class="line">                training_loss_, training_state, _ = sess.run([g['total_loss'],</div><div class="line">                                                           g['final_state'],</div><div class="line">                                                           g['train_step']],</div><div class="line">                                                          feed_dict=feed_dict)</div><div class="line">                training_loss += training_loss_ </div><div class="line">            if verbose:</div><div class="line">                print('epoch: &#123;0&#125;的平均损失值：&#123;1&#125;'.format(idx, training_loss/steps))</div><div class="line">            training_losses.append(training_loss/steps)</div><div class="line">        </div><div class="line">        if isinstance(save, str):</div><div class="line">            g['saver'].save(sess, save)</div><div class="line">    return training_losses</div></pre></td></tr></table></figure></li></ul></li><li><p>调用执行：</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">start_time = <span class="selector-tag">time</span>.time()</div><div class="line">g = build_basic_rnn_graph_with_list()</div><div class="line"><span class="function"><span class="title">print</span><span class="params">(<span class="string">"构建图耗时"</span>, time.time()</span></span>-start_time)</div><div class="line">start_time = <span class="selector-tag">time</span>.time()</div><div class="line"><span class="function"><span class="title">train_rnn</span><span class="params">(g, <span class="number">3</span>)</span></span></div><div class="line"><span class="function"><span class="title">print</span><span class="params">(<span class="string">"训练耗时："</span>, time.time()</span></span>-start_time)</div></pre></td></tr></table></figure></li><li><p>运行结果</p><ul><li>构建计算图耗时: <code>113.43532419204712</code></li><li><code>3</code>个<code>epoch</code>运行耗时：<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">epoch: 0的平均损失值：3.6314958388777985</div><div class="line">epoch: 1的平均损失值：3.287133811534136</div><div class="line">epoch: 2的平均损失值：3.250853428895446</div><div class="line">训练耗时： 84.2816972732544</div></pre></td></tr></table></figure></li></ul></li><li><p>可以看出在<strong>构建图的时候非常耗时</strong>，这里仅仅<strong>一层的cell</strong></p></li></ul><h2 id="3、dynamic-rnn的使用"><a href="#3、dynamic-rnn的使用" class="headerlink" title="3、dynamic_rnn的使用"></a>3、<code>dynamic_rnn</code>的使用</h2><ul><li>之前在我们<a href="http://lawlite.me/2017/06/16/RNN-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-02Tensorflow%E4%B8%AD%E7%9A%84%E5%AE%9E%E7%8E%B0/#2-%E4%BD%BF%E7%94%A8dynamic-rnn%E6%96%B9%E5%BC%8F">第一个例子</a>中实际已经使用过了，这里使用<code>MultiRNNCell</code>实现<strong>多层cell</strong>，具体下面再讲</li><li>构建模型：<ul><li><code>tf.nn.embedding_lookup(params, ids)</code>函数是在<code>params</code>中查找<code>ids</code>的<strong>表示</strong>， 和在<code>matrix</code>中用<code>array</code>索引类似, 这里是在<strong>二维embeddings</strong>中找<strong>二维的ids</strong>, <code>ids</code>每一行中的一个数对应<code>embeddings</code>中的一行，所以最后是<code>[batch_size, num_steps, state_size]</code>，关于具体的输出可以<a href="https://gist.github.com/lawlite19/02a598c6d9b91a53d0a38fd081f9cbc4" target="_blank" rel="external">查看这里</a></li><li>这里我认为就是某个<strong>字母的表示</strong>,之前上面我们的<code>statci_rnn</code>就是<code>one-hot</code>来表示的</li></ul></li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div></pre></td><td class="code"><pre><div class="line">'''使用dynamic_rnn方式</div><div class="line">   - 之前我们自己实现的cell和static_rnn的例子都是将得到的tensor使用list存起来，这种方式构建计算图时很慢</div><div class="line">   - dynamic可以在运行时构建计算图</div><div class="line">'''</div><div class="line">def build_multilayer_lstm_graph_with_dynamic_rnn(</div><div class="line">    state_size = state_size,</div><div class="line">    num_classes = num_classes,</div><div class="line">    batch_size = batch_size,</div><div class="line">    num_steps = num_steps,</div><div class="line">    num_layers = 3,</div><div class="line">    learning_rate = learning_rate</div><div class="line">    ):</div><div class="line">    reset_graph()</div><div class="line">    x = tf.placeholder(tf.int32, [batch_size, num_steps], name='x')</div><div class="line">    y = tf.placeholder(tf.int32, [batch_size, num_steps], name='y')</div><div class="line">    embeddings = tf.get_variable(name='embedding_matrix', shape=[num_classes, state_size])</div><div class="line">    '''这里的输入是三维的[batch_size, num_steps, state_size]</div><div class="line">        - embedding_lookup(params, ids)函数是在params中查找ids的表示， 和在matrix中用array索引类似,</div><div class="line">          这里是在二维embeddings中找二维的ids, ids每一行中的一个数对应embeddings中的一行，所以最后是[batch_size, num_steps, state_size]</div><div class="line">    '''</div><div class="line">    rnn_inputs = tf.nn.embedding_lookup(params=embeddings, ids=x)</div><div class="line">    cell = tf.nn.rnn_cell.LSTMCell(num_units=state_size, state_is_tuple=True)</div><div class="line">    cell = tf.nn.rnn_cell.MultiRNNCell(cells=[cell]*num_layers, state_is_tuple=True)</div><div class="line">    init_state = cell.zero_state(batch_size, dtype=tf.float32)</div><div class="line">    '''使用dynamic_rnn方式'''</div><div class="line">    rnn_outputs, final_state = tf.nn.dynamic_rnn(cell=cell, inputs=rnn_inputs, </div><div class="line">                                                 initial_state=init_state)    </div><div class="line">    with tf.variable_scope('softmax'):</div><div class="line">        W = tf.get_variable('W', [state_size, num_classes])</div><div class="line">        b = tf.get_variable('b', [num_classes], initializer=tf.constant_initializer(0.0))</div><div class="line">    </div><div class="line">    rnn_outputs = tf.reshape(rnn_outputs, [-1, state_size])   # 转成二维的矩阵</div><div class="line">    y_reshape = tf.reshape(y, [-1])</div><div class="line">    logits = tf.matmul(rnn_outputs, W) + b                    # 进行矩阵运算</div><div class="line">    total_loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=y_reshape))</div><div class="line">    train_step = tf.train.AdamOptimizer(learning_rate).minimize(total_loss)</div><div class="line">    </div><div class="line">    return dict(x = x,</div><div class="line">                y = y,</div><div class="line">                init_state = init_state,</div><div class="line">                final_state = final_state,</div><div class="line">                total_loss = total_loss,</div><div class="line">                train_step = train_step)</div></pre></td></tr></table></figure><ul><li>调用执行即可</li></ul><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">start_time = <span class="selector-tag">time</span>.time()</div><div class="line">g = build_multilayer_lstm_graph_with_dynamic_rnn()</div><div class="line"><span class="function"><span class="title">print</span><span class="params">(<span class="string">"构建图耗时"</span>, time.time()</span></span>-start_time)</div><div class="line">start_time = <span class="selector-tag">time</span>.time()</div><div class="line"><span class="function"><span class="title">train_rnn</span><span class="params">(g, <span class="number">3</span>)</span></span></div><div class="line"><span class="function"><span class="title">print</span><span class="params">(<span class="string">"训练耗时："</span>, time.time()</span></span>-start_time)</div></pre></td></tr></table></figure><ul><li>运行结果（注意这是<strong>3层的LSTM</strong>）：<ul><li>构建计算图耗时 <code>7.616888523101807</code>，相比第一种<code>static_rnn</code>很快</li><li>训练耗时(这是<strong>3层的LSTM</strong>，所以还是挺慢的)：<figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">epoch: <span class="number">0</span>的平均损失值：<span class="number">3.604653576324726</span></div><div class="line">epoch: <span class="number">1</span>的平均损失值：<span class="number">3.3202743626188957</span></div><div class="line">epoch: <span class="number">2</span>的平均损失值：<span class="number">3.3155322650383257</span></div><div class="line">训练耗时： <span class="number">303.5468375682831</span></div></pre></td></tr></table></figure></li></ul></li></ul><h2 id="4、tf-scan实现的方式"><a href="#4、tf-scan实现的方式" class="headerlink" title="4、tf.scan实现的方式"></a>4、<code>tf.scan</code>实现的方式</h2><ul><li>如果你不了解<code>tf.scan</code>，建议看下<a href="https://www.tensorflow.org/api_docs/python/tf/scan" target="_blank" rel="external">官方API</a>, 还是有点复杂的。<ul><li>或者Youtube上有个介绍，<a href="https://www.youtube.com/watch?v=A6qJMB3stE4&amp;t=621s" target="_blank" rel="external">点击这里查看</a></li></ul></li><li><code>scan</code>是个高阶函数，<strong>一般的计算方式是</strong>：给定一个<strong>序列</strong>$[x_0, x_1,…..,x_n]$和<strong>初试状态</strong>$y_{-1}$,根据$y_t = f(x_t, y_{t-1})$ 计算得到<strong>最终序列</strong>$[y_0, y_1,……,y_n]$</li><li>构建计算图<ul><li><code>tf.transpose(rnn_inputs, [1,0,2])</code>  是将<code>rnn_inputs</code>的<strong>第一个和第二个维度调换</strong>，即变成<code>[num_steps,batch_size, state_size]</code>, 在<code>dynamic_rnn</code>函数有个<strong>time_major参数</strong>，就是指定<code>num_steps</code>是否在第一个维度上，默认是<code>false</code>的,即不在第一维 </li><li><code>tf.scan</code>会将<code>elems</code>按照<strong>第一维拆开</strong>，所以一次就是一个<code>step</code>的数据（和我们<code>static_rnn</code>的例子类似） </li><li><strong>参数a</strong>的结构和<strong>initializer的结构一致</strong>，所以<code>a[1]</code>就是对应的<code>state</code>，<code>cell</code>需要传入<code>x</code>和<code>state</code>计算 </li><li>每次迭代<code>cell</code>返回的是一个<code>rnn_output, shape=(batch_size,state_size)</code>和对应的<code>state</code>,<code>num_steps</code>之后的<code>rnn_outputs</code>的<code>shape</code>就是<code>(num_steps, batch_size, state_size)</code> ，<code>state</code>同理</li><li>每次输入的<code>x</code>都会得到的<code>state--&gt;(final_states)</code>，我们只要的最后的<code>final_state</code> </li></ul></li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div></pre></td><td class="code"><pre><div class="line">'''使用scan实现dynamic_rnn的效果'''</div><div class="line">def build_multilayer_lstm_graph_with_scan(</div><div class="line">    state_size = state_size,</div><div class="line">    num_classes = num_classes,</div><div class="line">    batch_size = batch_size,</div><div class="line">    num_steps = num_steps,</div><div class="line">    num_layers = 3,</div><div class="line">    learning_rate = learning_rate</div><div class="line">    ):</div><div class="line">    reset_graph()</div><div class="line">    x = tf.placeholder(tf.int32, [batch_size, num_steps], name='x')</div><div class="line">    y = tf.placeholder(tf.int32, [batch_size, num_steps], name='y')</div><div class="line">    embeddings = tf.get_variable(name='embedding_matrix', shape=[num_classes, state_size])</div><div class="line">    '''这里的输入是三维的[batch_size, num_steps, state_size]'''</div><div class="line">    rnn_inputs = tf.nn.embedding_lookup(params=embeddings, ids=x)</div><div class="line">    '''构建多层的cell, 先构建一个cell, 然后使用MultiRNNCell函数构建即可'''</div><div class="line">    cell = tf.nn.rnn_cell.LSTMCell(num_units=state_size, state_is_tuple=True)</div><div class="line">    cell = tf.nn.rnn_cell.MultiRNNCell(cells=[cell]*num_layers, state_is_tuple=True)  </div><div class="line">    init_state = cell.zero_state(batch_size, dtype=tf.float32)</div><div class="line">    '''使用tf.scan方式</div><div class="line">       - tf.transpose(rnn_inputs, [1,0,2])  是将rnn_inputs的第一个和第二个维度调换，即[num_steps,batch_size, state_size],</div><div class="line">           在dynamic_rnn函数有个time_major参数，就是指定num_steps是否在第一个维度上，默认是false的,即不在第一维</div><div class="line">       - tf.scan会将elems按照第一维拆开，所以一次就是一个step的数据（和我们static_rnn的例子类似）</div><div class="line">       - a的结构和initializer的结构一致，所以a[1]就是对应的state，cell需要传入x和state计算</div><div class="line">       - 每次迭代cell返回的是一个rnn_output(batch_size,state_size)和对应的state,num_steps之后的rnn_outputs的shape就是(num_steps, batch_size, state_size)</div><div class="line">       - 每次输入的x都会得到的state(final_states)，我们只要的最后的final_state</div><div class="line">    '''</div><div class="line">    def testfn(a, x):</div><div class="line">        return cell(x, a[1])</div><div class="line">    rnn_outputs, final_states = tf.scan(fn=testfn, elems=tf.transpose(rnn_inputs, [1,0,2]),</div><div class="line">                                        initializer=(tf.zeros([batch_size,state_size]),init_state)</div><div class="line">                                        )</div><div class="line">    '''或者使用lambda的方式'''</div><div class="line">    #rnn_outputs, final_states = tf.scan(lambda a,x: cell(x, a[1]), tf.transpose(rnn_inputs, [1,0,2]),</div><div class="line">                                        #initializer=(tf.zeros([batch_size, state_size]),init_state))</div><div class="line">    final_state = tuple([tf.nn.rnn_cell.LSTMStateTuple(</div><div class="line">        tf.squeeze(tf.slice(c, [num_steps-1,0,0], [1,batch_size,state_size])),</div><div class="line">        tf.squeeze(tf.slice(h, [num_steps-1,0,0], [1,batch_size,state_size]))) for c, h in final_states])</div><div class="line"></div><div class="line">    with tf.variable_scope('softmax'):</div><div class="line">        W = tf.get_variable('W', [state_size, num_classes])</div><div class="line">        b = tf.get_variable('b', [num_classes], initializer=tf.constant_initializer(0.0))</div><div class="line">    </div><div class="line">    rnn_outputs = tf.reshape(rnn_outputs, [-1, state_size])</div><div class="line">    y_reshape = tf.reshape(y, [-1])</div><div class="line">    logits = tf.matmul(rnn_outputs, W) + b</div><div class="line">    total_loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=y_reshape))</div><div class="line">    train_step = tf.train.AdamOptimizer(learning_rate).minimize(total_loss)</div><div class="line">    </div><div class="line">    return dict(x = x,</div><div class="line">                y = y,</div><div class="line">                init_state = init_state,</div><div class="line">                final_state = final_state,</div><div class="line">                total_loss = total_loss,</div><div class="line">                train_step = train_step)</div></pre></td></tr></table></figure><ul><li>运行结果<ul><li>构建计算图耗时: <code>8.685610055923462</code> （比<code>dynamic_rnn</code>稍微慢一点）</li><li>训练耗时（和<code>dynamic_rnn</code>耗时差不多）</li></ul></li><li>使用<code>scan</code>的方式只比<code>dynamic_rnn</code>慢一点点，但是对我们来说更加灵活和清楚执行的过程。也方便我们修改代码（比如从<code>state</code>的<code>t-2</code>时刻跳过一个时刻直接到<code>t</code>）</li></ul><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">epoch: <span class="number">0</span>的平均损失值：<span class="number">3.6226147892831384</span></div><div class="line">epoch: <span class="number">1</span>的平均损失值：<span class="number">3.3211338095281318</span></div><div class="line">epoch: <span class="number">2</span>的平均损失值：<span class="number">3.3158331972429123</span></div><div class="line">训练耗时： <span class="number">303.2535448074341</span></div></pre></td></tr></table></figure><h1 id="三、关于多层RNN"><a href="#三、关于多层RNN" class="headerlink" title="三、关于多层RNN"></a>三、关于多层<code>RNN</code></h1><h2 id="1、结构"><a href="#1、结构" class="headerlink" title="1、结构"></a>1、结构</h2><ul><li><code>LSTM</code>中包含两个<code>state</code>,一个是<code>c</code>记忆单元（<code>memory cell</code>），另外一个是<code>h</code>隐藏状态(<code>hidden state</code>), 在<code>Tensorflow</code>中是以<code>tuple</code>元组的形式，所以才有上面构建<code>dynamic_rnn</code>时的参数<code>state_is_tuple</code>的参数，这种方式执行更快</li><li>多层的结构如下图<br><img src="/assets/blog_images/RNN/RNN_LSTM_27.png" alt="多层RNN" title="RNN_LSTM_27"></li><li>我们可以将其<strong>包装起来</strong>, 看起来像一个<code>cell</code>一样<br><img src="/assets/blog_images/RNN/RNN_LSTM_28.png" alt="包装 cell" title="RNN_LSTM_28"></li></ul><h2 id="2、代码"><a href="#2、代码" class="headerlink" title="2、代码"></a>2、代码</h2><ul><li><code>Tensorflow</code>中的实现就是使用<code>tf.nn.rnn_cell.MultiRNNCell</code><ul><li>声明一个<code>cell</code></li><li><code>MultiRNNCell</code>中传入<code>[cell]*num_layers</code>就可以了</li><li>注意如果是<code>LSTM</code>，定义参数<code>state_is_tuple=True</code><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">cell = tf<span class="selector-class">.nn</span><span class="selector-class">.rnn_cell</span><span class="selector-class">.LSTMCell</span>(num_units=state_size, state_is_tuple=True)</div><div class="line">cell = tf<span class="selector-class">.nn</span><span class="selector-class">.rnn_cell</span><span class="selector-class">.MultiRNNCell</span>(cells=[cell]*num_layers, state_is_tuple=True)</div><div class="line">init_state = cell.zero_state(batch_size, dtype=tf.float32)</div></pre></td></tr></table></figure></li></ul></li></ul><h1 id="四、Dropout操作"><a href="#四、Dropout操作" class="headerlink" title="四、Dropout操作"></a>四、Dropout操作</h1><ul><li>应用在一层<code>cell</code>的<strong>输入和输出</strong>，<strong>不应用在循环的部分</strong></li></ul><h2 id="1、一层的cell"><a href="#1、一层的cell" class="headerlink" title="1、一层的cell"></a>1、一层的<code>cell</code></h2><ul><li><code>static_rnn</code>中实现<ul><li>声明<code>placeholder</code>：<code>keep_prob = tf.placeholder(tf.float32, name=&#39;keep_prob&#39;)</code></li><li>输入：<code>rnn_inputs = [tf.nn.dropout(rnn_input, keep_prob) for rnn_input in rnn_inputs]</code></li><li>输出：<code>rnn_outputs = [tf.nn.dropout(rnn_output, keep_prob) for rnn_output in rnn_outputs]</code></li><li><code>feed_dict</code>中加入即可：<code>feed_dict = {g[&#39;x&#39;]: X, g[&#39;y&#39;]: Y, g[&#39;keep_prob&#39;]: keep_prob}</code></li></ul></li><li><code>dynamic_rnn</code>或者<code>scan</code>中实现<ul><li>直接添加即可，其余类似：<code>rnn_inputs = tf.nn.dropout(rnn_inputed, keep_prob)</code></li></ul></li></ul><h2 id="2、多层cell"><a href="#2、多层cell" class="headerlink" title="2、多层cell"></a>2、多层<code>cell</code></h2><ul><li>我们之前说使用<code>MultiRNNCell</code>将多层<code>cell</code>看作一个<code>cell</code>, 那么怎么实现对每层<code>cell</code>使用<code>dropout</code>呢</li><li>可以使用<code>tf.nn.rnn_cell.DropoutWrapper</code>来实现</li><li><strong>方式一</strong>：<code>cell = tf.nn.rnn_cell.DropoutWrapper(cell, input_keep_prob=input_keep_prob, output_keep_prob=output_drop_prob)</code><ul><li>如果同时使用了<code>input_keep_prob</code>和<code>output_keep_prob</code>都是<code>0.9</code>, 那么层之间的<code>drop_out=0.9*0.9=0.81</code></li></ul></li><li><strong>方式二</strong>: 对于<code>basic cell</code>只使用一个<code>input_keep_prob</code>或者<code>output_keep_prob</code>，对<code>MultiRNNCell</code>也使用一个<code>input_keep_prob</code>或者<code>output_keep_prob</code></li></ul><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">cell = tf<span class="selector-class">.nn</span><span class="selector-class">.rnn_cell</span><span class="selector-class">.LSTMCell</span>(num_units=state_size, state_is_tuple=True)</div><div class="line">cell = tf<span class="selector-class">.nn</span><span class="selector-class">.rnn_cell</span><span class="selector-class">.DropoutWrapper</span>(cell, input_keep_prob=keep_prob)</div><div class="line">cell = tf<span class="selector-class">.nn</span><span class="selector-class">.rnn_cell</span><span class="selector-class">.MultiRNNCell</span>(cells=[cell]*num_layers, state_is_tuple=True)</div><div class="line">cell = tf<span class="selector-class">.nn</span><span class="selector-class">.rnn_cell</span><span class="selector-class">.DropoutWrapper</span>(cell,output_keep_prob=keep_prob)</div></pre></td></tr></table></figure><h1 id="五、层标准化-Layer-Normalization"><a href="#五、层标准化-Layer-Normalization" class="headerlink" title="五、层标准化 (Layer Normalization)"></a>五、层标准化 (<code>Layer Normalization</code>)</h1><h2 id="1、说明"><a href="#1、说明" class="headerlink" title="1、说明"></a>1、说明</h2><ul><li><code>Layer Normalization</code>是受<code>Batch Normalization</code>的启发而来，针对于<strong>RNN</strong>，可以查看<a href="https://arxiv.org/abs/1607.06450" target="_blank" rel="external">相关论文</a></li><li><code>Batch Normalization</code>主要针对于<strong>传统的深度神经网络</strong>和<strong>CNN</strong>，关于<code>Batch Normalization</code>的操作和推导可以看<a href="http://lawlite.me/2017/06/23/%E8%AE%BA%E6%96%87%E8%AE%B0%E5%BD%95-Batch-Normalization/">我之前的博客</a></li><li>可以加快训练的速度，得到更好的结果等</li></ul><h2 id="2、代码-1"><a href="#2、代码-1" class="headerlink" title="2、代码"></a>2、代码</h2><ul><li>找到<code>LSTMCell</code>的源码拷贝一份修改即可</li><li><p><code>layer normalization</code>函数</p><ul><li>传入的<code>tensor</code>是二维的，对其进行<code>batch normalization</code>操作</li><li><code>tf.nn.moment</code>是计算<code>tensor</code>的<code>mean value</code>和<code>variance value</code></li><li>然后对其进行缩放(<code>scale</code>)和平移(<code>shift</code>)<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">'''layer normalization'''</div><div class="line">def ln(tensor, scope=None, epsilon=1e-5):</div><div class="line">  assert(len(tensor.get_shape()) == 2)</div><div class="line">  m, v = tf.nn.moments(tensor, [1], keep_dims=True)</div><div class="line">  if not isinstance(scope, str):</div><div class="line">    scope = ''</div><div class="line">  with tf.variable_scope(scope+'layer_norm'):</div><div class="line">    scale = tf.get_variable(name='scale', </div><div class="line">                            shape=[tensor.get_shape()[1]], </div><div class="line">                            initializer=tf.constant_initializer(1))</div><div class="line">    shift = tf.get_variable('shift',</div><div class="line">                            [tensor.get_shape()[1]],</div><div class="line">                            initializer=tf.constant_initializer(0))</div><div class="line">  LN_initial = (tensor - m) / tf.sqrt(v + epsilon)</div><div class="line">  return LN_initial*scale + shift</div></pre></td></tr></table></figure></li></ul></li><li><p><code>LSTMCell</code>中的<code>call</code>方法<code>i,j,f,o</code>调用<code>layer normalization</code>操作</p><ul><li><code>_linear</code>函数中的<code>bias</code>设为<code>False</code>， 因为<code>BN</code>会加上<code>shift</code><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">'''这里bias设置为false, 因为bn会加上shift'''</div><div class="line">lstm_matrix = _linear([inputs, m_prev], 4 * self._num_units, bias=False)</div><div class="line">i, j, f, o = array_ops.split(</div><div class="line">    value=lstm_matrix, num_or_size_splits=4, axis=1)</div><div class="line">'''执行ln'''</div><div class="line">i = ln(i, scope = 'i/')</div><div class="line">j = ln(j, scope = 'j/')</div><div class="line">f = ln(f, scope = 'f/')</div><div class="line">o = ln(o, scope = 'o/')</div></pre></td></tr></table></figure></li></ul></li><li><p>构建计算图</p><ul><li>可以选择<code>RNN GRU LSTM</code></li><li><code>Dropout</code></li><li><code>Layer Normalization</code><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div></pre></td><td class="code"><pre><div class="line">'''最终的整合模型，</div><div class="line">   - 普通RNN，GRU，LSTM</div><div class="line">   - dropout</div><div class="line">   - BN</div><div class="line">'''</div><div class="line">from LayerNormalizedLSTMCell import LayerNormalizedLSTMCell # 导入layer normalization的LSTMCell 文件</div><div class="line"></div><div class="line">def build_final_graph(</div><div class="line">    cell_type = None,</div><div class="line">    state_size = state_size,</div><div class="line">    num_classes = num_classes,</div><div class="line">    batch_size = batch_size,</div><div class="line">    num_steps = num_steps,</div><div class="line">    num_layers = 3,</div><div class="line">    build_with_dropout = False,</div><div class="line">    learning_rate = learning_rate):</div><div class="line">    </div><div class="line">    reset_graph()</div><div class="line">    x = tf.placeholder(tf.int32, [batch_size, num_steps], name='x')</div><div class="line">    y = tf.placeholder(tf.int32, [batch_size, num_steps], name='y')</div><div class="line">    keep_prob = tf.placeholder(tf.float32, name='keep_prob')</div><div class="line">    embeddings = tf.get_variable('embedding_matrix', [num_classes, state_size])</div><div class="line">    rnn_inputs = tf.nn.embedding_lookup(embeddings, x)</div><div class="line">    if cell_type == 'GRU':</div><div class="line">        cell = tf.nn.rnn_cell.GRUCell(state_size)</div><div class="line">    elif cell_type == 'LSTM':</div><div class="line">        cell = tf.nn.rnn_cell.LSTMCell(state_size, state_is_tuple=True)</div><div class="line">    elif cell_type == 'LN_LSTM':</div><div class="line">        cell = LayerNormalizedLSTMCell(state_size)  # 自己修改的代码，导入对应的文件</div><div class="line">    else:</div><div class="line">        cell = tf.nn.rnn_cell.BasicRNNCell(state_size)</div><div class="line">    if build_with_dropout:</div><div class="line">        cell = tf.nn.rnn_cell.DropoutWrapper(cell, input_keep_prob=keep_prob)</div><div class="line">        </div><div class="line">    init_state = cell.zero_state(batch_size, tf.float32)</div><div class="line">    '''dynamic_rnn'''</div><div class="line">    rnn_outputs, final_state = tf.nn.dynamic_rnn(cell, rnn_inputs, initial_state=init_state)</div><div class="line">    with tf.variable_scope('softmax'):</div><div class="line">        W = tf.get_variable('W', [state_size, num_classes])</div><div class="line">        b = tf.get_variable('b', [num_classes], initializer=tf.constant_initializer(0.0))</div><div class="line">    rnn_outputs = tf.reshape(rnn_outputs, [-1, state_size])</div><div class="line">    y_reshaped = tf.reshape(y, [-1])</div><div class="line">    logits = tf.matmul(rnn_outputs, W) + b</div><div class="line"></div><div class="line">    predictions = tf.nn.softmax(logits)</div><div class="line"></div><div class="line">    total_loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=y_reshaped))</div><div class="line">    train_step = tf.train.AdamOptimizer(learning_rate).minimize(total_loss)</div><div class="line"></div><div class="line">    return dict(</div><div class="line">        x = x,</div><div class="line">        y = y,</div><div class="line">        keep_prob = keep_prob,</div><div class="line">        init_state = init_state,</div><div class="line">        final_state = final_state,</div><div class="line">        total_loss = total_loss,</div><div class="line">        train_step = train_step,</div><div class="line">        preds = predictions,</div><div class="line">        saver = tf.train.Saver()</div><div class="line">    )</div></pre></td></tr></table></figure></li></ul></li></ul><h1 id="六、生成文本"><a href="#六、生成文本" class="headerlink" title="六、生成文本"></a>六、生成文本</h1><h2 id="1、说明-1"><a href="#1、说明-1" class="headerlink" title="1、说明"></a>1、说明</h2><ul><li>训练完成之后将计算图保存到本地磁盘，下次直接读取就可以了</li><li>我们给出<strong>第一个字符</strong>，<code>RNN</code>接着一个个生成字符，每次都是<strong>根据前一个字符</strong><ul><li>所以<code>num_steps=1</code>, <code>batch_size=1</code>（可以想象生成<code>prediction</code>的<code>shape</code>是<code>(1, num_classes)</code>中选择一个概率,–&gt; <code>num_steps=1</code> ）</li></ul></li></ul><h2 id="2、代码-2"><a href="#2、代码-2" class="headerlink" title="2、代码"></a>2、代码</h2><ul><li>构建图（直接传入参数即可）：<code>g = build_final_graph(cell_type=&#39;LN_LSTM&#39;, num_steps=1, batch_size=1)</code></li><li><p>生成文本</p><ul><li>读取训练好的文件</li><li>得到给出的第一个字符对应的数字</li><li>循环遍历要生成多少个字符， 每次循环生成一个字符<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line">'''生成文本'''</div><div class="line">def generate_characters(g, checkpoint, num_chars, prompt='A', pick_top_chars=None):</div><div class="line">    with tf.Session() as sess:</div><div class="line">        sess.run(tf.global_variables_initializer())</div><div class="line">        g['saver'].restore(sess, checkpoint)   # 读取文件</div><div class="line">        state = None</div><div class="line">        current_char = vocab_to_idx[prompt]    # 得到给出的字母对应的数字</div><div class="line">        chars = [current_char]                          </div><div class="line">        for i in range(num_chars):             # 总共生成多少数字</div><div class="line">            if state is not None:              # 第一次state为None,因为计算图中定义了刚开始为0</div><div class="line">                feed_dict=&#123;g['x']: [[current_char]], g['init_state']: state&#125; # 传入当前字符</div><div class="line">            else:</div><div class="line">                feed_dict=&#123;g['x']: [[current_char]]&#125;</div><div class="line">            preds, state = sess.run([g['preds'],g['final_state']], feed_dict)   # 得到预测结果（概率）preds的shape就是（1，num_classes）</div><div class="line">            if pick_top_chars is not None:              # 如果设置了概率较大的前多少个</div><div class="line">                p = np.squeeze(preds)</div><div class="line">                p[np.argsort(p)[:-pick_top_chars]] = 0  # 其余的置为0</div><div class="line">                p = p / np.sum(p)                       # 因为下面np.random.choice函数p的概率和要求是1，处理一下</div><div class="line">                current_char = np.random.choice(vocab_size, 1, p=p)[0]    # 根据概率选择一个</div><div class="line">            else:</div><div class="line">                current_char = np.random.choice(vocab_size, 1, p=np.squeeze(preds))[0]</div><div class="line"></div><div class="line">            chars.append(current_char)</div><div class="line">    chars = map(lambda x: idx_to_vocab[x], chars)</div><div class="line">    result = "".join(chars)</div><div class="line">    print(result)</div><div class="line">    return result</div></pre></td></tr></table></figure></li></ul></li><li><p>结果</p><ul><li>由于训练耗时很长，这里使用<code>LSTM</code>训练了<code>30</code>个<code>epoch</code>，结果如下</li><li>可以自己调整参数，可能会得到更好的结果<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div></pre></td><td class="code"><pre><div class="line">ANK</div><div class="line">O: HFOFMFRone s the statlighte thithe thit.</div><div class="line"></div><div class="line">BODEN --</div><div class="line"></div><div class="line">I I's a tomir.</div><div class="line">I't</div><div class="line">shis and on ar tald the theand this he sile be cares hat s ond tho fo hour he singe sime shind and somante tat ond treang tatsing of the an the to to fook.. Ir ard the with ane she stale..</div><div class="line">ANTE --</div><div class="line"></div><div class="line">KINE</div><div class="line">Show the ard and a beat the weringe be thing or.</div><div class="line"></div><div class="line">Bo hith tho he melan to the mute steres.</div><div class="line"></div><div class="line">The singer stis ard stis.</div><div class="line"></div><div class="line">BACE CANKONS CORE</div><div class="line">Sard the sids ing tho the the sackes tom the</div><div class="line"></div><div class="line">IN</div><div class="line">We stoe shit a dome thor</div><div class="line"></div><div class="line">ate seomser hith.</div><div class="line"></div><div class="line">That</div><div class="line">thow ound</div><div class="line"></div><div class="line"></div><div class="line">TANTONT. SEAT THONTITE SERTI                         1  23</div><div class="line">SHe the mathe a tomoner</div><div class="line">ind is ingit ofres treacentit. Sher stard on this the tor an the candin he whor he sath heres and</div><div class="line">stha dortour tit thas stand. I'd and or a</div></pre></td></tr></table></figure></li></ul></li><li><p><strong>#2017/06/25</strong> 运行结果更新</p><ul><li>更换了一个大点的数据集，<a href="https://gist.githubusercontent.com/spitis/59bfafe6966bfe60cc206ffbb760269f/raw/030a08754aada17cef14eed6fac7797cda830fe8/variousscripts.txt" target="_blank" rel="external">点击查看</a>，使用了<code>layer normalized</code>的<code>LSTM</code>模型</li><li>参数设置：<ul><li><code>num_steps=80</code>            </li><li><code>batch_size=50</code></li><li><code>state_size=512</code>            </li><li><code>num_classes = vocab_size</code></li><li><code>learning_rate = 5e-4</code></li><li><code>30</code>个<code>epochs</code></li></ul></li><li>在实验室电脑跑了一晚上，结果是不是好一点了<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div></pre></td><td class="code"><pre><div class="line">AKTIN:  Yousa hand it have to turn you, sir.</div><div class="line">I have. I've got to here hard on my</div><div class="line">play as a space state, and why he</div><div class="line">happened. What we alwaws who</div><div class="line">this?</div><div class="line"></div><div class="line">JOCASTAND :</div><div class="line"></div><div class="line">PADM </div><div class="line">You, sir!</div><div class="line"></div><div class="line">A battle. An arm of the ship is still.</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line">THE WINDEN'S CORUS</div><div class="line"></div><div class="line">Han's laser guns at the forest fire.  The crowd spots his black</div><div class="line">folkwark and sees the bedroom and twists and sees Leia</div><div class="line">who is shaking.  A huge creature has a long time,</div><div class="line">hold her hand and his timmed, that we see the saulyand.  The</div><div class="line">crowd ruised by the staircase.</div><div class="line"></div><div class="line">EXT. MAZ' CASTLE RUINS - DAY</div><div class="line"></div><div class="line">Rey and Wicket and CAMERA is heard.   Here as so they helf</div><div class="line">this tonight, he spins and sit in a startled bright.</div><div class="line"></div><div class="line">LUKE</div><div class="line">(into propecy)</div><div class="line">The defenstity! Thank you.</div><div class="line"></div><div class="line">LUKE</div><div class="line">I'm afraid to have a lossing live,</div><div class="line">or help. We're</div></pre></td></tr></table></figure></li></ul></li></ul><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul><li><a href="https://r2rt.com/recurrent-neural-networks-in-tensorflow-ii.html" target="_blank" rel="external">https://r2rt.com/recurrent-neural-networks-in-tensorflow-ii.html</a></li><li><a href="https://karpathy.github.io/2015/05/21/rnn-effectiveness/" target="_blank" rel="external">https://karpathy.github.io/2015/05/21/rnn-effectiveness/</a></li><li><a href="http://jmlr.org/proceedings/papers/v37/ioffe15.pdf" target="_blank" rel="external">http://jmlr.org/proceedings/papers/v37/ioffe15.pdf</a></li><li><code>tensorflow scan</code>：<ul><li><a href="https://www.tensorflow.org/api_docs/python/tf/scan" target="_blank" rel="external">https://www.tensorflow.org/api_docs/python/tf/scan</a></li><li><a href="https://www.youtube.com/watch?v=A6qJMB3stE4&amp;t=621s" target="_blank" rel="external">https://www.youtube.com/watch?v=A6qJMB3stE4&amp;t=621s</a></li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;ul&gt;
&lt;li&gt;全部代码：&lt;a href=&quot;https://github.com/lawlite19/Blog-Back-Up/tree/master/code/rnn/rnn_tensorflow&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;点击这里查看&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;关于&lt;code&gt;Tensorflow&lt;/code&gt;实现一个简单的&lt;strong&gt;二元序列的例子&lt;/strong&gt;可以&lt;a href=&quot;http://lawlite.me/2017/06/16/RNN-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-02Tensorflow%E4%B8%AD%E7%9A%84%E5%AE%9E%E7%8E%B0/&quot;&gt;点击这里查看&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;关于&lt;code&gt;RNN&lt;/code&gt;和&lt;code&gt;LSTM&lt;/code&gt;的基础可以&lt;a href=&quot;http://lawlite.me/2017/06/14/RNN-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8CLSTM-01%E5%9F%BA%E7%A1%80/&quot;&gt;查看这里&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;这篇博客主要包含以下内容&lt;ul&gt;
&lt;li&gt;训练一个&lt;code&gt;RNN&lt;/code&gt;模型&lt;strong&gt;逐字符生成文本数据&lt;/strong&gt;(最后的部分)&lt;/li&gt;
&lt;li&gt;使用&lt;code&gt;Tensorflow&lt;/code&gt;的&lt;code&gt;scan&lt;/code&gt;函数实现&lt;code&gt;dynamic_rnn&lt;/code&gt;&lt;strong&gt;动态创建&lt;/strong&gt;的效果&lt;/li&gt;
&lt;li&gt;使用&lt;code&gt;multiple RNN&lt;/code&gt;创建&lt;strong&gt;多层&lt;/strong&gt;的&lt;code&gt;RNN&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;实现&lt;code&gt;Dropout&lt;/code&gt;和&lt;code&gt;Layer Normalization&lt;/code&gt;的功能&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="Python" scheme="http://lawlite.me/tags/Python/"/>
    
      <category term="DeepLearning" scheme="http://lawlite.me/tags/DeepLearning/"/>
    
      <category term="RNN" scheme="http://lawlite.me/tags/RNN/"/>
    
      <category term="LSTM" scheme="http://lawlite.me/tags/LSTM/"/>
    
  </entry>
  
  <entry>
    <title>RNN-循环神经网络-02Tensorflow中的实现</title>
    <link href="http://lawlite.me/2017/06/16/RNN-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-02Tensorflow%E4%B8%AD%E7%9A%84%E5%AE%9E%E7%8E%B0/"/>
    <id>http://lawlite.me/2017/06/16/RNN-循环神经网络-02Tensorflow中的实现/</id>
    <published>2017-06-16T12:59:28.000Z</published>
    <updated>2017-06-25T08:49:26.223Z</updated>
    
    <content type="html"><![CDATA[<ul><li>关于基本的<code>RNN</code>和<code>LSTM</code>的概念和<code>BPTT</code>算法可以<a href="http://lawlite.me/2017/06/14/RNN-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8CLSTM-01%E5%9F%BA%E7%A1%80/">查看这里</a></li><li>参考文章：<ul><li><a href="https://r2rt.com/recurrent-neural-networks-in-tensorflow-i.html" target="_blank" rel="external">https://r2rt.com/recurrent-neural-networks-in-tensorflow-i.html</a></li><li><a href="https://r2rt.com/styles-of-truncated-backpropagation.html" target="_blank" rel="external">https://r2rt.com/styles-of-truncated-backpropagation.html</a></li></ul></li></ul><h1 id="一、源代码实现一个binary例子"><a href="#一、源代码实现一个binary例子" class="headerlink" title="一、源代码实现一个binary例子"></a>一、源代码实现一个<code>binary</code>例子</h1><h2 id="1、例子描述"><a href="#1、例子描述" class="headerlink" title="1、例子描述"></a>1、例子描述</h2><h3 id="1-数据描述"><a href="#1-数据描述" class="headerlink" title="(1) 数据描述"></a>(1) 数据描述</h3><ul><li>输入数据<code>X</code>是<strong>二进制的一串序列</strong>, 在<code>t</code>时刻，有<code>50%</code>的概率是<code>1</code>，<code>50%</code>的概率是<code>0</code>，比如：<code>X=[1,1,0,0,1,0.....]</code> </li></ul><a id="more"></a><ul><li>输出数据<code>Y</code>：<ul><li>在时刻<code>t</code>，<code>50%</code>的概率是<code>1</code>，<code>50%</code>的概率是<code>0</code>；</li><li>如果$X_{t-3}$是<code>1</code>，则$Y_{t}$ <code>100%</code>是<code>1</code>（增加<code>50%</code>）；</li><li>如果$X_{t-8}$是<code>1</code>，则$Y_{t}$ <code>25%</code>是<code>1</code>（减少<code>25%</code>）；<ul><li>所以如果$X_{t-3}$和$X_{t-8}$都是<code>1</code>，则$Y_{t}$ <code>50%+50%-25%=75%</code>的概率是<code>1</code></li></ul></li></ul></li><li>所以，输出数据是有两个<strong>依赖关系</strong>的</li></ul><h3 id="2-损失函数"><a href="#2-损失函数" class="headerlink" title="(2) 损失函数"></a>(2) 损失函数</h3><ul><li>使用<code>cross-entropy</code>损失函数进行训练</li><li>这里例子很简单，<strong>根据数据生成的规则</strong>，我们可以简单的计算一下不同情况下的<code>cross-entropy</code>值</li><li><strong>[1]</strong> 如果<code>rnn</code><strong>没有学到两个依赖关系</strong>, 则最终预测正确的概率是<code>62.5%</code>，<code>cross entropy</code>值为<strong>0.66</strong>计算如下<ul><li>${X_{t-3}}=\{ {\matrix{<br>{1} \rightarrow X_{t-8}=\{  {\matrix{<br>{ 1 \rightarrow 0.5+0.5-0.25=0.75} \cr<br>{ 0 \rightarrow 0.5+0.5=1 \quad \quad \quad \quad} \cr<br>}} \cr<br>{0} \rightarrow X_{t-8} = \{ {\matrix{<br>{1 \rightarrow 0.5-0.25=0.25 } \quad \quad\cr<br>{0 \rightarrow 0.5 \quad \quad \quad \quad \quad \quad \quad \quad} \cr<br>}}\cr<br>}}$</li><li>所以正确预测<code>1</code>的概率为：<code>(0.75+1+0.25+0.5)/4=0.625</code></li><li>所以<code>cross entropy</code>值为：<code>-[plog(p)+(1-p)log(1-p)]=0.66</code></li></ul></li><li><strong>[2]</strong> 如果<code>rnn</code>学到<strong>第一个</strong>依赖关系，<code>50%</code>的情况下预测准确度为<code>87.5%</code>，<code>50%</code>的情况下预测准确度为<code>62.5%</code>，<code>cross entropy</code>值为<strong>0.52</strong><ul><li>因为<code>X</code>是随机生成，<code>0/1</code>各占<code>50%</code>,想象生成了很多的数，根据<strong>大数定律</strong>，<code>50%</code>的情况是<code>1</code>，对应到 <strong>[1]</strong> 中的<strong>上面</strong>的情况就是:<code>(0.75+1)/2=0.875</code>的概率预测正确，其余的<code>50%</code>就和<strong>[1]</strong>中一样了（去除学到的一个依赖，其余就是没有学到依赖）<code>62.5%</code></li><li>损失值：<code>-0.5 * (0.875 * .log(0.875) + 0.125 * log(0.125))-0.5 * (0.625 * np.log(0.625) + 0.375 * log(0.375)))=0.52</code></li></ul></li><li><strong>[3]</strong> 如果<code>rnn</code>两个依赖都学到了，则<code>25%</code>的情况下<code>100%</code>预测正确，<code>25%</code>的情况下<code>50%</code>预测正确，<code>50%</code>的情况向<code>75%</code>预测正确，<code>cross entropy</code>值为<strong>0.45</strong><ul><li><code>1/4</code>的情况就是$X_{t-3}=1 和 X_{t-8}=0$ <code>100%</code>预测正确 </li><li><code>1/4</code>的情况就是$X_{t-3}=0 和 X_{t-8}=0$ <code>50%</code>预测正确</li><li><code>1/2</code>的情况<code>75%</code>预测正确（0.5+0.5-0.25）</li><li>损失值：<code>-0.50 * (0.75 * np.log(0.75) + 0.25 * np.log(0.25)) - 0.25 * (2 * 0.50 * np.log (0.50)) - 0.25 * (0) = 0.45</code></li></ul></li></ul><h2 id="2、网络结构"><a href="#2、网络结构" class="headerlink" title="2、网络结构"></a>2、网络结构</h2><ul><li>根据时刻<code>t</code>的输入向量$X_t$和时刻<code>t-1</code>的状态向量<code>state</code> $S_{t-1}$计算得出当前的状态向量$S_t$和输出的结果概率向量$P_t$</li><li>Label数据是<code>Y</code></li><li><p>所以有：$${S_t} = {tanh(W(X_t \bigoplus S_{t-1})) + b_s}$$ $${P_t = softmax(US_t + b_p)}$$</p><ul><li>这里$\bigoplus$表示<strong>向量的拼接</strong></li><li><p>$W \in R^{d \times (2+d)}, {b_s} \in R^d , U \in R^{2 \times d}, b_p \in R^2$ </p><ul><li><code>d</code>是 <code>state</code> 向量的长度</li><li>W是二维的矩阵，因为是将$X_t 和 S_{t-1}$拼接起来和<strong>W</strong>运算的，<code>2</code>对应输入的<strong>X</strong> <code>one-hot</code>之后，所以是<code>2</code></li><li><code>U</code>是最后输出预测的权值</li></ul></li><li><p>初始化<code>state</code> $S_{-1}$ 为<strong>0向量</strong></p></li></ul></li></ul><p><img src="/assets/blog_images/RNN/RNN_11.png" alt="RNN结构" title="RNN_11"></p><ul><li>需要注意的是 <code>cell</code> 并<strong>不一定是只有一个neuron unit，而是有n个hidden units</strong><ul><li>下图的<code>state size=4</code></li></ul></li></ul><p><img src="/assets/blog_images/RNN/RNN_13.png" alt="RNN详细的结构" title="RNN_13"></p><h2 id="3、Tensorflow中RNN-BPTT实现方式"><a href="#3、Tensorflow中RNN-BPTT实现方式" class="headerlink" title="3、Tensorflow中RNN BPTT实现方式"></a>3、Tensorflow中RNN BPTT实现方式</h2><h4 id="1-截断反向传播（truncated-backpropagation）"><a href="#1-截断反向传播（truncated-backpropagation）" class="headerlink" title="1) 截断反向传播（truncated backpropagation）"></a>1) 截断反向传播（truncated backpropagation）</h4><ul><li>假设我们训练含有<code>1000000</code>个数据的序列，如果<strong>全部训练</strong>的话，整个的序列都feed进<code>RNN</code>中，容易造成梯度消失或爆炸的问题</li><li>所以解决的方法就是<code>truncated backpropagation</code>，我们将序列<strong>截断</strong>来进行训练(<code>num_steps</code>)</li></ul><h3 id="2-tensorflow中的BPTT算法实现"><a href="#2-tensorflow中的BPTT算法实现" class="headerlink" title="2) tensorflow中的BPTT算法实现"></a>2) tensorflow中的BPTT算法实现</h3><ul><li>一般截断的反向传播是：在当前时间<code>t</code>,往前反向传播<code>num_steps</code>步即可<ul><li>如下图，长度为<code>6</code>的序列，截断步数是<code>3</code></li></ul></li></ul><p><img src="/assets/blog_images/RNN/RNN_truncated_bptt_16.png" alt="一般的截断反向传播" title="RNN_truncated_bptt_16"></p><ul><li>但是<code>Tensorflow</code>中的实现并不是这样(如下图)<ul><li>它是将长度为<code>6</code>的序列分为了<strong>两部分</strong>，每一部分长度为<code>3</code></li><li>前一部分计算得到的<code>final state</code>用于下一部分计算的<code>initial state</code></li></ul></li></ul><p><img src="/assets/blog_images/RNN/RNN_truncated_bptt_tensorflow_style17.png" alt="tensorflow 风格的bptt" title="RNN_truncated_bptt_tensorflow_style17"></p><ul><li>所以<code>tensorflow</code>风格的反向传播并没有有效的反向传播<code>num_steps</code>步(对比一般的方式，依赖关系变的弱一些)<ul><li>所以比如想要学习有<code>8</code>依赖关系的序列（我们的例子中就是），<strong>一般要设置的大于8</strong></li></ul></li><li>另外，有人做实验比较了两种方式<a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.56.7941&amp;rep=rep1&amp;type=pdf" target="_blank" rel="external">here</a>，发现一般的实现方式中的<code>n</code>步和<code>Tensorflow</code>中截断设置为<code>2n</code>的<strong>结果相似</strong></li></ul><h3 id="3-关于这个例子，tensorflow风格的实现"><a href="#3-关于这个例子，tensorflow风格的实现" class="headerlink" title="3) 关于这个例子，tensorflow风格的实现"></a>3) 关于这个例子，tensorflow风格的实现</h3><ul><li>如下图，<code>num_steps=5, state_size=4</code>，就是<strong>截断反向传播的步数</strong>truncated backprop steps是<code>5</code>步，<code>state_size</code>就是<code>cell</code>中的神经元的个数</li><li>如果需要截断的步数增多，可以适当增加<code>state_size</code>来记录更多的信息<ul><li>好比传统的神经网络，就是增加隐藏层的神经元个数</li></ul></li><li>途中的注释是下面的列子代码中定义变量的<code>shape</code>, 可以对照参考</li></ul><p><img src="/assets/blog_images/RNN/RNN_14.jpg" alt="例子，num_steps=5,state_size=4" title="RNN_14"></p><h2 id="4、自己实现例子中的RNN"><a href="#4、自己实现例子中的RNN" class="headerlink" title="4、自己实现例子中的RNN"></a>4、自己实现例子中的<code>RNN</code></h2><ul><li>全部代码：<a href="https://github.com/lawlite19/Blog-Back-Up/blob/master/code/rnn/rnn_implement.py" target="_blank" rel="external">https://github.com/lawlite19/Blog-Back-Up/blob/master/code/rnn/rnn_implement.py</a></li></ul><h3 id="1-实现过程"><a href="#1-实现过程" class="headerlink" title="1) 实现过程"></a>1) 实现过程</h3><ul><li>导入包：</li></ul><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">import numpy as np</div><div class="line">import tensorflow as tf</div><div class="line">from tensorflow<span class="selector-class">.python</span> import debug as tf_debug</div><div class="line">import matplotlib<span class="selector-class">.pyplot</span> as plt</div></pre></td></tr></table></figure><ul><li><p>超参数</p><ul><li>这里<code>num_steps=5</code>就是只能记忆<strong>5步</strong>, 所以只能学习到一个依赖(因为至少8步才能学到第二个依赖)，我们看结果最后的<code>cross entropy</code>是否在<code>0.52</code>左右<figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="string">''</span><span class="string">'超参数'</span><span class="string">''</span></div><div class="line">num_steps = <span class="number">5</span></div><div class="line">batch_size = <span class="number">200</span></div><div class="line">num_classes = <span class="number">2</span></div><div class="line">state_size = <span class="number">4</span></div><div class="line">learning_rate = <span class="number">0.1</span></div></pre></td></tr></table></figure></li></ul></li><li><p>生成数据</p><ul><li>就是按照我们描述的规则</li></ul></li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line">'''生成数据</div><div class="line">就是按照文章中提到的规则，这里生成1000000个</div><div class="line">'''</div><div class="line">def gen_data(size=1000000):</div><div class="line">    X = np.array(np.random.choice(2, size=(size,)))</div><div class="line">    Y = []</div><div class="line">    '''根据规则生成Y'''</div><div class="line">    for i in range(size):   </div><div class="line">        threshold = 0.5</div><div class="line">        if X[i-3] == 1:</div><div class="line">            threshold += 0.5</div><div class="line">        if X[i-8] == 1:</div><div class="line">            threshold -=0.25</div><div class="line">        if np.random.rand() &gt; threshold:</div><div class="line">            Y.append(0)</div><div class="line">        else:</div><div class="line">            Y.append(1)</div><div class="line">    return X, np.array(Y)</div></pre></td></tr></table></figure><ul><li>生成<code>batch</code>数据，因为我们使用<code>sgd</code>训练</li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line">'''生成batch数据'''</div><div class="line">def gen_batch(raw_data, batch_size, num_step):</div><div class="line">    raw_x, raw_y = raw_data</div><div class="line">    data_length = len(raw_x)</div><div class="line">    batch_patition_length = data_length // batch_size                         # -&gt;5000</div><div class="line">    data_x = np.zeros([batch_size, batch_patition_length], dtype=np.int32)    # -&gt;(200, 5000)</div><div class="line">    data_y = np.zeros([batch_size, batch_patition_length], dtype=np.int32)    # -&gt;(200, 5000)</div><div class="line">    '''填到矩阵的对应位置'''</div><div class="line">    for i in range(batch_size):</div><div class="line">        data_x[i] = raw_x[batch_patition_length*i:batch_patition_length*(i+1)]# 每一行取batch_patition_length个数，即5000</div><div class="line">        data_y[i] = raw_y[batch_patition_length*i:batch_patition_length*(i+1)]</div><div class="line">    epoch_size = batch_patition_length // num_steps                           # -&gt;5000/5=1000 就是每一轮的大小</div><div class="line">    for i in range(epoch_size):   # 抽取 epoch_size 个数据</div><div class="line">        x = data_x[:, i * num_steps:(i + 1) * num_steps]                      # -&gt;(200, 5)</div><div class="line">        y = data_y[:, i * num_steps:(i + 1) * num_steps]</div><div class="line">        yield (x, y)    # yield 是生成器，生成器函数在生成值后会自动挂起并暂停他们的执行和状态（最后就是for循环结束后的结果，共有1000个(x, y)）</div><div class="line">def gen_epochs(n, num_steps):</div><div class="line">    for i in range(n):</div><div class="line">        yield gen_batch(gen_data(), batch_size, num_steps)</div></pre></td></tr></table></figure><ul><li><p>定义RNN的输入</p><ul><li>这里每个数需要<code>one-hot</code>处理</li><li><code>unstack</code>方法就是将<code>n</code>维的数据拆成若开个<code>n-1</code>的数据，<code>axis</code>指定根据哪个维度拆的，比如<code>(200,5,2)</code>三维数据，按<code>axis=1</code>会有<code>5</code>个<code>(200,2)</code>的二维数据<figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="string">''</span><span class="string">'定义placeholder'</span><span class="string">''</span></div><div class="line">x = tf.placeholder(tf<span class="selector-class">.int32</span>, [batch_size, num_steps], name=<span class="string">"x"</span>)</div><div class="line">y = tf.placeholder(tf<span class="selector-class">.int32</span>, [batch_size, num_steps], name=<span class="string">'y'</span>)</div><div class="line">init_state = tf.zeros([batch_size, state_size])</div><div class="line"><span class="string">''</span><span class="string">'RNN输入'</span><span class="string">''</span></div><div class="line">x_one_hot = tf.one_hot(x, num_classes)</div><div class="line">rnn_inputs = tf.unstack(x_one_hot, axis=<span class="number">1</span>)</div></pre></td></tr></table></figure></li></ul></li><li><p>定义<code>RNN</code>的<code>cell</code>（<strong>关键步骤</strong>）</p><ul><li>这里关于<code>name_scope</code>和<code>variable_scope</code>的用法可以<a href="https://morvanzhou.github.io/tutorials/machine-learning/tensorflow/5-12-scope/" target="_blank" rel="external">查看这里</a><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">'''定义RNN cell'''</div><div class="line">with tf.variable_scope('rnn_cell'):</div><div class="line">    W = tf.get_variable('W', [num_classes + state_size, state_size])</div><div class="line">    b = tf.get_variable('b', [state_size], initializer=tf.constant_initializer(0.0))</div><div class="line">    </div><div class="line">def rnn_cell(rnn_input, state):</div><div class="line">    with tf.variable_scope('rnn_cell', reuse=True):</div><div class="line">        W = tf.get_variable('W', [num_classes+state_size, state_size])</div><div class="line">        b = tf.get_variable('b', [state_size], initializer=tf.constant_initializer(0.0))</div><div class="line">    return tf.tanh(tf.matmul(tf.concat([rnn_input, state],1),W) + b)</div></pre></td></tr></table></figure></li></ul></li><li><p>将<code>cell</code>添加到计算图中</p></li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">'''将rnn cell添加到计算图中'''</div><div class="line">state = init_state</div><div class="line">rnn_outputs = []</div><div class="line">for rnn_input in rnn_inputs:</div><div class="line">    state = rnn_cell(rnn_input, state)  # state会重复使用，循环</div><div class="line">    rnn_outputs.append(state)</div><div class="line">final_state = rnn_outputs[-1]        # 得到最后的state</div></pre></td></tr></table></figure><ul><li><p>定义预测，损失函数，和优化方法</p><ul><li><code>sparse_softmax_cross_entropy_with_logits</code>会自动<code>one-hot</code><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="string">''</span><span class="string">'预测，损失，优化'</span><span class="string">''</span></div><div class="line">with tf.variable_scope(<span class="string">'softmax'</span>):</div><div class="line">    W = tf.get_variable(<span class="string">'W'</span>, [state_size, num_classes])</div><div class="line">    <span class="selector-tag">b</span> = tf.get_variable(<span class="string">'b'</span>, [num_classes], initializer=tf.constant_initializer(<span class="number">0.0</span>))</div><div class="line">logits = [tf.matmul(rnn_output, W) + <span class="selector-tag">b</span> <span class="keyword">for</span> rnn_output <span class="keyword">in</span> rnn_outputs]</div><div class="line">predictions = [tf<span class="selector-class">.nn</span><span class="selector-class">.softmax</span>(logit) <span class="keyword">for</span> logit <span class="keyword">in</span> logits]</div><div class="line"></div><div class="line">y_as_list = tf.unstack(y, num=num_steps, axis=<span class="number">1</span>)</div><div class="line">losses = [tf<span class="selector-class">.nn</span><span class="selector-class">.sparse_softmax_cross_entropy_with_logits</span>(labels=<span class="selector-tag">label</span>,logits=logit) <span class="keyword">for</span> logit, <span class="selector-tag">label</span> <span class="keyword">in</span> zip(logits, y_as_list)]</div><div class="line">total_loss = tf.reduce_mean(losses)</div><div class="line">train_step = tf<span class="selector-class">.train</span><span class="selector-class">.AdagradOptimizer</span>(learning_rate).minimize(total_loss)</div></pre></td></tr></table></figure></li></ul></li><li><p>训练网络</p></li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line">'''训练网络'''</div><div class="line">def train_rnn(num_epochs, num_steps, state_size=4, verbose=True):</div><div class="line">    with tf.Session() as sess:</div><div class="line">        sess.run(tf.global_variables_initializer())</div><div class="line">        #sess = tf_debug.LocalCLIDebugWrapperSession(sess)</div><div class="line">        training_losses = []</div><div class="line">        for idx, epoch in enumerate(gen_epochs(num_epochs, num_steps)):</div><div class="line">            training_loss = 0</div><div class="line">            training_state = np.zeros((batch_size, state_size))   # -&gt;(200, 4)</div><div class="line">            if verbose:</div><div class="line">                print('\nepoch', idx)</div><div class="line">            for step, (X, Y) in enumerate(epoch):</div><div class="line">                tr_losses, training_loss_, training_state, _ = \</div><div class="line">                    sess.run([losses, total_loss, final_state, train_step], feed_dict=&#123;x:X, y:Y, init_state:training_state&#125;)</div><div class="line">                training_loss += training_loss_</div><div class="line">                if step % 100 == 0 and step &gt; 0:</div><div class="line">                    if verbose:</div><div class="line">                        print('第 &#123;0&#125; 步的平均损失 &#123;1&#125;'.format(step, training_loss/100))</div><div class="line">                    training_losses.append(training_loss/100)</div><div class="line">                    training_loss = 0</div><div class="line">    return training_losses</div></pre></td></tr></table></figure><ul><li>显示结果</li></ul><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">training_losses = train_rnn(num_epochs=<span class="number">1</span>, num_steps=num_steps, state_size=state_size)</div><div class="line"><span class="function"><span class="title">print</span><span class="params">(training_losses[<span class="number">0</span>])</span></span></div><div class="line">plt.plot(training_losses)</div><div class="line">plt.show()</div></pre></td></tr></table></figure><h3 id="2-实验结果"><a href="#2-实验结果" class="headerlink" title="2) 实验结果"></a>2) 实验结果</h3><ul><li><code>num_steps=5, state=4</code><ul><li>可以看到初试的损失值大约<code>0.66</code>, 最后学到一个依赖关系，最终损失值<code>0.52</code>左右</li></ul></li></ul><p><img src="/assets/blog_images/RNN/RNN_14.png" alt="num_step=5结果，只学到一个依赖" title="RNN_14"></p><ul><li><code>num_step=10, state=16</code><ul><li>学到了两个依赖，最终损失值接近<code>0.45</code></li></ul></li></ul><p><img src="/assets/blog_images/RNN/RNN_15.png" alt="num_step=10,学到两个依赖" title="RNN_15"></p><h2 id="5、使用Tensorflow的cell实现"><a href="#5、使用Tensorflow的cell实现" class="headerlink" title="5、使用Tensorflow的cell实现"></a>5、使用Tensorflow的cell实现</h2><h3 id="1-使用static-rnn方式"><a href="#1-使用static-rnn方式" class="headerlink" title="1) 使用static rnn方式"></a>1) 使用<code>static rnn</code>方式</h3><ul><li>将我们之前自己实现的<code>cell</code>和添加到计算图中步骤改为如下即可</li></ul><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">cell = tf<span class="selector-class">.contrib</span><span class="selector-class">.rnn</span><span class="selector-class">.BasicRNNCell</span>(num_units=state_size)</div><div class="line">rnn_outputs, final_state = tf<span class="selector-class">.contrib</span><span class="selector-class">.rnn</span><span class="selector-class">.static_rnn</span>(cell=cell, inputs=rnn_inputs, </div><div class="line">                                                    initial_state=init_state)</div></pre></td></tr></table></figure><h3 id="2-使用dynamic-rnn方式"><a href="#2-使用dynamic-rnn方式" class="headerlink" title="2) 使用dynamic_rnn方式"></a>2) 使用<code>dynamic_rnn</code>方式</h3><ul><li><p>这里仅仅替换<code>cell</code>就不行了，<code>RNN</code>输入</p><ul><li>直接就是<strong>三维</strong>的形式<figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="string">''</span><span class="string">'RNN输入'</span><span class="string">''</span></div><div class="line">rnn_inputs = tf.one_hot(x, num_classes)</div></pre></td></tr></table></figure></li></ul></li><li><p>使用<code>dynamic_rnn</code></p></li></ul><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">cell = tf<span class="selector-class">.contrib</span><span class="selector-class">.rnn</span><span class="selector-class">.BasicRNNCell</span>(num_units=state_size)</div><div class="line">rnn_outputs, final_state = tf<span class="selector-class">.nn</span><span class="selector-class">.dynamic_rnn</span>(cell, rnn_inputs, initial_state=init_state)</div></pre></td></tr></table></figure><ul><li>预测，损失<ul><li>由于<code>rnn_inputs</code>是三维的，所以先<strong>转成二维的</strong>，计算结束后再转换回三维<code>[batch_size, num_steps, num_classes]</code><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">    '''因为rnn_outputs是三维的，这里需要将其转成2维的，</div><div class="line">       矩阵运算后再转换回来[batch_size, num_steps, num_classes]'''</div><div class="line">logits = tf.reshape(tf.matmul(tf.reshape(rnn_outputs, [-1, state_size]), W) +b, \</div><div class="line">                    shape=[batch_size, num_steps, num_classes])</div><div class="line">predictions = tf.nn.softmax(logits)</div><div class="line"></div><div class="line">y_as_list = tf.unstack(y, num=num_steps, axis=1)</div><div class="line">losses = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,logits=logits)</div><div class="line">total_loss = tf.reduce_mean(losses)</div><div class="line">train_step = tf.train.AdagradOptimizer(learning_rate).minimize(total_loss)</div></pre></td></tr></table></figure></li></ul></li></ul><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul><li><a href="https://r2rt.com/recurrent-neural-networks-in-tensorflow-i.html" target="_blank" rel="external">https://r2rt.com/recurrent-neural-networks-in-tensorflow-i.html</a></li><li><a href="https://r2rt.com/styles-of-truncated-backpropagation.html" target="_blank" rel="external">https://r2rt.com/styles-of-truncated-backpropagation.html</a></li><li><a href="https://web.stanford.edu/class/psych209a/ReadingsByDate/02_25/Williams%20Zipser95RecNets.pdf" target="_blank" rel="external">https://web.stanford.edu/class/psych209a/ReadingsByDate/02_25/Williams%20Zipser95RecNets.pdf</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;ul&gt;
&lt;li&gt;关于基本的&lt;code&gt;RNN&lt;/code&gt;和&lt;code&gt;LSTM&lt;/code&gt;的概念和&lt;code&gt;BPTT&lt;/code&gt;算法可以&lt;a href=&quot;http://lawlite.me/2017/06/14/RNN-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8CLSTM-01%E5%9F%BA%E7%A1%80/&quot;&gt;查看这里&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;参考文章：&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://r2rt.com/recurrent-neural-networks-in-tensorflow-i.html&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://r2rt.com/recurrent-neural-networks-in-tensorflow-i.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://r2rt.com/styles-of-truncated-backpropagation.html&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://r2rt.com/styles-of-truncated-backpropagation.html&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&quot;一、源代码实现一个binary例子&quot;&gt;&lt;a href=&quot;#一、源代码实现一个binary例子&quot; class=&quot;headerlink&quot; title=&quot;一、源代码实现一个binary例子&quot;&gt;&lt;/a&gt;一、源代码实现一个&lt;code&gt;binary&lt;/code&gt;例子&lt;/h1&gt;&lt;h2 id=&quot;1、例子描述&quot;&gt;&lt;a href=&quot;#1、例子描述&quot; class=&quot;headerlink&quot; title=&quot;1、例子描述&quot;&gt;&lt;/a&gt;1、例子描述&lt;/h2&gt;&lt;h3 id=&quot;1-数据描述&quot;&gt;&lt;a href=&quot;#1-数据描述&quot; class=&quot;headerlink&quot; title=&quot;(1) 数据描述&quot;&gt;&lt;/a&gt;(1) 数据描述&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;输入数据&lt;code&gt;X&lt;/code&gt;是&lt;strong&gt;二进制的一串序列&lt;/strong&gt;, 在&lt;code&gt;t&lt;/code&gt;时刻，有&lt;code&gt;50%&lt;/code&gt;的概率是&lt;code&gt;1&lt;/code&gt;，&lt;code&gt;50%&lt;/code&gt;的概率是&lt;code&gt;0&lt;/code&gt;，比如：&lt;code&gt;X=[1,1,0,0,1,0.....]&lt;/code&gt; &lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="Python" scheme="http://lawlite.me/tags/Python/"/>
    
      <category term="DeepLearning" scheme="http://lawlite.me/tags/DeepLearning/"/>
    
      <category term="RNN" scheme="http://lawlite.me/tags/RNN/"/>
    
  </entry>
  
  <entry>
    <title>RNN-循环神经网络和LSTM_01基础</title>
    <link href="http://lawlite.me/2017/06/14/RNN-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8CLSTM-01%E5%9F%BA%E7%A1%80/"/>
    <id>http://lawlite.me/2017/06/14/RNN-循环神经网络和LSTM-01基础/</id>
    <published>2017-06-14T15:42:32.000Z</published>
    <updated>2017-06-25T08:49:35.108Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一、介绍"><a href="#一、介绍" class="headerlink" title="一、介绍"></a>一、介绍</h1><h2 id="1、什么是RNN"><a href="#1、什么是RNN" class="headerlink" title="1、什么是RNN"></a>1、什么是RNN</h2><ul><li>传统的神经网络是<strong>层与层</strong>之间是<strong>全连接</strong>的，但是每层之间的神经元是没有连接的（其实是假设各个数据之间是<strong>独立的</strong>）<ul><li>这种结构不善于处理<strong>序列化的问题</strong>。比如要预测句子中的下一个单词是什么，这往往与前面的单词有很大的关联，因为句子里面的单词并不是独立的。</li></ul></li><li><code>RNN</code> 的结构说明当前的的输出与前面的输出也有关，即隐层之间的节点不再是无连接的，而是有连接的<ul><li>基本的结构如图，可以看到有个<strong>循环的结构</strong>，将其展开就是右边的结构</li></ul></li></ul><a id="more"></a><p>  <img src="/assets/blog_images/RNN/rnn_01.jpg" alt="RNN基本结构" title="rnn_01"></p><h2 id="2、运算说明"><a href="#2、运算说明" class="headerlink" title="2、运算说明"></a>2、运算说明</h2><ul><li>如上图，<strong>输入单元</strong>(<code>inputs units</code>): $\{ {x_0},{x_1}, \cdots  \cdots ,{x_t},{x_{t + 1}}, \cdots  \cdots \}$, <ul><li><strong>输出单元</strong>(output units)为：$\{ {o_0},{o_1}, \cdots  \cdots ,{o_t},{o_{t + 1}}, \cdots  \cdots \}$, </li><li><strong>隐藏单元</strong>(hidden units)输出集: $\{ {s_0},{s_1}, \cdots  \cdots ,{ost},{s_{t + 1}}, \cdots  \cdots \}$</li></ul></li><li>时间 <code>t</code> 隐层单元的输出为：${s_t} = f(U{x_t} + W{s_{t - 1}})$<ul><li><code>f</code>就是激励函数，一般是<code>sigmoid,tanh, relu</code>等</li><li>计算${s_{0}}$时，即第一个的隐藏层状态，需要用到${s_{-1}}$，但是其并不存在，在实现中一般置为<strong>0向量</strong></li><li>（如果将上面的竖着立起来，其实很像传统的神经网络，哈哈）</li></ul></li><li>时间 <code>t</code> 的输出为：${o_t}=Softmax(V{s_t})$<ul><li>可以认为隐藏层状态${s_t}$是<strong>网络的记忆单元</strong>. ${s_t}$包含了前面所有步的隐藏层状态。而输出层的输出${o_t}$只与当前步的${s_t}$有关。</li><li>（在实践中，为了降低网络的复杂度，往往${s_t}$只包含<strong>前面若干步</strong>而不是所有步的隐藏层状态）</li></ul></li><li>在<code>RNNs</code>中，每输入一步，每一层都<strong>共享参数</strong><code>U,V,W</code>，（因为是将循环的部分展开，天然应该相等）</li><li><code>RNNs</code>的关键之处在于<strong>隐藏层</strong>，隐藏层能够捕捉序列的信息。</li></ul><h2 id="3、应用方面"><a href="#3、应用方面" class="headerlink" title="3、应用方面"></a>3、应用方面</h2><ul><li>循环神经网络(<code>Recurrent Neural Networks，RNNs</code>)已经在众多自然语言处理(<code>Natural Language Processing, NLP</code>)中取得了巨大成功以及广泛应用。目前使用最广泛最成功的模型便是<code>LSTMs</code>(Long Short-Term Memory，长短时记忆模型)模型<h3 id="1-语言模型和文本生成"><a href="#1-语言模型和文本生成" class="headerlink" title="(1) 语言模型和文本生成"></a>(1) 语言模型和文本生成</h3></li><li>给定一个单词序列，根据前面的单词预测下面单词的可能性</li><li>也可以根据概率生成新的词</li><li>这里给出了3篇论文<ul><li><a href="http://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf" target="_blank" rel="external">Recurrent neural network based language model</a></li><li><a href="http://www.fit.vutbr.cz/research/groups/speech/publi/2011/mikolov_icassp2011_5528.pdf" target="_blank" rel="external">Extensions of Recurrent neural network based language model</a></li><li><a href="http://machinelearning.wustl.edu/mlpapers/paper_files/ICML2011Sutskever_524.pdf" target="_blank" rel="external">Generating Text with Recurrent Neural Networks</a><h3 id="2-机器翻译"><a href="#2-机器翻译" class="headerlink" title="(2) 机器翻译"></a>(2) 机器翻译</h3></li></ul></li><li>和上面的语言模型很像，只不过是根据一段过生成另外的一段话</li><li>注意的是开始的输出是在全部输入结束后生成的</li><li>一些论文<ul><li><a href="http://www.aclweb.org/anthology/P14-1140.pdf" target="_blank" rel="external">A Recursive Recurrent Neural Network for Statistical Machine Translation</a></li><li><a href="http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf" target="_blank" rel="external">Sequence to Sequence Learning with Neural Networks</a></li></ul></li></ul><p><img src="/assets/blog_images/RNN/RNN_07.png" alt="机器翻译" title="RNN_07"></p><h3 id="3-语音识别"><a href="#3-语音识别" class="headerlink" title="(3) 语音识别"></a>(3) 语音识别</h3><ul><li>论文<ul><li><a href="http://proceedings.mlr.press/v32/graves14.pdf" target="_blank" rel="external">Towards End-to-End Speech Recognition with Recurrent Neural Networks</a><h3 id="4-图像描述生成"><a href="#4-图像描述生成" class="headerlink" title="(4) 图像描述生成"></a>(4) 图像描述生成</h3></li></ul></li><li>根据图像，生成一段描述图像的话</li><li>需要和<code>CNN</code>结合使用</li></ul><h1 id="二、结构"><a href="#二、结构" class="headerlink" title="二、结构"></a>二、结构</h1><h2 id="1、One-to-One"><a href="#1、One-to-One" class="headerlink" title="1、One to One"></a>1、One to One</h2><ul><li>即一个输入对应一个输出，就是上面的图<h2 id="2、Many-to-One"><a href="#2、Many-to-One" class="headerlink" title="2、Many to One"></a>2、Many to One</h2></li><li>即多个输入对应一个输出，比如<strong>情感分析</strong>，一段话中很多次，判断这段话的情感  </li><li>其中$x_{1},x_{2},\ldots,x_{t}$表示句子中的<code>t</code>个词，<code>o</code>代表最终输出的情感标签</li><li>前向计算就是：$$f(x)=Vs_{t}=V(Ux_{t}+Ws_{t-1})=V(Ux_{t}+W(Ux_{t-1}+Ws_{t-2}))\cdots$$<br><img src="/assets/blog_images/RNN/RNN_02.png" alt="Many to one" title="RNN_02"><h2 id="3、One-to-Many"><a href="#3、One-to-Many" class="headerlink" title="3、One to Many"></a>3、One to Many</h2></li><li>前向计算类似，不再给出<br><img src="/assets/blog_images/RNN/RNN_03.png" alt="One to Many" title="RNN_03"><h2 id="4、Many-to-Many"><a href="#4、Many-to-Many" class="headerlink" title="4、Many to Many"></a>4、Many to Many</h2></li><li>前向计算类似，不再给出<br><img src="/assets/blog_images/RNN/RNN_04.png" alt="Many to Many" title="RNN_04"><h2 id="5、双向RNN（Bidirectional-RNN）"><a href="#5、双向RNN（Bidirectional-RNN）" class="headerlink" title="5、双向RNN（Bidirectional RNN）"></a>5、双向RNN（Bidirectional RNN）</h2></li><li>比如翻译问题往往需要联系<strong>上下文内容</strong>才能正确的翻译，我们上面的结构线性传递允许“联系上文”，但是联系下文并没有，所以就有<strong>双向RNN</strong></li><li>前向运算稍微复杂一点，以<code>t</code>时刻为例<br>$<br>o_{t} = W_t^{(os)}s_t + W_t^{(oh)}h_t \\<br>\quad = W_t^{(os)} (W_{t-1}^{(ss)} s_{t-1} + W_{t}^{(sx)} x_{t-1}) + W_t^{(oh)} (W_t^{(hh)} h_{t+1} + W_t^{(hx)}x_t)<br>$<br><img src="/assets/blog_images/RNN/RNN_05.png" alt="RNN" title="RNN_05"></li></ul><h2 id="6、深层的RNN"><a href="#6、深层的RNN" class="headerlink" title="6、深层的RNN"></a>6、深层的RNN</h2><ul><li>上面的结构都是只含有一层的<code>state</code>层，根据传统NN和CNN，深层次的结构有更加号的效果，结构如图<br><img src="/assets/blog_images/RNN/RNN_06.png" alt="深层的RNN" title="RNN_06"></li></ul><h1 id="三、Back-Propagation-Through-Time-BPTT-训练"><a href="#三、Back-Propagation-Through-Time-BPTT-训练" class="headerlink" title="三、Back Propagation Through Time(BPTT)训练"></a>三、Back Propagation Through Time(BPTT)训练</h1><ul><li>关于传统神经网络BP算法可以<a href="https://github.com/lawlite19/MachineLearning_Python" target="_blank" rel="external">查看这里</a>神经网络部分的推导<h2 id="1、符号等说明"><a href="#1、符号等说明" class="headerlink" title="1、符号等说明"></a>1、符号等说明</h2></li><li>以下图为例</li></ul><p><img src="/assets/blog_images/RNN/RNN_09.png" alt="RNN基本结构" title="RNN_09"></p><ul><li>符号说明<ul><li>$\phi$………………………………………………隐藏层的激励函数</li><li>$\varphi$………………………………………………输出层的变换函数</li><li>$L_{t} = L_{t}\left( o_{t},y_{t} \right)$……………………………模型的损失函数<ul><li>标签数据$y_{t}$是一个 <code>one-hot</code> 向量</li></ul></li></ul></li></ul><h2 id="2、反向传播过程"><a href="#2、反向传播过程" class="headerlink" title="2、反向传播过程"></a>2、反向传播过程</h2><ul><li>接受完序列中所有样本后再<strong>统一计算损失</strong>，此时模型的总损失可以表示为（假设输入序列长度为<code>n</code>）：$$L = \sum_{t = 1}^{n}L_{t}$$<br><img src="/assets/blog_images/RNN/RNN_10.png" alt="RNN" title="RNN_10"></li><li>$o_{t} = \varphi(Vs_t) = \varphi(V(Ux_t + Ws_{t-1}))$<ul><li>其中$s_{0} = \mathbf{0 =}( 0,0,\ldots,0 )^{T}$</li></ul></li><li>令：${o_{t}^* = Vs_{t}}, \quad {s_{t}^{*} = Ux_{t} + Ws_{t - 1}}…………(1)$   (就是没有经过激励函数和变换函数前)<ul><li>则：$o_{t} = \varphi( o_{t}^*)$</li><li>$s_{t} = \phi(s_{t}^{*})$</li></ul></li></ul><h3 id="1-矩阵V的更新"><a href="#1-矩阵V的更新" class="headerlink" title="(1) 矩阵V的更新"></a>(1) 矩阵V的更新</h3><ul><li>对<strong>矩阵 V</strong> 的更新过程,根据(1)式可得， (和传统的神经网络一致，根据求导的<strong>链式法则</strong>):<ul><li>$${{{\partial {L_t} \over \partial o_t^{\ast}}} = {{\partial L_t \over \partial o_t } \ast {\partial o_t \over \partial o_{t}^{\ast} }} = {{\partial L_t \over \partial o_t} \ast \varphi ^{'} (o_t^{\ast})}}$$</li><li>$${{{\partial L_t} \over {\partial V }}} = {{\partial L_t \over \partial Vs_t} } \ast {{\partial Vs_t \over \partial V}} = {{\partial L_t \over \partial o_t^\ast}} \times s_t^T = ({{\partial L_t \over \partial o_t} \ast \varphi ^{'} (o_{t}^\ast)}) \times s_t^T$$- 因为${L = \sum_{t = 1}^{n}L_{t}}$，所以对矩阵V的更新对应的导数:<br>$${{\partial L \over \partial V} = {\sum\limits_{t=1}^n ({\partial L_t \over \partial o_t} \ast \varphi ^{'} (o_t^\ast)) \times s_t^T}}$$</li></ul></li></ul><h3 id="2-矩阵U和W的更新"><a href="#2-矩阵U和W的更新" class="headerlink" title="(2) 矩阵U和W的更新"></a>(2) 矩阵U和W的更新</h3><ul><li><code>RNN</code> 的 <code>BP</code> 算法的主要难点在于它 <code>State</code> 之间的通信</li><li>可以采用循环的方法来计算各个梯度，<code>t</code>应从<code>n</code>开始降序循环至 <code>1</code></li><li>计算时间通道上的局部梯度（同样根据链式法则）<br>$$ {{\partial L_t \over \partial s_t^{\ast}}} = {{\partial L_t \over \partial Vs_t}} \times {{\partial s_t^{T} V_t^{T} \over \partial s_t}} \ast {{\partial s_t \over \partial s_t^{\ast}}}  = V^T \times ({{\partial L_t \over \partial o_t}} * {\varphi ^{‘} (o_t^{\ast}))} $$</li></ul><p>$$ {{\partial L_t \over \partial s_{k-1}^\ast}} ={{\partial s_k^\ast \over \partial s_{k-1}^\ast}} \times {{\partial L_t \over \partial s_{k}^\ast}} = W_T \times ({{\partial L_t \over \partial s_k^\ast} * {\phi ^{'} (s_{k-1}^\ast)}}) , (k=1,……,t) ………(2)$$</p><ul><li>利用局部梯度计算<code>U</code>和<code>W</code>的梯度<ul><li>这里累加是因为权值是共享的，所以往前推算一直用的是一样的权值<br>$${\partial L_t \over \partial U} + = {\sum\limits_{k=1}^t {\partial L_t \over \partial s_k^\ast} \times {\partial s_k^\ast \over \partial U}} = {\sum\limits_{k=1}^t {\partial L_t\over \partial s_k^\ast}} \times x_t^T $$<br>$${\partial L_t \over \partial W} + = {\sum\limits_{k=1}^t {\partial L_t \over \partial s_k^\ast} \times {\partial s_k^\ast \over \partial W}} = {\sum\limits_{k=1}^t {\partial L_t\over \partial s_k^\ast}} \times s_{t-1}^T ………………..(3)$$</li></ul></li></ul><h2 id="3、训练问题"><a href="#3、训练问题" class="headerlink" title="3、训练问题"></a>3、训练问题</h2><ul><li>从<strong> 公式(2)和(3) </strong>中可以看出，时间维度上的权重<code>W</code>更新需要计算$\phi^{‘} (s_k^{\ast})$，即经过激励函数的导数</li><li>如果时间维度上很长，则这个梯度是<strong>累积</strong>的，所以造成<strong>梯度消失或爆炸</strong><ul><li>可以想象将结构图竖起来，就是一个深层的神经网络，所以容易出现梯度问题</li><li>关于梯度消失的问题可以查看我<a href="http://lawlite.me/2016/12/20/%E8%AE%BA%E6%96%87%E8%AE%B0%E5%BD%95-UnderstandingTheDifficultyOfTrainingDeepFeedforwardNeuralNetworks/">这里一遍博客</a></li></ul></li><li><code>RNN</code> 主要的作用就是能够记住之前的信息，但是<strong>梯度消失</strong>的问题又告诉我们不能记住太久之前的信息，改进的思路有两点<ul><li>一是使用一些<code>trick</code>,比如合适的激励函数，初始化，BN等等</li><li>二是改进<code>state</code>的传递方式，比如就是下面提及的<code>LSTM</code><ul><li>关于为何 <code>LSTMs</code> 能够解决梯度消失，直观上来说就是<strong>上方时间通道</strong>是简单的<strong>线性组合</strong></li></ul></li></ul></li></ul><h1 id="四、Long-Short-Term-Memory-LSTM，长短时记忆网络"><a href="#四、Long-Short-Term-Memory-LSTM，长短时记忆网络" class="headerlink" title="四、Long Short-Term Memory(LSTM，长短时记忆网络)"></a>四、Long Short-Term Memory(LSTM，长短时记忆网络)</h1><h2 id="1、介绍"><a href="#1、介绍" class="headerlink" title="1、介绍"></a>1、介绍</h2><ul><li><code>LSTM</code> 是一般 <code>RNN</code> 的升级，因为一些序列问题，我们可能需要忘记一些东西， <code>LSTM</code> 和普通 <code>RNN</code> 相比, 多出了三个<strong>控制器</strong>. (输入控制, 输出控制, 忘记控制)</li><li>在<code>LSTM</code>里，这个叫做<code>cell</code>（其实就是前面的<code>state</code>,只是这里更加复杂了）, 可以看作一个黑盒，这个<code>cell</code>结合前面<code>cell</code>的输出$h_{t-1}$和当前的输入$x_{t}$来决定是否记忆下来，该网络结构在对<strong>长序列依赖问题</strong>中非常有效</li></ul><h2 id="2、结构"><a href="#2、结构" class="headerlink" title="2、结构"></a>2、结构</h2><ul><li>一个经典的<code>cell</code>结构如下图<ul><li>$\phi_{1} $是<code>sigmoid</code>函数，$\phi_{2}$ 是<code>tanh</code>函数</li><li><code>*</code>表示 <code>element wise</code> 乘法(就是点乘)，使用<code>X</code>表示矩阵乘法</li></ul></li><li><code>LSTMs</code> 的 <code>cell</code> 的时间通道有<strong>两条</strong>。<ul><li>上方的时间通道（$h^{\left( {old} \right)} \rightarrow h^{\left( {new} \right)}$）仅包含了两个<strong>代数运算</strong>,这意味着它信息传递的方式会更为<strong>直接</strong> $$h^{(new)} = h^{(old)}*r_1 + r_2$$</li><li>位于下方的时间通道（$s^{\left( {old} \right)} \rightarrow s^{\left( {new} \right)}$）则运用了大量的<strong>层结构</strong>,在 <code>LSTMs</code> 中，我们通常称这些层结构为门（<code>Gates</code>）</li></ul></li></ul><p><img src="/assets/blog_images/RNN/RNN_LSTM_08.png" alt="LSTM cell结构" title="RNN_LSTM_08"></p><h2 id="3、运算说明"><a href="#3、运算说明" class="headerlink" title="3、运算说明"></a>3、运算说明</h2><ul><li><code>Sigmoid</code> 函数取值区间为 <code>0-1</code>，那么当 <code>Sigmoid</code> 对应的层结构输出 <code>0</code> 时，就对应着<strong>遗忘</strong>这个过程；当输出 <code>1</code> 时，自然就对应着<strong>接受</strong>这个过程。<ul><li>事实上这也是 <code>Sigmoid</code> 层叫门的原因——它能<strong>决定“放哪些数据进来”和决定“不让哪些数据通过”</strong></li></ul></li><li>最左边的<code>Sigmoid gate</code> 叫做<strong>遗忘门</strong>, 控制着时间通道信息的遗忘程度<ul><li>前向计算: $r_1 = \phi_1(W_1 \times x^*)$<ul><li>其中 $x^*  \buildrel \Delta \over =[x,s^{(old)}] $，表示当前输入样本和下方时间通道$s^{(old)}$连接(<code>concat</code>)起来</li></ul></li></ul></li><li>第二个 <code>Sigmoid Gate</code> 通常被称为<strong>输入门</strong>（Input Gate）, 控制着当前输入和下方通道信息对<strong>上方通道信息的影响</strong><ul><li>前向运算为：$g_{1} = \phi_{1} ( W_{2} \times x^{*} )$,</li></ul></li><li>第三个 <code>Tanh Gate</code> 则允许网络结构<strong>驳回</strong>历史信息, 因为<code>tanh</code>的值域是(-1,1)<ul><li>前向运算为：$g_{2} = \phi_{2} ( W_{3} \times x^{*} )$</li><li>$r_{2} = g_{1}*g_{2}$</li></ul></li><li>第四个 <code>Sigmoid Gate</code> 通常被称为<strong>输出门</strong>（Output Gate），它为输出和传向下一个 <code>cell</code> 的下方通道信息作出了贡献。<ul><li>对应的前向传导算法为：$g_{3} = \phi_{1}\left( W_{4} \times x^{*} \right)$</li></ul></li><li>最终<code>cell</code>的输出为：$o = s^{\left( \text{new} \right)} = \phi_{2}\left( h^{\left( \text{new} \right)} \right)*g_{3}$</li><li>每个 <code>Gate</code> 对应的权值矩阵是不同的（$W_{1}\sim W_{4}$），切勿以为它们会共享权值</li></ul><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul><li><a href="http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/" target="_blank" rel="external">http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/</a></li><li><a href="http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-2-implementing-a-language-model-rnn-with-python-numpy-and-theano/" target="_blank" rel="external">http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-2-implementing-a-language-model-rnn-with-python-numpy-and-theano/</a></li><li><a href="https://zhuanlan.zhihu.com/p/26891871" target="_blank" rel="external">https://zhuanlan.zhihu.com/p/26891871</a></li><li><a href="https://zhuanlan.zhihu.com/p/26892413" target="_blank" rel="external">https://zhuanlan.zhihu.com/p/26892413</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;一、介绍&quot;&gt;&lt;a href=&quot;#一、介绍&quot; class=&quot;headerlink&quot; title=&quot;一、介绍&quot;&gt;&lt;/a&gt;一、介绍&lt;/h1&gt;&lt;h2 id=&quot;1、什么是RNN&quot;&gt;&lt;a href=&quot;#1、什么是RNN&quot; class=&quot;headerlink&quot; title=&quot;1、什么是RNN&quot;&gt;&lt;/a&gt;1、什么是RNN&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;传统的神经网络是&lt;strong&gt;层与层&lt;/strong&gt;之间是&lt;strong&gt;全连接&lt;/strong&gt;的，但是每层之间的神经元是没有连接的（其实是假设各个数据之间是&lt;strong&gt;独立的&lt;/strong&gt;）&lt;ul&gt;
&lt;li&gt;这种结构不善于处理&lt;strong&gt;序列化的问题&lt;/strong&gt;。比如要预测句子中的下一个单词是什么，这往往与前面的单词有很大的关联，因为句子里面的单词并不是独立的。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;code&gt;RNN&lt;/code&gt; 的结构说明当前的的输出与前面的输出也有关，即隐层之间的节点不再是无连接的，而是有连接的&lt;ul&gt;
&lt;li&gt;基本的结构如图，可以看到有个&lt;strong&gt;循环的结构&lt;/strong&gt;，将其展开就是右边的结构&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="DeepLearning" scheme="http://lawlite.me/tags/DeepLearning/"/>
    
      <category term="RNN" scheme="http://lawlite.me/tags/RNN/"/>
    
      <category term="LSTM" scheme="http://lawlite.me/tags/LSTM/"/>
    
  </entry>
  
  <entry>
    <title>Seaborn绘图</title>
    <link href="http://lawlite.me/2017/06/14/Seaborn%E7%BB%98%E5%9B%BE/"/>
    <id>http://lawlite.me/2017/06/14/Seaborn绘图/</id>
    <published>2017-06-14T13:25:43.000Z</published>
    <updated>2017-06-25T08:49:50.072Z</updated>
    
    <content type="html"><![CDATA[<ul><li>全部代码：<a href="https://github.com/lawlite19/Blog-Back-Up/blob/master/code/seaborn_study.py" target="_blank" rel="external">https://github.com/lawlite19/Blog-Back-Up/blob/master/code/seaborn_study.py</a></li></ul><h1 id="一、介绍与安装"><a href="#一、介绍与安装" class="headerlink" title="一、介绍与安装"></a>一、介绍与安装</h1><h2 id="1、介绍"><a href="#1、介绍" class="headerlink" title="1、介绍"></a>1、介绍</h2><ul><li>官网：<a href="http://seaborn.pydata.org/index.html" target="_blank" rel="external">http://seaborn.pydata.org/index.html</a></li><li>Github: <a href="https://github.com/mwaskom/seaborn" target="_blank" rel="external">https://github.com/mwaskom/seaborn</a></li><li><code>Seaborn</code> 其实是在matplotlib的基础上进行了更高级的 <code>API</code> 封装，从而使得作图更加容易</li><li>在大多数情况下使用<code>seaborn</code>就能做出很具有吸引力的图，而使用<code>matplotlib</code>就能制作具有更多特色的图。应该把<code>Seaborn</code>视为<code>matplotlib</code>的补充<h2 id="2、安装"><a href="#2、安装" class="headerlink" title="2、安装"></a>2、安装</h2></li><li>直接 <code>pip3 install seaborn</code>即可</li></ul><a id="more"></a><h1 id="二、分布图"><a href="#二、分布图" class="headerlink" title="二、分布图"></a>二、分布图</h1><h2 id="1、distplot"><a href="#1、distplot" class="headerlink" title="1、distplot"></a>1、distplot</h2><ul><li>导入包</li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">#-*- coding: utf-8 -*-</div><div class="line">import numpy as np</div><div class="line">import pandas as pd</div><div class="line">import matplotlib as mpl</div><div class="line">import matplotlib.pyplot as plt</div><div class="line">import seaborn as sns</div><div class="line">#%matplotlib inline  # 为了在jupyter notebook里作图，需要用到这个命令</div></pre></td></tr></table></figure><ul><li>加载 <code>seaborn</code>中的数据集：<code>tips = sns.load_dataset(&#39;tips&#39;)</code></li><li><p>分布图</p><ul><li><code>kde</code>是高斯分布密度图，绘图在0-1之间</li><li><code>hist</code>是否画直方图</li><li><code>rug</code>在X轴上画一些分布线</li><li><code>fit</code>可以制定某个分布进行<strong>拟合</strong></li><li><code>label</code> legend时的值</li><li><code>axlabel</code>制定横轴的说明<figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">sns.distplot(tips[<span class="string">'total_bill'</span>], bins=None, hist=True, kde=False, rug=True, fit=None, </div><div class="line">            hist_kws=None, kde_kws=None, rug_kws=None, </div><div class="line">            fit_kws=None, <span class="attribute">color</span>=None, vertical=False, </div><div class="line">            norm_hist=False, axlabel=None, label=None, ax=None)</div><div class="line">sns<span class="selector-class">.plt</span><span class="selector-class">.show</span>()</div></pre></td></tr></table></figure></li></ul></li><li><p>拟合分布</p><ul><li>这里使用了<strong>gamma分布</strong>拟合<figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">from scipy import stats</div><div class="line">sns.distplot(tips<span class="selector-class">.total_bill</span>, fit=stats<span class="selector-class">.gamma</span>, kde=False)</div><div class="line">sns<span class="selector-class">.plt</span><span class="selector-class">.show</span>()</div></pre></td></tr></table></figure></li></ul></li></ul><p><img src="/assets/blog_images/seaborn_images/seaborn_dis_01.png" alt="seaborn gamma分布拟合" title="seaborn_dis_01"></p><h2 id="2、kdeplot"><a href="#2、kdeplot" class="headerlink" title="2、kdeplot"></a>2、kdeplot</h2><ul><li>高斯概率密度图<ul><li><code>data2</code>可以是二维的分布</li><li><code>shade</code>是否填充 </li><li><code>kernel</code>核函数，还有很多核函数，比如<strong>cos, biw</strong>等</li><li><code>cumulative</code>累积的作图，最后的值应该是接近<strong>1</strong></li><li><code>gridsize</code>多少个点估计</li></ul></li></ul><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">ax = sns.kdeplot(tips[<span class="string">'total_bill'</span>], data2=tips<span class="selector-class">.tip</span>, shade=False, vertical=False, </div><div class="line">                kernel=<span class="string">"gau"</span>, bw=<span class="string">"scott"</span>, </div><div class="line">                gridsize=<span class="number">100</span>, cut=<span class="number">3</span>, <span class="attribute">clip</span>=None, </div><div class="line">                legend=True, cumulative=False, </div><div class="line">                shade_lowest=True, ax=None)</div><div class="line">sns<span class="selector-class">.plt</span><span class="selector-class">.show</span>()</div></pre></td></tr></table></figure><p><img src="/assets/blog_images/seaborn_images/seaborn_kde_01.png" alt="二维的分布" title="seaborn_kde_01"></p><h1 id="二、pairplot"><a href="#二、pairplot" class="headerlink" title="二、pairplot"></a>二、pairplot</h1><h2 id="1、两两作图"><a href="#1、两两作图" class="headerlink" title="1、两两作图"></a>1、两两作图</h2><ul><li><code>iris</code> 为例<ul><li><code>data</code>: DataFrame格式的数据</li><li><code>hue</code>: label类别对应的column name</li><li><code>vars</code>: 指定feature的列名</li><li><code>kind</code>: 作图的方式，可以是<strong>reg或scatter</strong></li><li><code>diag_kind</code>: 对角线作图的方式，可以是<strong>hist或kde</strong><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">iris = sns.load_dataset(<span class="string">'iris'</span>)</div><div class="line">g = sns.pairplot(iris, hue=<span class="string">'species'</span>, hue_order=None, palette=None, </div><div class="line">                 vars=list(iris<span class="selector-class">.columns</span>[<span class="number">0</span>:-<span class="number">1</span>]), </div><div class="line">                 x_vars=None, y_vars=None, </div><div class="line">                 kind=<span class="string">"reg"</span>, diag_kind=<span class="string">"hist"</span>, </div><div class="line">                 markers=[<span class="string">'o'</span>,<span class="string">'s'</span>,<span class="string">'D'</span>], size=<span class="number">1.5</span>, aspect=<span class="number">1</span>, </div><div class="line">                 dropna=True, plot_kws=None, </div><div class="line">                 diag_kws=None, grid_kws=None)</div><div class="line">sns<span class="selector-class">.plt</span><span class="selector-class">.show</span>()</div></pre></td></tr></table></figure></li></ul></li></ul><p><img src="/assets/blog_images/seaborn_images/seaborn_pair_01.png" alt="seaborn pairplot作图" title="seaborn_pair_01"></p><h1 id="三、stripplot和swarmplot"><a href="#三、stripplot和swarmplot" class="headerlink" title="三、stripplot和swarmplot"></a>三、stripplot和swarmplot</h1><h2 id="1、stripplot"><a href="#1、stripplot" class="headerlink" title="1、stripplot"></a>1、stripplot</h2><ul><li>tips为例，查看每天的数据信息<ul><li><code>x</code>: X轴数据</li><li><code>y</code>: Y轴数据</li><li><code>hue</code>: 区分不同种类数据的column name</li><li><code>data</code>: DataFrame类型数据</li><li><code>jitter</code>: 将数据分开点，防止重叠<figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">tips = sns.load_dataset(<span class="string">'tips'</span>)</div><div class="line">ax = sns.stripplot(x=<span class="string">'day'</span>, y=<span class="string">'total_bill'</span>, hue=None, data=tips, <span class="attribute">order</span>=None, </div><div class="line">                  hue_order=None, jitter=True, </div><div class="line">                  split=False, orient=None, </div><div class="line">                  <span class="attribute">color</span>=None, palette=None, size=<span class="number">5</span>, </div><div class="line">                  edgecolor=<span class="string">"gray"</span>, linewidth=<span class="number">0</span>, </div><div class="line">                  ax=None)</div></pre></td></tr></table></figure></li></ul></li></ul><p><img src="/assets/blog_images/seaborn_images/seaborn_stripplot_01.png" alt="stripplot 查看每天的数据信息" title="seaborn_stripplot_01"></p><ul><li>查看关于性别消费的信息</li></ul><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">ax = sns.stripplot(x=<span class="string">'sex'</span>, y=<span class="string">'total_bill'</span>, hue=<span class="string">'day'</span>, data=tips, <span class="attribute">order</span>=None, </div><div class="line">                  hue_order=None, jitter=True, </div><div class="line">                  split=False, orient=None, </div><div class="line">                  <span class="attribute">color</span>=None, palette=None, size=<span class="number">5</span>, </div><div class="line">                  edgecolor=<span class="string">"gray"</span>, linewidth=<span class="number">0</span>, </div><div class="line">                  ax=None)</div></pre></td></tr></table></figure><p><img src="/assets/blog_images/seaborn_images/seaborn_stripplot_02.png" alt="sripplot 查看敢于性别消费的信息" title="seaborn_stripplot_02"></p><h2 id="2、swarmplot"><a href="#2、swarmplot" class="headerlink" title="2、swarmplot"></a>2、swarmplot</h2><ul><li>与stripplot类似，只是数据点不会重叠 (适合小数据量)</li></ul><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">tips = sns.load_dataset(<span class="string">'tips'</span>)</div><div class="line">ax = sns.swarmplot(x=<span class="string">'sex'</span>, y=<span class="string">'total_bill'</span>, hue=<span class="string">'day'</span>, data=tips)</div><div class="line">sns<span class="selector-class">.plt</span><span class="selector-class">.show</span>()</div></pre></td></tr></table></figure><p><img src="/assets/blog_images/seaborn_images/seaborn_swarmplot_01.png" alt="seaborn swarmplot" title="seaborn_swarmplot_01"></p><h1 id="四、boxplot"><a href="#四、boxplot" class="headerlink" title="四、boxplot"></a>四、boxplot</h1><h2 id="1、boxplot示意图"><a href="#1、boxplot示意图" class="headerlink" title="1、boxplot示意图"></a>1、boxplot示意图</h2><p><img src="/assets/blog_images/seaborn_images/seaborn_boxplot_01.jpg" alt="box图" title="seaborn_boxplot_01"></p><ul><li>函数<ul><li><code>x, y</code>：指定X轴，Y轴的columns name值</li><li><code>hue</code>: 指定要区分的类别<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">tips = sns.load_dataset('tips')</div><div class="line">ax = sns.boxplot(x='day', y='total_bill', hue=None, data=tips, order=None, </div><div class="line">                hue_order=None, orient=None, </div><div class="line">                color=None, palette=None, </div><div class="line">                saturation=.75, width=.8, </div><div class="line">                fliersize=5, linewidth=None, </div><div class="line">                whis=1.5, notch=False, ax=None)</div><div class="line">sns.plt.show()</div></pre></td></tr></table></figure></li></ul></li></ul><p><img src="/assets/blog_images/seaborn_images/seaborn_boxplot_02.png" alt="box图" title="seaborn_boxplot_02"></p><ul><li>可以和上面的stripplot一起用</li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">tips = sns.load_dataset('tips')</div><div class="line">ax = sns.boxplot(x='day', y='total_bill', hue=None, data=tips, order=None, </div><div class="line">                hue_order=None, orient=None, </div><div class="line">                color=None, palette=None, </div><div class="line">                saturation=.75, width=.8, </div><div class="line">                fliersize=5, linewidth=None, </div><div class="line">                whis=1.5, notch=False, ax=None)</div><div class="line">sns.stripplot(x='day', y='total_bill', hue=None, data=tips, order=None, </div><div class="line">             hue_order=None, jitter=True, split=False, </div><div class="line">             orient=None, color=None, palette=None, </div><div class="line">             size=5, edgecolor="gray", linewidth=0, </div><div class="line">             ax=None)</div><div class="line">sns.plt.show()</div></pre></td></tr></table></figure><p><img src="/assets/blog_images/seaborn_images/seaborn_boxplot_03.png" alt="seaborn boxplot和stripplot" title="seaborn_boxplot_03"></p><h1 id="五、jointplot"><a href="#五、jointplot" class="headerlink" title="五、jointplot"></a>五、jointplot</h1><h2 id="1、jointplot"><a href="#1、jointplot" class="headerlink" title="1、jointplot"></a>1、jointplot</h2><ul><li>联合作图<ul><li><code>kind</code>: 有<strong>scatter” | “reg” | “resid” | “kde” | “hex</strong><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">tips = sns.load_dataset(<span class="string">'tips'</span>)</div><div class="line">from scipy import stats</div><div class="line">g = sns.jointplot(x=<span class="string">'total_bill'</span>, y=<span class="string">'tip'</span>,</div><div class="line">                  data=tips, kind=<span class="string">"reg"</span>, </div><div class="line">                  stat_func=stats<span class="selector-class">.pearsonr</span>, </div><div class="line">                  <span class="attribute">color</span>=None, size=<span class="number">6</span>, ratio=<span class="number">5</span>, </div><div class="line">                  space=.<span class="number">2</span>, dropna=True, xlim=None, </div><div class="line">                  ylim=None, joint_kws=None, </div><div class="line">                  marginal_kws=None, annot_kws=None)</div><div class="line">sns<span class="selector-class">.plt</span><span class="selector-class">.show</span>()</div></pre></td></tr></table></figure></li></ul></li></ul><p><img src="/assets/blog_images/seaborn_images/seaborn_jointplot_01.png" alt="jointplot" title="seaborn_jointplot_01"></p><ul><li>可以在基础上再作图<ul><li>plot_joint就是在联合分布上作图</li><li>plot_marginals就是在边缘分布上再作图<figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">g = (sns.jointplot(x=<span class="string">'total_bill'</span>, y=<span class="string">'tip'</span>,data=tips).plot_joint(sns.kdeplot))</div></pre></td></tr></table></figure></li></ul></li></ul><p><img src="/assets/blog_images/seaborn_images/seaborn_jointplot_02.png" alt="joint plot" title="seaborn_jointplot_02"></p><h1 id="六、violinplot"><a href="#六、violinplot" class="headerlink" title="六、violinplot"></a>六、violinplot</h1><h2 id="1、小提琴图，和boxplot很像"><a href="#1、小提琴图，和boxplot很像" class="headerlink" title="1、小提琴图，和boxplot很像"></a>1、小提琴图，和<strong>boxplot</strong>很像</h2><ul><li>对称的<strong>kde图</strong></li><li>中间的白点是<strong>中位数</strong>，黑色粗线对应<strong>分位数</strong><br><img src="/assets/blog_images/seaborn_images/seaborn_violinplot_02.png" alt="小提琴图1" title="seaborn_violinplot_02"></li><li><ul><li><code>inner</code>: 指定图里面用什么划分，有<code>&quot;box&quot;, &quot;quartile&quot;, &quot;point&quot;, &quot;stick&quot;, None</code><ul><li><code>quartile</code>为四分位数划分</li><li><code>stick</code>很像<strong>rug</strong>，就是可以看出密度情况</li></ul></li><li><code>scale</code>: 缩放每个图对应的area, 取值有 <code>&quot;area&quot;, &quot;count&quot;, &quot;width&quot;</code><ul><li><code>area</code>指定每个有相同的area</li><li><code>count</code>会按数量缩放（数量少的就比较<strong>窄扁</strong>）<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">tips = sns.load_dataset('tips')</div><div class="line">ax = sns.violinplot(x='day', y='total_bill', </div><div class="line">                    hue='smoker', data=tips, order=None, </div><div class="line">                    hue_order=None, bw="scott", </div><div class="line">                    cut=2, scale="area", </div><div class="line">                    scale_hue=True, gridsize=100, </div><div class="line">                    width=.8, inner="quartile", </div><div class="line">                    split=False, orient=None, </div><div class="line">                    linewidth=None, color=None, </div><div class="line">                    palette='muted', saturation=.75, </div><div class="line">                    ax=None) </div><div class="line">sns.plt.show()</div></pre></td></tr></table></figure></li></ul></li></ul></li></ul><p><img src="/assets/blog_images/seaborn_images/seaborn_violinplot_01.png" alt="小提琴图" title="seaborn_violinplot_01"></p><h1 id="七、pointplot-bar"><a href="#七、pointplot-bar" class="headerlink" title="七、pointplot, bar"></a>七、pointplot, bar</h1><h2 id="1、pointplot"><a href="#1、pointplot" class="headerlink" title="1、pointplot"></a>1、pointplot</h2><ul><li>点图<ul><li><code>estimator</code>：点的取值是，默认是<code>np.mean</code><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">tips = sns.load_dataset(<span class="string">'tips'</span>)</div><div class="line">sns.pointplot(x=<span class="string">'time'</span>, y=<span class="string">'total_bill'</span>, hue=<span class="string">'smoker'</span>, data=tips, <span class="attribute">order</span>=None, </div><div class="line">             hue_order=None, estimator=np<span class="selector-class">.mean</span>, ci=<span class="number">95</span>, </div><div class="line">             n_boot=<span class="number">1000</span>, units=None, markers=<span class="string">"o"</span>, </div><div class="line">             linestyles=<span class="string">"-"</span>, dodge=False, join=True, </div><div class="line">             scale=<span class="number">1</span>, orient=None, <span class="attribute">color</span>=None, </div><div class="line">             palette=None, ax=None, errwidth=None, </div><div class="line">             capsize=None)</div><div class="line">sns<span class="selector-class">.plt</span><span class="selector-class">.show</span>()</div></pre></td></tr></table></figure></li></ul></li></ul><p><img src="/assets/blog_images/seaborn_images/seaborn_pointplot_01.png" alt="pointplot" title="seaborn_pointplot_01"></p><h2 id="2、barplot"><a href="#2、barplot" class="headerlink" title="2、barplot"></a>2、barplot</h2><ul><li>条形图<ul><li>y轴是<code>mean value</code>，和点图其实差不多<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">tips = sns.load_dataset('tips')</div><div class="line">sns.barplot(x='day', y='total_bill', hue='sex', data=tips, order=None, </div><div class="line">           hue_order=None, estimator=np.mean, ci=95, </div><div class="line">           n_boot=1000, units=None, orient=None, </div><div class="line">           color=None, palette=None, saturation=.75, </div><div class="line">           errcolor=".26", errwidth=None, capsize=None, </div><div class="line">           ax=None)</div><div class="line">sns.plt.show()</div></pre></td></tr></table></figure></li></ul></li></ul><p><img src="/assets/blog_images/seaborn_images/seaborn_barplot_01.png" alt="bar" title="seaborn_barplot_01"></p><h2 id="3、countplot"><a href="#3、countplot" class="headerlink" title="3、countplot"></a>3、countplot</h2><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">tips = sns.load_dataset(<span class="string">'tips'</span>)</div><div class="line">sns.countplot(x=<span class="string">'day'</span>, hue=<span class="string">'sex'</span>, data=tips)</div><div class="line">sns<span class="selector-class">.plt</span><span class="selector-class">.show</span>()</div></pre></td></tr></table></figure><p><img src="/assets/blog_images/seaborn_images/seaborn_countplot_01.png" alt="count" title="seaborn_countplot_01"></p><h1 id="八、factorplot"><a href="#八、factorplot" class="headerlink" title="八、factorplot"></a>八、factorplot</h1><h2 id="1、可以通过这个函数绘制以上几种图"><a href="#1、可以通过这个函数绘制以上几种图" class="headerlink" title="1、可以通过这个函数绘制以上几种图"></a>1、可以通过这个函数绘制以上几种图</h2><ul><li>指定<code>kind</code>即可，有<code>point</code>, <code>bar</code>, <code>count</code>, <code>box</code>, <code>violin</code>, <code>strip</code></li><li><code>row</code>和<code>col</code>指定绘制的行数和列数，给出一个<strong>种类类型的列名</strong>即可 <figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">titanic = sns.load_dataset(<span class="string">'titanic'</span>)</div><div class="line">sns.factorplot(x=<span class="string">'age'</span>, y=<span class="string">'embark_town'</span>, </div><div class="line">               hue=<span class="string">'sex'</span>, data=titanic,</div><div class="line">               row=<span class="string">'class'</span>, col=<span class="string">'sex'</span>, </div><div class="line">               col_wrap=None, estimator=np<span class="selector-class">.mean</span>, ci=<span class="number">95</span>, </div><div class="line">               n_boot=<span class="number">1000</span>, units=None, <span class="attribute">order</span>=None, </div><div class="line">               hue_order=None, row_order=None, </div><div class="line">               col_order=None, kind=<span class="string">"box"</span>, size=<span class="number">4</span>, </div><div class="line">               aspect=<span class="number">1</span>, orient=None, <span class="attribute">color</span>=None, </div><div class="line">               palette=None, legend=True, </div><div class="line">               legend_out=True, sharex=True, </div><div class="line">               sharey=True, margin_titles=False, </div><div class="line">               facet_kws=None)</div><div class="line">sns<span class="selector-class">.plt</span><span class="selector-class">.show</span>()</div></pre></td></tr></table></figure></li></ul><p><img src="/assets/blog_images/seaborn_images/seaborn_factorplot_01.png" alt="factorplot box" title="seaborn_factorplot_01"></p><h1 id="九、heatmap"><a href="#九、heatmap" class="headerlink" title="九、heatmap"></a>九、heatmap</h1><h2 id="1、heatmap"><a href="#1、heatmap" class="headerlink" title="1、heatmap"></a>1、heatmap</h2><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">flight = sns.load_dataset(<span class="string">'flights'</span>)</div><div class="line">flights = flight.pivot(<span class="string">'month'</span>,<span class="string">'year'</span>,<span class="string">'passengers'</span>)</div><div class="line">sns.heatmap(flights, annot=True, fmt=<span class="string">'d'</span>)</div><div class="line">sns<span class="selector-class">.plt</span><span class="selector-class">.show</span>()</div></pre></td></tr></table></figure><p><img src="/assets/blog_images/seaborn_images/seaborn_heatmap_01.png" alt="heatmap" title="seaborn_heatmap_01"></p><h1 id="十、时序绘图"><a href="#十、时序绘图" class="headerlink" title="十、时序绘图"></a>十、时序绘图</h1><h2 id="1、tsplot"><a href="#1、tsplot" class="headerlink" title="1、tsplot"></a>1、tsplot</h2><ul><li>condition: 和<code>hue</code>差不多，指定类别</li><li>estimator: 默认为<code>np.mean</code><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">gammas = sns.load_dataset('gammas')</div><div class="line">sns.tsplot(data=gammas, time='timepoint', unit='subject', </div><div class="line">           condition='ROI', value='BOLD signal', </div><div class="line">           err_style="ci_band", ci=68, interpolate=True, </div><div class="line">           color=None, estimator=np.mean, n_boot=5000, </div><div class="line">           err_palette=None, err_kws=None, legend=True, </div><div class="line">           ax=None)</div><div class="line">sns.plt.show()</div></pre></td></tr></table></figure></li></ul><p><img src="/assets/blog_images/seaborn_images/seaborn_tsplot_01.png" alt="tsplot" title="seaborn_tsplot_01"></p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul><li>Youtube: <a href="https://www.youtube.com/playlist?list=PLgJhDSE2ZLxYlhQx0UfVlnF1F7OWF-9rp" target="_blank" rel="external">https://www.youtube.com/playlist?list=PLgJhDSE2ZLxYlhQx0UfVlnF1F7OWF-9rp</a></li><li>Github: <a href="https://github.com/knathanieltucker/seaborn-weird-parts" target="_blank" rel="external">https://github.com/knathanieltucker/seaborn-weird-parts</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;ul&gt;
&lt;li&gt;全部代码：&lt;a href=&quot;https://github.com/lawlite19/Blog-Back-Up/blob/master/code/seaborn_study.py&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://github.com/lawlite19/Blog-Back-Up/blob/master/code/seaborn_study.py&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&quot;一、介绍与安装&quot;&gt;&lt;a href=&quot;#一、介绍与安装&quot; class=&quot;headerlink&quot; title=&quot;一、介绍与安装&quot;&gt;&lt;/a&gt;一、介绍与安装&lt;/h1&gt;&lt;h2 id=&quot;1、介绍&quot;&gt;&lt;a href=&quot;#1、介绍&quot; class=&quot;headerlink&quot; title=&quot;1、介绍&quot;&gt;&lt;/a&gt;1、介绍&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;官网：&lt;a href=&quot;http://seaborn.pydata.org/index.html&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://seaborn.pydata.org/index.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Github: &lt;a href=&quot;https://github.com/mwaskom/seaborn&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://github.com/mwaskom/seaborn&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Seaborn&lt;/code&gt; 其实是在matplotlib的基础上进行了更高级的 &lt;code&gt;API&lt;/code&gt; 封装，从而使得作图更加容易&lt;/li&gt;
&lt;li&gt;在大多数情况下使用&lt;code&gt;seaborn&lt;/code&gt;就能做出很具有吸引力的图，而使用&lt;code&gt;matplotlib&lt;/code&gt;就能制作具有更多特色的图。应该把&lt;code&gt;Seaborn&lt;/code&gt;视为&lt;code&gt;matplotlib&lt;/code&gt;的补充&lt;h2 id=&quot;2、安装&quot;&gt;&lt;a href=&quot;#2、安装&quot; class=&quot;headerlink&quot; title=&quot;2、安装&quot;&gt;&lt;/a&gt;2、安装&lt;/h2&gt;&lt;/li&gt;
&lt;li&gt;直接 &lt;code&gt;pip3 install seaborn&lt;/code&gt;即可&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="Python" scheme="http://lawlite.me/tags/Python/"/>
    
      <category term="可视化" scheme="http://lawlite.me/tags/%E5%8F%AF%E8%A7%86%E5%8C%96/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch学习</title>
    <link href="http://lawlite.me/2017/05/10/PyTorch/"/>
    <id>http://lawlite.me/2017/05/10/PyTorch/</id>
    <published>2017-05-10T11:30:05.000Z</published>
    <updated>2017-07-06T08:50:11.260Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一、PyTorch介绍"><a href="#一、PyTorch介绍" class="headerlink" title="一、PyTorch介绍"></a>一、PyTorch介绍</h1><h2 id="1、说明"><a href="#1、说明" class="headerlink" title="1、说明"></a>1、说明</h2><ul><li><code>PyTorch</code> 是 <code>Torch</code> 在 <code>Python</code> 上的衍生（<code>Torch</code> 是一个使用 <code>Lua</code> 语言的神经网络库）</li><li>和<code>tensorflow</code>比较<ul><li><code>PyTorch</code>建立的神经网络是<strong>动态的</strong></li><li><code>Tensorflow</code>是建立<strong>静态图</strong></li><li><code>Tensorflow</code> 的高度工业化, 它的底层代码是很难看懂的. </li><li><code>PyTorch</code> 好那么一点点, 如果你深入 <code>API</code>, 你至少能比看 <code>Tensorflow</code> 多看懂一点点 <code>PyTorch</code> 的底层在干嘛.</li></ul></li></ul><a id="more"></a><h2 id="2、安装PyTorch"><a href="#2、安装PyTorch" class="headerlink" title="2、安装PyTorch"></a>2、安装<code>PyTorch</code></h2><ul><li>官网：<a href="http://pytorch.org/" target="_blank" rel="external">http://pytorch.org/</a></li><li>进入官网之后可以选择对应的安装选项<ul><li>目前只支持<code>Linux</code>和<code>MacOS</code>版本（<code>2017-05-06</code>）</li><li>执行下面对应的安装命令即可<br><img src="/assets/blog_images/PyTorch/PyTorch_01.png" alt="安装" title="PyTorch_01"></li></ul></li><li>安装 <code>PyTorch</code> 会安装两个模块<ul><li>一个是 <code>torch</code>, 一个 <code>torchvision</code>, <code>torch</code> 是<strong>主模块</strong>, 用来搭建神经网络的, </li><li><code>torchvision</code> 是<strong>辅模块</strong>,有数据库,还有一些已经训练好的神经网络等着你直接用, 比如 (<code>VGG, AlexNet, ResNet</code>).</li></ul></li></ul><hr><ul><li>上面在<code>ubuntu14</code>下自带的<code>python2.7</code>安装没有问题，在<code>CentOS6.5</code>下的<code>python3.5</code>中安装可能报错<ul><li>安装<code>python3.5</code>时的配置：</li></ul></li></ul><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">./configure --enable-shared \</div><div class="line">            --prefix=/usr/local/python3.<span class="number">5</span> \</div><div class="line">            LDFLAGS=<span class="string">"-Wl,--rpath=/usr/local/lib"</span></div></pre></td></tr></table></figure><ul><li>然后运行<code>python</code>可能报<code>loading shared libraries: libpython3.5m.so.1.0: cannot open shared object file: No such file or directory</code>的错误，拷贝一份<code>libpython3.5m.so.1.0</code>到<code>/usr/lib64</code>目录下即可</li></ul><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">cp /home/Python/Python-<span class="number">3.5</span>.<span class="number">3</span>/libpython3.<span class="number">5</span>m<span class="selector-class">.so</span>.<span class="number">1.0</span> /usr/lib64</div></pre></td></tr></table></figure><h1 id="二、基础知识"><a href="#二、基础知识" class="headerlink" title="二、基础知识"></a>二、基础知识</h1><h2 id="1、和Numpy相似之处"><a href="#1、和Numpy相似之处" class="headerlink" title="1、和Numpy相似之处"></a>1、和<code>Numpy</code>相似之处</h2><h3 id="1-数据转换"><a href="#1-数据转换" class="headerlink" title="(1) 数据转换"></a>(1) 数据转换</h3><ul><li>导入包：<code>import torch</code></li><li>将<code>numpy</code>数据转为<code>torch</code>数据</li></ul><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">np_data = np.arange(<span class="number">6</span>).reshape((<span class="number">2</span>, <span class="number">3</span>))</div><div class="line">torch_data = torch.from_numpy(np_data)</div></pre></td></tr></table></figure><ul><li>将<code>torch</code>数据转为<code>numpy</code>数据</li></ul><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">tensor2array = torch_data.numpy()</div></pre></td></tr></table></figure><h3 id="2-Torch中的运算"><a href="#2-Torch中的运算" class="headerlink" title="(2) Torch中的运算"></a>(2) <code>Torch</code>中的运算</h3><ul><li>API：<a href="http://pytorch.org/docs/torch.html#math-operations" target="_blank" rel="external">http://pytorch.org/docs/torch.html#math-operations</a></li><li><code>torch</code> 中 <code>tensor</code> 的运算和 <code>numpy array</code>运算很相似，比如<ul><li><code>np.abs() --&gt; torch.abs()</code></li><li><code>np.sin() --&gt; torch.sin()</code>等</li></ul></li><li>矩阵相乘：<ul><li><code>data = [[1,2], [3,4]]</code></li><li><code>tensor = torch.FloatTensor(data)  # 转换成32位浮点 tensor</code></li><li><code>torch.mm(tensor, tensor)</code><h2 id="2、变量Variable"><a href="#2、变量Variable" class="headerlink" title="2、变量Variable"></a>2、变量<code>Variable</code></h2><h3 id="1-说明"><a href="#1-说明" class="headerlink" title="(1) 说明"></a>(1) 说明</h3></li></ul></li><li><code>Variable</code> 就是一个存放会<strong>变化的值</strong>的位置</li><li>这里<strong>变化的值</strong>就是<strong>tensor</strong>数据<h3 id="2-使用"><a href="#2-使用" class="headerlink" title="(2) 使用"></a>(2) 使用</h3></li><li><p>导入包</p><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">import torch</div><div class="line">from torch.autograd import Variable # torch 中 Variable 模块</div></pre></td></tr></table></figure></li><li><p>定义<code>tensor</code>: <code>tensor = torch.FloatTensor([[1,2],[3,4]])</code></p></li><li>将<code>tensor</code>放入<code>Variable</code>: <code>variable = Variable(tensor, requires_grad=True)</code><ul><li><code>requires_grad</code> 是参不参与误差反向传播, 要不要计算<strong>梯度</strong></li><li><code>print(variable)</code> 会输出，(多出<code>Variable containing:</code>，表明是<code>Variable</code>)</li></ul></li></ul><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">Variable containing:</div><div class="line"> <span class="number">1</span>  <span class="number">2</span></div><div class="line"> <span class="number">3</span>  <span class="number">4</span></div><div class="line">[torch<span class="selector-class">.FloatTensor</span> of size <span class="number">2</span>x2]</div></pre></td></tr></table></figure><h3 id="3-计算梯度"><a href="#3-计算梯度" class="headerlink" title="(3) 计算梯度"></a>(3) 计算梯度</h3><ul><li><code>v_out = torch.mean(variable*variable)   # x^2</code></li><li><code>v_out.backward()      # 模拟 v_out 的误差反向传递</code></li><li><code>print(variable.grad)  # 显示 Variable 的梯度</code><ul><li>输出结果如下</li><li>因为<code>torch.mean(variable*variable)</code>是<code>1/4*x^2</code>，导数就是<code>1/2x</code><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="number">0.5000</span>  <span class="number">1.0000</span></div><div class="line"><span class="number">1.5000</span>  <span class="number">2.0000</span></div></pre></td></tr></table></figure></li></ul></li></ul><h3 id="4-Variable里面的数据"><a href="#4-Variable里面的数据" class="headerlink" title="(4) Variable里面的数据"></a>(4) Variable里面的数据</h3><ul><li>直接<code>print(variable)</code>只会输出 <code>Variable</code> 形式的数据, 在很多时候是用不了的(比如想要用 <code>plt</code> 画图), 所以我们要转换一下, 将它变成 <code>tensor</code> 形式.</li><li>获取 <code>tensor</code> 数据：<code>print(variable.data)    # tensor 形式</code><ul><li>然后也可以转而<code>numpy</code>数据：<code>print(variable.data.numpy())    # numpy 形式</code><h2 id="3、Torch-中的激励函数"><a href="#3、Torch-中的激励函数" class="headerlink" title="3、Torch 中的激励函数"></a>3、<code>Torch</code> 中的激励函数</h2></li></ul></li><li>导入包：<code>import torch.nn.functional as F     # 激励函数都在这</code></li><li>平时要用到的就这几个. <code>relu, sigmoid, tanh, softplus</code></li><li>激励函数<ul><li><code>x</code>是<code>Variable</code>数据，<code>F.relu(x)</code>也是返回<code>Variable</code>数据，然后<code>.data</code>获取 <code>tensor</code> 数据<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"># 做一些假数据来观看图像</div><div class="line">x = torch.linspace(-5, 5, 200)  # x data (tensor), shape=(100, 1)</div><div class="line">x = Variable(x)</div><div class="line">x_np = x.data.numpy()   # 换成 numpy array, 出图时用</div></pre></td></tr></table></figure></li></ul></li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">y_relu = F.relu(x).data.numpy()</div><div class="line">y_sigmoid = F.sigmoid(x).data.numpy()</div><div class="line">y_tanh = F.tanh(x).data.numpy()</div><div class="line">y_softplus = F.softplus(x).data.numpy()</div><div class="line"># y_softmax = F.softmax(x)  softmax 比较特殊, 不能直接显示, 不过他是关于概率的, 用于分类</div></pre></td></tr></table></figure><ul><li><code>softplus</code>的公式为：<code>f(x)=ln(1+ex)</code></li></ul><h1 id="三、建立基础的神经网络"><a href="#三、建立基础的神经网络" class="headerlink" title="三、建立基础的神经网络"></a>三、建立基础的神经网络</h1><h2 id="1、回归问题"><a href="#1、回归问题" class="headerlink" title="1、回归问题"></a>1、回归问题</h2><h3 id="1-准备工作"><a href="#1-准备工作" class="headerlink" title="(1) 准备工作"></a>(1) 准备工作</h3><ul><li>导入包</li></ul><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">import torch</div><div class="line">from torch<span class="selector-class">.autograd</span> import Variable</div><div class="line">import torch<span class="selector-class">.nn</span><span class="selector-class">.functional</span> as F </div><div class="line">import matplotlib<span class="selector-class">.pyplot</span> as plt</div></pre></td></tr></table></figure><ul><li><p>制造假数据</p><ul><li><code>torch.unsqueeze</code>是转成2维的数据<code>[[]]</code>,加上一个<strong>假的维度</strong><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">x = torch.unsqueeze(torch.linspace(-1, 1, 100), dim=1)  # x data (tensor), shape=(100, 1)</div><div class="line">y = x.pow(2) + 0.2*torch.rand(x.size())                 # noisy y data (tensor), shape=(100, 1)</div></pre></td></tr></table></figure></li></ul></li><li><p>定义<code>Variable</code>: <code>x, y = torch.autograd.Variable(x), Variable(y)</code></p><h3 id="2-建立神经网络"><a href="#2-建立神经网络" class="headerlink" title="(2) 建立神经网络"></a>(2) 建立神经网络</h3></li><li><p>使用类的方式class</p><ul><li>继承<code>torch.nn.Module</code></li><li>这里只包含一个隐层，<code>__init__</code>只是定义了几个层</li><li><code>forward</code>进行传播，也就是整个网络的搭建，<strong>因为是预测，最后一层不需要激励函数</strong><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">class Net(torch.nn.Module):  # 继承 torch 的 Module</div><div class="line">    def __init__(self, n_feature, n_hidden, n_output):</div><div class="line">        super(Net, self).__init__()     # 继承 __init__ 功能</div><div class="line">        # 定义每层用什么样的形式</div><div class="line">        self.hidden = torch.nn.Linear(n_feature, n_hidden)   # 隐藏层线性输出</div><div class="line">        self.predict = torch.nn.Linear(n_hidden, n_output)   # 输出层线性输出</div><div class="line"></div><div class="line">    def forward(self, x):   # 这同时也是 Module 中的 forward 功能</div><div class="line">        # 正向传播输入值, 神经网络分析出输出值</div><div class="line">        x = F.relu(self.hidden(x))      # 激励函数(隐藏层的线性值)</div><div class="line">        x = self.predict(x)             # 输出值</div><div class="line">        return x</div></pre></td></tr></table></figure></li></ul></li><li><p>使用：<code>net = Net(n_feature=1, n_hidden=10, n_output=1)</code></p></li><li>输出：<code>print(net)</code>，结果为</li></ul><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">Net (</div><div class="line">  (hidden): Linear (<span class="number">1</span> -&gt; <span class="number">10</span>)</div><div class="line">  (predict): Linear (<span class="number">10</span> -&gt; <span class="number">1</span>)</div><div class="line">)</div></pre></td></tr></table></figure><h3 id="3-训练网络"><a href="#3-训练网络" class="headerlink" title="(3) 训练网络"></a>(3) 训练网络</h3><ul><li>定义优化器：<code>optimizer = torch.optim.SGD(net.parameters(), lr=0.5)  # 传入 net 的所有参数, 学习率lr</code></li><li>定义损失函数：<code>loss_func = torch.nn.MSELoss()      # 预测值和真实值的误差计算公式 (均方差)</code></li><li>训练</li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">for t in range(100):</div><div class="line">    prediction = net(x)     # 喂给 net 训练数据 x, 输出预测值</div><div class="line"></div><div class="line">    loss = loss_func(prediction, y)     # 计算两者的误差</div><div class="line"></div><div class="line">    optimizer.zero_grad()   # 清空上一步的残余更新参数值</div><div class="line">    loss.backward()         # 误差反向传播, 计算参数更新值</div><div class="line">    optimizer.step()        # 将参数更新值施加到 net 的 parameters 上</div></pre></td></tr></table></figure><h2 id="2、分类问题"><a href="#2、分类问题" class="headerlink" title="2、分类问题"></a>2、分类问题</h2><h3 id="1-准备工作-1"><a href="#1-准备工作-1" class="headerlink" title="(1) 准备工作"></a>(1) 准备工作</h3><ul><li>导入包</li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">import torch</div><div class="line">from torch.autograd import Variable</div><div class="line">import torch.nn.functional as F     # 激励函数都在这</div><div class="line">import matplotlib.pyplot as plt</div></pre></td></tr></table></figure><ul><li><p>制造假数据</p><ul><li><code>x0</code>是一个类别的<code>x1,x2</code></li><li><code>y0</code>就是对应这个类别的 <code>label</code>，这里是<code>0</code></li><li>然后将<code>x0,x1</code>，<code>y0,y1</code>合并在一起<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"># 假数据</div><div class="line">n_data = torch.ones(100, 2)         # 数据的基本形态</div><div class="line">x0 = torch.normal(2*n_data, 1)      # 类型0 x data (tensor), shape=(100, 2)</div><div class="line">y0 = torch.zeros(100)               # 类型0 y data (tensor), shape=(100, 1)</div><div class="line">x1 = torch.normal(-2*n_data, 1)     # 类型1 x data (tensor), shape=(100, 1)</div><div class="line">y1 = torch.ones(100)                # 类型1 y data (tensor), shape=(100, 1)</div><div class="line"># 注意 x, y 数据的数据形式是一定要像下面一样 (torch.cat 是在合并数据)</div><div class="line">x = torch.cat((x0, x1), 0).type(torch.FloatTensor)  # FloatTensor = 32-bit floating</div><div class="line">y = torch.cat((y0, y1), ).type(torch.LongTensor)    # LongTensor = 64-bit integer</div></pre></td></tr></table></figure></li></ul></li><li><p>定义Variable: <code>x, y = Variable(x), Variable(y)</code></p></li></ul><h3 id="2-建立网络"><a href="#2-建立网络" class="headerlink" title="(2) 建立网络"></a>(2) 建立网络</h3><ul><li><p>与上面回归的例子类似</p><ul><li>使用<code>relu</code>激励函数</li><li>这里最后一层并没有使用<strong>激励函数</strong>或是<code>softmax</code>，因为下面使用了<code>CrossEntropyLoss</code>，这个里面默认会调用<code>log_softmax</code>函数(<code>nll_loss(log_softmax(input), target, weight, size_average)</code>)</li><li>当然这里也可以最后返回<code>F.softmax(x)</code>, 那么下面的损失函数就是<code>loss = F.nll_loss(out, y)</code><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">class Net(torch.nn.Module):     # 继承 torch 的 Module</div><div class="line">    def __init__(self, n_feature, n_hidden, n_output):</div><div class="line">        super(Net, self).__init__()     # 继承 __init__ 功能</div><div class="line">        self.hidden = torch.nn.Linear(n_feature, n_hidden)   # 隐藏层线性输出</div><div class="line">        self.out = torch.nn.Linear(n_hidden, n_output)       # 输出层线性输出</div><div class="line"></div><div class="line">    def forward(self, x):</div><div class="line">        # 正向传播输入值, 神经网络分析出输出值</div><div class="line">        x = F.relu(self.hidden(x))      # 激励函数(隐藏层的线性值)</div><div class="line">        x = self.out(x)                 # 输出值, 但是这个不是预测值, 预测值还需要再另外计算</div><div class="line">        return x</div></pre></td></tr></table></figure></li></ul></li><li><p>建立网络：<code>net = Net(n_feature=2, n_hidden=10, n_output=2) # 几个类别就几个 output</code></p><h3 id="3-训练网络-1"><a href="#3-训练网络-1" class="headerlink" title="(3) 训练网络"></a>(3) 训练网络</h3></li><li>优化器：<code>optimizer = torch.optim.SGD(net.parameters(), lr=0.02)  # 传入 net 的所有参数, 学习率</code></li><li>损失函数：<code>loss_func = torch.nn.CrossEntropyLoss()</code></li><li>训练：</li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">for t in range(100):</div><div class="line">    out = net(x)     # 喂给 net 训练数据 x, 输出分析值</div><div class="line"></div><div class="line">    loss = loss_func(out, y)     # 计算两者的误差</div><div class="line"></div><div class="line">    optimizer.zero_grad()   # 清空上一步的残余更新参数值</div><div class="line">    loss.backward()         # 误差反向传播, 计算参数更新值</div><div class="line">    optimizer.step()        # 将参数更新值施加到 net 的 parameters 上</div></pre></td></tr></table></figure><ul><li>预测：<ul><li>输出至最大的那个（概率最大的）坐标<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"># 过了一道 softmax 的激励函数后的最大概率才是预测值</div><div class="line">        prediction = torch.max(F.softmax(out), 1)[1]</div></pre></td></tr></table></figure></li></ul></li></ul><h2 id="3、快速搭建神经网络"><a href="#3、快速搭建神经网络" class="headerlink" title="3、快速搭建神经网络"></a>3、快速搭建神经网络</h2><ul><li>使用<code>torch.nn.Sequential</code></li></ul><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">net2 = torch<span class="selector-class">.nn</span><span class="selector-class">.Sequential</span>(</div><div class="line">    torch<span class="selector-class">.nn</span><span class="selector-class">.Linear</span>(<span class="number">1</span>, <span class="number">10</span>),</div><div class="line">    torch<span class="selector-class">.nn</span><span class="selector-class">.ReLU</span>(),</div><div class="line">    torch<span class="selector-class">.nn</span><span class="selector-class">.Linear</span>(<span class="number">10</span>, <span class="number">1</span>)</div><div class="line">)</div></pre></td></tr></table></figure><ul><li>输出：<code>print(net2)</code><ul><li>相比我们之前自己定义的类，<strong>激励函数</strong>也显示出来了<figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">Sequential (</div><div class="line">  (<span class="number">0</span>): Linear (<span class="number">1</span> -&gt; <span class="number">10</span>)</div><div class="line">  (<span class="number">1</span>): ReLU ()</div><div class="line">  (<span class="number">2</span>): Linear (<span class="number">10</span> -&gt; <span class="number">1</span>)</div><div class="line">)</div></pre></td></tr></table></figure></li></ul></li></ul><h2 id="4、保存和提取"><a href="#4、保存和提取" class="headerlink" title="4、保存和提取"></a>4、保存和提取</h2><h3 id="1-保存（两种方法）"><a href="#1-保存（两种方法）" class="headerlink" title="(1) 保存（两种方法）"></a>(1) 保存（两种方法）</h3><ul><li>保存整个网络<ul><li><code>torch.save(net1, &#39;net.pkl&#39;)</code>  # 保存整个网络，<code>net1</code> 就是定义的网络</li></ul></li><li>只保存网络中的参数<ul><li><code>torch.save(net1.state_dict(), &#39;net_params.pkl&#39;)</code>   # 只保存网络中的参数 (速度快, 占内存少)<h3 id="2-提取（也是两种方法）"><a href="#2-提取（也是两种方法）" class="headerlink" title="(2) 提取（也是两种方法）"></a>(2) 提取（也是两种方法）</h3></li></ul></li><li>提取整个网络：<ul><li><code>net2 = torch.load(&#39;net.pkl&#39;)</code></li><li><code>prediction = net2(x)</code></li></ul></li><li>只提取网络参数<ul><li>首先需要定义一样的神经网络</li><li><code>net3.load_state_dict(torch.load(&#39;net_params.pkl&#39;))</code></li><li><code>prediction = net3(x)</code><h2 id="5、批训练SGD"><a href="#5、批训练SGD" class="headerlink" title="5、批训练SGD"></a>5、批训练SGD</h2></li></ul></li><li>上面我们虽然是<code>torch.optim.SGD</code>进行优化，但是还是将所有数据放进去训练<h3 id="1-DataLoader"><a href="#1-DataLoader" class="headerlink" title="(1) DataLoader"></a>(1) DataLoader</h3></li><li>是 <code>torch</code> 用来包装你的数据(<code>tensor</code>)的工具</li><li>导入包： <code>import torch.utils.data as Data</code></li><li><p>将tensor数据转为<code>torch</code>能识别的<code>Dataset</code></p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">torch_dataset = Data.TensorDataset(data_tensor=x, target_tensor=y)</div></pre></td></tr></table></figure></li><li><p>把 dataset 放入 DataLoader</p><ul><li><code>BATCH_SIZE</code>是我们定义的<code>batch</code>大小<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">loader = Data.DataLoader(</div><div class="line">    dataset=torch_dataset,      # torch TensorDataset format</div><div class="line">    batch_size=BATCH_SIZE,      # mini batch size</div><div class="line">    shuffle=True,               # 要不要打乱数据 (打乱比较好)</div><div class="line">    num_workers=2,              # 多线程来读数据</div><div class="line">)</div></pre></td></tr></table></figure></li></ul></li><li><p>就可以得到<code>batch</code>数据了</p><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">for epoch in range(3):   # 训练所有!整套!数据 3 次</div><div class="line">    for step, (batch_x, batch_y) in enumerate(loader):  # 每一步 loader 释放一小批数据用来学习</div><div class="line">        # 假设这里就是你训练的地方...</div><div class="line"></div><div class="line">        # 打出来一些数据</div><div class="line">        print('Epoch: ', epoch, '| Step: ', step, '| batch x: ',</div><div class="line">              batch_x.numpy(), '| batch y: ', batch_y.numpy())</div></pre></td></tr></table></figure></li><li><p>这里还是<code>tensor</code>数据，真正训练时还要放到<code>Variable</code>中</p></li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">b_x = Variable(batch_x)  # 务必要用 Variable 包一下</div><div class="line">b_y = Variable(batch_y)</div></pre></td></tr></table></figure><h2 id="6、优化器-optimizer"><a href="#6、优化器-optimizer" class="headerlink" title="6、优化器 optimizer"></a>6、优化器 optimizer</h2><ul><li><code>SGD</code><ul><li>就是随机梯度下降</li></ul></li><li><code>momentum</code><ul><li>动量加速</li><li>在<code>SGD</code>函数里指定<code>momentum</code>的值即可</li></ul></li><li><code>RMSprop</code><ul><li>指定参数<code>alpha</code></li></ul></li><li><code>Adam</code><ul><li>参数<code>betas=(0.9, 0.99)</code><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">opt_SGD         = torch<span class="selector-class">.optim</span><span class="selector-class">.SGD</span>(net_SGD.parameters(), lr=LR)</div><div class="line">opt_Momentum    = torch<span class="selector-class">.optim</span><span class="selector-class">.SGD</span>(net_Momentum.parameters(), lr=LR, momentum=<span class="number">0.8</span>)</div><div class="line">opt_RMSprop     = torch<span class="selector-class">.optim</span><span class="selector-class">.RMSprop</span>(net_RMSprop.parameters(), lr=LR, alpha=<span class="number">0.9</span>)</div><div class="line">opt_Adam        = torch<span class="selector-class">.optim</span><span class="selector-class">.Adam</span>(net_Adam.parameters(), lr=LR, betas=(<span class="number">0.9</span>, <span class="number">0.99</span>))</div></pre></td></tr></table></figure></li></ul></li></ul><h1 id="四、高级神经网络"><a href="#四、高级神经网络" class="headerlink" title="四、高级神经网络"></a>四、高级神经网络</h1><h2 id="1、卷积神经网络-CNN"><a href="#1、卷积神经网络-CNN" class="headerlink" title="1、卷积神经网络 CNN"></a>1、卷积神经网络 <code>CNN</code></h2><ul><li>使用mnist数据集</li><li>导入包</li></ul><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">import torch</div><div class="line">from torch<span class="selector-class">.autograd</span> import Variable</div><div class="line">import torch<span class="selector-class">.utils</span><span class="selector-class">.data</span> as Data</div><div class="line">import torchvision</div><div class="line">from matplotlib import pyplot as plt</div></pre></td></tr></table></figure><ul><li>下载数据集</li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">EPOCH = 10</div><div class="line">BATCH_SIZE = 50</div><div class="line">LR = 0.001</div><div class="line">train_data = torchvision.datasets.MNIST(root='./mnist', </div><div class="line">                                        transform=torchvision.transforms.ToTensor(),</div><div class="line">                                        download=False)  # first set True, then set False</div><div class="line"></div><div class="line">print(train_data.train_data.size())</div><div class="line">test_data = torchvision.datasets.MNIST(root='./mnist', train=False)</div></pre></td></tr></table></figure><ul><li><p>处理数据</p><ul><li>使用 <code>DataLoader</code> 进行<code>batch</code>训练</li><li>将测试数据放到<code>Variable</code>里，并加上一个维度（在第二维位置dim=1），因为下面训练时是<code>(batch_size, 1, 28, 28)</code><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">train_loader = Data.DataLoader(dataset=train_data, batch_size=128, shuffle=True)</div><div class="line"># shape from (total_size, 28, 28) to (total_size, 1, 28, 28)</div><div class="line">test_x = Variable(torch.unsqueeze(test_data.test_data, dim=1), volatile=True).type(torch.FloatTensor)/255.0</div><div class="line"></div><div class="line">test_y = test_data.test_labels</div></pre></td></tr></table></figure></li></ul></li><li><p>建立计算图模型</p></li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line">class CNN(torch.nn.Module):</div><div class="line">    def __init__(self):</div><div class="line">        super(CNN, self).__init__()</div><div class="line">        self.conv1 = torch.nn.Sequential( # input shape (1, 28, 28)</div><div class="line">            torch.nn.Conv2d(in_channels=1,</div><div class="line">                            out_channels=16,</div><div class="line">                            kernel_size=5,</div><div class="line">                            stride=1, </div><div class="line">                            padding=2),</div><div class="line">            torch.nn.ReLU(),</div><div class="line">            torch.nn.MaxPool2d(kernel_size=2) </div><div class="line">        )   # output shape (16, 14, 14)</div><div class="line">        self.conv2 = torch.nn.Sequential(</div><div class="line">            torch.nn.Conv2d(16, 32, 5, 1, 2),</div><div class="line">            torch.nn.ReLU(),</div><div class="line">            torch.nn.MaxPool2d(2)</div><div class="line">        )  # output shape (32, 7, 7)</div><div class="line">        self.out = torch.nn.Linear(in_features=32*7*7, out_features=10)</div><div class="line">    def forward(self, x):</div><div class="line">        x = self.conv1(x)</div><div class="line">        x = self.conv2(x)</div><div class="line">        x = x.view(x.size(0), -1) # 展平多维的卷积图成 (batch_size, 32 * 7 * 7)</div><div class="line">        output = self.out(x)</div><div class="line">        return output</div><div class="line">cnn = CNN()</div></pre></td></tr></table></figure><ul><li>定义优化器<code>optimizer</code>和损失</li></ul><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">optimizer = torch<span class="selector-class">.optim</span><span class="selector-class">.Adam</span>(cnn.parameters(), lr=LR)</div><div class="line">loss_func = torch<span class="selector-class">.nn</span><span class="selector-class">.CrossEntropyLoss</span>()</div></pre></td></tr></table></figure><ul><li>进行batch训练</li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">for epoch in range(EPOCH):</div><div class="line">    for i, (x, y) in enumerate(train_loader):</div><div class="line">        batch_x = Variable(x)</div><div class="line">        batch_y = Variable(y)</div><div class="line">        output = cnn(batch_x)</div><div class="line">        loss = loss_func(output, batch_y)</div><div class="line">        optimizer.zero_grad()</div><div class="line">        loss.backward()</div><div class="line">        optimizer.step()</div><div class="line">        if i % 50 == 0:</div><div class="line">            # 用 test 数据来验证准确率</div><div class="line">            test_output = cnn(test_x)</div><div class="line">            pred_y = torch.max(test_output, 1)[1].data.squeeze()</div><div class="line">            accuracy = sum(pred_y == test_y) / float(test_y.size(0))</div><div class="line">            print('Epoch: ', epoch, '| train loss: %.4f' % loss.data[0], '| test accuracy: %.2f' % accuracy)</div></pre></td></tr></table></figure><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul><li><a href="https://morvanzhou.github.io/tutorials/machine-learning/torch/" target="_blank" rel="external">https://morvanzhou.github.io/tutorials/machine-learning/torch/</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;一、PyTorch介绍&quot;&gt;&lt;a href=&quot;#一、PyTorch介绍&quot; class=&quot;headerlink&quot; title=&quot;一、PyTorch介绍&quot;&gt;&lt;/a&gt;一、PyTorch介绍&lt;/h1&gt;&lt;h2 id=&quot;1、说明&quot;&gt;&lt;a href=&quot;#1、说明&quot; class=&quot;headerlink&quot; title=&quot;1、说明&quot;&gt;&lt;/a&gt;1、说明&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;code&gt;PyTorch&lt;/code&gt; 是 &lt;code&gt;Torch&lt;/code&gt; 在 &lt;code&gt;Python&lt;/code&gt; 上的衍生（&lt;code&gt;Torch&lt;/code&gt; 是一个使用 &lt;code&gt;Lua&lt;/code&gt; 语言的神经网络库）&lt;/li&gt;
&lt;li&gt;和&lt;code&gt;tensorflow&lt;/code&gt;比较&lt;ul&gt;
&lt;li&gt;&lt;code&gt;PyTorch&lt;/code&gt;建立的神经网络是&lt;strong&gt;动态的&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Tensorflow&lt;/code&gt;是建立&lt;strong&gt;静态图&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Tensorflow&lt;/code&gt; 的高度工业化, 它的底层代码是很难看懂的. &lt;/li&gt;
&lt;li&gt;&lt;code&gt;PyTorch&lt;/code&gt; 好那么一点点, 如果你深入 &lt;code&gt;API&lt;/code&gt;, 你至少能比看 &lt;code&gt;Tensorflow&lt;/code&gt; 多看懂一点点 &lt;code&gt;PyTorch&lt;/code&gt; 的底层在干嘛.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="Python" scheme="http://lawlite.me/tags/Python/"/>
    
      <category term="PyTorch" scheme="http://lawlite.me/tags/PyTorch/"/>
    
  </entry>
  
  <entry>
    <title>Hexo+yilia主题实现文章目录和添加视频</title>
    <link href="http://lawlite.me/2017/04/17/Hexo-yilia%E4%B8%BB%E9%A2%98%E5%AE%9E%E7%8E%B0%E6%96%87%E7%AB%A0%E7%9B%AE%E5%BD%95%E5%92%8C%E6%B7%BB%E5%8A%A0%E8%A7%86%E9%A2%91/"/>
    <id>http://lawlite.me/2017/04/17/Hexo-yilia主题实现文章目录和添加视频/</id>
    <published>2017-04-17T07:00:21.000Z</published>
    <updated>2017-09-08T10:40:44.248Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一、说明"><a href="#一、说明" class="headerlink" title="一、说明"></a>一、说明</h1><ul><li>文章目录功能可以<a href="http://lawlite.me/2017/04/13/Hexo-Github%E5%AE%9E%E7%8E%B0%E7%9B%B8%E5%86%8C%E5%8A%9F%E8%83%BD/">点击这里查看</a></li><li>视频页面可以<a href="http://lawlite.me/photos/videos.html">点击这里查看</a></li><li>粗略实现，有问题可以在下方评论区留言，或者<a href="http://lawlite.me/%E7%95%99%E8%A8%80%E6%9D%BF/">留言板</a>留言</li></ul><h1 id="二、文章目录功能"><a href="#二、文章目录功能" class="headerlink" title="二、文章目录功能"></a>二、文章目录功能</h1><h2 id="1、添加CSS样式"><a href="#1、添加CSS样式" class="headerlink" title="1、添加CSS样式"></a>1、添加CSS样式</h2><ul><li>打开<code>themes\yilia\source</code>下的<code>main.234bc0.css</code>文件，添加如下代码：<ul><li><code>css</code>样式我也放到了<code>github</code>上：<a href="https://raw.githubusercontent.com/lawlite19/Blog-Back-Up/master/css/main.234bc0.css" target="_blank" rel="external">https://raw.githubusercontent.com/lawlite19/Blog-Back-Up/master/css/main.234bc0.css</a></li><li>使用的是别人的<code>css</code>，可能有冗余的部分</li></ul></li></ul><a id="more"></a><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">/* 新添加的 */</div><div class="line">#container .show-toc-btn,#container .toc-article&#123;display:block&#125;</div><div class="line">.toc-article&#123;z-index:100;background:#fff;border:1px solid #ccc;max-width:250px;min-width:150px;max-height:500px;overflow-y:auto;-webkit-box-shadow:5px 5px 2px #ccc;box-shadow:5px 5px 2px #ccc;font-size:12px;padding:10px;position:fixed;right:35px;top:129px&#125;.toc-article .toc-close&#123;font-weight:700;font-size:20px;cursor:pointer;float:right;color:#ccc&#125;.toc-article .toc-close:hover&#123;color:#000&#125;.toc-article .toc&#123;font-size:12px;padding:0;line-height:20px&#125;.toc-article .toc .toc-number&#123;color:#333&#125;.toc-article .toc .toc-text:hover&#123;text-decoration:underline;color:#2a6496&#125;.toc-article li&#123;list-style-type:none&#125;.toc-article .toc-level-1&#123;margin:4px 0&#125;.toc-article .toc-child&#123;&#125;@-moz-keyframes cd-bounce-1&#123;0%&#123;opacity:0;-o-transform:scale(1);-webkit-transform:scale(1);-moz-transform:scale(1);-ms-transform:scale(1);transform:scale(1)&#125;60%&#123;opacity:1;-o-transform:scale(1.01);-webkit-transform:scale(1.01);-moz-transform:scale(1.01);-ms-transform:scale(1.01);transform:scale(1.01)&#125;100%&#123;-o-transform:scale(1);-webkit-transform:scale(1);-moz-transform:scale(1);-ms-transform:scale(1);transform:scale(1)&#125;&#125;@-webkit-keyframes cd-bounce-1&#123;0%&#123;opacity:0;-o-transform:scale(1);-webkit-transform:scale(1);-moz-transform:scale(1);-ms-transform:scale(1);transform:scale(1)&#125;60%&#123;opacity:1;-o-transform:scale(1.01);-webkit-transform:scale(1.01);-moz-transform:scale(1.01);-ms-transform:scale(1.01);transform:scale(1.01)&#125;100%&#123;-o-transform:scale(1);-webkit-transform:scale(1);-moz-transform:scale(1);-ms-transform:scale(1);transform:scale(1)&#125;&#125;@-o-keyframes cd-bounce-1&#123;0%&#123;opacity:0;-o-transform:scale(1);-webkit-transform:scale(1);-moz-transform:scale(1);-ms-transform:scale(1);transform:scale(1)&#125;60%&#123;opacity:1;-o-transform:scale(1.01);-webkit-transform:scale(1.01);-moz-transform:scale(1.01);-ms-transform:scale(1.01);transform:scale(1.01)&#125;100%&#123;-o-transform:scale(1);-webkit-transform:scale(1);-moz-transform:scale(1);-ms-transform:scale(1);transform:scale(1)&#125;&#125;@keyframes cd-bounce-1&#123;0%&#123;opacity:0;-o-transform:scale(1);-webkit-transform:scale(1);-moz-transform:scale(1);-ms-transform:scale(1);transform:scale(1)&#125;60%&#123;opacity:1;-o-transform:scale(1.01);-webkit-transform:scale(1.01);-moz-transform:scale(1.01);-ms-transform:scale(1.01);transform:scale(1.01)&#125;100%&#123;-o-transform:scale(1);-webkit-transform:scale(1);-moz-transform:scale(1);-ms-transform:scale(1);transform:scale(1)&#125;&#125;.show-toc-btn&#123;display:none;z-index:10;width:30px;min-height:14px;overflow:hidden;padding:4px 6px 8px 5px;border:1px solid #ddd;border-right:none;position:fixed;right:40px;text-align:center;background-color:#f9f9f9&#125;.show-toc-btn .btn-bg&#123;margin-top:2px;display:block;width:16px;height:14px;background:url(http://7xtawy.com1.z0.glb.clouddn.com/show.png) no-repeat;-webkit-background-size:100%;-moz-background-size:100%;background-size:100%&#125;.show-toc-btn .btn-text&#123;color:#999;font-size:12px&#125;.show-toc-btn:hover&#123;cursor:pointer&#125;.show-toc-btn:hover .btn-bg&#123;background-position:0 -16px&#125;.show-toc-btn:hover .btn-text&#123;font-size:12px;color:#ea8010&#125;</div><div class="line"></div><div class="line">.toc-article li ol, .toc-article li ul &#123;</div><div class="line">    margin-left: 30px;</div><div class="line">&#125;</div><div class="line">.toc-article ol, .toc-article ul &#123;</div><div class="line">    margin: 10px 0;</div><div class="line">&#125;</div></pre></td></tr></table></figure><h2 id="2、修改article-ejs文件"><a href="#2、修改article-ejs文件" class="headerlink" title="2、修改article.ejs文件"></a>2、修改article.ejs文件</h2><ul><li>使用的是<code>Hexo</code>的<code>toc</code>函数</li><li>打开<code>themes\yilia\layout\_partial</code>文件夹下的<code>article.ejs</code>文件</li><li>在<code>&lt;/header&gt; &lt;% } %&gt;</code>下面加入如下内容（注意位置）</li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line">&lt;!-- 目录内容 --&gt;</div><div class="line">&lt;% if (!index &amp;&amp; post.toc)&#123; %&gt;</div><div class="line">    &lt;p class="show-toc-btn" id="show-toc-btn" onclick="showToc();" style="display:none"&gt;</div><div class="line">          &lt;span class="btn-bg"&gt;&lt;/span&gt;</div><div class="line">          &lt;span class="btn-text"&gt;文章导航&lt;/span&gt;</div><div class="line">          &lt;/p&gt;</div><div class="line">&lt;div id="toc-article" class="toc-article"&gt;</div><div class="line">    &lt;span id="toc-close" class="toc-close" title="隐藏导航" onclick="showBtn();"&gt;×&lt;/span&gt;</div><div class="line">&lt;strong class="toc-title"&gt;文章目录&lt;/strong&gt;</div><div class="line">           &lt;%- toc(post.content) %&gt;</div><div class="line">         &lt;/div&gt;</div><div class="line">   &lt;script type="text/javascript"&gt;</div><div class="line">function showToc()&#123;</div><div class="line">var toc_article = document.getElementById("toc-article");</div><div class="line">var show_toc_btn = document.getElementById("show-toc-btn");</div><div class="line">toc_article.setAttribute("style","display:block");</div><div class="line">show_toc_btn.setAttribute("style","display:none");</div><div class="line">&#125;;</div><div class="line">function showBtn()&#123;</div><div class="line">var toc_article = document.getElementById("toc-article");</div><div class="line">var show_toc_btn = document.getElementById("show-toc-btn");</div><div class="line">toc_article.setAttribute("style","display:none");</div><div class="line">show_toc_btn.setAttribute("style","display:block");</div><div class="line">&#125;;</div><div class="line">   &lt;/script&gt;</div><div class="line">      &lt;% &#125; %&gt;</div><div class="line">&lt;!-- 目录内容结束 --&gt;</div></pre></td></tr></table></figure><ul><li>然后若想要文章显示目录，在每篇文章开头加入：<code>toc: true</code></li></ul><h2 id="3、最终效果"><a href="#3、最终效果" class="headerlink" title="3、最终效果"></a>3、最终效果</h2><h3 id="1-电脑端"><a href="#1-电脑端" class="headerlink" title="(1) 电脑端"></a>(1) 电脑端</h3><ul><li><img src="/assets/blog_images/hexo+github/15.png" alt="电脑端显示目录" title="15"></li><li><img src="/assets/blog_images/hexo+github/16.png" alt="电脑端关闭目录" title="16"></li></ul><h3 id="2-手机端"><a href="#2-手机端" class="headerlink" title="(2) 手机端"></a>(2) 手机端</h3><ul><li><img src="/assets/blog_images/hexo+github/17.png" alt="手机端显示目录" title="17"></li><li><img src="/assets/blog_images/hexo+github/18.png" alt="手机端关闭目录" title="18"></li></ul><h1 id="三、添加视频"><a href="#三、添加视频" class="headerlink" title="三、添加视频"></a>三、添加视频</h1><ul><li>是在之前<strong>相册功能</strong>的基础之上，相册功能的实现<a href="http://lawlite.me/2017/04/13/Hexo-Github%E5%AE%9E%E7%8E%B0%E7%9B%B8%E5%86%8C%E5%8A%9F%E8%83%BD/">点击这里查看</a></li></ul><h2 id="1、添加视频样式css"><a href="#1、添加视频样式css" class="headerlink" title="1、添加视频样式css"></a>1、添加视频样式css</h2><ul><li>打开<strong>当前博客</strong><code>source\photos</code>文件夹下的<code>ins.css</code>文件</li><li>加入如下内容</li></ul><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/* ====== video ===== */</span></div><div class="line"><span class="selector-class">.video-container</span> &#123;</div><div class="line"><span class="attribute">z-index</span>: <span class="number">1</span>;</div><div class="line"><span class="attribute">position</span>: relative;</div><div class="line"><span class="attribute">padding-bottom</span>: <span class="number">56.25%</span>;</div><div class="line"><span class="attribute">margin</span>: <span class="number">0</span> auto;</div><div class="line"></div><div class="line">&#125;</div><div class="line"><span class="selector-class">.video-container</span> <span class="selector-tag">iframe</span>, <span class="selector-class">.video-container</span> <span class="selector-tag">object</span>, <span class="selector-class">.video-container</span> embed &#123;<span class="attribute">z-index</span>: <span class="number">1</span>;<span class="attribute">position</span>: absolute;<span class="attribute">top</span>: <span class="number">0</span>;<span class="attribute">left</span>: <span class="number">7%</span>;<span class="attribute">width</span>: <span class="number">85%</span>;<span class="attribute">height</span>: <span class="number">85%</span>;<span class="attribute">box-shadow</span>: <span class="number">0px</span> <span class="number">0px</span> <span class="number">20px</span> <span class="number">2px</span> <span class="number">#888888</span>;&#125;</div></pre></td></tr></table></figure><h2 id="2、添加视频"><a href="#2、添加视频" class="headerlink" title="2、添加视频"></a>2、添加视频</h2><ul><li>我这里添加的是<strong>优酷</strong>上面的视频</li><li>在<strong>当前博客</strong><code>source\photos</code>文件夹下建立<code>videos.ejs</code>文件</li><li>内容如下：</li></ul><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div></pre></td><td class="code"><pre><div class="line">---</div><div class="line">layout: post</div><div class="line">slug: <span class="string">"photos"</span></div><div class="line">title: <span class="string">"相册"</span></div><div class="line">noDate: <span class="string">"true"</span></div><div class="line">comments: <span class="string">"true"</span></div><div class="line">reward: <span class="string">"true"</span></div><div class="line">open_in_new: false</div><div class="line">---</div><div class="line">&lt;link rel=<span class="string">"stylesheet"</span> href=<span class="string">"./ins.css"</span>&gt;</div><div class="line">&lt;<span class="selector-tag">div</span> class=<span class="string">"photos-btn-wrap"</span>&gt;</div><div class="line">&lt;<span class="selector-tag">a</span> class=<span class="string">"photos-btn"</span> href=<span class="string">"/photos"</span>&gt;Photos&lt;/a&gt;</div><div class="line">&lt;<span class="selector-tag">a</span> class=<span class="string">"photos-btn active"</span> href=<span class="string">"/photos/videos.html"</span>&gt;Videos&lt;/a&gt;</div><div class="line">&lt;/div&gt;</div><div class="line"></div><div class="line"></div><div class="line">&lt;center&gt;&lt;h1&gt;指弹_女儿情&lt;/h1&gt;&lt;/center&gt;</div><div class="line">&lt;hr/&gt;</div><div class="line"></div><div class="line">&lt;center&gt;</div><div class="line">&lt;<span class="selector-tag">div</span> class=<span class="string">"video-container"</span>&gt;</div><div class="line">&lt;<span class="selector-tag">iframe</span> <span class="attribute">height</span>=<span class="string">"80%"</span> width=<span class="string">"80%"</span> src=<span class="string">"http://player.youku.com/embed/XMjUzMzY4OTM3Ng=="</span> </div><div class="line">frameborder=<span class="number">0</span> allowfullscreen&gt;</div><div class="line">&lt;/iframe&gt;</div><div class="line">&lt;/div&gt;</div><div class="line">&lt;/center&gt;</div><div class="line"></div><div class="line">&lt;hr/&gt;</div><div class="line"></div><div class="line">&lt;center&gt;&lt;h1&gt;指弹_友谊地久天长&lt;/h1&gt;&lt;/center&gt;</div><div class="line">&lt;hr/&gt;</div><div class="line">&lt;center&gt;</div><div class="line">&lt;<span class="selector-tag">div</span> class=<span class="string">"video-container"</span>&gt;</div><div class="line">&lt;<span class="selector-tag">iframe</span> <span class="attribute">height</span>=<span class="string">"80%"</span> width=<span class="string">"80%"</span> src=<span class="string">"http://player.youku.com/embed/XMjQ5MDExOTY2MA=="</span> </div><div class="line">frameborder=<span class="number">0</span> allowfullscreen&gt;</div><div class="line">&lt;/iframe&gt;</div><div class="line">&lt;/div&gt;</div><div class="line">&lt;/center&gt;</div><div class="line">&lt;hr/&gt;</div><div class="line"></div><div class="line">&lt;center&gt;&lt;h1&gt;指弹_Always with me&lt;/h1&gt;&lt;/center&gt;</div><div class="line">&lt;hr/&gt;</div><div class="line">&lt;center&gt;</div><div class="line">&lt;<span class="selector-tag">div</span> class=<span class="string">"video-container"</span>&gt;</div><div class="line">&lt;<span class="selector-tag">iframe</span> <span class="attribute">height</span>=<span class="string">"80%"</span> width=<span class="string">"80%"</span> src=<span class="string">"http://player.youku.com/embed/XMjQ4MDQyNTQ0MA=="</span> </div><div class="line">frameborder=<span class="number">0</span> allowfullscreen&gt;</div><div class="line">&lt;/iframe&gt;</div><div class="line">&lt;/div&gt;</div><div class="line">&lt;/center&gt;</div></pre></td></tr></table></figure><h2 id="3、最终效果-1"><a href="#3、最终效果-1" class="headerlink" title="3、最终效果"></a>3、最终效果</h2><h3 id="1-电脑端-1"><a href="#1-电脑端-1" class="headerlink" title="(1) 电脑端"></a>(1) 电脑端</h3><ul><li><img src="/assets/blog_images/hexo+github/19.png" alt="电脑端" title="19"></li></ul><h3 id="2-手机端-1"><a href="#2-手机端-1" class="headerlink" title="(2) 手机端"></a>(2) 手机端</h3><ul><li><img src="/assets/blog_images/hexo+github/20.png" alt="手机端" title="20"></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;一、说明&quot;&gt;&lt;a href=&quot;#一、说明&quot; class=&quot;headerlink&quot; title=&quot;一、说明&quot;&gt;&lt;/a&gt;一、说明&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;文章目录功能可以&lt;a href=&quot;http://lawlite.me/2017/04/13/Hexo-Github%E5%AE%9E%E7%8E%B0%E7%9B%B8%E5%86%8C%E5%8A%9F%E8%83%BD/&quot;&gt;点击这里查看&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;视频页面可以&lt;a href=&quot;http://lawlite.me/photos/videos.html&quot;&gt;点击这里查看&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;粗略实现，有问题可以在下方评论区留言，或者&lt;a href=&quot;http://lawlite.me/%E7%95%99%E8%A8%80%E6%9D%BF/&quot;&gt;留言板&lt;/a&gt;留言&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&quot;二、文章目录功能&quot;&gt;&lt;a href=&quot;#二、文章目录功能&quot; class=&quot;headerlink&quot; title=&quot;二、文章目录功能&quot;&gt;&lt;/a&gt;二、文章目录功能&lt;/h1&gt;&lt;h2 id=&quot;1、添加CSS样式&quot;&gt;&lt;a href=&quot;#1、添加CSS样式&quot; class=&quot;headerlink&quot; title=&quot;1、添加CSS样式&quot;&gt;&lt;/a&gt;1、添加CSS样式&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;打开&lt;code&gt;themes\yilia\source&lt;/code&gt;下的&lt;code&gt;main.234bc0.css&lt;/code&gt;文件，添加如下代码：&lt;ul&gt;
&lt;li&gt;&lt;code&gt;css&lt;/code&gt;样式我也放到了&lt;code&gt;github&lt;/code&gt;上：&lt;a href=&quot;https://raw.githubusercontent.com/lawlite19/Blog-Back-Up/master/css/main.234bc0.css&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://raw.githubusercontent.com/lawlite19/Blog-Back-Up/master/css/main.234bc0.css&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;使用的是别人的&lt;code&gt;css&lt;/code&gt;，可能有冗余的部分&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="Github" scheme="http://lawlite.me/tags/Github/"/>
    
      <category term="Hexo" scheme="http://lawlite.me/tags/Hexo/"/>
    
  </entry>
  
  <entry>
    <title>Hexo+Github实现相册功能</title>
    <link href="http://lawlite.me/2017/04/13/Hexo-Github%E5%AE%9E%E7%8E%B0%E7%9B%B8%E5%86%8C%E5%8A%9F%E8%83%BD/"/>
    <id>http://lawlite.me/2017/04/13/Hexo-Github实现相册功能/</id>
    <published>2017-04-13T12:27:03.000Z</published>
    <updated>2017-06-25T08:48:03.007Z</updated>
    
    <content type="html"><![CDATA[<ul><li>最终效果请看这里：<a href="http://lawlite.me/photos/">http://lawlite.me/photos/</a></li><li>粗略实现，有问题可以在下方评论区评论，或者<a href="http://lawlite.me/%E7%95%99%E8%A8%80%E6%9D%BF/">留言区</a>留言</li></ul><h2 id="一、说明"><a href="#一、说明" class="headerlink" title="一、说明"></a>一、说明</h2><h3 id="1、关于"><a href="#1、关于" class="headerlink" title="1、关于"></a>1、关于</h3><ul><li>我使用的主题是<a href="https://github.com/litten/hexo-theme-yilia" target="_blank" rel="external">hexo-theme-yilia</a>，其中实现相册功能的方案是同步<code>instagram</code>上面的图片，但是现在<code>instagram</code>被禁，不能使用了</li><li>下面是通过自己的方式实现了相册功能，其中的<strong>样式</strong>还是使用该主题提供的<h3 id="2、方案"><a href="#2、方案" class="headerlink" title="2、方案"></a>2、方案</h3></li><li>在<code>github</code>上<strong>新建一个仓库</strong>，主要用于存储图片，可以通过<code>url</code>访问到，也方便管理</li><li>将要放到相册的图片处理成<code>json</code>格式的数据，然后进行访问，这里<code>json</code>的格式需要配合要使用的样式，所以需要处理成<strong>特定格式</strong>的<code>json</code>数据，下面会给出</li><li><strong>图片裁剪</strong>，因为相册显示的样式最好是<strong>正方形</strong>的的图片，这里使用脚本处理一下</li><li><strong>图片压缩</strong>，相册显示的图片是压缩后的图片，提高加载的速度，打开后的图片是原图。</li></ul><a id="more"></a><h2 id="二、实现"><a href="#二、实现" class="headerlink" title="二、实现"></a>二、实现</h2><h3 id="1、github操作"><a href="#1、github操作" class="headerlink" title="1、github操作"></a>1、github操作</h3><ul><li>建立一个用于存储相册的仓库，我这里建立了名为<code>Blog-Back-Up</code>的仓库<br><img src="/assets/blog_images/hexo+github/10.png" alt="enter description here" title="10"></li><li>关于<code>git</code>的命令行操作和配置不再给出</li></ul><h3 id="2、博客操作"><a href="#2、博客操作" class="headerlink" title="2、博客操作"></a>2、博客操作</h3><ul><li>在<strong>博客</strong>的<code>source</code>文件夹下建立一个<code>photos</code>文件夹</li><li>将样式文件放到<code>photos</code>文件夹下，样式文件我都放到了<code>github</code>上：<a href="https://github.com/lawlite19/Blog-Back-Up/tree/master/blog_photos_copy" target="_blank" rel="external">https://github.com/lawlite19/Blog-Back-Up/tree/master/blog_photos_copy</a></li><li>修改<code>ins.js</code>文件，主要是里面的<code>render</code>函数<ul><li>其中的<code>url</code>对应到你的<code>github</code>放图片的地址<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line">var render = function render(res) &#123;</div><div class="line">  var ulTmpl = "";</div><div class="line">  for (var j = 0, len2 = res.list.length; j &lt; len2; j++) &#123;</div><div class="line">    var data = res.list[j].arr;</div><div class="line">    var liTmpl = "";</div><div class="line">    for (var i = 0, len = data.link.length; i &lt; len; i++) &#123;</div><div class="line">      var minSrc = 'https://raw.githubusercontent.com/lawlite19/blog-back-up/master/min_photos/' + data.link[i];</div><div class="line">      var src = 'https://raw.githubusercontent.com/lawlite19/blog-back-up/master/photos/' + data.link[i];</div><div class="line">      var type = data.type[i];</div><div class="line">      var target = src + (type === 'video' ? '.mp4' : '.jpg');</div><div class="line">      src += '';</div><div class="line"></div><div class="line">      liTmpl += '&lt;figure class="thumb" itemprop="associatedMedia" itemscope="" itemtype="http://schema.org/ImageObject"&gt;\</div><div class="line">            &lt;a href="' + src + '" itemprop="contentUrl" data-size="1080x1080" data-type="' + type + '" data-target="' + src + '"&gt;\</div><div class="line">              &lt;img class="reward-img" data-type="' + type + '" data-src="' + minSrc + '" src="/assets/img/empty.png" itemprop="thumbnail" onload="lzld(this)"&gt;\</div><div class="line">            &lt;/a&gt;\</div><div class="line">            &lt;figcaption style="display:none" itemprop="caption description"&gt;' + data.text[i] + '&lt;/figcaption&gt;\</div><div class="line">        &lt;/figure&gt;';</div><div class="line">    &#125;</div><div class="line">    ulTmpl = ulTmpl + '&lt;section class="archives album"&gt;&lt;h1 class="year"&gt;' + data.year + '年&lt;em&gt;' + data.month + '月&lt;/em&gt;&lt;/h1&gt;\</div><div class="line">    &lt;ul class="img-box-ul"&gt;' + liTmpl + '&lt;/ul&gt;\</div><div class="line">    &lt;/section&gt;';</div><div class="line">  &#125;</div></pre></td></tr></table></figure></li></ul></li></ul><h3 id="3、图片处理"><a href="#3、图片处理" class="headerlink" title="3、图片处理"></a>3、图片处理</h3><ul><li><strong>python脚本文件</strong>都放在了这里：<a href="https://github.com/lawlite19/Blog-Back-Up" target="_blank" rel="external">https://github.com/lawlite19/Blog-Back-Up</a><h4 id="1-裁剪图片"><a href="#1-裁剪图片" class="headerlink" title="(1) 裁剪图片"></a>(1) 裁剪图片</h4></li><li>去图片的中间部分，裁剪为<strong>正方形</strong></li><li>对应的裁剪函数<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line">def cut_by_ratio(self):  </div><div class="line">    """按照图片长宽进行分割</div><div class="line">    </div><div class="line">    ------------</div><div class="line">    取中间的部分，裁剪成正方形</div><div class="line">    """  </div><div class="line">    im = Image.open(self.infile)  </div><div class="line">    (x, y) = im.size  </div><div class="line">    if x &gt; y:  </div><div class="line">        region = (int(x/2-y/2), 0, int(x/2+y/2), y)  </div><div class="line">        #裁切图片  </div><div class="line">        crop_img = im.crop(region)  </div><div class="line">        #保存裁切后的图片  </div><div class="line">        crop_img.save(self.outfile)             </div><div class="line">    elif x &lt; y:  </div><div class="line">        region = (0, int(y/2-x/2), x, int(y/2+x/2))</div><div class="line">        #裁切图片  </div><div class="line">        crop_img = im.crop(region)  </div><div class="line">        #保存裁切后的图片  </div><div class="line">        crop_img.save(self.outfile)</div></pre></td></tr></table></figure></li></ul><h4 id="2-压缩图片"><a href="#2-压缩图片" class="headerlink" title="(2) 压缩图片"></a>(2) 压缩图片</h4><ul><li>把图片进行压缩，方便相册的加载<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line">def compress(choose, des_dir, src_dir, file_list):</div><div class="line">    """压缩算法，img.thumbnail对图片进行压缩，</div><div class="line">    </div><div class="line">    参数</div><div class="line">    -----------</div><div class="line">    choose: str</div><div class="line">            选择压缩的比例，有4个选项，越大压缩后的图片越小</div><div class="line">    """</div><div class="line">    if choose == '1':</div><div class="line">        scale = SIZE_normal</div><div class="line">    if choose == '2':</div><div class="line">        scale = SIZE_small</div><div class="line">    if choose == '3':</div><div class="line">        scale = SIZE_more_small</div><div class="line">    if choose == '4':</div><div class="line">        scale = SIZE_more_small_small</div><div class="line">    for infile in file_list:</div><div class="line">        img = Image.open(src_dir+infile)</div><div class="line">        # size_of_file = os.path.getsize(infile)</div><div class="line">        w, h = img.size</div><div class="line">        img.thumbnail((int(w/scale), int(h/scale)))</div><div class="line">        img.save(des_dir + infile)</div></pre></td></tr></table></figure></li></ul><h3 id="4、github提交"><a href="#4、github提交" class="headerlink" title="4、github提交"></a>4、github提交</h3><ul><li>处理完成之后需要将处理后的图片提交到<code>github</code>上</li><li>这里同样使用脚本的方式，需要将<code>git</code>命令行配置到环境变量中<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">def git_operation():</div><div class="line">    '''</div><div class="line">    git 命令行函数，将仓库提交</div><div class="line">    </div><div class="line">    ----------</div><div class="line">    需要安装git命令行工具，并且添加到环境变量中</div><div class="line">    '''</div><div class="line">    os.system('git add --all')</div><div class="line">    os.system('git commit -m "add photos"')</div><div class="line">    os.system('git push origin master')</div></pre></td></tr></table></figure></li></ul><h3 id="5、json数据处理"><a href="#5、json数据处理" class="headerlink" title="5、json数据处理"></a>5、json数据处理</h3><ul><li>下面就需要将图片信息处理成<code>json</code>数据格式了，这里为重点</li><li>最终需要的json格式的数据如下图：<br><img src="/assets/blog_images/hexo+github/11.png" alt="json数据格式" title="11"></li><li>这里我采用的方式是读取<strong>图片的名字</strong>作为其中的<strong>text</strong>的内容，图片的命名如下图<ul><li>最前面是日期，然后用<code>_</code>进行分隔</li><li>后面是图片的描述信息，注意不要包含<code>_</code>和<code>.</code>符号<br><img src="/assets/blog_images/hexo+github/12.png" alt="enter description here" title="12"></li></ul></li><li>实现代码：<ul><li>注意代码中<code>../lawlite19.github.io/source/photos/data.json</code>是对应到我的博客的路径，这里根据需要改成自己博客的路径</li></ul></li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div></pre></td><td class="code"><pre><div class="line">def handle_photo():</div><div class="line">    '''根据图片的文件名处理成需要的json格式的数据</div><div class="line">    </div><div class="line">    -----------</div><div class="line">    最后将data.json文件存到博客的source/photos文件夹下</div><div class="line">    '''</div><div class="line">    src_dir, des_dir = "photos/", "min_photos/"</div><div class="line">    file_list = list_img_file(src_dir)</div><div class="line">    list_info = []</div><div class="line">    for i in range(len(file_list)):</div><div class="line">        filename = file_list[i]</div><div class="line">        date_str, info = filename.split("_")</div><div class="line">        info, _ = info.split(".")</div><div class="line">        date = datetime.strptime(date_str, "%Y-%m-%d")</div><div class="line">        year_month = date_str[0:7]            </div><div class="line">        if i == 0:  # 处理第一个文件</div><div class="line">            new_dict = &#123;"date": year_month, "arr":&#123;'year': date.year,</div><div class="line">                                                                   'month': date.month,</div><div class="line">                                                                   'link': [filename],</div><div class="line">                                                                   'text': [info],</div><div class="line">                                                                   'type': ['image']</div><div class="line">                                                                   &#125;</div><div class="line">                                        &#125; </div><div class="line">            list_info.append(new_dict)</div><div class="line">        elif year_month != list_info[-1]['date']:  # 不是最后的一个日期，就新建一个dict</div><div class="line">            new_dict = &#123;"date": year_month, "arr":&#123;'year': date.year,</div><div class="line">                                                   'month': date.month,</div><div class="line">                                                   'link': [filename],</div><div class="line">                                                   'text': [info],</div><div class="line">                                                   'type': ['image']</div><div class="line">                                                   &#125;</div><div class="line">                        &#125;</div><div class="line">            list_info.append(new_dict)</div><div class="line">        else:  # 同一个日期</div><div class="line">            list_info[-1]['arr']['link'].append(filename)</div><div class="line">            list_info[-1]['arr']['text'].append(info)</div><div class="line">            list_info[-1]['arr']['type'].append('image')</div><div class="line">    list_info.reverse()  # 翻转</div><div class="line">    final_dict = &#123;"list": list_info&#125;</div><div class="line">    with open("../lawlite19.github.io/source/photos/data.json","w") as fp:</div><div class="line">        json.dump(final_dict, fp)</div></pre></td></tr></table></figure><ul><li>每次图片有改动都需要执行此脚本文件<h2 id="三、其他"><a href="#三、其他" class="headerlink" title="三、其他"></a>三、其他</h2></li><li>你可以根据需要进行修改<code>python</code>脚本代码，这里一些细节可能处理的不好</li><li>留言板：<a href="http://lawlite.me/%E7%95%99%E8%A8%80%E6%9D%BF/">http://lawlite.me/%E7%95%99%E8%A8%80%E6%9D%BF/</a></li><li>效果展示<ul><li>相册<br><img src="/assets/blog_images/hexo+github/13.png" alt="enter description here" title="13"></li><li>留言板<br><img src="/assets/blog_images/hexo+github/14.png" alt="enter description here" title="14"></li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;ul&gt;
&lt;li&gt;最终效果请看这里：&lt;a href=&quot;http://lawlite.me/photos/&quot;&gt;http://lawlite.me/photos/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;粗略实现，有问题可以在下方评论区评论，或者&lt;a href=&quot;http://lawlite.me/%E7%95%99%E8%A8%80%E6%9D%BF/&quot;&gt;留言区&lt;/a&gt;留言&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;一、说明&quot;&gt;&lt;a href=&quot;#一、说明&quot; class=&quot;headerlink&quot; title=&quot;一、说明&quot;&gt;&lt;/a&gt;一、说明&lt;/h2&gt;&lt;h3 id=&quot;1、关于&quot;&gt;&lt;a href=&quot;#1、关于&quot; class=&quot;headerlink&quot; title=&quot;1、关于&quot;&gt;&lt;/a&gt;1、关于&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;我使用的主题是&lt;a href=&quot;https://github.com/litten/hexo-theme-yilia&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;hexo-theme-yilia&lt;/a&gt;，其中实现相册功能的方案是同步&lt;code&gt;instagram&lt;/code&gt;上面的图片，但是现在&lt;code&gt;instagram&lt;/code&gt;被禁，不能使用了&lt;/li&gt;
&lt;li&gt;下面是通过自己的方式实现了相册功能，其中的&lt;strong&gt;样式&lt;/strong&gt;还是使用该主题提供的&lt;h3 id=&quot;2、方案&quot;&gt;&lt;a href=&quot;#2、方案&quot; class=&quot;headerlink&quot; title=&quot;2、方案&quot;&gt;&lt;/a&gt;2、方案&lt;/h3&gt;&lt;/li&gt;
&lt;li&gt;在&lt;code&gt;github&lt;/code&gt;上&lt;strong&gt;新建一个仓库&lt;/strong&gt;，主要用于存储图片，可以通过&lt;code&gt;url&lt;/code&gt;访问到，也方便管理&lt;/li&gt;
&lt;li&gt;将要放到相册的图片处理成&lt;code&gt;json&lt;/code&gt;格式的数据，然后进行访问，这里&lt;code&gt;json&lt;/code&gt;的格式需要配合要使用的样式，所以需要处理成&lt;strong&gt;特定格式&lt;/strong&gt;的&lt;code&gt;json&lt;/code&gt;数据，下面会给出&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;图片裁剪&lt;/strong&gt;，因为相册显示的样式最好是&lt;strong&gt;正方形&lt;/strong&gt;的的图片，这里使用脚本处理一下&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;图片压缩&lt;/strong&gt;，相册显示的图片是压缩后的图片，提高加载的速度，打开后的图片是原图。&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="Github" scheme="http://lawlite.me/tags/Github/"/>
    
      <category term="Hexo" scheme="http://lawlite.me/tags/Hexo/"/>
    
  </entry>
  
  <entry>
    <title>Hexo+Github搭建自己的博客</title>
    <link href="http://lawlite.me/2017/04/10/Hexo-Github%E6%90%AD%E5%BB%BA%E8%87%AA%E5%B7%B1%E7%9A%84%E5%8D%9A%E5%AE%A2/"/>
    <id>http://lawlite.me/2017/04/10/Hexo-Github搭建自己的博客/</id>
    <published>2017-04-10T14:49:15.000Z</published>
    <updated>2017-09-08T10:40:29.671Z</updated>
    
    <content type="html"><![CDATA[<ul><li>最终效果可以查看：<a href="http://lawlite.me/">http://lawlite.me/</a></li><li>后序继续完善，有问题可以联系我或是下面评论</li></ul><h2 id="一、说明"><a href="#一、说明" class="headerlink" title="一、说明"></a>一、说明</h2><ul><li>关于一些基本软件的安装和配置这里不再给出<ul><li>安装<code>NodeJS</code>：<a href="http://nodejs.cn/download/" target="_blank" rel="external">http://nodejs.cn/download/</a><ul><li>需要配置环境变量</li></ul></li><li>安装<code>git</code>工具：<a href="https://git-for-windows.github.io/" target="_blank" rel="external">https://git-for-windows.github.io/</a><ul><li>注册<code>github</code>账号</li><li>配置<code>SSH-key</code></li><li>创建名为<code>userName.github.io</code>的仓库,<code>userName</code>是你申请的用户名</li></ul></li></ul></li></ul><a id="more"></a><h2 id="二、安装Hexo和基本使用"><a href="#二、安装Hexo和基本使用" class="headerlink" title="二、安装Hexo和基本使用"></a>二、安装Hexo和基本使用</h2><ul><li>安装<strong>Hexo</strong>: <code>npm install -g hexo</code></li><li>初始化<strong>Hexo</strong>: <code>hexo init</code></li><li>生成静态页面：<code>hexo generate</code> 或者 <code>hexo g</code></li><li>启动服务器：<code>hexo server</code> 或者 <code>hexo s</code><ul><li>注意：服务器默认是<code>4000</code>端口，若是安装了<strong>福昕阅读器</strong>可能端口冲突</li><li>可以制定端口：<code>hexo s -p5000</code></li></ul></li><li>浏览器中访问：<code>http://localhost:4000</code></li></ul><h2 id="三、更换主题Theme及基本配置"><a href="#三、更换主题Theme及基本配置" class="headerlink" title="三、更换主题Theme及基本配置"></a>三、更换主题Theme及基本配置</h2><h3 id="1、更换主题"><a href="#1、更换主题" class="headerlink" title="1、更换主题"></a>1、更换主题</h3><ul><li>默认主题是<code>landscape</code>，在<code>themes</code>文件夹下，可以使用别人开发好的主题，<a href="https://github.com/hexojs/hexo/wiki/Themes" target="_blank" rel="external">这里</a>有很多，我使用的是这一个: <a href="https://github.com/litten/hexo-theme-yilia" target="_blank" rel="external">https://github.com/litten/hexo-theme-yilia</a><ul><li>下载之后放到<code>themes</code>文件夹下即可：<code>git clone git@github.com:litten/hexo-theme-yilia.git</code></li></ul></li></ul><h3 id="2、主题基本配置"><a href="#2、主题基本配置" class="headerlink" title="2、主题基本配置"></a>2、主题基本配置</h3><ul><li>配置在<code>_config.yml</code>文件中，基本的配置尝试一下就知道了，不在给出<h4 id="1-图片的位置："><a href="#1-图片的位置：" class="headerlink" title="(1) 图片的位置："></a>(1) 图片的位置：</h4></li><li>比如<strong>打赏</strong>的支付宝二维码图片，是在<strong>当前博客</strong>的<code>source/assets/img/</code>下 （不是当前主题）<br><img src="/assets/blog_images/hexo+github/01.png" alt="enter description here" title="01"></li><li>配置：<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"># 打赏基础设定：0-关闭打赏； 1-文章对应的md文件里有reward:true属性，才有打赏； 2-所有文章均有打赏</div><div class="line">reward_type: 1</div><div class="line"># 打赏wording</div><div class="line">reward_wording: '谢谢你请我吃糖果'</div><div class="line"># 支付宝二维码图片地址，跟你设置头像的方式一样。比如：/assets/img/alipay.jpg</div><div class="line">alipay: /assets/img/alipay.jpg</div><div class="line"># 微信二维码图片地址</div><div class="line">weixin: /assets/img/weixin.png</div></pre></td></tr></table></figure></li></ul><h4 id="2-百度、谷歌统计配置"><a href="#2-百度、谷歌统计配置" class="headerlink" title="(2) 百度、谷歌统计配置"></a>(2) 百度、谷歌统计配置</h4><ul><li>申请账号：<a href="https://tongji.baidu.com/web/welcome/login" target="_blank" rel="external">https://tongji.baidu.com/web/welcome/login</a></li><li>在<strong>代码获取</strong>的地方只要填入<strong>key</strong>即可<br><img src="/assets/blog_images/hexo+github/02.png" alt="enter description here" title="02"></li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"># Miscellaneous</div><div class="line">baidu_analytics: '454d1a5ba8ed29xxxxxxxx'</div><div class="line">google_analytics: 'UA-9700xxxxxxxx'</div></pre></td></tr></table></figure><ul><li>就可以统计网站访问情况了，如下图，<br><img src="/assets/blog_images/hexo+github/03.png" alt="enter description here" title="03"></li><li>谷歌统计同理</li></ul><h4 id="3-文章评论设置"><a href="#3-文章评论设置" class="headerlink" title="(3) 文章评论设置"></a>(3) 文章评论设置</h4><ul><li>由于主题之实现了<strong>多说</strong>和<strong>disqus</strong>的第三方评论功能，这里不配置</li><li>因为<strong>多说</strong>6月份要关闭了，<strong>disqus</strong>需要翻墙访问才行，还有<strong>友言</strong>不支持<code>https</code>协议，因为<code>github</code>使用的是<code>https</code>协议</li><li>下面会给出使用<strong>网易云跟帖</strong>和<strong>来必力</strong>的第三方评论功能</li></ul><h2 id="四、博客的基本配置"><a href="#四、博客的基本配置" class="headerlink" title="四、博客的基本配置"></a>四、博客的基本配置</h2><h3 id="1、部署配置"><a href="#1、部署配置" class="headerlink" title="1、部署配置"></a>1、部署配置</h3><ul><li>配置到<code>github</code>对应的仓库中<ul><li>使用<code>hexo deploy</code>或<code>hexo d</code>命令即可发布到github仓库中</li><li>浏览器输入网址<code>https://userName.github.io</code>即可访问（<code>userName</code>对应你的用户名）</li></ul></li></ul><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">deploy:</div><div class="line">  type: git</div><div class="line">  repo: git@github<span class="selector-class">.com</span>:lawlite19/lawlite19<span class="selector-class">.github</span><span class="selector-class">.io</span><span class="selector-class">.git</span></div><div class="line">  branch: master</div></pre></td></tr></table></figure><h3 id="2、主题配置"><a href="#2、主题配置" class="headerlink" title="2、主题配置"></a>2、主题配置</h3><ul><li>设置为你下载的主题：<code>theme: yilia</code><h3 id="3、其他"><a href="#3、其他" class="headerlink" title="3、其他"></a>3、其他</h3></li><li>加入如下配置，<figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line">jsonContent:</div><div class="line">  meta: false</div><div class="line">  pages: false</div><div class="line">  posts:</div><div class="line">    title: true</div><div class="line">    date: true</div><div class="line">    path: true</div><div class="line">    text: true</div><div class="line">    raw: false</div><div class="line">    <span class="attribute">content</span>: false</div><div class="line">    slug: false</div><div class="line">    updated: false</div><div class="line">    comments: false</div><div class="line">    link: false</div><div class="line">    permalink: false</div><div class="line">    excerpt: false</div><div class="line">    categories: false</div><div class="line">    tags: true</div></pre></td></tr></table></figure></li></ul><h2 id="五、进阶功能配置"><a href="#五、进阶功能配置" class="headerlink" title="五、进阶功能配置"></a>五、进阶功能配置</h2><h3 id="1、网站访问量显示"><a href="#1、网站访问量显示" class="headerlink" title="1、网站访问量显示"></a>1、网站访问量显示</h3><h4 id="1-效果"><a href="#1-效果" class="headerlink" title="(1) 效果"></a>(1) 效果</h4><p><img src="/assets/blog_images/hexo+github/04.png" alt="enter description here" title="04"></p><h4 id="2-实现"><a href="#2-实现" class="headerlink" title="(2) 实现"></a>(2) 实现</h4><ul><li>我使用了<strong>不蒜子</strong>第三方的统计插件，网址：<a href="http://ibruce.info/2015/04/04/busuanzi/" target="_blank" rel="external">http://ibruce.info/2015/04/04/busuanzi/</a></li><li>在<code>themes\yilia\layout\_partial</code>下的<code>footer.ejs</code>中加入如下代码即可</li></ul><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">&lt;script async src=<span class="string">"//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"</span>&gt;</div><div class="line">&lt;/script&gt;</div><div class="line">&lt;<span class="selector-tag">span</span> id=<span class="string">"busuanzi_container_site_pv"</span>&gt;</div><div class="line">  本站总访问量&lt;<span class="selector-tag">span</span> id=<span class="string">"busuanzi_value_site_pv"</span>&gt;&lt;/span&gt;次</div><div class="line">&lt;/span&gt;</div><div class="line">&lt;<span class="selector-tag">span</span> id=<span class="string">"busuanzi_container_site_uv"</span>&gt;</div><div class="line">总访客数&lt;<span class="selector-tag">span</span> id=<span class="string">"busuanzi_value_site_uv"</span>&gt;&lt;/span&gt;人次</div><div class="line">&lt;/span&gt;</div></pre></td></tr></table></figure><h3 id="2、实现单篇文章浏览统计和评论统计"><a href="#2、实现单篇文章浏览统计和评论统计" class="headerlink" title="2、实现单篇文章浏览统计和评论统计"></a>2、实现单篇文章浏览统计和评论统计</h3><ul><li>评论数的统计是<strong>网易云跟帖</strong>中获取的，下面给出<h4 id="1-效果-1"><a href="#1-效果-1" class="headerlink" title="(1) 效果"></a>(1) 效果</h4><img src="/assets/blog_images/hexo+github/05.png" alt="enter description here" title="05"><h4 id="2-实现-1"><a href="#2-实现-1" class="headerlink" title="(2) 实现"></a>(2) 实现</h4></li><li>修改<code>themes\yilia\layout\_partial</code>文件夹下的<code>article.ejs</code>文件</li><li>在<code>&lt;%- partial(&#39;post/title&#39;, {class_name: &#39;article-title&#39;}) %&gt;</code>节点下加入：<ul><li>注意这里<strong>网易云跟帖</strong>还没设置，而<strong>评论数</strong>中使用到了，这里运行会有问题，下面给出<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">&lt;!-- 显示阅读和评论数 --&gt;</div><div class="line">&lt;% if (!index &amp;&amp; post.comments &amp;&amp; config.wangYi)&#123; %&gt;</div><div class="line">&lt;br/&gt;</div><div class="line">&lt;a class="cloud-tie-join-count" href="javascript:void(0);" style="color:gray;font-size:14px;"&gt;</div><div class="line">&lt;span class="icon-sort"&gt;&lt;/span&gt;</div><div class="line">&lt;span id="busuanzi_container_page_pv" style="color:#ef7522;font-size:14px;"&gt;</div><div class="line">          阅读数: &lt;span id="busuanzi_value_page_pv"&gt;&lt;/span&gt;次 &amp;nbsp;&amp;nbsp;</div><div class="line">&lt;/span&gt;</div><div class="line">&lt;/a&gt;</div><div class="line">&lt;a class="cloud-tie-join-count" href="javascript:void(0);" style="color:#ef7522;font-size:14px;"&gt;</div><div class="line">&lt;span class="icon-comment"&gt;&lt;/span&gt;</div><div class="line">&lt;span class="join-text" style="color:#ef7522;font-size:14px;"&gt;评论数:&lt;/span&gt;</div><div class="line">&lt;span class="join-count"&gt;0&lt;/span&gt;次</div><div class="line">&lt;/a&gt;</div><div class="line">&lt;% &#125; %&gt;</div></pre></td></tr></table></figure></li></ul></li></ul><h3 id="3、实现网易云跟帖评论"><a href="#3、实现网易云跟帖评论" class="headerlink" title="3、实现网易云跟帖评论"></a>3、实现网易云跟帖评论</h3><h4 id="1-效果-2"><a href="#1-效果-2" class="headerlink" title="(1) 效果"></a>(1) 效果</h4><p><img src="/assets/blog_images/hexo+github/06.png" alt="enter description here" title="06"></p><h4 id="2-实现-2"><a href="#2-实现-2" class="headerlink" title="(2) 实现"></a>(2) 实现</h4><ul><li>注册账号：<a href="https://gentie.163.com/info.html" target="_blank" rel="external">https://gentie.163.com/info.html</a></li><li>填写完成之后获取<code>WEB</code>代码</li><li>修改<code>themes\yilia\layout\_partial</code>文件夹下的<code>article.ejs</code>文件</li><li>在最后加入<ul><li>这里需要注意下，一个站点不同端标识文章的方式必须统一（同一站点可以采用以下方式标识文章：①<code>URL</code>；②<code>Sourceid+productKey</code> ；③<code>URL+Sourceid+productKey</code>，建议用②或者③），否则跟贴数据可能错乱。比如同一站点PC端采用URL，APP采用Sourceid+productKey，这种情况跟贴数据会错乱，必选采用统一方式标识。</li><li>这里使用方式②</li></ul></li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">&lt;!-- 网易云跟帖 --&gt;</div><div class="line">&lt;% if (!index &amp;&amp; post.comments &amp;&amp; config.wangYi)&#123; %&gt;</div><div class="line">&lt;section class="duoshuo" id="comments"&gt;</div><div class="line">&lt;div id="cloud-tie-wrapper" class="cloud-tie-wrapper"&gt;&lt;/div&gt;</div><div class="line">&lt;script&gt;</div><div class="line">  var cloudTieConfig = &#123;</div><div class="line">    url: "", </div><div class="line">    sourceId: "&lt;%= post.path%&gt;",</div><div class="line">    productKey: "&lt;%= config.wangYi%&gt;",</div><div class="line">    target: "cloud-tie-wrapper"</div><div class="line">  &#125;;</div><div class="line">&lt;/script&gt;</div><div class="line">&lt;script src="https://img1.cache.netease.com/f2e/tie/yun/sdk/loader.js"&gt;&lt;/script&gt;</div><div class="line">&lt;/section&gt;</div><div class="line">&lt;% &#125; %&gt;</div></pre></td></tr></table></figure><ul><li>在<strong>博客</strong>的配置文件<code>_config.yml</code>最后加入获取代码中的<code>productKey</code><br><img src="/assets/blog_images/hexo+github/07.png" alt="enter description here" title="07"></li></ul><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">wangYi: <span class="number">06</span>ab5cdc0b4c45efb39xxxxxxxxxxxx</div></pre></td></tr></table></figure><ul><li>发布到<code>github</code>上可以查看效果</li></ul><h2 id="六、绑定到申请的域名"><a href="#六、绑定到申请的域名" class="headerlink" title="六、绑定到申请的域名"></a>六、绑定到申请的域名</h2><ul><li>可以绑定到自己申请的域名上，不用使用<code>userName.github.io</code>访问了，直接使用自己的域名访问<h3 id="1、申请域名"><a href="#1、申请域名" class="headerlink" title="1、申请域名"></a>1、申请域名</h3></li><li>我在万网购买的域名，地址：<a href="https://wanwang.aliyun.com/domain/com?spm=5176.8142029.388261.137.LoKzy7" target="_blank" rel="external">https://wanwang.aliyun.com/domain/com?spm=5176.8142029.388261.137.LoKzy7</a></li><li>我这里是<code>.me</code>结尾的域名，一年<code>13</code>大洋<h3 id="2、解析域名"><a href="#2、解析域名" class="headerlink" title="2、解析域名"></a>2、解析域名</h3></li><li>添加如下的解析<br><img src="/assets/blog_images/hexo+github/09.png" alt="enter description here" title="09"><h3 id="3、配置一下"><a href="#3、配置一下" class="headerlink" title="3、配置一下"></a>3、配置一下</h3></li><li>在<strong>博客</strong>的<code>source</code>文件夹下建立一个<code>CNAME</code>的文件</li><li>内容写入你的域名信息，比如我这里是<code>lawlite.me</code></li><li>发布到<code>github</code>即可<h3 id="4、细节说明"><a href="#4、细节说明" class="headerlink" title="4、细节说明"></a>4、细节说明</h3></li><li>之前<strong>网易云跟帖</strong>，<strong>百度统计</strong>设置的域名这里对应该过来一下<h2 id="七、写作的一些说明"><a href="#七、写作的一些说明" class="headerlink" title="七、写作的一些说明"></a>七、写作的一些说明</h2></li><li>执行命令：<code>hexo new  &quot;xxxx&quot;</code>创建<code>Markdown</code>文件，在<strong>博客</strong>的<code>source\_posts</code>文件夹下</li><li>比如如下例子，<ul><li><code>comments</code>设置为<code>true</code>允许评论，若设置为<code>false</code>则不能评论</li><li><code>reward</code>设置为<code>true</code>允许打赏，若设置为<code>false</code>则不能打赏，（注意对应主题的配置文件<code>reward_type</code>: 设置的为<code>1</code>）<br><img src="/assets/blog_images/hexo+github/08.png" alt="enter description here" title="08"></li></ul></li><li>在文章中加入<code>&lt;!-- more --&gt;</code>将文章<strong>截断</strong>显示在<strong>主页</strong></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;ul&gt;
&lt;li&gt;最终效果可以查看：&lt;a href=&quot;http://lawlite.me/&quot;&gt;http://lawlite.me/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;后序继续完善，有问题可以联系我或是下面评论&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;一、说明&quot;&gt;&lt;a href=&quot;#一、说明&quot; class=&quot;headerlink&quot; title=&quot;一、说明&quot;&gt;&lt;/a&gt;一、说明&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;关于一些基本软件的安装和配置这里不再给出&lt;ul&gt;
&lt;li&gt;安装&lt;code&gt;NodeJS&lt;/code&gt;：&lt;a href=&quot;http://nodejs.cn/download/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://nodejs.cn/download/&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;需要配置环境变量&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;安装&lt;code&gt;git&lt;/code&gt;工具：&lt;a href=&quot;https://git-for-windows.github.io/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://git-for-windows.github.io/&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;注册&lt;code&gt;github&lt;/code&gt;账号&lt;/li&gt;
&lt;li&gt;配置&lt;code&gt;SSH-key&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;创建名为&lt;code&gt;userName.github.io&lt;/code&gt;的仓库,&lt;code&gt;userName&lt;/code&gt;是你申请的用户名&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="Github" scheme="http://lawlite.me/tags/Github/"/>
    
      <category term="Hexo" scheme="http://lawlite.me/tags/Hexo/"/>
    
  </entry>
  
  <entry>
    <title>致火影</title>
    <link href="http://lawlite.me/2017/03/24/%E8%87%B4%E7%81%AB%E5%BD%B1/"/>
    <id>http://lawlite.me/2017/03/24/致火影/</id>
    <published>2017-03-24T13:57:12.000Z</published>
    <updated>2017-06-25T09:17:26.463Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/assets/随笔_images/鸣人-雏田.png" alt="enter description here" title="鸣人-雏田"><br> <center> <h1>致火影</h1></center></p><p><center>——只要有树叶飞舞的地方，火就会燃烧。</center><br>　　昨天就知道火影动漫也完结了，但是没有马上去看，想抽个正式点的时间。<br><a id="more"></a></p><p>　　漫画是700集完结，当时动漫到700的时候就有个打算想写点东西记录一下，但是没有动手。今天准备看时还在思考，看完一些回忆涌上，果断提笔。<br>　　初三的暑假，当时是在补课，一位小伙伴有火影的光盘，当时就借来看看。记得每天最多能看几十集，当时光盘里面应该是有300集左右。<br>　　暑假结束，步入高一，当时并不知道有漫画（毕竟高一才有的QQ），还在军训，班级里面有同学买的关于火影的海报，那时漫画里讲到鼬双重间谍的身份，以及多么爱他的弟弟佐助。后来有了个诺亚舟学习机（当然现在还在），有时周末就去网吧下载火影动漫看。现在来说有的一集看了不止10遍，当然我周围的小伙伴也有一块看的。<br>　　高一结束分班，我后面一排的一位小伙伴也看火影，每次周日下午回校，他都和我讨论，当时讨论的还有死神（死神、柯南都有看，但火影是我唯一看的完整的动漫（不算死亡笔记这种比较短的动漫））。<br>　　大三的时候火影漫画700完结（700之后的5话是番外），当时写了一段话，但没有发出来。当动漫700之后几集的片尾曲唱到：さようなら（再见）的时候，些许感慨，之后看的时候的片头曲和片尾曲很少跳过。<br>　　还记得岸本齐史（AB大叔）有说过，刚开始画火影的时候他还没有结婚，就像鸣人一样希望得到别人的注意，后来结婚，漫画里的鸣人也渐渐的有了朋友。<br>　　最后定格在鸣人雏田结婚。<br>　　16岁到24岁，谢谢鸣人，谢谢火影!</p><p align="right">——思念你的人所在的地方就是你的归宿！</p><p align="right">2017年3月24日</p><p><img src="/assets/随笔_images/记录.jpg" alt="enter description here" title="IMG_20170324_214803"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;/assets/随笔_images/鸣人-雏田.png&quot; alt=&quot;enter description here&quot; title=&quot;鸣人-雏田&quot;&gt;&lt;br&gt; &lt;center&gt; &lt;h1&gt;致火影&lt;/h1&gt;&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;&lt;center&gt;——只要有树叶飞舞的地方，火就会燃烧。&lt;/center&gt;&lt;br&gt;　　昨天就知道火影动漫也完结了，但是没有马上去看，想抽个正式点的时间。&lt;br&gt;
    
    </summary>
    
    
      <category term="随笔" scheme="http://lawlite.me/tags/%E9%9A%8F%E7%AC%94/"/>
    
  </entry>
  
  <entry>
    <title>Keras学习</title>
    <link href="http://lawlite.me/2017/02/14/Keras%E5%AD%A6%E4%B9%A0/"/>
    <id>http://lawlite.me/2017/02/14/Keras学习/</id>
    <published>2017-02-14T12:25:43.000Z</published>
    <updated>2017-06-25T08:48:43.490Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一、Keras概述"><a href="#一、Keras概述" class="headerlink" title="一、Keras概述"></a>一、Keras概述</h2><h3 id="1、介绍"><a href="#1、介绍" class="headerlink" title="1、介绍"></a>1、介绍</h3><ul><li><code>Keras</code> 是一个兼容 <code>Theano</code> 和 <code>Tensorflow</code> 的神经网络高级包</li><li>用他来组件一个神经网络更加快速, 几条语句就搞定</li><li><code>Keras</code> 可以再在 <code>Windows</code> 和 <code>MacOS</code> 或者 <code>Linux</code> 上运行</li><li>网站：<a href="https://keras.io/" target="_blank" rel="external">https://keras.io/</a></li></ul><a id="more"></a><h3 id="2、安装Keras"><a href="#2、安装Keras" class="headerlink" title="2、安装Keras"></a>2、安装Keras</h3><ul><li>需要事先安装好<code>numpy</code>和<code>scipy</code></li><li>直接pip安装：<code>pip install keras</code></li><li><code>Keras</code>有两个<strong>backend</strong>,就是是基于什么进行运算的，一个是<strong>Tensorflow</strong>，一个是<strong>Theano</strong></li><li><p>通过修改配置文件永久修改</p><ul><li>默认配置是<strong>Tensorflow</strong>，这里改为<strong>Theano</strong></li><li>Windows在用户的文件夹下有个配置文件：<code>C:\Users\bob\.keras</code>文件夹下的<code>keras.json</code>文件</li><li>修改即可<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">&#123;</div><div class="line">  &quot;image_dim_ordering&quot;: &quot;tf&quot;, </div><div class="line">  &quot;epsilon&quot;: 1e-07, </div><div class="line">  &quot;floatx&quot;: &quot;float32&quot;, </div><div class="line">  &quot;backend&quot;: &quot;theano&quot;</div><div class="line">&#125;</div></pre></td></tr></table></figure></li></ul></li><li><p>修改当前脚本的环境变量</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">import os </div><div class="line">os.environ[&apos;KERAS_BACKEND&apos;]=&apos;tensorflow&apos;  # 或者theano</div><div class="line">import keras</div></pre></td></tr></table></figure></li></ul><h2 id="二、搭建神经网络"><a href="#二、搭建神经网络" class="headerlink" title="二、搭建神经网络"></a>二、搭建神经网络</h2><h3 id="1、一个神经网络例子"><a href="#1、一个神经网络例子" class="headerlink" title="1、一个神经网络例子"></a>1、一个神经网络例子</h3><ul><li><p>导入包</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">import keras</div><div class="line">import numpy as np</div><div class="line">from keras.models import Sequential   # Sequential顺序建立</div><div class="line">from keras.layers import Dense        # 全连接层</div><div class="line">import matplotlib.pyplot as plt</div></pre></td></tr></table></figure></li><li><p>制造数据</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">&apos;&apos;&apos;制造数据，并且显示&apos;&apos;&apos;</div><div class="line">X = np.linspace(-1,1,200)</div><div class="line">np.random.shuffle(X)</div><div class="line">Y = 0.5 * X + 2 + np.random.normal(0,0.05,(200,))</div><div class="line">plt.scatter(X,Y)</div><div class="line">plt.show()</div><div class="line">X_train,Y_train = X[:160],Y[:160]</div><div class="line">X_test,Y_test = X[160:],Y[160:]</div></pre></td></tr></table></figure></li><li><p>建立模型</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">&apos;&apos;&apos;建立模型&apos;&apos;&apos;</div><div class="line">    model = Sequential()   # 通过Sequential建立model</div><div class="line">    model.add(Dense(output_dim=1, input_dim=1))   # model.add添加神经层，指定输入和输出维度</div></pre></td></tr></table></figure></li><li><p>激活模型</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">&apos;&apos;&apos;激活模型&apos;&apos;&apos;</div><div class="line">    model.compile(optimizer=&apos;sgd&apos;, loss=&apos;mse&apos;)</div></pre></td></tr></table></figure></li><li><p>训练模型</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">for i in range(500):</div><div class="line">    cost = model.train_on_batch(X_train,Y_train)  # 使用批训练</div><div class="line">    if i % 50 == 0:</div><div class="line">        print(cost)</div></pre></td></tr></table></figure></li><li><p>测试集的cost误差</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">cost = model.evaluate(X_test, Y_test, batch_size=40)</div><div class="line">print(cost)</div></pre></td></tr></table></figure></li><li><p>学到的权重和偏置</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">&apos;&apos;&apos;输出学到的权重和偏置&apos;&apos;&apos;</div><div class="line">    W,b = model.layers[0].get_weights()</div><div class="line">    print(W,b)</div></pre></td></tr></table></figure></li><li><p>预测</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">Y_pred = model.predict(X_test)</div></pre></td></tr></table></figure></li></ul><h3 id="2、手写数字识别例子–mnist"><a href="#2、手写数字识别例子–mnist" class="headerlink" title="2、手写数字识别例子–mnist"></a>2、手写数字识别例子–mnist</h3><ul><li><p>导入包：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">import keras</div><div class="line">from keras.datasets import mnist</div><div class="line">from keras.utils import np_utils</div><div class="line">import numpy as np</div><div class="line">from keras.models import Sequential   # Sequential顺序建立</div><div class="line">from keras.layers import Dense,Activation  # 全连接层</div><div class="line">from keras.optimizers import RMSprop</div></pre></td></tr></table></figure></li><li><p>加载并预处理数据</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">&apos;&apos;&apos;加载和预处理数据&apos;&apos;&apos;</div><div class="line">(X_train,y_train),(X_test,y_test) = mnist.load_data()   # 下载数据集，windows在当前用户的对应目录下：C:\Users\bob\.keras\datasets</div><div class="line">X_train = X_train.reshape(X_train.shape[0],-1)/255      # X_train是(60000, 28, 28)，reshape一下变成(60000,784),然后在标准化</div><div class="line">X_test  = X_test.reshape(X_test.shape[0],-1)/255</div><div class="line">y_train = np_utils.to_categorical(y_train,nb_classes=10) # y_train对应的数字1，2，3....转换为0/1映射</div><div class="line">y_test  = np_utils.to_categorical(y_test,nb_classes=10)</div></pre></td></tr></table></figure></li><li><p>建立模型</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">&apos;&apos;&apos;建立模型&apos;&apos;&apos;</div><div class="line">model = Sequential(layers=[</div><div class="line">    Dense(output_dim=32,input_dim=784),   # 第一层，输入为784维，输出为32维</div><div class="line">    Activation(&apos;relu&apos;),                   # 激励函数为relu</div><div class="line">    Dense(10),                            # 第二层，这里不需要指定输入层维度，全连接会使用上一层的输出，这里即32</div><div class="line">    Activation(&apos;softmax&apos;),                # 激励函数，也是最后的预测函数使用softmax</div><div class="line">    ])</div></pre></td></tr></table></figure></li><li><p>激活模型</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">&apos;&apos;&apos;定义optimizer&apos;&apos;&apos;</div><div class="line">rmsprop = RMSprop()</div><div class="line">&apos;&apos;&apos;激活模型&apos;&apos;&apos;</div><div class="line">model.compile(optimizer=rmsprop, </div><div class="line">              loss=&apos;categorical_crossentropy&apos;,   # 分类中使用交叉熵损失函数</div><div class="line">              metrics=[&apos;accuracy&apos;])              # 计算准确度</div></pre></td></tr></table></figure></li><li><p>训练模型</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">model.fit(X_train,y_train,nb_epoch=2,batch_size=100)  # nb_epoch整个训练集训练次数</div></pre></td></tr></table></figure></li><li><p>测试集上预测信息</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">&apos;&apos;&apos;测试集测试训练出的模型&apos;&apos;&apos;</div><div class="line">loss,accuracy = model.evaluate(X_test,y_test)</div><div class="line">print(&apos;loss:&apos;,loss)</div><div class="line">print(&apos;accuracy&apos;,accuracy)</div></pre></td></tr></table></figure></li></ul><h3 id="3、卷积神经网络CNN–mnist"><a href="#3、卷积神经网络CNN–mnist" class="headerlink" title="3、卷积神经网络CNN–mnist"></a>3、卷积神经网络CNN–mnist</h3><ul><li><p>导入包</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">import keras</div><div class="line">from keras.datasets import mnist</div><div class="line">from keras.utils import np_utils</div><div class="line">import numpy as np</div><div class="line">from keras.models import Sequential   # Sequential顺序建立</div><div class="line">from keras.layers import Dense,Activation,Convolution2D,MaxPooling2D,Flatten</div><div class="line">from keras.optimizers import RMSprop,Adam</div></pre></td></tr></table></figure></li><li><p>建立模型</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div></pre></td><td class="code"><pre><div class="line">model = Sequential()</div><div class="line">## 第一层卷积</div><div class="line">model.add(Convolution2D(nb_filter=32,    # 32个filter，即从32个特征提取</div><div class="line">                        nb_row=5,        # patch大小</div><div class="line">                        nb_col=5, </div><div class="line">                        border_mode=&apos;same&apos;, </div><div class="line">                        dim_ordering=&apos;th&apos;,  # theano使用th,TensorFlow使用tf</div><div class="line">                        input_shape=(1,28,28,)  # 输入的大小，1表示输入的channel通道，由于是黑白图所以是1,若是rgb是3个通道</div><div class="line">                        ))</div><div class="line">## 第一层激活层</div><div class="line">model.add(Activation(&apos;relu&apos;))</div><div class="line">## 第一层池化层</div><div class="line">model.add(MaxPooling2D(</div><div class="line">    pool_size=(2,2),    # 2x2的大小</div><div class="line">    strides=(2,2),      # 步长为2，纵向和横向</div><div class="line">    border_mode=&apos;same&apos;</div><div class="line">))</div><div class="line">### 第二层卷积层</div><div class="line">model.add(Convolution2D(nb_filter=64,      # 不需要指定输入的大小了</div><div class="line">                        nb_row=5,</div><div class="line">                        nb_col=5, </div><div class="line">                       border_mode=&apos;same&apos;</div><div class="line">                       ))</div><div class="line">### 第二层激活层</div><div class="line">model.add(Activation(&apos;relu&apos;))</div><div class="line">### 第二层池化层</div><div class="line">model.add(MaxPooling2D(border_mode=&apos;same&apos;))</div><div class="line">#### 全连接层</div><div class="line">model.add(Flatten())   # 展开</div><div class="line">model.add(Dense(output_dim=1024))  # 输出维度为1024</div><div class="line">model.add(Activation(&apos;relu&apos;))</div><div class="line">model.add(Dense(output_dim=10))    # 最终输出为10类</div><div class="line">model.add(Activation(&apos;softmax&apos;))</div></pre></td></tr></table></figure></li><li><p>激活模型</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">adam = Adam()</div><div class="line">model.compile(optimizer=adam,     # 使用adam的optimizer</div><div class="line">              loss=&apos;categorical_crossentropy&apos;,</div><div class="line">              metrics=[&apos;accuracy&apos;])</div></pre></td></tr></table></figure></li><li><p>训练模型</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">model.fit(X_train, y_train)</div></pre></td></tr></table></figure></li><li><p>测试集计算结果</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">&apos;&apos;&apos;测试集模型&apos;&apos;&apos;</div><div class="line">loss,accuracy = model.evaluate(X_test,y_test)</div><div class="line">print(&quot;loss&quot;,loss)</div><div class="line">print(&apos;accuracy&apos;,accuracy)</div></pre></td></tr></table></figure></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;一、Keras概述&quot;&gt;&lt;a href=&quot;#一、Keras概述&quot; class=&quot;headerlink&quot; title=&quot;一、Keras概述&quot;&gt;&lt;/a&gt;一、Keras概述&lt;/h2&gt;&lt;h3 id=&quot;1、介绍&quot;&gt;&lt;a href=&quot;#1、介绍&quot; class=&quot;headerlink&quot; title=&quot;1、介绍&quot;&gt;&lt;/a&gt;1、介绍&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;code&gt;Keras&lt;/code&gt; 是一个兼容 &lt;code&gt;Theano&lt;/code&gt; 和 &lt;code&gt;Tensorflow&lt;/code&gt; 的神经网络高级包&lt;/li&gt;
&lt;li&gt;用他来组件一个神经网络更加快速, 几条语句就搞定&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Keras&lt;/code&gt; 可以再在 &lt;code&gt;Windows&lt;/code&gt; 和 &lt;code&gt;MacOS&lt;/code&gt; 或者 &lt;code&gt;Linux&lt;/code&gt; 上运行&lt;/li&gt;
&lt;li&gt;网站：&lt;a href=&quot;https://keras.io/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://keras.io/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="Python" scheme="http://lawlite.me/tags/Python/"/>
    
      <category term="DeepLearning" scheme="http://lawlite.me/tags/DeepLearning/"/>
    
  </entry>
  
  <entry>
    <title>Theano学习</title>
    <link href="http://lawlite.me/2017/02/10/Theano%E5%AD%A6%E4%B9%A0/"/>
    <id>http://lawlite.me/2017/02/10/Theano学习/</id>
    <published>2017-02-10T13:25:43.000Z</published>
    <updated>2017-06-25T08:50:20.700Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一、Theano概述"><a href="#一、Theano概述" class="headerlink" title="一、Theano概述"></a>一、Theano概述</h2><h3 id="1、介绍"><a href="#1、介绍" class="headerlink" title="1、介绍"></a>1、介绍</h3><ul><li><code>Theano</code> 是神经网络python机器学习的模块，和 <code>Tensowflow</code> 类似</li><li>可以在MacOS、Linux、Windows上运行</li><li>theano 可以使用 GPU 进行运算</li><li>网址：<a href="http://deeplearning.net/software/theano/" target="_blank" rel="external">http://deeplearning.net/software/theano/</a> </li></ul><h3 id="2、安装"><a href="#2、安装" class="headerlink" title="2、安装"></a>2、安装</h3><ul><li>Windows上直接：<code>pip install theano</code><ul><li>可能提示个警告：<code>WARNING (theano.configdefaults): g++ not detected ! Theano will be unable to execute optimized C-implementations (for both CPU and GPU) and will default to Python implementations. Performance will be severely degraded. To remove this warning, set Theano flags cxx to an empty string.</code></li></ul></li></ul><a id="more"></a><h2 id="二、Theano基础"><a href="#二、Theano基础" class="headerlink" title="二、Theano基础"></a>二、Theano基础</h2><h3 id="1、基本用法"><a href="#1、基本用法" class="headerlink" title="1、基本用法"></a>1、基本用法</h3><ul><li><p>导入包</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">import numpy as np</div><div class="line">import theano.tensor as T</div><div class="line">from theano import function</div></pre></td></tr></table></figure></li><li><p>常量和方程定义</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">x = T.dscalar(&apos;x&apos;)  # 建立 x 的容器</div><div class="line">y = T.dscalar(&apos;y&apos;)  # 建立 y 的容器</div><div class="line">z = x + y           # 建立方程</div><div class="line">f = function([x, y],z) # 使用function定义方程，将输入值 x, y 放在 [] 里,  输出值 z 放在后面</div></pre></td></tr></table></figure></li><li><p><strong>pretty print</strong>打印原始方程</p><ul><li>导入包：<code>from theano import pp</code></li><li>打印即可：<code>print(pp(z))</code></li></ul></li><li>矩阵相乘<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">x = T.dmatrix(&apos;x&apos;)  # float64的矩阵,fmatrix对应float32</div><div class="line">y = T.dmatrix(&apos;y&apos;)</div><div class="line">z = T.dot(x,y)      # 相乘</div><div class="line">f = function([x,y],z)  # 定义function</div><div class="line">print(f(np.arange(12).reshape(3,4),</div><div class="line">      np.ones((4,3))))</div><div class="line">print(pp(z))</div></pre></td></tr></table></figure></li></ul><p>输出：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">[[  6.   6.   6.]</div><div class="line"> [ 22.  22.  22.]</div><div class="line"> [ 38.  38.  38.]]</div><div class="line">(x \dot y)</div></pre></td></tr></table></figure></p><h3 id="2、function用法"><a href="#2、function用法" class="headerlink" title="2、function用法"></a>2、function用法</h3><ul><li>theano 当中的 function 就和 python 中的 function类似，但是在theano中由于涉及到GPU加速以及CPU的并行的运算，所以他的function会有不同。</li><li><p>多输入和多输出</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">a,b = T.dmatrices(&apos;a&apos;,&apos;b&apos;)   # 定义两个容器</div><div class="line">diff = a - b</div><div class="line">abs_diff = abs(diff)         # 绝对值</div><div class="line">diff_square = diff ** 2      </div><div class="line">f = function([a,b],[diff,abs_diff,diff_square])  # function同样前面指定输入，后面是输出</div><div class="line">x1,x2,x3 = f(np.ones((2,2)),</div><div class="line">             np.arange(4).reshape(2,2))</div><div class="line">print(x1,x2,x3)</div></pre></td></tr></table></figure></li><li><p>指定function默认值和名字name</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">x,y,z = T.dscalars(&apos;x&apos;,&apos;y&apos;,&apos;z&apos;)  # 定义三个scalar容器</div><div class="line">w = (x + y) * z</div><div class="line">f = function([x,</div><div class="line">              theano.In(y,value=1),  # 输入y,默认值为1</div><div class="line">              theano.In(z,value=2,name=&apos;weights&apos;)], # 输入z,默认值为2，同时指定名字为weights，后面可以通过名字复制</div><div class="line">             w)</div><div class="line">print(f(2))         # 使用默认值</div><div class="line">print(f(2,2,3))     # 指定值</div><div class="line">print(f(2,2,weights=3)) # 通过name赋值</div></pre></td></tr></table></figure></li></ul><h3 id="3、Shared变量"><a href="#3、Shared变量" class="headerlink" title="3、Shared变量"></a>3、Shared变量</h3><ul><li><strong>Shared 变量</strong>，意思是这些变量可以在运算过程中，不停地进行交换和更新值。 在定义 <code>weights</code> 和 <code>bias</code> 的情况下，会需要用到这样的变量</li><li>定义shared变量<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">state = theano.shared(np.array(0,dtype=np.float64), name=&apos;state&apos;)  # state初值为0，为float64</div><div class="line">increase = T.scalar(&apos;increase&apos;,dtype=state.dtype)   # 定义一个容器，这里注意dtype为state.dtype,若是np.float64会报错</div><div class="line">accmulator = theano.function([increase],   # 输入</div><div class="line">                             state,        # 输出</div><div class="line">                             updates=[(state,state+increase)]) # 指定每次更新为累加</div><div class="line">print(state.get_value())# get_value()获取值</div><div class="line">accmulator(1)</div><div class="line">print(state.get_value())</div><div class="line">state.set_value(-1)     # set_value()设置值</div><div class="line">accmulator(1)</div><div class="line">print(state.get_value())</div></pre></td></tr></table></figure></li></ul><p>输出：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">0.0</div><div class="line">1.0</div><div class="line">0.0</div></pre></td></tr></table></figure></p><ul><li>临时使用shared变量<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">state = theano.shared(np.array(0,dtype=np.float64), name=&apos;state&apos;)  # state初值为0，为float64</div><div class="line">increase = T.scalar(&apos;increase&apos;,dtype=state.dtype)   # 定义一个容器，这里注意dtype为state.dtype,若是np.float64会报错</div><div class="line">tmp_func = state *2 +increase</div><div class="line">a = T.scalar(dtype=state.dtype)   # 定义标量a，后面用来带起state</div><div class="line">skip_shared = theano.function([increase,a],</div><div class="line">                              tmp_func,</div><div class="line">                              givens=[(state,a)])   # 指定givens参数用a代替state</div><div class="line">print(skip_shared(2,3))</div><div class="line">print(state.get_value())  # 输出state还是0</div></pre></td></tr></table></figure></li></ul><h3 id="4、Theano中的激励函数"><a href="#4、Theano中的激励函数" class="headerlink" title="4、Theano中的激励函数"></a>4、Theano中的激励函数</h3><ul><li><strong>sigmoid</strong>: <code>theano.tensor.nnet.nnet.sigmoid(x)</code></li><li>还有<code>relu</code>,<code>tanh</code>,<code>softmax</code>,<code>softplus</code>等</li><li>在隐含层中常用<code>relu,tanh,softplus</code>等非线性激励函数</li><li>在输出层常用<code>sigmoid，softmax</code>求概率</li></ul><h2 id="三、搭建神经网络"><a href="#三、搭建神经网络" class="headerlink" title="三、搭建神经网络"></a>三、搭建神经网络</h2><h3 id="1、定义Layer类或者函数"><a href="#1、定义Layer类或者函数" class="headerlink" title="1、定义Layer类或者函数"></a>1、定义<strong>Layer类</strong>或者<strong>函数</strong></h3><ul><li><p>以后想这样直接使用：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">l1 = Layer(inputs,1,10,T.nnet.relu)</div><div class="line">l2 = Layer(l1.outputs,10,1,None)</div></pre></td></tr></table></figure></li><li><p>实现代码：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">import theano</div><div class="line">import theano.tensor as T</div><div class="line">import numpy as np</div><div class="line">class Layer(object):</div><div class="line">    def __init__(self,inputs,in_size,out_size,activation_function=None):</div><div class="line">        self.W = theano.shared(np.random.normal(0,1,(in_size,out_size)))  # 使用高斯函数初始化，大小为in_size*out_size</div><div class="line">        self.b = theano.shared(np.zeros(out_size) + 0.1)                  # 指定偏置，大小为out_size，注意+0.1在shared内，否则使用会报错</div><div class="line">        self.Wx_plus_b = T.dot(inputs,self.W) + self.b</div><div class="line">        self.activation_function = activation_function</div><div class="line">        if activation_function is None:</div><div class="line">            self.outpus = self.Wx_plus_b</div><div class="line">        else:</div><div class="line">            self.outputs = self.activation_function(self.Wx_plus_b)</div></pre></td></tr></table></figure></li><li><p>指定构造函数的参数：<code>inputs,in_size,out_size,activation_function</code></p></li></ul><h3 id="2、一个神经网络例子"><a href="#2、一个神经网络例子" class="headerlink" title="2、一个神经网络例子"></a>2、一个神经网络例子</h3><ul><li><p>制造数据</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">&apos;&apos;&apos;制造假数据，并显示&apos;&apos;&apos;</div><div class="line">x_data = np.linspace(-1,1,300)[:,np.newaxis]  # [:,np.newaxis]是将(300,)转为(300,1),增加一个维度，将列表转化为矩阵</div><div class="line">noise = np.random.normal(0,0.05,x_data.shape)</div><div class="line">y_data = np.square(x_data) -0.5 + noise</div><div class="line">plt.scatter(x_data,y_data)</div><div class="line">plt.show()</div></pre></td></tr></table></figure></li><li><p>定义输入，相当于TensorFlow中的placeholder，后面传入真实数据</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">x = T.dmatrix(&apos;x&apos;)</div><div class="line">y = T.dmatrix(&apos;y&apos;)</div></pre></td></tr></table></figure></li><li><p>定义网络，使用上面定义的<code>Layer</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">l1 = Layer(x, 1, 10,T.nnet.relu)</div><div class="line">l2 = Layer(l1.outputs,10,1,None)</div></pre></td></tr></table></figure></li><li><p>定义<strong>cost</strong>,并且计算其<strong>梯度</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">cost = T.mean(T.square(l2.outputs-y))</div><div class="line">g_w1,g_b1,g_w2,g_b2 = T.grad(cost, [l1.W,l1.b,l2.W,l2.b])</div></pre></td></tr></table></figure></li><li><p>使用<code>theano</code>中的<strong>function</strong>进行梯度下降</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">learning_rate = 0.05</div><div class="line">&apos;&apos;&apos;调用function，使用梯度下降求解&apos;&apos;&apos;</div><div class="line">train = theano.function(inputs=[x,y],</div><div class="line">                        outputs=cost,</div><div class="line">                        updates=[(l1.W, l1.W - learning_rate * g_w1),</div><div class="line">                                 (l1.b, l1.b - learning_rate * g_b1),</div><div class="line">                                 (l2.W, l2.W - learning_rate * g_w2),</div><div class="line">                                 (l2.b, l2.b - learning_rate * g_b2)]</div><div class="line">                        )</div></pre></td></tr></table></figure></li><li><p>传入之前制造的数据训练</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">for i in range(1000):</div><div class="line">    err = train(x_data,y_data)  # 训练，传入真实数据</div><div class="line">    if i%50 == 0:</div><div class="line">        print(err)</div></pre></td></tr></table></figure></li><li><p>预测</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">&apos;&apos;&apos;预测，输入为x，输出为layer2的输出&apos;&apos;&apos;</div><div class="line">prediction = theano.function([x],l2.outputs)</div></pre></td></tr></table></figure></li><li><p>计算准确度函数</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">def compute_accuracy(y_target,y_predict):</div><div class="line">    correct_prediction = np.equal(y_target,y_predict)</div><div class="line">    accuracy = np.sum(correct_prediction)/len(correct_prediction)</div><div class="line">    return accuracy</div></pre></td></tr></table></figure></li></ul><h3 id="3、保存和提取模型"><a href="#3、保存和提取模型" class="headerlink" title="3、保存和提取模型"></a>3、保存和提取模型</h3><ul><li>导入包：<code>import pickle</code></li><li><p>保存模型–即学到的参数<strong>权重</strong>和<strong>偏置</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">&apos;&apos;&apos;保存神经网络--保存学到的参数&apos;&apos;&apos;</div><div class="line">with open(&apos;model.pickle&apos;, mode=&apos;wb&apos;) as file:</div><div class="line">    #model = [l1.W.get_value(),l1.b.get_value(),</div><div class="line">    #         l2.W.get_value(),l2.b.get_value()]</div><div class="line">    model = &#123;&apos;layer1_w&apos;:l1.W.get_value(),&apos;layer1_b&apos;:l1.b.get_value(),</div><div class="line">             &apos;layer2_w&apos;:l2.W.get_value(),&apos;layer2_b&apos;:l2.b.get_value()&#125;    # 保存为字典形式，通过get_value()获取值</div><div class="line">    pickle.dump(model, file)</div><div class="line">    print(model[&apos;layer1_w&apos;])</div></pre></td></tr></table></figure></li><li><p>提取模型</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">with open(&apos;model.pickle&apos;,&apos;rb&apos;) as file:</div><div class="line">    model = pickle.load(file)</div><div class="line">    l1.W.set_value(model[&apos;layer1.w&apos;])    # 通过set_value()将值设置进去</div><div class="line">    ...</div></pre></td></tr></table></figure></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;一、Theano概述&quot;&gt;&lt;a href=&quot;#一、Theano概述&quot; class=&quot;headerlink&quot; title=&quot;一、Theano概述&quot;&gt;&lt;/a&gt;一、Theano概述&lt;/h2&gt;&lt;h3 id=&quot;1、介绍&quot;&gt;&lt;a href=&quot;#1、介绍&quot; class=&quot;headerlink&quot; title=&quot;1、介绍&quot;&gt;&lt;/a&gt;1、介绍&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;code&gt;Theano&lt;/code&gt; 是神经网络python机器学习的模块，和 &lt;code&gt;Tensowflow&lt;/code&gt; 类似&lt;/li&gt;
&lt;li&gt;可以在MacOS、Linux、Windows上运行&lt;/li&gt;
&lt;li&gt;theano 可以使用 GPU 进行运算&lt;/li&gt;
&lt;li&gt;网址：&lt;a href=&quot;http://deeplearning.net/software/theano/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://deeplearning.net/software/theano/&lt;/a&gt; &lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;2、安装&quot;&gt;&lt;a href=&quot;#2、安装&quot; class=&quot;headerlink&quot; title=&quot;2、安装&quot;&gt;&lt;/a&gt;2、安装&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;Windows上直接：&lt;code&gt;pip install theano&lt;/code&gt;&lt;ul&gt;
&lt;li&gt;可能提示个警告：&lt;code&gt;WARNING (theano.configdefaults): g++ not detected ! Theano will be unable to execute optimized C-implementations (for both CPU and GPU) and will default to Python implementations. Performance will be severely degraded. To remove this warning, set Theano flags cxx to an empty string.&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="Python" scheme="http://lawlite.me/tags/Python/"/>
    
      <category term="DeepLearning" scheme="http://lawlite.me/tags/DeepLearning/"/>
    
  </entry>
  
  <entry>
    <title>论文记录-Batch-Normalization</title>
    <link href="http://lawlite.me/2017/01/09/%E8%AE%BA%E6%96%87%E8%AE%B0%E5%BD%95-Batch-Normalization/"/>
    <id>http://lawlite.me/2017/01/09/论文记录-Batch-Normalization/</id>
    <published>2017-01-09T13:37:38.000Z</published>
    <updated>2017-06-25T08:50:34.356Z</updated>
    
    <content type="html"><![CDATA[<ul><li>参考论文：<a href="http://jmlr.org/proceedings/papers/v37/ioffe15.pdf" target="_blank" rel="external">http://jmlr.org/proceedings/papers/v37/ioffe15.pdf</a></li></ul><h2 id="一、论文概述"><a href="#一、论文概述" class="headerlink" title="一、论文概述"></a>一、论文概述</h2><ul><li><code>2015</code>年<code>Google</code>提出的<code>Batch Normalization</code></li><li>训练深层的神经网络很复杂，因为训练时每一层输入的分布在变化，导致训练过程中的饱和，称这种现象为：<code>internal covariate shift</code></li><li>需要降低<strong>学习率Learning Rate</strong>和注意<strong>参数的初始化</strong></li></ul><a id="more"></a><ul><li>论文中提出的方法是对于每一个小的训练<code>batch</code>都进行<strong>标准化（正态化）</strong><ul><li>允许使用较大的<strong>学习率</strong></li><li>不必太关心<strong>初始化的问题</strong></li><li>同时一些例子中不需要使用<code>Dropout</code>方法避免过拟合</li><li>此方法在<code>ImageNet classification</code>比赛中获得<code>4.82% top-5</code>的测试错误率</li></ul></li></ul><h2 id="二、BN思路"><a href="#二、BN思路" class="headerlink" title="二、BN思路"></a>二、<code>BN</code>思路</h2><h3 id="1、问题"><a href="#1、问题" class="headerlink" title="1、问题"></a>1、问题</h3><ul><li><p>如果输入数据是<strong>白化</strong>的（whitened），网络会更快的<strong>收敛</strong></p><ul><li>白化<strong>目的</strong>是<strong>降低数据的冗余性和特征的相关性</strong>，例如通过<strong>线性变换</strong>使数据为<strong>0均值</strong>和<strong>单位方差</strong></li></ul></li><li><p><strong>并非直接标准化每一层</strong>那么简单，如果不考虑归一化的影响，可能会降低梯度下降的影响</p></li><li>标准化与某个样本和所有样本都有关系<ul><li>解决上面的问题，我们希望对于任何参数值，都要满足想要的分布；$$\widehat x Norm(x,\chi )$$</li><li>对于反向传播，需要计算:${\partial Norm(x,\chi )} \over {\partial x}$和${\partial Norm({x},\chi )} \over {\partial \chi }$</li><li>这样做的<strong>计算代价</strong>是非常大的，因为需要计算x的<strong>协方差矩阵</strong> </li><li>然后<strong>白化</strong>操作：$${x - E[x]} \over {\sqrt {Cov[x]} }$$</li></ul></li><li>上面两种都不行或是不好，进而得到了<strong>BN</strong>的方法</li><li>既然<strong>白化每一层</strong>的<strong>输入代价非常大</strong>，我们可以进行简化</li></ul><h3 id="2、简化1"><a href="#2、简化1" class="headerlink" title="2、简化1"></a>2、简化1</h3><ul><li>标准化特征的<strong>每一个维度</strong>而不是去标准化<strong>所有的特征</strong>，这样就不用求<strong>协方差矩阵</strong>了<ul><li>例如<code>d</code>维的输入：$$x = ({x^{(1)}},{x^{(2)}}, \cdots ,{x^{(d)}})$$</li><li>标准化操作：<br>$${\widehat x^k} = {x^{(k) - E[x^{(k)}]} \over {\sqrt {Var[x^{(k)}]} }}$$</li></ul></li><li><p>需要注意的是标准化操作可能会<strong>降低数据的表达能力</strong>,例如我们之前提到的<strong>Sigmoid函数</strong>，标准化之后<strong>均值为0</strong>，<strong>方差为1</strong>，数据就会落在<strong>近似线性</strong>的函数区域内，这样激活函数的意义就不明显<br><img src="/assets/blog_images/BN/sigmoid.png" alt="Sigmoid激励函数" title="sigmoid"></p></li><li><p>所以对于每个标准化之后的$\widehat x^{(k)}$，对应一对<strong>参数</strong>：${\gamma ^{(k)}},{\beta ^{(k)}}$ ，然后令：${y^{(k)}} = {\gamma ^{(k)}}{\widehat x^{(k)}} + {\beta ^{(k)}}$</p></li><li>从式子来看就是对标准化的数据进行<strong>缩放和平移</strong>，不至于使数据落在线性区域内，增加数据的表达能力（式子中如果：${\gamma ^{(k)}} = \sqrt {Var[x^{(k)}]},  {\beta ^{(k)}} = E[x^{(k)}]$ ，就会使<strong>恢复到原来的值</strong>了）</li><li>但是这里还是使用的<strong>全部的数据集</strong>，但是如果使用<strong>随机梯度下降</strong>，可以选取一个<strong>batch</strong>进行训练</li></ul><h3 id="3、简化2"><a href="#3、简化2" class="headerlink" title="3、简化2"></a>3、简化2</h3><ul><li>第二种简化就是使用<code>mini-batch</code>进行<code>随机梯度下降</code></li><li>注意这里使用<code>mini-batch</code>也是标准化<strong>每一个维度</strong>上的特征，而不是所有的特征一起，因为若果<code>mini-batch</code>中的数据量小于特征的维度时，会产生<strong>奇异协方差矩阵</strong>， 对应的<strong>行列式</strong>的值为0，非满秩</li><li>假设<code>mini-batch</code> 大小为<code>m</code>的<code>B</code></li><li>$B = \{ {x_{1 \ldots m}}\}$对应的变换操作为：$$B{N_{\gamma ,\beta }}:{x_{1 \ldots m}} \to {y_{1 \ldots m}}$$</li><li>作者给出的批标准化的算法如下：<br><img src="/assets/blog_images/BN/bn-algorithm.png" alt="BN算法" title="bn-algorithm"></li><li>算法中的<code>ε</code>是一个<strong>很小的常量</strong>，为了保证数值的稳定性（就是<strong>防止除数为0</strong>）</li></ul><h3 id="4、反向传播求梯度："><a href="#4、反向传播求梯度：" class="headerlink" title="4、反向传播求梯度："></a>4、反向传播求梯度：</h3><ul><li>因为：$$y^{(k)} = \gamma ^{(k)}\widehat x^{(k)} + \beta ^{(k)}$$<ul><li>所以：$${\partial l \over \partial \widehat x_i} = {\partial l \over \partial y_i}\gamma $$</li></ul></li><li><p>因为：$$\widehat x_i = {x_i - \mu _B \over {\sqrt {\sigma _B^2 + \varepsilon } }}$$</p><ul><li>所以：$${\partial l \over \partial \sigma _B^2} = \sum\limits_{i=1}^m {\partial l \over \partial \widehat x_i} (x_i- \mu_B) {-1 \over 2}(\sigma_B^2 + \varepsilon)^{-{3\over2}}$$<br>$${\partial l \over \partial u_B} = \sum\limits_{i = 1}^m {\partial l \over \partial \widehat x_i} { - 1 \over \sqrt {\sigma _B^2 + \varepsilon }}$$</li></ul></li><li><p>因为：${\mu _B} = {1 \over m}\sum\limits_{i = 1}^m $和$\sigma _B^2 = {1 \over m}\sum\limits_{i = 1}^m {({x_i}}  - {\mu _B}{)^2}$</p><ul><li>所以：$${\partial l \over \partial x_i} = {\partial l \over \partial \widehat x_i}{1 \over \sqrt {\sigma _B^2 + \varepsilon } } + {\partial l \over \partial \sigma _B^2}{2(x_i - \mu _B) \over m} + {\partial l \over \partial u_B}{1 \over m}$$<ul><li>所以：$${\partial l \over \partial \gamma } = \sum\limits_{i = 1}^m {\partial l \over \partial y_i} {\widehat x_i}$$<br>$${\partial l \over \partial \beta } = \sum\limits_{i = 1}^m {\partial l \over \partial y_i} $$</li></ul></li></ul></li><li>对于<strong>BN变换</strong>是<strong>可微分</strong>的，随着网络的训练，网络层可以持续学到输入的分布。</li></ul><h2 id="三、BN网络的训练和推断（预测）"><a href="#三、BN网络的训练和推断（预测）" class="headerlink" title="三、BN网络的训练和推断（预测）"></a>三、<code>BN</code>网络的训练和推断（预测）</h2><h3 id="1、预测的问题"><a href="#1、预测的问题" class="headerlink" title="1、预测的问题"></a>1、预测的问题</h3><ul><li>按照<code>BN</code>方法，输入数据<code>x</code>会经过变化得到<code>BN（x）</code>，然后可以通过<strong>随机梯度下降</strong>进行训练，标准化是在<code>mini-batch</code>上所以是非常高效的。</li><li>但是对于推断我们希望输出只取决于输入，而对于输入<strong>只有一个实例数据</strong>，无法得到<code>mini-batch</code>的其他实例，就<strong>无法求对应的均值和方差</strong>了。</li></ul><h3 id="2、解决方法"><a href="#2、解决方法" class="headerlink" title="2、解决方法"></a>2、解决方法</h3><ul><li>可以通过从所有<strong>训练实例中获得的统计量</strong>来代替<code>mini-batch</code>中<code>m</code>个训练实例获得<strong>统计量均值和方差</strong><ul><li>比如我们机器学习算法，在训练集上进行了<strong>标准化</strong>，在测试集上的标准化操作时利用的训练集上的数据(<code>Standarscaler</code>中的<code>mean</code>和<code>variance</code>)</li></ul></li><li>我们对每个<code>mini-batch</code>做标准化，可以对记住每个<code>mini-batch</code>的B，然后得到<strong>全局统计量</strong></li><li>$$E[x] \leftarrow E_B[{\mu _B}]$$</li><li>$$Var[x] \leftarrow {m \over {m - 1}}E_B[\sigma _B^2]$$（这里方差采用的是<strong>无偏</strong>方差估计，所以是<code>m-1</code>）</li><li>所以<strong>推断</strong>采用<code>BN</code>的方式为：<br>$$\eqalign{<br>&amp; y = \gamma {x - E(x) \over \sqrt {Var[x] + \varepsilon }} + \beta   \cr<br>&amp; \quad= {\gamma  \over \sqrt {Var[x] + \varepsilon }}x + (\beta  - {\gamma E[x] \over \sqrt {Var[x] + \varepsilon }})} $$<h3 id="3、完整算法"><a href="#3、完整算法" class="headerlink" title="3、完整算法"></a>3、完整算法</h3></li><li>作者给出的完整算法：<br><img src="/assets/blog_images/BN/bn-algorithm-complete.png" alt="BN 完整算法" title="bn-algorithm-complete"></li></ul><h2 id="四、实验"><a href="#四、实验" class="headerlink" title="四、实验"></a>四、实验</h2><ul><li>最后给出的实验可以看出使用BN的方式训练<strong>精准度</strong>很高而且很<strong>稳定</strong>。<br><img src="/assets/blog_images/BN/experience-result.png" alt="实验结果" title="experience-result"></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;ul&gt;
&lt;li&gt;参考论文：&lt;a href=&quot;http://jmlr.org/proceedings/papers/v37/ioffe15.pdf&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://jmlr.org/proceedings/papers/v37/ioffe15.pdf&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;一、论文概述&quot;&gt;&lt;a href=&quot;#一、论文概述&quot; class=&quot;headerlink&quot; title=&quot;一、论文概述&quot;&gt;&lt;/a&gt;一、论文概述&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;code&gt;2015&lt;/code&gt;年&lt;code&gt;Google&lt;/code&gt;提出的&lt;code&gt;Batch Normalization&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;训练深层的神经网络很复杂，因为训练时每一层输入的分布在变化，导致训练过程中的饱和，称这种现象为：&lt;code&gt;internal covariate shift&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;需要降低&lt;strong&gt;学习率Learning Rate&lt;/strong&gt;和注意&lt;strong&gt;参数的初始化&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="DeepLearning" scheme="http://lawlite.me/tags/DeepLearning/"/>
    
      <category term="Paper阅读记录" scheme="http://lawlite.me/tags/Paper%E9%98%85%E8%AF%BB%E8%AE%B0%E5%BD%95/"/>
    
  </entry>
  
  <entry>
    <title>论文记录-Relu激励函数权重初始化</title>
    <link href="http://lawlite.me/2017/01/09/%E8%AE%BA%E6%96%87%E8%AE%B0%E5%BD%95-Relu%E6%BF%80%E5%8A%B1%E5%87%BD%E6%95%B0%E6%9D%83%E9%87%8D%E5%88%9D%E5%A7%8B%E5%8C%96/"/>
    <id>http://lawlite.me/2017/01/09/论文记录-Relu激励函数权重初始化/</id>
    <published>2017-01-09T07:20:00.000Z</published>
    <updated>2017-06-25T08:50:41.140Z</updated>
    
    <content type="html"><![CDATA[<ul><li>参考论文：<a href="https://arxiv.org/pdf/1502.01852v1.pdf" target="_blank" rel="external">点击这里查看</a></li><li><a href="http://lawlite.me/2016/12/20/%E8%AE%BA%E6%96%87%E8%AE%B0%E5%BD%95-UnderstandingTheDifficultyOfTrainingDeepFeedforwardNeuralNetworks/">上一篇博客</a>谈到了关于<code>Sigmoid，tanh</code>激励函数的<strong>权重初始化方法</strong>，以及深度神经网络为什么难训练</li><li>这篇博客主要推导关于<code>Relu</code>类激励函数的<strong>权重初始化方法</strong></li></ul><a id="more"></a><h2 id="一、ReLu-PReLu激励函数"><a href="#一、ReLu-PReLu激励函数" class="headerlink" title="一、ReLu/PReLu激励函数"></a>一、<code>ReLu/PReLu</code>激励函数</h2><ul><li>目前<code>ReLu</code>激活函数使用比较多，而<a href="http://lawlite.me/2016/12/20/%E8%AE%BA%E6%96%87%E8%AE%B0%E5%BD%95-UnderstandingTheDifficultyOfTrainingDeepFeedforwardNeuralNetworks/">上一篇论文博客</a>中没有讨论，如果还是使用同样初始化权重的方法（<strong>Xavier初始化</strong>）会有问题</li><li><code>PReLu</code>函数定义如下：<ul><li><img src="/assets/blog_images/Weights-Initialization/Weights_initialization_06.png" alt="PRelu激励函数" title="Weights_initialization_06"></li><li>等价于：$$f(y_i) = \max (0,y_i) + a_i\min (0,y_i)$$</li></ul></li><li><code>ReLu</code>（左）和<code>PReLu</code>（右）激活函数图像<br><img src="/assets/blog_images/Weights-Initialization/Weights_initialization_07.png" alt="Relu和PRelu激励函数" title="Weights_initialization_07"></li></ul><h2 id="二、前向传播推导"><a href="#二、前向传播推导" class="headerlink" title="二、前向传播推导"></a>二、前向传播推导</h2><h3 id="1、符号说明"><a href="#1、符号说明" class="headerlink" title="1、符号说明"></a>1、符号说明</h3><p>$\varepsilon$……………………………………目标函数<br>$\mu$……………………………………动量<br>$\alpha$……………………………………学习率<br>$f()$…………………………………激励函数<br>$l$……………………………………当前层<code>layer</code><br>$L$……………………………………神经网络总层数<br>$b$…………………………..…………偏置向量</p><h3 id="2、推导过程"><a href="#2、推导过程" class="headerlink" title="2、推导过程"></a>2、推导过程</h3><ul><li><p>可以得到：$$y_l = W_l x_l + b_l……………………………………..(1)$$<br>$$x_l= f(y_{l - 1})$$</p></li><li><p>根据式<code>(1)</code>得：<br>$$Var[y_l] = n_lVar[w_lx_l]………………………………….(2)$$</p></li><li>因为初始化权重<code>w</code>均值为<code>0</code>，所以<ul><li><strong>期望</strong>：$$E(w_l) = 0$$</li><li><strong>方差</strong>：$$Var[w_l] = E(w_l^2) - E^2(w_l) = E(w_l^2)$$</li></ul></li><li><p>根据 <strong>公式(2)</strong> 继续推导：</p><ul><li><p>$Var[y_l] = n_l Var[w_l x_l]\\<br>\quad \quad\quad = n_l[E(w_l^2 x_l^2) - E^2(w_l x_l)]\\<br>\quad \quad\quad = n_l[E(w_l^2) - E^2(w_l)]E(x_l^2)\\<br>\quad \quad\quad = n_lVar[w_l]E(x_l^2)……………………………………..(3)$</p></li><li><p>对于<code>x</code>来说：$Var[x_l] \ne E[x_l^2]$，除非<code>x</code>的均值也是<code>0</code>,</p></li><li>对于<code>ReLu</code>函数来说：$x_l = \max (0,y_{l - 1})$，所以<strong>不可能均值为0</strong></li></ul></li><li><p><code>w</code>满足对称区间的分布，并且偏置${b_{l - 1}} = 0$，所以${y_{l - 1}}$也满足<strong>对称区间的分布</strong>，所以： </p><ul><li>$E(x_l^2) = E[max(0, y_{l-1})^2]\\<br>\quad \quad\quad= {1\over 2} [E(y_{l-1}^2)]\\<br>\quad \quad\quad= {1 \over 2} [E(y_{l-1}^2) - E^2(y_{l-1})]……………………………………(4)$</li></ul></li><li>将上式<code>(4)</code>代入<code>(3)</code>中得：<br>$$Var[y_l] = {1 \over 2}{n_l}Var[w_l]Var[y_{l - 1}]……………………………………….(5)$$</li><li>所以对于<code>L</code>层:<br>$$Var[y_L] = Var[y_1]\prod\limits_{l = 2}^L {1 \over 2}n_lVar[w_l] …………………………………..(6)$$<ul><li>从上式可以看出，因为<strong>累乘</strong>的存在，若是$${1 \over 2}n_lVar[w_l] &lt; 1$$，每次累乘都会使方差缩小，若是大于<code>1</code>，每次会使方差当大。</li><li>所以我们希望：<br>$${1 \over 2}n_lVar[w_l] = 1$$</li></ul></li><li>所以<strong>初始化方法</strong>为：是<code>w</code>满足<strong>均值为0</strong>，<strong>标准差</strong>为$\sqrt {2 \over n_l}$的<strong>高斯分布</strong>，同时<strong>偏置</strong>初始化为<code>0</code></li></ul><h2 id="三、反向传播推导"><a href="#三、反向传播推导" class="headerlink" title="三、反向传播推导"></a>三、反向传播推导</h2><ul><li><p>$\Delta x_l = \widehat W_l\Delta y_l…………………………………………….(7)$</p><ul><li>假设$\widehat W_l$和$\Delta y_l$相互独立</li><li>当$\widehat W_l$初始化为对称区间的分布时，可以得到：$\Delta x_l$的<strong>均值</strong>为<code>0</code></li><li><code>△x,△y</code>都表示梯度，即：<br>$$\Delta x = {\partial \varepsilon  \over \partial x}$$，$$\Delta y = {\partial \varepsilon  \over \partial y}$$</li></ul></li><li><p>根据<strong>反向传播</strong>：<br>$$\Delta {y_l} = f^{‘}(y_l)\Delta x_{l + 1}$$</p><ul><li>对于<code>ReLu</code>函数，<strong>f的导数</strong>为<code>0</code>或<code>1</code>，且<strong>概率是相等的</strong>，假设$f^{‘}(y_l)$和$\Delta x_{l + 1}$是相互独立的，</li><li>所以：$$E[\Delta y_l] = E[\Delta x_{l + 1}]/2 = 0$$</li></ul></li><li>所以：$$E[(\Delta y_l)^2] = Var[\Delta y_l] = {1 \over 2}Var[\Delta x_{l + 1}]……………………………………………(8)$$</li><li>根据<code>(7)</code>可以得到：              <ul><li>$Var[\Delta x_l] = \widehat n_l Var[w_l] Var[\Delta y_l] \\<br>\quad\quad\quad\quad= {1\over2} {\widehat n_l Var[w_l]Var[\Delta x_{l+1}]}$</li></ul></li><li>将<code>L</code>层展开得：<br>$$Var[\Delta x_2] = Var[\Delta x_{L + 1}]\prod\limits_{l = 2}^L {1 \over 2}\widehat n_lVar[w_l]…………………………………………………..(9)$$</li><li><p>同样令：$${1 \over 2}\widehat n_lVar[w_l] = 1$$</p><ul><li>注意这里：$\widehat n_l = k_l^2d_l$，而$n_l = k_l^2c_l = k_l^2d_{l - 1}$</li></ul></li><li><p>所以$w_l$应满足均值为0，标准差为$\sqrt {2 \over \widehat n_l}$的的分布</p></li></ul><h2 id="四、正向和反向传播讨论、实验和PReLu函数"><a href="#四、正向和反向传播讨论、实验和PReLu函数" class="headerlink" title="四、正向和反向传播讨论、实验和PReLu函数"></a>四、正向和反向传播讨论、实验和<strong>PReLu</strong>函数</h2><h2 id="1、正向和方向传播"><a href="#1、正向和方向传播" class="headerlink" title="1、正向和方向传播"></a>1、正向和方向传播</h2><ul><li>对于<strong>正向和反向</strong>两种初始化权重的方式都是可以的，论文中的模型都能够<strong>收敛</strong></li><li>比如利用<strong>反向传播</strong>得到的初始化得到：$$\prod\limits_{l = 2}^L {1 \over 2}\widehat n_lVar[{w_l}]  = 1$$</li><li><p>对应到<strong>正向传播</strong>中得到：</p><ul><li>$\prod\limits_{l=2}^L{1\over2} {n_lVar[w_l]} = \prod\limits_{l=2}^L {n_l \over \widehat n_l}\\<br>\quad\quad\quad\quad\quad\quad= {k_2^2 c_2 \over k_2^2 d_2} \cdot {k_3^2 d_2 \over k_3^2d_3} \cdot {k_L^2d_L \over K_L^2 d_L} \\<br>\quad\quad\quad\quad\quad\quad= {c_2 \over d_L}$</li></ul></li><li><p>所以也不是逐渐缩小的</p></li><li><p>实验给出了与<strong>第一篇论文</strong>的比较，如下图所示，当神经网络有30层时，<strong>Xavier初始化权重</strong>的方法（第一篇论文中的方法）已经不能收敛。<br><img src="/assets/blog_images/Weights-Initialization/Weights_initialization_12.png" alt="实验对比" title="Weights_initialization_12"></p><h3 id="2、PRelu对应方差"><a href="#2、PRelu对应方差" class="headerlink" title="2、PRelu对应方差"></a>2、<code>PRelu</code>对应方差</h3></li><li><p>对于<strong>PReLu激励函数</strong>可以得到：$${1 \over 2}(1 + a^2)n_lVar[w_l] = 1$$</p><ul><li>当<code>a=0</code>时就是对应的<strong>ReLu激励函数</strong></li><li>当<code>a=1</code>是就是对应<strong>线性函数</strong></li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;ul&gt;
&lt;li&gt;参考论文：&lt;a href=&quot;https://arxiv.org/pdf/1502.01852v1.pdf&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;点击这里查看&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://lawlite.me/2016/12/20/%E8%AE%BA%E6%96%87%E8%AE%B0%E5%BD%95-UnderstandingTheDifficultyOfTrainingDeepFeedforwardNeuralNetworks/&quot;&gt;上一篇博客&lt;/a&gt;谈到了关于&lt;code&gt;Sigmoid，tanh&lt;/code&gt;激励函数的&lt;strong&gt;权重初始化方法&lt;/strong&gt;，以及深度神经网络为什么难训练&lt;/li&gt;
&lt;li&gt;这篇博客主要推导关于&lt;code&gt;Relu&lt;/code&gt;类激励函数的&lt;strong&gt;权重初始化方法&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="DeepLearning" scheme="http://lawlite.me/tags/DeepLearning/"/>
    
      <category term="Paper阅读记录" scheme="http://lawlite.me/tags/Paper%E9%98%85%E8%AF%BB%E8%AE%B0%E5%BD%95/"/>
    
  </entry>
  
  <entry>
    <title>Python机器学习</title>
    <link href="http://lawlite.me/2017/01/08/Python%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    <id>http://lawlite.me/2017/01/08/Python机器学习/</id>
    <published>2017-01-08T15:01:58.000Z</published>
    <updated>2017-06-25T08:48:51.685Z</updated>
    
    <content type="html"><![CDATA[<h1 id="机器学习算法Python实现"><a href="#机器学习算法Python实现" class="headerlink" title="机器学习算法Python实现"></a>机器学习算法Python实现</h1><h2 id="说明"><a href="#说明" class="headerlink" title="说明"></a>说明</h2><ul><li>github地址：<a href="https://github.com/lawlite19/MachineLearning_Python" target="_blank" rel="external">https://github.com/lawlite19/MachineLearning_Python</a></li><li><strong>因为里面的公式加载出现问题，这里只给出了目录，可以去github中查看</strong><h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2></li></ul><ul><li><a href="#机器学习算法python实现">机器学习算法Python实现</a><ul><li><a href="#一-线性回归">一、线性回归</a><ul><li><a href="#1-代价函数">1、代价函数</a></li><li><a href="#2-梯度下降算法">2、梯度下降算法</a></li><li><a href="#3-均值归一化">3、均值归一化</a></li><li><a href="#4-最终运行结果">4、最终运行结果</a></li><li><a href="#5-使用scikit-learn库中的线性模型实现">5、使用scikit-learn库中的线性模型实现</a><a id="more"></a></li></ul></li><li><a href="#二-逻辑回归">二、逻辑回归</a><ul><li><a href="#1-代价函数">1、代价函数</a></li><li><a href="#2-梯度">2、梯度</a></li><li><a href="#3-正则化">3、正则化</a></li><li><a href="#4-s型函数即">4、S型函数（即）</a></li><li><a href="#5-映射为多项式">5、映射为多项式</a></li><li><a href="#6-使用的优化方法">6、使用的优化方法</a></li><li><a href="#7-运行结果">7、运行结果</a></li><li><a href="#8-使用scikit-learn库中的逻辑回归模型实现">8、使用scikit-learn库中的逻辑回归模型实现</a></li></ul></li><li><a href="#逻辑回归_手写数字识别_onevsall">逻辑回归_手写数字识别_OneVsAll</a><ul><li><a href="#1-随机显示100个数字">1、随机显示100个数字</a></li><li><a href="#2-onevsall">2、OneVsAll</a></li><li><a href="#3-手写数字识别">3、手写数字识别</a></li><li><a href="#4-预测">4、预测</a></li><li><a href="#5-运行结果">5、运行结果</a></li><li><a href="#6-使用scikit-learn库中的逻辑回归模型实现">6、使用scikit-learn库中的逻辑回归模型实现</a></li></ul></li><li><a href="#三-bp神经网络">三、BP神经网络</a><ul><li><a href="#1-神经网络model">1、神经网络model</a></li><li><a href="#2-代价函数">2、代价函数</a></li><li><a href="#3-正则化">3、正则化</a></li><li><a href="#4-反向传播bp">4、反向传播BP</a></li><li><a href="#5-bp可以求梯度的原因">5、BP可以求梯度的原因</a></li><li><a href="#6-梯度检查">6、梯度检查</a></li><li><a href="#7-权重的随机初始化">7、权重的随机初始化</a></li><li><a href="#8-预测">8、预测</a></li><li><a href="#9-输出结果">9、输出结果</a></li></ul></li><li><a href="#四-svm支持向量机">四、SVM支持向量机</a><ul><li><a href="#1-代价函数">1、代价函数</a></li><li><a href="#2-large-margin">2、Large Margin</a></li><li><a href="#3-svm-kernel核函数">3、SVM Kernel（核函数）</a></li><li><a href="#4-使用中的模型代码">4、使用中的模型代码</a></li><li><a href="#5-运行结果">5、运行结果</a></li></ul></li><li><a href="#五-k-means聚类算法">五、K-Means聚类算法</a><ul><li><a href="#1-聚类过程">1、聚类过程</a></li><li><a href="#2-目标函数">2、目标函数</a></li><li><a href="#3-聚类中心的选择">3、聚类中心的选择</a></li><li><a href="#4-聚类个数k的选择">4、聚类个数K的选择</a></li><li><a href="#5-应用图片压缩">5、应用——图片压缩</a></li><li><a href="#6-使用scikit-learn库中的线性模型实现聚类">6、使用scikit-learn库中的线性模型实现聚类</a></li><li><a href="#7-运行结果">7、运行结果</a></li></ul></li><li><a href="#六-pca主成分分析降维">六、PCA主成分分析（降维）</a><ul><li><a href="#1-用处">1、用处</a></li><li><a href="#2-2d-1dnd-kd">2、2D–&gt;1D，nD–&gt;kD</a></li><li><a href="#3-主成分分析pca与线性回归的区别">3、主成分分析PCA与线性回归的区别</a></li><li><a href="#4-pca降维过程">4、PCA降维过程</a></li><li><a href="#5-数据恢复">5、数据恢复</a></li><li><a href="#6-主成分个数的选择即要降的维度">6、主成分个数的选择（即要降的维度）</a></li><li><a href="#7-使用建议">7、使用建议</a></li><li><a href="#8-运行结果">8、运行结果</a></li><li><a href="#9-使用scikit-learn库中的pca实现降维">9、使用scikit-learn库中的PCA实现降维</a></li></ul></li><li><a href="#七-异常检测-anomaly-detection">七、异常检测 Anomaly Detection</a><ul><li><a href="#1-高斯分布正态分布">1、高斯分布（正态分布）</a></li><li><a href="#2-异常检测算法">2、异常检测算法</a></li><li><a href="#3-评价的好坏以及的选取">3、评价的好坏，以及的选取</a></li><li><a href="#4-选择使用什么样的feature单元高斯分布">4、选择使用什么样的feature（单元高斯分布）</a></li><li><a href="#5-多元高斯分布">5、多元高斯分布</a></li><li><a href="#6-单元和多元高斯分布特点">6、单元和多元高斯分布特点</a></li><li><a href="#7-程序运行结果">7、程序运行结果</a></li></ul></li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;机器学习算法Python实现&quot;&gt;&lt;a href=&quot;#机器学习算法Python实现&quot; class=&quot;headerlink&quot; title=&quot;机器学习算法Python实现&quot;&gt;&lt;/a&gt;机器学习算法Python实现&lt;/h1&gt;&lt;h2 id=&quot;说明&quot;&gt;&lt;a href=&quot;#说明&quot; class=&quot;headerlink&quot; title=&quot;说明&quot;&gt;&lt;/a&gt;说明&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;github地址：&lt;a href=&quot;https://github.com/lawlite19/MachineLearning_Python&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://github.com/lawlite19/MachineLearning_Python&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;因为里面的公式加载出现问题，这里只给出了目录，可以去github中查看&lt;/strong&gt;&lt;h2 id=&quot;目录&quot;&gt;&lt;a href=&quot;#目录&quot; class=&quot;headerlink&quot; title=&quot;目录&quot;&gt;&lt;/a&gt;目录&lt;/h2&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#机器学习算法python实现&quot;&gt;机器学习算法Python实现&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#一-线性回归&quot;&gt;一、线性回归&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#1-代价函数&quot;&gt;1、代价函数&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#2-梯度下降算法&quot;&gt;2、梯度下降算法&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#3-均值归一化&quot;&gt;3、均值归一化&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#4-最终运行结果&quot;&gt;4、最终运行结果&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#5-使用scikit-learn库中的线性模型实现&quot;&gt;5、使用scikit-learn库中的线性模型实现&lt;/a&gt;
    
    </summary>
    
    
      <category term="Python" scheme="http://lawlite.me/tags/Python/"/>
    
      <category term="机器学习" scheme="http://lawlite.me/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>论文记录-UnderstandingTheDifficultyOfTrainingDeepFeedforwardNeuralNetworks</title>
    <link href="http://lawlite.me/2016/12/20/%E8%AE%BA%E6%96%87%E8%AE%B0%E5%BD%95-UnderstandingTheDifficultyOfTrainingDeepFeedforwardNeuralNetworks/"/>
    <id>http://lawlite.me/2016/12/20/论文记录-UnderstandingTheDifficultyOfTrainingDeepFeedforwardNeuralNetworks/</id>
    <published>2016-12-20T11:03:24.000Z</published>
    <updated>2017-06-25T08:50:53.224Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1、说明"><a href="#1、说明" class="headerlink" title="1、说明"></a>1、说明</h3><ul><li><code>2010</code>年的一篇论文，说明<strong>深度神经网络为什么难以训练</strong>，当时只讨论了<strong>Sigmoid，tanh和Softsign激活函数</strong></li><li>提出了一种<strong>初始化权重weights</strong>的方法，能够解决训练中梯度消失的问题<ul><li>但是使用现在的<code>ReLu</code>激活函数，同样使用此初始化方法就会出现问题。</li></ul></li></ul><a id="more"></a><h3 id="2、Sigmoid激励函数实验"><a href="#2、Sigmoid激励函数实验" class="headerlink" title="2、Sigmoid激励函数实验"></a>2、<code>Sigmoid</code>激励函数实验</h3><ul><li>说明<ul><li>论文首先通过实验观察激活函数的影响，指出<code>Sigmoid</code>函数是<strong>不适合</strong>作为深度神经网络激活函数的</li><li>因为它的<strong>均值总是大于0</strong>的，如下图，导致后面的隐藏层<code>hidden layer</code>趋于饱和，并且发现饱和的神经元可以自发移出饱和趋于，<strong>但是非常慢</strong>。接着发现一个新的非线性的激活函数是非常有益的。<br><img src="/assets/blog_images/Weights-Initialization/01.png" alt="Sigmoid函数" title="01"></li><li>最后观察每一层激活值和梯度的变化，给出了一种<strong>新的初始化权重的方法</strong>。</li></ul></li><li>实验部分<ul><li>初始化偏置<code>biases</code>为<code>0</code>，权重<code>w</code>服从<strong>均匀分布</strong>，即：<br>$${{W_{ij}} \sim U[ - {1 \over {\sqrt n }},{1 \over {\sqrt n }}]}$$</li><li>其中<code>n</code>为前一层的神经元个数。然后构建了一个含有<strong>4个隐含层</strong>的神经网络，激活函数使用的是<code>Sigmoid</code></li><li>观察每一层的<strong>激活值的均值和标准差</strong>随着<strong>训练次数</strong>的变化，<code>layer1</code>表示<strong>第一个隐含层的输出</strong>，以此类推。如图所示：<strong>实线表示均值mean value，垂直的条表示标准差。</strong><br><img src="/assets/blog_images/Weights-Initialization/02.png" alt="均值和方差的变化" title="02"></li></ul></li><li>实验的直观理解<ul><li>最后我们使用 ${Softmax(b+Wh)}$ 作为输出预测的，刚开始训练的时候不能够很好的预测<code>y</code>的值，因此误差梯度会迫使<code>Wh</code>趋于<code>0</code>，所以会是<code>h</code>的值趋于<code>0</code>，<code>h</code>就是上一层的输出，所以激活值很快为<code>0</code>。</li><li>但是对于<code>tanh</code>函数是关于<strong>原点对称</strong>的，图像如下，值趋于<code>0</code>是好的，因为梯度能够<strong>反向传播</strong>回去，但是对于<code>sigmoid</code>函数来说就<strong>趋于饱和</strong>的位置了，梯度很难反向传回去，也就学习不到东西了。<br><img src="/assets/blog_images/Weights-Initialization/03.png" alt="Sigmoid和tanh函数" title="03"></li></ul></li></ul><h3 id="3、梯度计算和公式推导"><a href="#3、梯度计算和公式推导" class="headerlink" title="3、梯度计算和公式推导"></a>3、梯度计算和公式推导</h3><h4 id="1-代价函数"><a href="#1-代价函数" class="headerlink" title="1) 代价函数"></a>1) 代价函数</h4><ul><li>代价函数使用的是<strong>交叉熵代价函数</strong>，相比对于<strong>二次代价函数</strong>会更好<br><img src="/assets/blog_images/Weights-Initialization/04.png" alt="交叉熵和二次代价函数图像" title="04"> </li><li>二次代价函数较为平坦，所以使用梯度下降会比较慢。<h4 id="2-公式推导"><a href="#2-公式推导" class="headerlink" title="2) 公式推导"></a>2) 公式推导</h4></li><li>符号说明<br>${z^i}$………………………………第i层的激活值向量<br>${s^i}$………………………………第i+1层的输入<br>$X$………………………………输入<br>${n_i}$………………………………第i层神经元个数<br>$W$………………………………权重</li><li>可以得到：<br>$${s^i = {z^i}{W^i} + {b^i}}$$<br>$${z^{i + 1} = f({s^i})}$$</li><li>所以分别对上面两式求偏导可以得到：<br>$${{\partial Cost \over \partial s_k^i}=f^{'}W_{k,\bullet}^{i+1}}{ \partial Cost \over \partial s^{i+1}}…………………………….(1)$$</li></ul><p>$${{\partial Cost \over \partial w_{l,k}^i} = z_l^i}{\partial Cost \over \partial s_k^i}........................................(2)$$推导如下![BP推导][5]- 上面公式推导说明  - 其中 $${{\partial Cost \over \partial s^{i-1}}={\delta ^{i-1}}}$$</p><ul><li>这里<code>W</code>从<code>1</code>开始，上面给出的最终公式是从<code>0</code>开始。</li><li>对权重的偏导（梯度）再乘以输入 ${z^i}$ 即可。</li><li>因为我们使用均匀分布进行初始化，所以方差是一样的，对于<code>tanh</code>函数的导数，$${[\tanh (x)]^{‘}} = 1 - {[\tanh (x)]^2}$$</li><li>所以：$${f^,}(s_k^i) \approx 1$$</li><li>实际这里作者假设了<strong>这个区间内激活函数是线性的</strong>，第二篇论文中也有提到。（下面会给出）<ul><li>根据方差的公式： $$Var(x) = E({x^2}) - {E^2}(x)$$</li></ul></li><li><p>可以得到: $${Var[z^i] = Var[x] \prod\limits_{j=0}^{i-1}n_j Var[W^j]}…………………………(3)$$</p></li><li><p>推导如下：</p><ul><li>${Var(s) = Var(\sum\limits_i^n w_i x_i)}=\sum\limits_i^n Var(w_ix_i)$</li><li>${Var(wx) = E(w^2x^2) - E^2(wx)} \\<br>\quad\quad\quad\quad=E(w^2)E(x^2) - E^2(w)E^2(x) \\<br>\quad\quad\quad\quad=[E(w^2)-E^2(w)][E(x^2)-E^2(x)] + E^2(w)[E(x^2)-E^2(x)] + E^2(x)[E(w^2)-E^2(w)] \\<br>\quad\quad\quad\quad=Var(w)Var(x)+E^2Var(x)+E^2Var(w)$</li><li>因为输入的<strong>均值为0</strong>，所以$${E(w) = E(x) = 0}$$</li><li>所以：$${Var(wx) = Var(w)Var(x)}$$</li><li>又因为${f^{‘}(s_k^i) \approx 1}$成立，然后代入上面的式子即可</li><li>根据<strong>公式（1）</strong>，所以对${S^i}$偏导数的方差为：$${Var[{\partial Cost \over \partial s^i}]} = {Var[{\partial Cost \over \partial s^n}]}{\prod\limits_{j=i}^n}{n_{j+1}Var[W^j]}$$</li></ul></li></ul><ul><li><p>根据<strong>公式（2）</strong>，代入到对权重<code>w</code>偏导（即为梯度）的方差为: $${Var[{\partial Cost \over \partial w^i}]} = {\prod \limits_{j=0}^{i-1} n_j Var[W^j]}{\prod \limits_{j=i}^{n-1} n_{j+1}Var[W^j] \ast Var[x] Var[{\partial Cost \over \partial s^n}]}$$</p></li><li><p>对于正向传播，希望：$$\forall (i,j),Var[{z^i}] = Var[{z^j}]$$</p><ul><li>从反向传播的角度同样可以有：$${\forall (i,j), Var[{\partial Cost \over \partial s^i}]} = Var[{\partial Cost \over \partial s^j}]$$</li><li>就可以转化为：$$\left\{ {\matrix{<br>{n_iVar[w^i]}=1  \cr<br>{n_{i+1}Var[w^i]}=1  \cr<br>} }…………………………(4) \right.$$<ul><li>比如第一种(公式（3）)： $${Var[z^i] = Var[x] \prod\limits_{j=0}^{i-1}n_j Var[W^j]}$$ $$Var(z^i) = Var(x)$$</li><li>所以${n_i}Var[{w^i}] = 1$ ，第二种情况同理</li></ul></li></ul></li><li>所以将 <strong>（4）</strong> 中的两式相加可得：$${Var[{W^i}]}={2 \over {n_i + n_{i+1}}}$$<ul><li>如果所有层的神经元个数一样时：$$\left\{ {\matrix{<br>{Var[{\partial Cost \over \partial s^i}] = [nVar[W]]^{d-i}Var[x]} \cr<br>{Var[{\partial Cost \over \partial w^i}] = [nVar[w]]^{d}Var[x]Var[{\partial Cost \over \partial w^n}]} \cr<br>}} \right.$$</li><li>可以看到，所有层的梯度的方差都是一样的。但是对于很深层的神经网络还是有可能导致梯度消失。</li></ul></li></ul><h3 id="4、初始化权重方法"><a href="#4、初始化权重方法" class="headerlink" title="4、初始化权重方法"></a>4、初始化权重方法</h3><ul><li>最后提出了一个归一化的初始化方法，因为<code>W</code>服从均匀分布，根据均匀分布的方差公式可以得到：$${[c-(-c)]^2 \over 12} = {c^2 \over 3}$$</li><li>所以得到：$${2 \over {n_i + n_{i+1}} =}{ c^2 \over 3}$$</li><li>求出: <code> </code>$$c={\sqrt 6 \over \sqrt {n_i + n_{i+1}}}$$</li><li>所以最终给出初始化权重的方法为：$${W \sim U[-{{\sqrt 6}\over {\sqrt n_i+n_{i+1}}},{\sqrt 6 \over \sqrt {n_i + n_{i+1}}}]}$$</li></ul><h3 id="5、总结"><a href="#5、总结" class="headerlink" title="5、总结"></a>5、总结</h3><ul><li>论文讨论了<code>Sigmoid，tanh</code>激励函数权重初始化的问题，并给出了初始化的方法，- 但是针对<code>ReLu</code>这种激励函数是不适用的，第二篇会提到。</li><li>并且推导过程中假设了激励函数在初始化对应区间上是<strong>线性的</strong>，即假设导数恒为1我感觉也是存在问题的。</li><li>作者给出的实验部分网络的深度还是很有限的，随着网络的不断加深，对应的初始化权重的分布范围还是会不断减小的。可以通过控制学习率参数等方式来进行对应处理。</li></ul><h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><ul><li><a href="http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf" target="_blank" rel="external">http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;1、说明&quot;&gt;&lt;a href=&quot;#1、说明&quot; class=&quot;headerlink&quot; title=&quot;1、说明&quot;&gt;&lt;/a&gt;1、说明&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;code&gt;2010&lt;/code&gt;年的一篇论文，说明&lt;strong&gt;深度神经网络为什么难以训练&lt;/strong&gt;，当时只讨论了&lt;strong&gt;Sigmoid，tanh和Softsign激活函数&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;提出了一种&lt;strong&gt;初始化权重weights&lt;/strong&gt;的方法，能够解决训练中梯度消失的问题&lt;ul&gt;
&lt;li&gt;但是使用现在的&lt;code&gt;ReLu&lt;/code&gt;激活函数，同样使用此初始化方法就会出现问题。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="DeepLearning" scheme="http://lawlite.me/tags/DeepLearning/"/>
    
      <category term="Paper阅读记录" scheme="http://lawlite.me/tags/Paper%E9%98%85%E8%AF%BB%E8%AE%B0%E5%BD%95/"/>
    
  </entry>
  
  <entry>
    <title>Tensorflow学习</title>
    <link href="http://lawlite.me/2016/12/08/Tensorflow%E5%AD%A6%E4%B9%A0/"/>
    <id>http://lawlite.me/2016/12/08/Tensorflow学习/</id>
    <published>2016-12-08T08:07:33.000Z</published>
    <updated>2017-06-25T08:50:03.497Z</updated>
    
    <content type="html"><![CDATA[<ul><li>github地址：<a href="https://github.com/lawlite19/MachineLearning_TensorFlow" target="_blank" rel="external">https://github.com/lawlite19/MachineLearning_TensorFlow</a></li></ul><h2 id="一、TensorFlow介绍"><a href="#一、TensorFlow介绍" class="headerlink" title="一、TensorFlow介绍"></a>一、TensorFlow介绍</h2><a id="more"></a><h3 id="1、什么是TensorFlow"><a href="#1、什么是TensorFlow" class="headerlink" title="1、什么是TensorFlow"></a>1、什么是TensorFlow</h3><ul><li>官网：<a href="https://www.tensorflow.org/" target="_blank" rel="external">https://www.tensorflow.org/</a></li><li>TensorFlow是Google开发的一款神经网络的Python外部的结构包, 也是一个采用数据流图来进行数值计算的开源软件库.</li><li>先绘制计算结构图, 也可以称是一系列可人机交互的计算操作, 然后把编辑好的Python文件 转换成 更高效的C++, 并在后端进行计算.</li></ul><h3 id="2、TensorFlow强大之处"><a href="#2、TensorFlow强大之处" class="headerlink" title="2、TensorFlow强大之处"></a>2、TensorFlow强大之处</h3><ul><li>擅长的任务就是训练深度神经网络</li><li>快速的入门神经网络,大大降低了深度学习（也就是深度神经网络）的开发成本和开发难度</li><li>TensorFlow 的开源性, 让所有人都能使用并且维护</li></ul><h3 id="3、安装TensorFlow"><a href="#3、安装TensorFlow" class="headerlink" title="3、安装TensorFlow"></a>3、安装TensorFlow</h3><ul><li>暂不支持Windows下安装TensorFlow,可以在虚拟机里使用或者安装Docker安装</li><li>这里在CentOS6.5下进行安装</li><li><p>安装<strong>Python2.7</strong>，默认CentOS中安装的是<strong>Python2.6</strong></p><ul><li><p>先安装<strong>zlib</strong>的依赖，下面安装<strong>easy_install</strong>时会用到</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">yum install zlib</div><div class="line">yum install zlib-devel</div></pre></td></tr></table></figure></li><li><p>在安装<strong>openssl</strong>的依赖，下面安装<strong>pip</strong>时会用到</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">yum install openssl</div><div class="line">yum install openssl-devel</div></pre></td></tr></table></figure></li><li><p>下载安装包，我传到<code>github</code>上的安装包，<code>https</code>协议后面加上<code>--no-check-certificate</code>，：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">wget https://raw.githubusercontent.com/lawlite19/LinuxSoftware/master/python/Python-2.7.12.tgz --no-check-certificate</div></pre></td></tr></table></figure></li><li><p>解压缩：<code>tar -zxvf xxx</code></p></li><li>进入，配置：<code>./configure --prefix=/usr/local/python2.7</code></li><li>编译并安装：<code>make &amp;&amp; make install</code></li><li>创建链接来使系统默认python变为python2.7,<br><code>ln -fs /usr/local/python2.7/bin/python2.7 /usr/bin/python</code></li><li>修改一下<strong>yum</strong>，因为yum的执行文件还是需要原来的<strong>python2.6</strong>,<code>vim /usr/bin/yum</code><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">#!/usr/bin/python</div></pre></td></tr></table></figure></li></ul><p>修改为系统原有的python版本地址</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">#!/usr/bin/python2.6</div></pre></td></tr></table></figure></li></ul><ul><li><p>安装<strong>easy_install</strong></p><ul><li>下载：<code>wget https://raw.githubusercontent.com/lawlite19/LinuxSoftware/blob/master/python/setuptools-26.1.1.tar.gz --no-check-certificate</code></li><li>解压缩：<code>tar -zxvf xxx</code></li><li><code>python setup.py build</code>  #注意这里python是新的python2.7</li><li><code>python setup.py install</code></li><li>到<code>/usr/local/python2.7/bin</code>目录下查看就会看到<code>easy_install</code>了</li><li>创建一个软连接：<code>ln -s /usr/local/python2.7/bin/easy_install /usr/local/bin/easy_install</code></li><li>就可以使用<code>easy_install 包名</code> 进行安装</li></ul></li><li><p>安装<strong>pip</strong></p><ul><li>下载:</li><li>解压缩：<code>tar -zxvf xxx</code></li><li>安装：<code>python setup.py install</code></li><li>到<code>/usr/local/python2.7/bin</code>目录下查看就会看到<code>pip</code>了</li><li>同样创建软连接：<code>ln -s /usr/local/python2.7/bin/pip /usr/local/bin/pip</code></li><li>就可以使用<code>pip install 包名</code>进行安装包了</li></ul></li><li><p>安装<strong>wingIDE</strong></p><ul><li>默认安装到<code>/usr/local/lib</code>下，进入，执行<code>./wing</code>命令即可执行</li><li>创建软连接：<code>ln -s /usr/local/lib/wingide5.1/wing /usr/local/bin/wing</code></li><li>破解：</li></ul></li></ul><ul><li><p>[另]安装<strong>VMwareTools</strong>，可以在windows和Linux之间复制粘贴</p><ul><li>启动CentOS</li><li>选择VMware中的虚拟机–&gt;安装VMware Tools</li><li>会自动弹出VMware Tools的文件夹</li><li>拷贝一份到root目录下 <code>cp VMwareTools-9.9.3-2759765.tar.gz /root</code></li><li>解压缩 <code>tar -zxvf VMwareTools-9.9.3-2759765.tar.gz</code></li><li>进入目录执行，<code>vmware-install.pl</code>，一路回车下去即可</li><li>重启CentOS即可</li></ul></li><li><p>安装<strong>numpy</strong></p><ul><li>直接安装没有出错</li></ul></li><li><p>安装<strong>scipy</strong></p><ul><li>安装依赖：<code>yum install bzip2-devel pcre-devel ncurses-devel  readline-devel tk-devel gcc-c++ lapack-devel</code></li><li>安装即可：<code>pip install scipy</code></li></ul></li><li><p>安装<strong>matplotlib</strong></p><ul><li>安装依赖：<code>yum install libpng-devel</code></li><li>安装即可：<code>pip install matplotlib</code></li><li>运行可能有以下的错误：<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">ImportError: No module named _tkinter</div></pre></td></tr></table></figure></li></ul><p>安装：<code>tcl8.5.9-src.tar.gz</code></p><ul><li>进入安装即可,<code>./confgiure  make  make install</code><br>安装：<code>tk8.5.9-src.tar.gz</code></li><li>进入安装即可。</li><li><strong>[注意]</strong>要重新安装一下<strong>Pyhton2.7</strong>才能链接到<code>tkinter</code></li></ul></li><li><p>安装<strong>scikit-learn</strong></p><ul><li>直接安装没有出错，但是缺少包<code>bz2</code></li><li>将系统中<code>python2.6</code>的<code>bz2</code>复制到<code>python2.7</code>对应文件夹下<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">cp /usr/lib/python2.6/lib-dynload/bz2.so /usr/local/python2.7/lib/python2.7/lib-dynload</div></pre></td></tr></table></figure></li></ul></li><li><p>安装<strong>TensorFlow</strong></p><ul><li><a href="https://www.tensorflow.org/" target="_blank" rel="external">官网点击</a></li><li><p>选择对应的版本</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div></pre></td><td class="code"><pre><div class="line"> # Ubuntu/Linux 64-bit, CPU only, Python 2.7</div><div class="line">$ export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.12.0rc0-cp27-none-linux_x86_64.whl</div><div class="line"></div><div class="line"># Ubuntu/Linux 64-bit, GPU enabled, Python 2.7</div><div class="line"># Requires CUDA toolkit 8.0 and CuDNN v5. For other versions, see &quot;Installing from sources&quot; below.</div><div class="line">$ export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-0.12.0rc0-cp27-none-linux_x86_64.whl</div><div class="line"></div><div class="line"># Mac OS X, CPU only, Python 2.7:</div><div class="line">$ export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-0.12.0rc0-py2-none-any.whl</div><div class="line"></div><div class="line"># Mac OS X, GPU enabled, Python 2.7:</div><div class="line">$ export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/mac/gpu/tensorflow_gpu-0.12.0rc0-py2-none-any.whl</div><div class="line"></div><div class="line"># Ubuntu/Linux 64-bit, CPU only, Python 3.4</div><div class="line">$ export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.12.0rc0-cp34-cp34m-linux_x86_64.whl</div><div class="line"></div><div class="line"># Ubuntu/Linux 64-bit, GPU enabled, Python 3.4</div><div class="line"># Requires CUDA toolkit 8.0 and CuDNN v5. For other versions, see &quot;Installing from sources&quot; below.</div><div class="line">$ export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-0.12.0rc0-cp34-cp34m-linux_x86_64.whl</div><div class="line"></div><div class="line"># Ubuntu/Linux 64-bit, CPU only, Python 3.5</div><div class="line">$ export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.12.0rc0-cp35-cp35m-linux_x86_64.whl</div><div class="line"></div><div class="line"># Ubuntu/Linux 64-bit, GPU enabled, Python 3.5</div><div class="line"># Requires CUDA toolkit 8.0 and CuDNN v5. For other versions, see &quot;Installing from sources&quot; below.</div><div class="line">$ export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-0.12.0rc0-cp35-cp35m-linux_x86_64.whl</div><div class="line"></div><div class="line"># Mac OS X, CPU only, Python 3.4 or 3.5:</div><div class="line">$ export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-0.12.0rc0-py3-none-any.whl</div><div class="line"></div><div class="line"># Mac OS X, GPU enabled, Python 3.4 or 3.5:</div><div class="line">$ export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/mac/gpu/tensorflow_gpu-0.12.0rc0-py3-none-any.whl</div></pre></td></tr></table></figure></li><li><p>对应<code>python</code>版本</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"> # Python 2</div><div class="line">$ sudo pip install --upgrade $TF_BINARY_URL</div><div class="line"></div><div class="line"># Python 3</div><div class="line">$ sudo pip3 install --upgrade $TF_BINARY_URL</div></pre></td></tr></table></figure></li><li><p>可能缺少依赖<code>glibc</code>,看对应提示的版本，</p></li><li>还有可能报错<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">ImportError: /usr/lib64/libstdc++.so.6: version `GLIBCXX_3.4.19&apos; not found (required by /usr/local/python2.7/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so)</div></pre></td></tr></table></figure></li></ul></li><li><p>安装对应版本的<strong>glibc</strong></p><ul><li>查看现有版本的glibc, <code>strings /lib64/libc.so.6 |grep GLIBC</code></li><li>下载对应版本：<code>wget http://ftp.gnu.org/gnu/glibc/glibc-2.17.tar.gz</code></li><li>解压缩：<code>tar -zxvf glibc-2.17</code></li><li>进入文件夹创建<code>build</code>文件夹<code>cd glibc-2.17 &amp;&amp; mkdir build</code></li><li><p>配置：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">../configure  \</div><div class="line">   --prefix=/usr          \</div><div class="line">   --disable-profile      \</div><div class="line">   --enable-add-ons       \</div><div class="line">   --enable-kernel=2.6.25 \</div><div class="line">   --libexecdir=/usr/lib/glibc</div></pre></td></tr></table></figure></li><li><p>编译安装：<code>make &amp;&amp; make install</code></p></li><li>可以再用命令：<code>strings /lib64/libc.so.6 |grep GLIBC</code>查看</li></ul></li><li><p>添加<strong>GLIBCXX_3.4.19</strong>的支持</p><ul><li>下载：<code>wget https://raw.githubusercontent.com/lawlite19/LinuxSoftware/master/python2.7_tensorflow/libstdc++.so.6.0.20</code> </li><li>复制到<code>/usr/lib64</code>文件夹下：<code>cp libstdc++.so.6.0.20 /usr/lib64/</code></li><li>添加执行权限：<code>chmod +x /usr/lib64/libstdc++.so.6.0.20</code></li><li>删除原来的：<code>rm -rf /usr/lib64/libstdc++.so.6</code></li><li>创建软连接：<code>ln -s /usr/lib64/libstdc++.so.6.0.20 /usr/lib64/libstdc++.so.6</code></li><li>可以查看是否有个版本：<code>strings /usr/lib64/libstdc++.so.6 | grep GLIBCXX</code></li></ul></li></ul><ul><li><p>运行还可能报错编码的问题，这里安装<code>0.10.0</code>版本:<code>pip install --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.10.0rc0-cp27-none-linux_x86_64.whl</code></p></li><li><p>安装<code>pandas</code></p><ul><li><code>pip install pandas</code>没有问题</li></ul></li></ul><h2 id="二、TensorFlow基础架构"><a href="#二、TensorFlow基础架构" class="headerlink" title="二、TensorFlow基础架构"></a>二、TensorFlow基础架构</h2><h3 id="1、处理结构"><a href="#1、处理结构" class="headerlink" title="1、处理结构"></a>1、处理结构</h3><ul><li>Tensorflow 首先要定义神经网络的结构,然后再把数据放入结构当中去运算和 training<br><img src="https://raw.githubusercontent.com/lawlite19/MachineLearning_TensorFlow/master/images/tensors_flowing.gif" alt="enter description here" title="tensors_flowing.gif"></li><li>TensorFlow是采用数据流图（data　flow　graphs）来计算</li><li>首先我们得创建一个数据流流图</li><li>然后再将我们的数据（数据以张量(tensor)的形式存在）放在数据流图中计算</li><li>张量（tensor):<ul><li>张量有多种. 零阶张量为 纯量或标量 (scalar) 也就是一个数值. 比如 <a href="https://raw.githubusercontent.com/lawlite19/MachineLearning_TensorFlow/master/images/tensors_flowing.gif" title="tensors_flowing.gif" target="_blank" rel="external">1</a></li><li>一阶张量为 向量 (vector), 比如 一维的 [1, 2, 3]</li><li>二阶张量为 矩阵 (matrix), 比如 二维的 [[1, 2, 3],[4, 5, 6],[7, 8, 9]]</li><li>以此类推, 还有 三阶 三维的 …</li></ul></li></ul><h3 id="2、一个例子"><a href="#2、一个例子" class="headerlink" title="2、一个例子"></a>2、一个例子</h3><ul><li><p>求<code>y=1*x+3</code>中的权重<code>1</code>和偏置<code>3</code></p><ul><li><p>定义这个函数</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">x_data = np.random.rand(100).astype(np.float32)</div><div class="line">y_data = x_data*1.0+3.0</div></pre></td></tr></table></figure></li><li><p>创建TensorFlow结构</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">Weights = tf.Variable(tf.random_uniform([1], -1.0, 1.0)) # 创建变量Weight是，范围是 -1.0~1.0</div><div class="line">biases = tf.Variable(tf.zeros([1]))                      # 创建偏置，初始值为0</div><div class="line">y = Weights*x_data+biases                                # 定义方程</div><div class="line">loss = tf.reduce_mean(tf.square(y-y_data))               # 定义损失，为真实值减去我们每一步计算的值</div><div class="line">optimizer = tf.train.GradientDescentOptimizer(0.5)       # 0.5 是学习率</div><div class="line">train = optimizer.minimize(loss)                         # 使用梯度下降优化</div><div class="line">init = tf.initialize_all_variables()                     # 初始化所有变量</div></pre></td></tr></table></figure></li><li><p>定义<code>Session</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">sess = tf.Session()</div><div class="line">sess.run(init)</div></pre></td></tr></table></figure></li><li><p>输出结果</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">for i in range(201):</div><div class="line">   sess.run(train)</div><div class="line">   if i%20 == 0:</div><div class="line">       print i,sess.run(Weights),sess.run(biases)</div></pre></td></tr></table></figure></li></ul><p>结果为：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"> 0 [ 1.60895896] [ 3.67376709]</div><div class="line">20 [ 1.04673827] [ 2.97489643]</div><div class="line">40 [ 1.011392] [ 2.99388123]</div><div class="line">60 [ 1.00277638] [ 2.99850869]</div><div class="line">80 [ 1.00067675] [ 2.99963641]</div><div class="line">100 [ 1.00016499] [ 2.99991131]</div><div class="line">120 [ 1.00004005] [ 2.99997854]</div><div class="line">140 [ 1.00000978] [ 2.99999475]</div><div class="line">160 [ 1.0000025] [ 2.99999857]</div><div class="line">180 [ 1.00000119] [ 2.99999928]</div><div class="line">200 [ 1.00000119] [ 2.99999928]</div></pre></td></tr></table></figure></li></ul><h3 id="3、Session会话控制"><a href="#3、Session会话控制" class="headerlink" title="3、Session会话控制"></a>3、Session会话控制</h3><ul><li>运行 <code>session.run()</code> 可以获得你要得知的运算结果, 或者是你所要运算的部分</li><li>定义常量矩阵：<code>tf.constant([[3,3]])</code></li><li>矩阵乘法 ：<code>tf.matmul(matrix1,matrix2)</code></li><li><p>运行Session的两种方法：</p><ul><li><p>手动关闭</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">sess = tf.Session()</div><div class="line">print sess.run(product)</div><div class="line">sess.close()</div></pre></td></tr></table></figure></li><li><p>使用<code>with</code>，执行完会自动关闭</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">with tf.Session() as sess:</div><div class="line">print sess.run(product)</div></pre></td></tr></table></figure></li></ul></li></ul><h3 id="4、Variable变量"><a href="#4、Variable变量" class="headerlink" title="4、Variable变量"></a>4、<code>Variable</code>变量</h3><ul><li>定义变量：<code>tf.Variable()</code></li><li>初始化所有变量：<code>init = tf.initialize_all_variables()</code> </li><li>需要再在 sess 里, <code>sess.run(init)</code> , 激活变量</li><li>输出时，一定要把 sess 的指针指向变量再进行 <code>print</code> 才能得到想要的结果</li></ul><h3 id="5、Placeholder传入值"><a href="#5、Placeholder传入值" class="headerlink" title="5、Placeholder传入值"></a>5、<code>Placeholder</code>传入值</h3><ul><li>首先定义<code>Placeholder</code>，然后在<code>Session.run()</code>的时候输入值</li><li><code>placeholder</code> 与 <code>feed_dict={}</code> 是绑定在一起出现的<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">input1 = tf.placeholder(tf.float32) #在 Tensorflow 中需要定义 placeholder 的 type ，一般为 float32 形式</div><div class="line">input2 = tf.placeholder(tf.float32)</div><div class="line"></div><div class="line">output = tf.mul(input1,input2)  # 乘法运算</div><div class="line"></div><div class="line">with tf.Session() as sess:</div><div class="line">    print sess.run(output,feed_dict=&#123;input1:7.,input2:2.&#125;) # placeholder 与 feed_dict=&#123;&#125; 是绑定在一起出现的</div></pre></td></tr></table></figure></li></ul><h2 id="三、定义一个神经网络"><a href="#三、定义一个神经网络" class="headerlink" title="三、定义一个神经网络"></a>三、定义一个神经网络</h2><h3 id="1、添加层函数add-layer"><a href="#1、添加层函数add-layer" class="headerlink" title="1、添加层函数add_layer()"></a>1、添加层函数<code>add_layer()</code></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">&apos;&apos;&apos;参数：输入数据，前一层size，当前层size，激活函数&apos;&apos;&apos;</div><div class="line">def add_layer(inputs,in_size,out_size,activation_function=None):</div><div class="line">    Weights = tf.Variable(tf.random_normal([in_size,out_size]))  #随机初始化权重</div><div class="line">    biases = tf.Variable(tf.zeros([1,out_size]) + 0.1)  # 初始化偏置，+0.1</div><div class="line">    Ws_plus_b = tf.matmul(inputs,Weights) + biases      # 未使用激活函数的值</div><div class="line">    if activation_function is None:</div><div class="line">        outputs = Ws_plus_b</div><div class="line">    else:</div><div class="line">        outputs = activation_function(Ws_plus_b)   # 使用激活函数激活</div><div class="line">    return outputs</div></pre></td></tr></table></figure><h3 id="2、构建神经网络"><a href="#2、构建神经网络" class="headerlink" title="2、构建神经网络"></a>2、构建神经网络</h3><ul><li><p>定义二次函数</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">x_data = np.linspace(-1,1,300,dtype=np.float32)[:,np.newaxis]</div><div class="line">noise = np.random.normal(0,0.05,x_data.shape).astype(np.float32)</div><div class="line">y_data = np.square(x_data)-0.5+noise</div></pre></td></tr></table></figure></li><li><p>定义<code>Placeholder</code>,用于后期输入数据</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">xs = tf.placeholder(tf.float32,[None,1]) # None代表无论输入有多少都可以,只有一个特征，所以这里是1</div><div class="line">ys = tf.placeholder(tf.float32,[None,1])</div></pre></td></tr></table></figure></li><li><p>定义神经层<code>layer</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">layer1 = add_layer(xs, 1, 10, activation_function=tf.nn.relu) # 第一层，输入层为1，隐含层为10个神经元，Tensorflow 自带的激励函数tf.nn.relu</div></pre></td></tr></table></figure></li><li><p>定义输出层</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">prediction = add_layer(layer1, 10, 1) # 利用上一层作为输入</div></pre></td></tr></table></figure></li><li><p>计算<code>loss</code>损失</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">loss = tf.reduce_mean(tf.reduce_sum(tf.square(ys-prediction),reduction_indices=[1])) # 对二者差的平方求和再取平均</div></pre></td></tr></table></figure></li><li><p>梯度下降最小化损失</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">train = tf.train.GradientDescentOptimizer(0.1).minimize(loss)</div></pre></td></tr></table></figure></li><li><p>初始化所有变量</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">init = tf.initialize_all_variables()</div></pre></td></tr></table></figure></li><li><p>定义Session</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">sess = tf.Session()</div><div class="line">sess.run(init)</div></pre></td></tr></table></figure></li><li><p>输出</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">for i in range(1000):</div><div class="line">    sess.run(train,feed_dict=&#123;xs:x_data,ys:y_data&#125;)</div><div class="line">    if i%50==0:</div><div class="line">        print sess.run(loss,feed_dict=&#123;xs:x_data,ys:y_data&#125;)</div></pre></td></tr></table></figure></li></ul><p>结果：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line">0.45402</div><div class="line">0.0145364</div><div class="line">0.00721318</div><div class="line">0.0064215</div><div class="line">0.00614493</div><div class="line">0.00599307</div><div class="line">0.00587578</div><div class="line">0.00577039</div><div class="line">0.00567172</div><div class="line">0.00558008</div><div class="line">0.00549546</div><div class="line">0.00541595</div><div class="line">0.00534059</div><div class="line">0.00526139</div><div class="line">0.00518873</div><div class="line">0.00511403</div><div class="line">0.00504063</div><div class="line">0.0049613</div><div class="line">0.0048874</div><div class="line">0.004819</div></pre></td></tr></table></figure></p><h3 id="3、可视化结果"><a href="#3、可视化结果" class="headerlink" title="3、可视化结果"></a>3、可视化结果</h3><ul><li>显示数据<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">fig = plt.figure()</div><div class="line">ax = fig.add_subplot(111)</div><div class="line">ax.scatter(x_data,y_data)</div><div class="line">plt.ion()   # 绘画之后不暂停</div><div class="line">plt.show()</div></pre></td></tr></table></figure></li></ul><p><img src="https://raw.githubusercontent.com/lawlite19/MachineLearning_TensorFlow/master/images/example_01.png" alt="enter description here" title="example_01.png">  </p><ul><li>动态绘画<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">        try:</div><div class="line">            ax.lines.remove(lines[0])   # 每次绘画需要移除上次绘画的结果，放在try catch里因为第一次执行没有，所以直接pass</div><div class="line">        except Exception:</div><div class="line">            pass</div><div class="line">        prediction_value = sess.run(prediction, feed_dict=&#123;xs: x_data&#125;)</div><div class="line">        # plot the prediction</div><div class="line">        lines = ax.plot(x_data, prediction_value, &apos;r-&apos;, lw=3)  # 绘画</div><div class="line">        plt.pause(0.1)  # 停0.1s</div><div class="line">```    </div><div class="line">![enter description here][3]</div><div class="line"></div><div class="line">## 四、TensorFlow可视化</div><div class="line"></div><div class="line">### 1、TensorFlow的可视化工具`tensorboard`，可视化神经网路额结构</div><div class="line">- 输入`input`</div></pre></td></tr></table></figure></li></ul><p>with tf.name_scope(‘input’):<br>    xs = tf.placeholder(tf.float32,[None,1],name=’x_in’)  #<br>    ys = tf.placeholder(tf.float32,[None,1],name=’y_in’)<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">![enter description here][4]</div><div class="line"></div><div class="line">- `layer`层</div></pre></td></tr></table></figure></p><p>def add_layer(inputs,in_size,out_size,activation_function=None):<br>    with tf.name_scope(‘layer’):<br>        with tf.name_scope(‘Weights’):<br>            Weights = tf.Variable(tf.random_normal([in_size,out_size]),name=’W’)<br>        with tf.name_scope(‘biases’):<br>            biases = tf.Variable(tf.zeros([1,out_size]) + 0.1,name=’b’)<br>        with tf.name_scope(‘Ws_plus_b’):<br>            Ws_plus_b = tf.matmul(inputs,Weights) + biases<br>        if activation_function is None:                                       outputs = Ws_plus_b<br>        else:<br>            outputs = activation_function(Ws_plus_b)<br>        return outputs<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">![enter description here][5]</div><div class="line"></div><div class="line">- `loss`和`train`</div></pre></td></tr></table></figure></p><p>with tf.name_scope(‘loss’):<br>    loss = tf.reduce_mean(tf.reduce_sum(tf.square(ys-prediction),reduction_indices=<a href="https://raw.githubusercontent.com/lawlite19/MachineLearning_TensorFlow/master/images/tensors_flowing.gif" title="tensors_flowing.gif" target="_blank" rel="external">1</a>))</p><p>with tf.name_scope(‘train’):<br>    train = tf.train.GradientDescentOptimizer(0.1).minimize(loss)<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">![enter description here][6]</div><div class="line"></div><div class="line">- 写入文件中</div></pre></td></tr></table></figure></p><p>writer = tf.train.SummaryWriter(“logs/“, sess.graph)<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">- 浏览器中查看（chrome浏览器）</div><div class="line"> - 在终端输入：`tensorboard --logdir=&apos;logs/&apos;`，它会给出访问地址</div><div class="line"> - 浏览器中查看即可。</div><div class="line"> - `tensorboard`命令在安装**python**目录的**bin**目录下，可以创建一个软连接</div><div class="line"></div><div class="line">### 2、可视化训练过程</div><div class="line">- 可视化Weights权重和biases偏置</div><div class="line"> - 每一层起个名字</div></pre></td></tr></table></figure></p><p> layer_name = ‘layer%s’%n_layer<br> <figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">- tf.histogram_summary(name,value)</div></pre></td></tr></table></figure></p><p> def add_layer(inputs,in_size,out_size,n_layer,activation_function=None):<br>    layer_name = ‘layer%s’%n_layer<br>    with tf.name_scope(layer_name):<br>        with tf.name_scope(‘Weights’):<br>            Weights = tf.Variable(tf.random_normal([in_size,out_size]),name=’W’)<br>            tf.histogram_summary(layer_name+’/weights’, Weights)<br>        with tf.name_scope(‘biases’):<br>            biases = tf.Variable(tf.zeros([1,out_size]) + 0.1,name=’b’)<br>            tf.histogram_summary(layer_name+’/biases’,biases)<br>        with tf.name_scope(‘Ws_plus_b’):<br>            Ws_plus_b = tf.matmul(inputs,Weights) + biases</p><pre><code>if activation_function is None:                 outputs = Ws_plus_b else:                                                             outputs = activation_function(Ws_plus_b)      tf.histogram_summary(layer_name+&apos;/outputs&apos;,outputs)return outputs</code></pre> <figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">- merge所有的summary</div></pre></td></tr></table></figure><p> merged =tf.merge_all_summaries()<br> <figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">- 写入文件中</div></pre></td></tr></table></figure></p><p> writer = tf.train.SummaryWriter(“logs/“, sess.graph)<br> <figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">- 训练1000次，每50步显示一次：</div></pre></td></tr></table></figure></p><p> for i in range(1000):<br>    sess.run(train,feed_dict={xs:x_data,ys:y_data})<br>    if i%50==0:<br>        summary = sess.run(merged, feed_dict={xs: x_data, ys:y_data})<br>        writer.add_summary(summary, i)<br> <figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"> - 同样适用`tensorboard`查看   </div><div class="line"> ![enter description here][7]</div><div class="line"> </div><div class="line">- 可视化损失函数（代价函数）</div><div class="line"> - 添加：`tf.scalar_summary(&apos;loss&apos;,loss)`    </div><div class="line"> ![enter description here][8]</div><div class="line"></div><div class="line">## 五、手写数字识别_1</div><div class="line">### 1、说明</div><div class="line">- [全部代码](https://github.com/lawlite19/MachineLearning_TensorFlow/blob/master/Mnist_01/mnist.py)：`https://github.com/lawlite19/MachineLearning_TensorFlow/blob/master/Mnist_02/mnist.py`</div><div class="line">- 自己的数据集，没有使用tensorflow中mnist数据集，</div><div class="line">- 之前在机器学习中用Python实现过，地址：`https://github.com/lawlite19/MachineLearning_Python`,这里使用`tensorflow`实现</div><div class="line">- 神经网络只有两层</div><div class="line"></div><div class="line">### 2、代码实现</div><div class="line">- 添加一层</div></pre></td></tr></table></figure></p><p>‘’’添加一层神经网络’’’<br>def add_layer(inputs,in_size,out_size,activation_function=None):<br>    Weights = tf.Variable(tf.random_normal([in_size,out_size]))    # 权重，in*out<br>    biases = tf.Variable(tf.zeros([1,out_size]) + 0.1)<br>    Ws_plus_b = tf.matmul(inputs,Weights) + biases   # 计算权重和偏置之后的值<br>    if activation_function is None:<br>        outputs = Ws_plus_b<br>    else:<br>        outputs = activation_function(Ws_plus_b)    # 调用激励函数运算<br>    return outputs<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">- 运行函数</div></pre></td></tr></table></figure></p><p>‘’’运行函数’’’<br>def NeuralNetwork():<br>    data_digits = spio.loadmat(‘data_digits.mat’)<br>    X = data_digits[‘X’]<br>    y = data_digits[‘y’]<br>    m,n = X.shape<br>    class_y = np.zeros((m,10))      # y是0,1,2,3…9,需要映射0/1形式<br>    for i in range(10):<br>        class_y[:,i] = np.float32(y==i).reshape(1,-1) </p><pre><code>xs = tf.placeholder(tf.float32, shape=[None,400])  # 像素是20x20=400，所以有400个featureys = tf.placeholder(tf.float32, shape=[None,10])   # 输出有10个prediction = add_layer(xs, 400, 10, activation_function=tf.nn.softmax) # 两层神经网络，400x10#prediction = add_layer(layer1, 25, 10, activation_function=tf.nn.softmax)#loss = tf.reduce_mean(tf.reduce_sum(tf.square(ys-prediction),reduction_indices=[1]))loss = tf.reduce_mean(-tf.reduce_sum(ys*tf.log(prediction),reduction_indices=[1]))  # 定义损失函数（代价函数），train = tf.train.GradientDescentOptimizer(learning_rate=0.5).minimize(loss)     # 使用梯度下降最小化损失init = tf.initialize_all_variables()   # 初始化所有变量sess = tf.Session()  # 创建Sessionsess.run(init)for i in range(4000): # 迭代训练4000次    sess.run(train, feed_dict={xs:X,ys:class_y})  # 训练train，填入数据    if i%50==0:  # 每50次输出当前的准确度        print(compute_accuracy(xs,ys,X,class_y,sess,prediction))</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">- 计算准确度</div></pre></td></tr></table></figure><p>‘’’计算预测准确度’’’<br>def compute_accuracy(xs,ys,X,y,sess,prediction):<br>    y_pre = sess.run(prediction,feed_dict={xs:X})<br>    correct_prediction = tf.equal(tf.argmax(y_pre,1),tf.argmax(y,1))  #tf.argmax 给出某个tensor对象在某一维上的其数据最大值所在的索引值,即为对应的数字，tf.equal 来检测我们的预测是否真实标签匹配<br>    accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32)) # 平均值即为准确度<br>    result = sess.run(accuracy,feed_dict={xs:X,ys:y})<br>    return result<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">- 输出每一次预测的结果准确度    </div><div class="line">![enter description here][9]</div><div class="line"></div><div class="line">## 六、手写数字识别_2</div><div class="line">### 1、说明</div><div class="line">- [全部代码](https://github.com/lawlite19/MachineLearning_TensorFlow/blob/master/Mnist_02/mnist.py)：`https://github.com/lawlite19/MachineLearning_TensorFlow/blob/master/Mnist_02/mnist.py`</div><div class="line">- 采用TensorFlow中的mnist数据集（可以取网站下载它的数据集，http://yann.lecun.com/exdb/mnist/）</div><div class="line">- 实现代码与上面类似，它有专门的测试集</div><div class="line"></div><div class="line">### 2、代码</div><div class="line">- 随机梯度下降`SGD`,每次选出`100`个数据进行训练</div></pre></td></tr></table></figure></p><p>for i in range(2000):<br>        batch_xs, batch_ys = minist.train.next_batch(100)<br>        sess.run(train_step,feed_dict={xs:batch_xs,ys:batch_ys})<br>        if i%50==0:<br>            print(compute_accuracy(xs,ys,minist.test.images, minist.test.labels,sess,prediction))</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">- 输出每一次预测的结果准确度     </div><div class="line">![enter description here][10]</div><div class="line"></div><div class="line">## 七、手写数字识别_3_CNN卷积神经网络</div><div class="line">### 1、说明</div><div class="line">- 关于**卷积神经网络CNN**可以查看[我的博客](http://blog.csdn.net/u013082989/article/details/53673602)：http://blog.csdn.net/u013082989/article/details/53673602</div><div class="line"> - 或者[github](https://github.com/lawlite19/DeepLearning_Python)：https://github.com/lawlite19/DeepLearning_Python</div><div class="line">- [全部代码](https://github.com/lawlite19/MachineLearning_TensorFlow/blob/master/Mnist_03_CNN/mnist_cnn.py)：`https://github.com/lawlite19/MachineLearning_TensorFlow/blob/master/Mnist_03_CNN/mnist_cnn.py`</div><div class="line">- 采用TensorFlow中的mnist数据集（可以取网站下载它的数据集，http://yann.lecun.com/exdb/mnist/）</div><div class="line"></div><div class="line">### 2、代码实现</div><div class="line">- 权重和偏置初始化函数</div><div class="line"> - 权重使用的`truncated_normal`进行初始化,`stddev`标准差定义为0.1</div><div class="line"> - 偏置初始化为常量0.1</div></pre></td></tr></table></figure><p>‘’’权重初始化函数’’’<br>def weight_variable(shape):<br>    inital = tf.truncated_normal(shape, stddev=0.1)  # 使用truncated_normal进行初始化<br>    return tf.Variable(inital)</p><p>‘’’偏置初始化函数’’’<br>def bias_variable(shape):<br>    inital = tf.constant(0.1,shape=shape)  # 偏置定义为常量<br>    return tf.Variable(inital)<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">- 卷积函数</div><div class="line"> - `strides[0]`和`strides[3]`的两个1是默认值，中间两个1代表padding时在x方向运动1步，y方向运动1步</div><div class="line"> - `padding=&apos;SAME&apos;`代表经过卷积之后的输出图像和原图像大小一样</div></pre></td></tr></table></figure></p><p>‘’’卷积函数’’’<br>def conv2d(x,W):#x是图片的所有参数，W是此卷积层的权重<br>    return tf.nn.conv2d(x,W,strides=[1,1,1,1],padding=’SAME’)#strides[0]和strides<a href="https://raw.githubusercontent.com/lawlite19/MachineLearning_TensorFlow/master/images/example_02.gif" title="example_02.gif" target="_blank" rel="external">3</a>的两个1是默认值，中间两个1代表padding时在x方向运动1步，y方向运动1步<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"></div><div class="line"></div><div class="line">- 池化函数</div><div class="line"> - `ksize`指定池化核函数的大小</div><div class="line"> - 根据池化核函数的大小定义`strides`的大小</div></pre></td></tr></table></figure></p><p>‘’’池化函数’’’<br>def max_pool_2x2(x):<br>    return tf.nn.max_pool(x,ksize=[1,2,2,1],<br>                          strides=[1,2,2,1],                          padding=’SAME’)#池化的核函数大小为2x2，因此ksize=[1,2,2,1]，步长为2，因此strides=[1,2,2,1]<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">- 加载`mnist`数据和定义`placeholder`</div><div class="line"> - 输入数据`x_image`最后一个`1`代表`channel`的数量,若是`RGB`3个颜色通道就定义为3</div><div class="line"> - `keep_prob` 用于**dropout**防止过拟合</div></pre></td></tr></table></figure></p><pre><code>mnist = input_data.read_data_sets(&apos;MNIST_data&apos;, one_hot=True)  # 下载数据xs = tf.placeholder(tf.float32,[None,784])  # 输入图片的大小，28x28=784ys = tf.placeholder(tf.float32,[None,10])   # 输出0-9共10个数字keep_prob = tf.placeholder(tf.float32)      # 用于接收dropout操作的值，dropout为了防止过拟合x_image = tf.reshape(xs,[-1,28,28,1])       #-1代表先不考虑输入的图片例子多少这个维度，后面的1是channel的数量，因为我们输入的图片是黑白的，因此channel是1，例如如果是RGB图像，那么channel就是3</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">- 第一层卷积和池化</div><div class="line">  - 使用**ReLu**激活函数</div></pre></td></tr></table></figure><pre><code>&apos;&apos;&apos;第一层卷积，池化&apos;&apos;&apos;W_conv1 = weight_variable([5,5,1,32])  # 卷积核定义为5x5,1是输入的通道数目，32是输出的通道数目b_conv1 = bias_variable([32])          # 每个输出通道对应一个偏置h_conv1 = tf.nn.relu(conv2d(x_image,W_conv1)+b_conv1) # 卷积运算，并使用ReLu激活函数激活h_pool1 = max_pool_2x2(h_conv1)        # pooling操作 </code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"></div><div class="line"></div><div class="line">- 第二层卷积和池化</div></pre></td></tr></table></figure><pre><code>&apos;&apos;&apos;第二层卷积，池化&apos;&apos;&apos;W_conv2 = weight_variable([5,5,32,64]) # 卷积核还是5x5,32个输入通道，64个输出通道b_conv2 = bias_variable([64])          # 与输出通道一致h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2)+b_conv2)h_pool2 = max_pool_2x2(h_conv2)</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"></div><div class="line"></div><div class="line">- 全连接第一层</div></pre></td></tr></table></figure><pre><code>&apos;&apos;&apos;全连接层&apos;&apos;&apos;h_pool2_flat = tf.reshape(h_pool2, [-1,7*7*64])   # 将最后操作的数据展开W_fc1 = weight_variable([7*7*64,1024])            # 下面就是定义一般神经网络的操作了，继续扩大为1024b_fc1 = bias_variable([1024])                     # 对应的偏置h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat,W_fc1)+b_fc1)  # 运算、激活（这里不是卷积运算了，就是对应相乘）</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"></div><div class="line"></div><div class="line">- `dropout`防止过拟合</div></pre></td></tr></table></figure><pre><code>&apos;&apos;&apos;dropout&apos;&apos;&apos;h_fc1_drop = tf.nn.dropout(h_fc1,keep_prob)       # dropout操作</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"></div><div class="line"></div><div class="line">- 最后一层全连接预测,使用梯度下降优化**交叉熵损失函数**</div><div class="line"> - 使用**softmax**分类器分类</div></pre></td></tr></table></figure><pre><code>&apos;&apos;&apos;最后一层全连接&apos;&apos;&apos;W_fc2 = weight_variable([1024,10])                # 最后一层权重初始化b_fc2 = bias_variable([10])                       # 对应偏置prediction = tf.nn.softmax(tf.matmul(h_fc1_drop,W_fc2)+b_fc2)  # 使用softmax分类器cross_entropy = tf.reduce_mean(-tf.reduce_sum(ys*tf.log(prediction),reduction_indices=[1]))  # 交叉熵损失函数来定义cost functiontrain_step = tf.train.AdamOptimizer(1e-3).minimize(cross_entropy)  # 调用梯度下降</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"></div><div class="line"></div><div class="line">- 定义Session，使用`SGD`训练</div></pre></td></tr></table></figure><pre><code>&apos;&apos;&apos;下面就是tf的一般操作，定义Session，初始化所有变量，placeholder传入值训练&apos;&apos;&apos;sess = tf.Session()sess.run(tf.initialize_all_variables())for i in range(1000):    batch_xs, batch_ys = mnist.train.next_batch(100)  # 使用SGD，每次选取100个数据训练    sess.run(train_step, feed_dict={xs: batch_xs, ys: batch_ys, keep_prob: 0.5})  # dropout值定义为0.5    if i % 50 == 0:        print compute_accuracy(xs,ys,mnist.test.images, mnist.test.labels,keep_prob,sess,prediction)  # 每50次输出一下准确度</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">- 计算准确度函数</div><div class="line">  - 和上面的两个计算准确度的函数一致，就是多了个**dropout**的参数`keep_prob`</div></pre></td></tr></table></figure><p>‘’’计算准确度函数’’’<br>def compute_accuracy(xs,ys,X,y,keep_prob,sess,prediction):<br>    y_pre = sess.run(prediction,feed_dict={xs:X,keep_prob:1.0})       # 预测，这里的keep_prob是dropout时用的，防止过拟合<br>    correct_prediction = tf.equal(tf.argmax(y_pre,1),tf.argmax(y,1))  #tf.argmax 给出某个tensor对象在某一维上的其数据最大值所在的索引值,即为对应的数字，tf.equal 来检测我们的预测是否真实标签匹配<br>    accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32)) # 平均值即为准确度<br>    result = sess.run(accuracy,feed_dict={xs:X,ys:y,keep_prob:1.0})<br>    return result<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">### 3、运行结果</div><div class="line">- 测试集上准确度   </div><div class="line">![enter description here][11]   </div><div class="line">- 使用`top`命令查看占用的CPU和内存，还是很消耗CPU和内存的，所以上面只输出了四次我就终止了</div><div class="line">![enter description here][12]   </div><div class="line">- 由于我在虚拟机里运行的`TensorFlow`程序，分配了`5G`的内存，若是内存不够会报一个错误。</div><div class="line"></div><div class="line">-------------------------------------------------------------</div><div class="line"></div><div class="line">## 八、保存和提取神经网络</div><div class="line">### 1、保存</div><div class="line">- 定义要保存的数据</div></pre></td></tr></table></figure></p><p>W = tf.Variable(initial_value=[[1,2,3],[3,4,5]],<br>               name=’weights’, dtype=tf.float32)   # 注意需要指定name和dtype<br>b = tf.Variable(initial_value=[1,2,3],<br>               name=’biases’, dtype=tf.float32)<br>init = tf.initialize_all_variables()<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">- 保存</div></pre></td></tr></table></figure></p><p>saver = tf.train.Saver()<br>with tf.Session() as sess:<br>    sess.run(init)<br>    save_path = saver.save(sess, ‘my_network/save_net.ckpt’) # 保存目录，注意要在当前项目下建立my_network的目录<br>    print (‘保存到 :’,save_path)<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">### 2、提取</div><div class="line">- 定义数据</div></pre></td></tr></table></figure></p><p>W = tf.Variable(np.arange(6).reshape((2,3)),<br>               name=’weights’, dtype=tf.float32) # 注意与之前保存的一致<br>b = tf.Variable(np.arange((3)),<br>               name=’biases’, dtype=tf.float32)<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">- `restore`提取</div></pre></td></tr></table></figure></p><p>saver = tf.train.Saver()<br>with tf.Session() as sess:<br>    saver.restore(sess,’my_network/save_net.ckpt’)<br>    print(‘weights:’,sess.run(W))  # 输出一下结果<br>    print(‘biases:’,sess.run(b))<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">-------------------------------------------------</div><div class="line"></div><div class="line">- 以下来自`tensorflow-turorial`，使用`python3.5`</div><div class="line"></div><div class="line"></div><div class="line">## 九、线性模型Linear Model</div><div class="line">- [全部代码][13]</div><div class="line">- 使用`MNIST`数据集</div><div class="line"></div><div class="line">### 1、加载MNIST数据集，并输出信息</div><div class="line"></div><div class="line">``` stylus</div><div class="line">&apos;&apos;&apos;Load MNIST data and print some information&apos;&apos;&apos;</div><div class="line">data = input_data.read_data_sets(&quot;MNIST_data&quot;, one_hot = True)</div><div class="line">print(&quot;Size of:&quot;)</div><div class="line">print(&quot;\t training-set:\t\t&#123;&#125;&quot;.format(len(data.train.labels)))</div><div class="line">print(&quot;\t test-set:\t\t\t&#123;&#125;&quot;.format(len(data.test.labels)))</div><div class="line">print(&quot;\t validation-set:\t&#123;&#125;&quot;.format(len(data.validation.labels)))</div><div class="line">print(data.test.labels[0:5])</div><div class="line">data.test.cls = np.array([label.argmax() for label in data.test.labels])   # get the actual value</div><div class="line">print(data.test.cls[0:5])</div></pre></td></tr></table></figure></p><h3 id="2、绘制9张图像"><a href="#2、绘制9张图像" class="headerlink" title="2、绘制9张图像"></a>2、绘制9张图像</h3><ul><li><p>实现函数</p><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line">'''define a funciton to plot 9 images'''</div><div class="line">def plot_images(images, cls_true, cls_pred = None):</div><div class="line">    '''</div><div class="line">    @parameter images:   the images info</div><div class="line">    @parameter cls_true: the true value of image</div><div class="line">    @parameter cls_pred: the prediction value, default is None</div><div class="line">    '''</div><div class="line">    assert len(images) == len(cls_true) == 9  # only show 9 images</div><div class="line">    fig, axes = plt.subplots(nrows=3, ncols=3)</div><div class="line">    for i, ax in enumerate(axes.flat):</div><div class="line">        ax.imshow(images[i].reshape(img_shape), cmap="binary")  # binary means black_white image</div><div class="line">        # show the true and pred values</div><div class="line">        if cls_pred is None:</div><div class="line">            xlabel = "True: &#123;0&#125;".format(cls_true[i])</div><div class="line">        else:</div><div class="line">            xlabel = "True: &#123;0&#125;,Pred: &#123;1&#125;".format(cls_true[i],cls_pred[i])</div><div class="line">        ax.set_xlabel(xlabel)</div><div class="line">        ax.set_xticks([])  # remove the ticks</div><div class="line">        ax.set_yticks([])</div><div class="line">    plt.show()</div></pre></td></tr></table></figure></li><li><p>选择测试集中的9张图显示</p></li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">'''show 9 images'''</div><div class="line">images = data.test.images[0:9]</div><div class="line">cls_true = data.test.cls[0:9]</div><div class="line">plot_images(images, cls_true)</div><div class="line">```                   </div><div class="line">![enter description here][14]</div><div class="line"></div><div class="line">### 3、定义要训练的模型</div><div class="line">- 定义`placeholder`</div><div class="line">``` stylus</div><div class="line">'''define the placeholder'''</div><div class="line">X = tf.placeholder(tf.float32, [None, img_size_flat])    # None means the arbitrary number of labels, the features size is img_size_flat </div><div class="line">y_true = tf.placeholder(tf.float32, [None, num_classes]) # output size is num_classes</div><div class="line">y_true_cls = tf.placeholder(tf.int64, [None])</div></pre></td></tr></table></figure><ul><li>定义<code>weights</code>和<code>biases</code></li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">'''define weights and biases'''</div><div class="line">weights = tf.Variable(tf.zeros([img_size_flat, num_classes]))  # img_size_flat*num_classes</div><div class="line">biases = tf.Variable(tf.zeros([num_classes]))</div></pre></td></tr></table></figure><ul><li>定义模型</li></ul><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="string">''</span><span class="string">'define the model'</span><span class="string">''</span></div><div class="line">logits = tf.matmul(X,weights) + biases </div><div class="line">y_pred = tf<span class="selector-class">.nn</span><span class="selector-class">.softmax</span>(logits)</div><div class="line">y_pred_cls = tf.argmax(y_pred, dimension=<span class="number">1</span>)</div><div class="line">cross_entropy = tf<span class="selector-class">.nn</span><span class="selector-class">.softmax_cross_entropy_with_logits</span>(labels=y_true, </div><div class="line">                                                       logits=logits)</div><div class="line">cost = tf.reduce_mean(cross_entropy)</div><div class="line"><span class="string">''</span><span class="string">'define the optimizer'</span><span class="string">''</span></div><div class="line">optimizer = tf<span class="selector-class">.train</span><span class="selector-class">.GradientDescentOptimizer</span>(learning_rate=<span class="number">0.5</span>).minimize(cost)</div></pre></td></tr></table></figure><ul><li>定义求准确度</li></ul><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="string">''</span><span class="string">'define the accuracy'</span><span class="string">''</span></div><div class="line">correct_prediction = tf.equal(y_pred_cls, y_true_cls)</div><div class="line">accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</div></pre></td></tr></table></figure><ul><li>定义<code>session</code></li></ul><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="string">''</span><span class="string">'run the datagraph and use batch gradient descent'</span><span class="string">''</span></div><div class="line">session = tf.Session()</div><div class="line">session.run(tf.global_variables_initializer())</div><div class="line">batch_size = <span class="number">100</span></div></pre></td></tr></table></figure><h3 id="4、定义函数optimize进行bgd训练"><a href="#4、定义函数optimize进行bgd训练" class="headerlink" title="4、定义函数optimize进行bgd训练"></a>4、定义函数<code>optimize</code>进行bgd训练</h3><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">'''define a function to run the optimizer'''</div><div class="line">def optimize(num_iterations):</div><div class="line">    '''</div><div class="line">    @parameter num_iterations: the traning times</div><div class="line">    '''</div><div class="line">    for i in range(num_iterations):</div><div class="line">        x_batch, y_true_batch = data.train.next_batch(batch_size)</div><div class="line">        feed_dict_train = &#123;X: x_batch,y_true: y_true_batch&#125;</div><div class="line">        session.run(optimizer, feed_dict=feed_dict_train)</div></pre></td></tr></table></figure><h3 id="5、定义输出准确度的函数"><a href="#5、定义输出准确度的函数" class="headerlink" title="5、定义输出准确度的函数"></a>5、定义输出准确度的函数</h3><ul><li><p>代码</p><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">feed_dict_test = &#123;X: data.test.images, </div><div class="line">                  y_true: data.test.labels, </div><div class="line">                  y_true_cls: data.test.cls&#125;        </div><div class="line">'''define a function to print the accuracy'''    </div><div class="line">def print_accuracy():</div><div class="line">    acc = session.run(accuracy, feed_dict=feed_dict_test)</div><div class="line">    print("Accuracy on test-set:&#123;0:.1%&#125;".format(acc))</div></pre></td></tr></table></figure></li><li><p>输出：<code>Accuracy on test-set:89.4%</code></p></li></ul><h3 id="6、定义绘制错误预测的图片函数"><a href="#6、定义绘制错误预测的图片函数" class="headerlink" title="6、定义绘制错误预测的图片函数"></a>6、定义绘制错误预测的图片函数</h3><ul><li><p>代码</p><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">'''define a function to plot the error prediciton'''    </div><div class="line">def plot_example_errors():</div><div class="line">    correct, cls_pred = session.run([correct_prediction, y_pred_cls], feed_dict=feed_dict_test) </div><div class="line">    incorrect = (correct == False)</div><div class="line">    images = data.test.images[incorrect]  # get the prediction error images</div><div class="line">    cls_pred = cls_pred[incorrect]        # get prediction value</div><div class="line">    cls_true = data.test.cls[incorrect]   # get true value</div><div class="line">    plot_images(images[0:9], cls_true[0:9], cls_pred[0:9])</div></pre></td></tr></table></figure></li><li><p>输出：<br><img src="https://raw.githubusercontent.com/lawlite19/MachineLearning_TensorFlow/master/images/LinearModel_02.png" alt="enter description here" title="LinearModel_02"></p><h3 id="7、定义可视化权重的函数"><a href="#7、定义可视化权重的函数" class="headerlink" title="7、定义可视化权重的函数"></a>7、定义可视化权重的函数</h3></li><li><p>代码</p><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">'''define a fucntion to plot weights'''</div><div class="line">def plot_weights():</div><div class="line">    w = session.run(weights)</div><div class="line">    w_min = np.min(w)</div><div class="line">    w_max = np.max(w)</div><div class="line">    fig, axes = plt.subplots(3, 4)</div><div class="line">    fig.subplots_adjust(0.3, 0.3)</div><div class="line">    for i, ax in enumerate(axes.flat):</div><div class="line">        if i&lt;10:</div><div class="line">            image = w[:,i].reshape(img_shape)</div><div class="line">            ax.set_xlabel("Weights: &#123;0&#125;".format(i))</div><div class="line">            ax.imshow(image, vmin=w_min,vmax=w_max,cmap="seismic")</div><div class="line">        ax.set_xticks([])</div><div class="line">        ax.set_yticks([])</div><div class="line">    plt.show()</div></pre></td></tr></table></figure></li><li><p>输出：<br><img src="https://raw.githubusercontent.com/lawlite19/MachineLearning_TensorFlow/master/images/LinearModel_03.png" alt="enter description here" title="LinearModel_03"></p><h3 id="8、定义输出confusion-matrix的函数"><a href="#8、定义输出confusion-matrix的函数" class="headerlink" title="8、定义输出confusion_matrix的函数"></a>8、定义输出<code>confusion_matrix</code>的函数</h3></li><li><p>代码：</p><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">'''define a function to printand plot the confusion matrix using scikit-learn.'''   </div><div class="line">def print_confusion_martix():</div><div class="line">    cls_true = data.test.cls  # test set actual value </div><div class="line">    cls_pred = session.run(y_pred_cls, feed_dict=feed_dict_test)  # test set predict value</div><div class="line">    cm = confusion_matrix(y_true=cls_true,y_pred=cls_pred)        # use sklearn confusion_matrix</div><div class="line">    print(cm)</div><div class="line">    plt.imshow(cm, interpolation='nearest',cmap=plt.cm.Blues) # Plot the confusion matrix as an image.</div><div class="line">    plt.tight_layout()</div><div class="line">    plt.colorbar()</div><div class="line">    tick_marks = np.arange(num_classes)</div><div class="line">    tick_marks = np.arange(num_classes)</div><div class="line">    plt.xticks(tick_marks, range(num_classes))</div><div class="line">    plt.yticks(tick_marks, range(num_classes))</div><div class="line">    plt.xlabel('Predicted')</div><div class="line">    plt.ylabel('True')    </div><div class="line">    plt.show()</div></pre></td></tr></table></figure></li><li><p>输出：<br><img src="https://raw.githubusercontent.com/lawlite19/MachineLearning_TensorFlow/master/images/LinearModel_04.png" alt="enter description here" title="LinearModel_04"></p></li></ul><h2 id="十：CNN"><a href="#十：CNN" class="headerlink" title="十：CNN"></a>十：CNN</h2><ul><li><a href="https://github.com/lawlite19/MachineLearning_TensorFlow/blob/master/CNNModel/CNN_Model.py" target="_blank" rel="external">全部代码</a></li><li>使用<code>MNIST</code>数据集</li><li>加载数据，绘制9张图等函数与上面一致，<code>readme</code>中不再写出</li></ul><h3 id="1、定义CNN所需要的变量"><a href="#1、定义CNN所需要的变量" class="headerlink" title="1、定义CNN所需要的变量"></a>1、定义CNN所需要的变量</h3><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">'''define cnn description'''</div><div class="line">filter_size1 = 5     # the first conv filter size is 5x5 </div><div class="line">num_filters1 = 32    # there are 32 filters</div><div class="line">filter_size2 = 5     # the second conv filter size</div><div class="line">num_filters2 = 64    # there are 64 filters</div><div class="line">fc_size = 1024       # fully-connected layer</div></pre></td></tr></table></figure><h3 id="2、初始化weights和biases的函数"><a href="#2、初始化weights和biases的函数" class="headerlink" title="2、初始化weights和biases的函数"></a>2、初始化weights和biases的函数</h3><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">'''define a function to intialize weights'''</div><div class="line">def initialize_weights(shape):</div><div class="line">    '''</div><div class="line">    @param shape：the shape of weights</div><div class="line">    '''</div><div class="line">    return tf.Variable(tf.truncated_normal(shape=shape, stddev=0.1))</div><div class="line">'''define a function to intialize biases'''</div><div class="line">def initialize_biases(length):</div><div class="line">    '''</div><div class="line">    @param length: the length of biases, which is a vector</div><div class="line">    '''</div><div class="line">    return tf.Variable(tf.constant(0.1,shape=[length]))</div></pre></td></tr></table></figure><h3 id="3、定义卷积操作和池化（如果使用的话）的函数"><a href="#3、定义卷积操作和池化（如果使用的话）的函数" class="headerlink" title="3、定义卷积操作和池化（如果使用的话）的函数"></a>3、定义卷积操作和池化（如果使用的话）的函数</h3><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line">'''define a function to do conv and pooling if used'''</div><div class="line">def conv_layer(input, </div><div class="line">               num_input_channels,</div><div class="line">               filter_size,</div><div class="line">               num_output_filters,</div><div class="line">               use_pooling=True):</div><div class="line">    '''</div><div class="line">    @param input: the input of previous layer's output</div><div class="line">    @param num_input_channels: input channels</div><div class="line">    @param filter_size: the weights filter size</div><div class="line">    @param num_output_filters: the output number channels</div><div class="line">    @param use_pooling: if use pooling operation</div><div class="line">    '''</div><div class="line">    shape = [filter_size, filter_size, num_input_channels, num_output_filters]</div><div class="line">    weights = initialize_weights(shape=shape)</div><div class="line">    biases = initialize_biases(length=num_output_filters)   # one for each filter</div><div class="line">    layer = tf.nn.conv2d(input=input, filter=weights, strides=[1,1,1,1], padding='SAME')</div><div class="line">    layer += biases</div><div class="line">    if use_pooling:</div><div class="line">        layer = tf.nn.max_pool(value=layer,</div><div class="line">                               ksize=[1,2,2,1],</div><div class="line">                               strides=[1,2,2,1],</div><div class="line">                               padding="SAME")   # the kernel function size is 2x2,so the ksize=[1,2,2,1]</div><div class="line">    layer = tf.nn.relu(layer)</div><div class="line">    return layer, weights</div></pre></td></tr></table></figure><h3 id="4、定义将卷积层展开的函数"><a href="#4、定义将卷积层展开的函数" class="headerlink" title="4、定义将卷积层展开的函数"></a>4、定义将卷积层展开的函数</h3><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">'''define a function to flat conv layer'''</div><div class="line">def flatten_layer(layer):</div><div class="line">    '''</div><div class="line">    @param layer: the conv layer</div><div class="line">    '''</div><div class="line">    layer_shape = layer.get_shape() # get the shape of the layer(layer_shape == [num_images, img_height, img_width, num_channels])</div><div class="line">    num_features = layer_shape[1:4].num_elements()  # [1:4] means the last three demension, namely the flatten size</div><div class="line">    layer_flat = tf.reshape(layer, [-1, num_features])   # reshape to flat,-1 means don't care about the number of images</div><div class="line">    return layer_flat, num_features</div></pre></td></tr></table></figure><h3 id="5、定义全连接层的函数"><a href="#5、定义全连接层的函数" class="headerlink" title="5、定义全连接层的函数"></a>5、定义全连接层的函数</h3><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">'''define a function to do fully-connected'''</div><div class="line">def fc_layer(input, num_inputs, num_outputs, use_relu=True):</div><div class="line">    '''</div><div class="line">    @param input: the input</div><div class="line">    @param num_inputs: the input size</div><div class="line">    @param num_outputs: the output size</div><div class="line">    @param use_relu: if use relu activation function</div><div class="line">    '''</div><div class="line">    weights = initialize_weights(shape=[num_inputs, num_outputs])</div><div class="line">    biases = initialize_biases(num_outputs)</div><div class="line">    layer = tf.matmul(input, weights) + biases</div><div class="line">    if use_relu:</div><div class="line">        layer = tf.nn.relu(layer)</div><div class="line">    return layer</div></pre></td></tr></table></figure><h3 id="6、定义模型"><a href="#6、定义模型" class="headerlink" title="6、定义模型"></a>6、定义模型</h3><ul><li>定义<code>placeholder</code></li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">'''define the placeholder'''</div><div class="line">X = tf.placeholder(tf.float32, shape=[None, img_flat_size], name="X")</div><div class="line">X_image = tf.reshape(X, shape=[-1, img_size, img_size, num_channels])  # reshape to the image shape</div><div class="line">y_true = tf.placeholder(tf.float32, [None, num_classes], name="y_true")</div><div class="line">y_true_cls = tf.argmax(y_true, axis=1)</div><div class="line">keep_prob = tf.placeholder(tf.float32)  # drop out placeholder</div></pre></td></tr></table></figure><ul><li>定义卷积、dropout、和全连接</li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line">'''define the cnn model'''</div><div class="line">layer_conv1, weights_conv1 = conv_layer(input=X_image, num_input_channels=num_channels, </div><div class="line">                                       filter_size=filter_size1, </div><div class="line">                                       num_output_filters=num_filters1,</div><div class="line">                                       use_pooling=True)</div><div class="line">print("conv1:",layer_conv1)</div><div class="line">layer_conv2, weights_conv2 = conv_layer(input=layer_conv1, num_input_channels=num_filters1, </div><div class="line">                                        filter_size=filter_size2,</div><div class="line">                                        num_output_filters=num_filters2,</div><div class="line">                                        use_pooling=True)</div><div class="line">print("conv2:",layer_conv2)</div><div class="line">layer_flat, num_features = flatten_layer(layer_conv2) # the num_feature is 7x7x36=1764</div><div class="line">print("flatten layer:", layer_flat)  </div><div class="line">layer_fc1 = fc_layer(layer_flat, num_features, fc_size, use_relu=True)</div><div class="line">print("fully-connected layer1:", layer_fc1)</div><div class="line">layer_drop_out = tf.nn.dropout(layer_fc1, keep_prob)   # dropout operation</div><div class="line">layer_fc2 = fc_layer(layer_drop_out, fc_size, num_classes,use_relu=False)</div><div class="line">print("fully-connected layer2:", layer_fc2)</div><div class="line">y_pred = tf.nn.softmax(layer_fc2)</div><div class="line">y_pred_cls = tf.argmax(y_pred, axis=1)</div><div class="line">cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=y_true, </div><div class="line">                                                       logits=layer_fc2)</div><div class="line">cost = tf.reduce_mean(cross_entropy)</div><div class="line">optimizer = tf.train.AdamOptimizer(learning_rate=1e-3).minimize(cost)  # use AdamOptimizer优化</div></pre></td></tr></table></figure><ul><li>定义求准确度</li></ul><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="string">''</span><span class="string">'define accuracy'</span><span class="string">''</span></div><div class="line">correct_prediction = tf.equal(y_true_cls, y_pred_cls)</div><div class="line">accuracy = tf.reduce_mean(tf.cast(correct_prediction,dtype=tf.float32))</div></pre></td></tr></table></figure><h3 id="7、定义训练的函数optimize，使用bgd"><a href="#7、定义训练的函数optimize，使用bgd" class="headerlink" title="7、定义训练的函数optimize，使用bgd"></a>7、定义训练的函数<code>optimize</code>，使用bgd</h3><ul><li><p>代码：</p><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line">'''define a function to run train the model with bgd'''</div><div class="line">total_iterations = 0  # record the total iterations</div><div class="line">def optimize(num_iterations):</div><div class="line">    '''</div><div class="line">    @param num_iterations: the total interations of train batch_size operation</div><div class="line">    '''</div><div class="line">    global total_iterations</div><div class="line">    start_time = time.time()</div><div class="line">    for i in range(total_iterations,total_iterations + num_iterations):</div><div class="line">        x_batch, y_batch = data.train.next_batch(batch_size)</div><div class="line">        feed_dict = &#123;X: x_batch, y_true: y_batch, keep_prob: 0.5&#125;</div><div class="line">        session.run(optimizer, feed_dict=feed_dict)</div><div class="line">        if i % 10 == 0:</div><div class="line">            acc = session.run(accuracy, feed_dict=feed_dict)</div><div class="line">            msg = "Optimization Iteration: &#123;0:&gt;6&#125;, Training Accuracy: &#123;1:&gt;6.1%&#125;"    # &#123;:&gt;6&#125;means the fixed width,&#123;1:&gt;6.1%&#125;means the fixed width is 6 and keep 1 decimal place         </div><div class="line">            print(msg.format(i + 1, acc))</div><div class="line">    total_iterations += num_iterations</div><div class="line">    end_time = time.time()</div><div class="line">    time_dif = end_time-start_time</div><div class="line">    print("time usage:"+str(timedelta(seconds=int(round(time_dif)))))</div></pre></td></tr></table></figure></li><li><p>输出：</p></li></ul><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div></pre></td><td class="code"><pre><div class="line">Optimization Iteration:    <span class="number">651</span>, Training Accuracy:  <span class="number">99.0%</span></div><div class="line">Optimization Iteration:    <span class="number">661</span>, Training Accuracy:  <span class="number">99.0%</span></div><div class="line">Optimization Iteration:    <span class="number">671</span>, Training Accuracy:  <span class="number">99.0%</span></div><div class="line">Optimization Iteration:    <span class="number">681</span>, Training Accuracy:  <span class="number">99.0%</span></div><div class="line">Optimization Iteration:    <span class="number">691</span>, Training Accuracy:  <span class="number">99.0%</span></div><div class="line">Optimization Iteration:    <span class="number">701</span>, Training Accuracy:  <span class="number">99.0%</span></div><div class="line">Optimization Iteration:    <span class="number">711</span>, Training Accuracy:  <span class="number">99.0%</span></div><div class="line">Optimization Iteration:    <span class="number">721</span>, Training Accuracy:  <span class="number">99.0%</span></div><div class="line">Optimization Iteration:    <span class="number">731</span>, Training Accuracy:  <span class="number">99.0%</span></div><div class="line">Optimization Iteration:    <span class="number">741</span>, Training Accuracy: <span class="number">100.0%</span></div><div class="line">Optimization Iteration:    <span class="number">751</span>, Training Accuracy:  <span class="number">99.0%</span></div><div class="line">Optimization Iteration:    <span class="number">761</span>, Training Accuracy:  <span class="number">99.0%</span></div><div class="line">Optimization Iteration:    <span class="number">771</span>, Training Accuracy:  <span class="number">97.0%</span></div><div class="line">Optimization Iteration:    <span class="number">781</span>, Training Accuracy:  <span class="number">96.0%</span></div><div class="line">Optimization Iteration:    <span class="number">791</span>, Training Accuracy:  <span class="number">98.0%</span></div><div class="line">Optimization Iteration:    <span class="number">801</span>, Training Accuracy: <span class="number">100.0%</span></div><div class="line">Optimization Iteration:    <span class="number">811</span>, Training Accuracy: <span class="number">100.0%</span></div><div class="line">Optimization Iteration:    <span class="number">821</span>, Training Accuracy:  <span class="number">97.0%</span></div><div class="line">Optimization Iteration:    <span class="number">831</span>, Training Accuracy:  <span class="number">98.0%</span></div><div class="line">Optimization Iteration:    <span class="number">841</span>, Training Accuracy:  <span class="number">99.0%</span></div><div class="line">Optimization Iteration:    <span class="number">851</span>, Training Accuracy:  <span class="number">99.0%</span></div><div class="line">Optimization Iteration:    <span class="number">861</span>, Training Accuracy:  <span class="number">99.0%</span></div><div class="line">Optimization Iteration:    <span class="number">871</span>, Training Accuracy:  <span class="number">96.0%</span></div><div class="line">Optimization Iteration:    <span class="number">881</span>, Training Accuracy:  <span class="number">99.0%</span></div><div class="line">Optimization Iteration:    <span class="number">891</span>, Training Accuracy:  <span class="number">99.0%</span></div><div class="line">Optimization Iteration:    <span class="number">901</span>, Training Accuracy:  <span class="number">98.0%</span></div><div class="line">Optimization Iteration:    <span class="number">911</span>, Training Accuracy:  <span class="number">99.0%</span></div><div class="line">Optimization Iteration:    <span class="number">921</span>, Training Accuracy:  <span class="number">99.0%</span></div><div class="line">Optimization Iteration:    <span class="number">931</span>, Training Accuracy:  <span class="number">99.0%</span></div><div class="line">Optimization Iteration:    <span class="number">941</span>, Training Accuracy:  <span class="number">98.0%</span></div><div class="line">Optimization Iteration:    <span class="number">951</span>, Training Accuracy: <span class="number">100.0%</span></div><div class="line">Optimization Iteration:    <span class="number">961</span>, Training Accuracy:  <span class="number">99.0%</span></div><div class="line">Optimization Iteration:    <span class="number">971</span>, Training Accuracy:  <span class="number">98.0%</span></div><div class="line">Optimization Iteration:    <span class="number">981</span>, Training Accuracy:  <span class="number">99.0%</span></div><div class="line">Optimization Iteration:    <span class="number">991</span>, Training Accuracy: <span class="number">100.0%</span></div><div class="line"><span class="selector-tag">time</span> usage:<span class="number">0</span>:<span class="number">07</span>:<span class="number">07</span></div></pre></td></tr></table></figure><h3 id="8、定义批量预测的函数，方便输出训练错的图像"><a href="#8、定义批量预测的函数，方便输出训练错的图像" class="headerlink" title="8、定义批量预测的函数，方便输出训练错的图像"></a>8、定义批量预测的函数，方便输出训练错的图像</h3><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line">batch_size_test = 256</div><div class="line">def print_test_accuracy(print_error=False,print_confusion_matrix=False):</div><div class="line">    '''</div><div class="line">    @param print_error: whether plot the error images</div><div class="line">    @param print_confusion_matrix: whether plot the confusion_matrix</div><div class="line">    '''</div><div class="line">    num_test = len(data.test.images)   </div><div class="line">    cls_pred = np.zeros(shape=num_test, dtype=np.int)  # declare the cls_pred</div><div class="line">    i = 0</div><div class="line">    #predict the test set using batch_size</div><div class="line">    while i &lt; num_test:</div><div class="line">        j = min(i + batch_size_test, num_test)</div><div class="line">        images = data.test.images[i:j,:]</div><div class="line">        labels = data.test.labels[i:j,:]</div><div class="line">        feed_dict = &#123;X:images,y_true:labels,keep_prob:0.5&#125;</div><div class="line">        cls_pred[i:j] = session.run(y_pred_cls,feed_dict=feed_dict)</div><div class="line">        i = j</div><div class="line">    cls_true = data.test.cls</div><div class="line">    correct = (cls_true == cls_pred)</div><div class="line">    correct_sum = correct.sum()   # correct predictions</div><div class="line">    acc = float(correct_sum)/num_test</div><div class="line">    msg = "Accuracy on Test-Set: &#123;0:.1%&#125; (&#123;1&#125; / &#123;2&#125;)"</div><div class="line">    print(msg.format(acc, correct_sum, num_test))    </div><div class="line">    if print_error:</div><div class="line">        plot_error_pred(cls_pred,correct)</div><div class="line">    if print_confusion_matrix:</div><div class="line">        plot_confusin_martrix(cls_pred)</div></pre></td></tr></table></figure><h3 id="9、定义可视化卷积核权重的函数"><a href="#9、定义可视化卷积核权重的函数" class="headerlink" title="9、定义可视化卷积核权重的函数"></a>9、定义可视化卷积核权重的函数</h3><ul><li><p>代码：</p><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line">'''define a function to plot conv weights'''</div><div class="line">def plot_conv_weights(weights,input_channel=0):</div><div class="line">    '''</div><div class="line">    @param weights: the conv filter weights, for example: the weights_conv1 and weights_conv2, which are 4 dimension [filter_size, filter_size, num_input_channels, num_output_filters]</div><div class="line">    @param input_channel: the input_channels</div><div class="line">    '''</div><div class="line">    w = session.run(weights)</div><div class="line">    w_min = np.min(w)</div><div class="line">    w_max = np.max(w)</div><div class="line">    num_filters = w.shape[3]   # get the number of filters</div><div class="line">    num_grids = math.ceil(math.sqrt(num_filters))</div><div class="line">    fig, axes = plt.subplots(num_grids, num_grids)</div><div class="line">    for i, ax in enumerate(axes.flat):</div><div class="line">        if i &lt; num_filters:</div><div class="line">            img = w[:,:,input_channel,i]   # the ith weight</div><div class="line">            ax.imshow(img,vmin=w_min,vmax=w_max,interpolation="nearest",cmap='seismic')</div><div class="line">        ax.set_xticks([])</div><div class="line">        ax.set_yticks([])</div><div class="line">    plt.show()</div></pre></td></tr></table></figure></li><li><p>输出：</p><ul><li>第一层：<br><img src="https://raw.githubusercontent.com/lawlite19/MachineLearning_TensorFlow/master/images/CNNModel_01.png" alt="enter description here" title="CNNModel_01"></li><li>第二层：<br><img src="https://raw.githubusercontent.com/lawlite19/MachineLearning_TensorFlow/master/images/CNNModel_03.png" alt="enter description here" title="CNNModel_03"><h3 id="10、定义可视化卷积层输出的函数"><a href="#10、定义可视化卷积层输出的函数" class="headerlink" title="10、定义可视化卷积层输出的函数"></a>10、定义可视化卷积层输出的函数</h3></li></ul></li><li><p>代码：</p><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line">'''define a function to plot conv output layer'''</div><div class="line">def plot_conv_layer(layer, image):</div><div class="line">    '''</div><div class="line">    @param layer: the conv layer, which is also a image after conv</div><div class="line">    @param image: the image info</div><div class="line">    '''</div><div class="line">    feed_dict = &#123;X:[image]&#125;</div><div class="line">    values = session.run(layer, feed_dict=feed_dict)</div><div class="line">    num_filters = values.shape[3]   # get the number of filters</div><div class="line">    num_grids = math.ceil(math.sqrt(num_filters))</div><div class="line">    fig, axes = plt.subplots(num_grids,num_grids)</div><div class="line">    for i, ax in enumerate(axes.flat):</div><div class="line">        if i &lt; num_filters:</div><div class="line">            img = values[0,:,:,i]</div><div class="line">            ax.imshow(img, interpolation="nearest",cmap="binary")</div><div class="line">        ax.set_xticks([])</div><div class="line">        ax.set_yticks([])</div><div class="line">    plt.show()</div></pre></td></tr></table></figure></li><li><p>输出：</p><ul><li>第一层：<br><img src="https://raw.githubusercontent.com/lawlite19/MachineLearning_TensorFlow/master/images/CNNModel_02.png" alt="enter description here" title="CNNModel_02"></li><li>第二层：<br><img src="https://raw.githubusercontent.com/lawlite19/MachineLearning_TensorFlow/master/images/CNNModel_04.png" alt="enter description here" title="CNNModel_04"></li></ul></li></ul><h2 id="十一：使用prettytensor实现CNNModel"><a href="#十一：使用prettytensor实现CNNModel" class="headerlink" title="十一：使用prettytensor实现CNNModel"></a>十一：使用prettytensor实现CNNModel</h2><ul><li><a href="https://github.com/lawlite19/MachineLearning_TensorFlow/blob/master/CNNModel_PrettyTensor/CNNModel_prettytensor.py" target="_blank" rel="external">全部代码</a></li><li>使用<code>MNIST</code>数据集</li><li>加载数据，绘制9张图等函数与<strong>九</strong>一致，<code>readme</code>中不再写出<h3 id="1、定义模型"><a href="#1、定义模型" class="headerlink" title="1、定义模型"></a>1、定义模型</h3></li><li>定义<code>placeholder</code>,与之前的一致</li></ul><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="string">''</span><span class="string">'declare the placeholder'</span><span class="string">''</span></div><div class="line">X = tf.placeholder(tf<span class="selector-class">.float32</span>, [None, img_flat_size], name=<span class="string">"X"</span>)</div><div class="line">X_img = tf.reshape(X, shape=[-<span class="number">1</span>,img_size,img_size, num_channels])</div><div class="line">y_true = tf.placeholder(tf<span class="selector-class">.float32</span>, shape=[None, num_classes], name=<span class="string">"y_true"</span>)</div><div class="line">y_true_cls = tf.argmax(y_true,<span class="number">1</span>)</div></pre></td></tr></table></figure><ul><li>使用<code>prettytensor</code>实现CNN模型</li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">'''define the cnn model with prettytensor'''</div><div class="line">x_pretty = pt.wrap(X_img)</div><div class="line">with pt.defaults_scope():   # or pt.defaults_scope(activation_fn=tf.nn.relu) if just use one activation function</div><div class="line">    y_pred, loss = x_pretty.\</div><div class="line">        conv2d(kernel=5, depth=16, activation_fn=tf.nn.relu, name="conv_layer1").\</div><div class="line">        max_pool(kernel=2, stride=2).\</div><div class="line">        conv2d(kernel=5, depth=36, activation_fn=tf.nn.relu, name="conv_layer2").\</div><div class="line">        max_pool(kernel=2, stride=2).\</div><div class="line">        flatten().\</div><div class="line">        fully_connected(size=128, activation_fn=tf.nn.relu, name="fc_layer1").\</div><div class="line">        softmax_classifier(num_classes=num_classes, labels=y_true)</div></pre></td></tr></table></figure><ul><li>获取卷积核的权重(后续可视化)</li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">'''define a function to get weights'''</div><div class="line">def get_weights_variable(layer_name):</div><div class="line">    with tf.variable_scope(layer_name, reuse=True):</div><div class="line">        variable = tf.get_variable("weights")</div><div class="line">    return variable</div><div class="line">conv1_weights = get_weights_variable("conv_layer1")</div><div class="line">conv2_weights = get_weights_variable("conv_layer2")</div></pre></td></tr></table></figure><ul><li>定义<code>optimizer</code>训练，和之前的一样了</li></ul><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="string">''</span><span class="string">'define optimizer to train'</span><span class="string">''</span></div><div class="line">optimizer = tf<span class="selector-class">.train</span><span class="selector-class">.AdamOptimizer</span>().minimize(loss)</div><div class="line">y_pred_cls = tf.argmax(y_pred,<span class="number">1</span>)</div><div class="line">correct_prediction = tf.equal(y_pred_cls, y_true_cls)</div><div class="line">accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</div><div class="line">session = tf.Session()</div><div class="line">session.run(tf.global_variables_initializer())</div></pre></td></tr></table></figure><h2 id="十二：CNN-保存和加载模型，使用Early-Stopping"><a href="#十二：CNN-保存和加载模型，使用Early-Stopping" class="headerlink" title="十二：CNN,保存和加载模型，使用Early Stopping"></a>十二：CNN,保存和加载模型，使用Early Stopping</h2><ul><li><a href="https://github.com/lawlite19/MachineLearning_TensorFlow/blob/master/CNNModel_EarlyStopping_Save_Restore/CNNModel_EarlyStopping_Save_Restore.py" target="_blank" rel="external">全部代码</a></li><li>使用<code>MNIST</code>数据集</li><li>加载数据，绘制9张图等函数与<strong>九</strong>一致，<code>readme</code>中不再写出</li><li>CNN模型的定义和<strong>十一</strong>中的一致，<code>readme</code>中不再写出<h3 id="1、保存模型"><a href="#1、保存模型" class="headerlink" title="1、保存模型"></a>1、保存模型</h3></li><li>创建saver,和保存的目录</li></ul><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="string">''</span><span class="string">'define a Saver to save the network'</span><span class="string">''</span></div><div class="line">saver = tf<span class="selector-class">.train</span><span class="selector-class">.Saver</span>()</div><div class="line">save_dir = <span class="string">"checkpoints/"</span></div><div class="line"><span class="keyword">if</span> not os<span class="selector-class">.path</span><span class="selector-class">.exists</span>(save_dir):</div><div class="line">    os.makedirs(save_dir)</div><div class="line">save_path = os<span class="selector-class">.path</span><span class="selector-class">.join</span>(save_dir, <span class="string">'best_validation'</span>)</div></pre></td></tr></table></figure><ul><li>保存session,对应到下面2中的Early Stopping，将最好的模型保存</li></ul><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">saver.save(sess=session, save_path=save_path)</div></pre></td></tr></table></figure><h3 id="2、Early-Stopping"><a href="#2、Early-Stopping" class="headerlink" title="2、Early Stopping"></a>2、Early Stopping</h3><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div></pre></td><td class="code"><pre><div class="line">'''declear the train info'''</div><div class="line">train_batch_size = 64</div><div class="line">best_validation_accuracy = 0.0</div><div class="line">last_improvement = 0</div><div class="line">require_improvement_iterations = 1000</div><div class="line">total_iterations = 0</div><div class="line">'''define a function to optimize the optimizer'''</div><div class="line">def optimize(num_iterations):</div><div class="line">    global total_iterations</div><div class="line">    global best_validation_accuracy</div><div class="line">    global last_improvement</div><div class="line">    start_time = time.time()</div><div class="line">    for i in range(num_iterations):</div><div class="line">        total_iterations += 1</div><div class="line">        X_batch, y_true_batch = data.train.next_batch(train_batch_size)</div><div class="line">        feed_dict_train = &#123;X: X_batch,</div><div class="line">                     y_true: y_true_batch&#125;</div><div class="line">        session.run(optimizer, feed_dict=feed_dict_train)</div><div class="line">        if (total_iterations%100 == 0) or (i == num_iterations-1):</div><div class="line">            acc_train = session.run(accuracy, feed_dict=feed_dict_train)</div><div class="line">            acc_validation, _ = validation_accuracy()</div><div class="line">            if acc_validation &gt; best_validation_accuracy:</div><div class="line">                best_validation_accuracy = acc_validation</div><div class="line">                last_improvement = total_iterations</div><div class="line">                saver.save(sess=session, save_path=save_path)</div><div class="line">                improved_str = "*"</div><div class="line">            else:</div><div class="line">                improved_str = ""</div><div class="line">            msg = "Iter: &#123;0:&gt;6&#125;, Train_batch accuracy:&#123;1:&gt;6.1%&#125;, validation acc:&#123;2:&gt;6.1%&#125; &#123;3&#125;"</div><div class="line">            print(msg.format(i+1, acc_train, acc_validation, improved_str))</div><div class="line">        if total_iterations-last_improvement &gt; require_improvement_iterations:</div><div class="line">            print('No improvement found in a while, stop running')</div><div class="line">            break</div><div class="line">    end_time = time.time()</div><div class="line">    time_diff = end_time-start_time</div><div class="line">    print("Time usage:" + str(timedelta(seconds=int(round(time_diff)))))</div></pre></td></tr></table></figure><ul><li>调用<code>optimize(10000)</code>输出信息</li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line">Iter:   5100, Train_batch accuracy:100.0%, validation acc: 98.8% *</div><div class="line">Iter:   5200, Train_batch accuracy:100.0%, validation acc: 98.3% </div><div class="line">Iter:   5300, Train_batch accuracy:100.0%, validation acc: 98.7% </div><div class="line">Iter:   5400, Train_batch accuracy: 98.4%, validation acc: 98.6% </div><div class="line">Iter:   5500, Train_batch accuracy: 98.4%, validation acc: 98.6% </div><div class="line">Iter:   5600, Train_batch accuracy:100.0%, validation acc: 98.7% </div><div class="line">Iter:   5700, Train_batch accuracy: 96.9%, validation acc: 98.9% *</div><div class="line">Iter:   5800, Train_batch accuracy:100.0%, validation acc: 98.6% </div><div class="line">Iter:   5900, Train_batch accuracy:100.0%, validation acc: 98.6% </div><div class="line">Iter:   6000, Train_batch accuracy: 98.4%, validation acc: 98.7% </div><div class="line">Iter:   6100, Train_batch accuracy:100.0%, validation acc: 98.7% </div><div class="line">Iter:   6200, Train_batch accuracy:100.0%, validation acc: 98.7% </div><div class="line">Iter:   6300, Train_batch accuracy: 98.4%, validation acc: 98.8% </div><div class="line">Iter:   6400, Train_batch accuracy: 98.4%, validation acc: 98.8% </div><div class="line">Iter:   6500, Train_batch accuracy:100.0%, validation acc: 98.7% </div><div class="line">Iter:   6600, Train_batch accuracy:100.0%, validation acc: 98.7% </div><div class="line">Iter:   6700, Train_batch accuracy:100.0%, validation acc: 98.8% </div><div class="line">No improvement found in a while, stop running</div><div class="line">Time usage:0:18:43</div></pre></td></tr></table></figure><p>可以看到最后10次输出（每100次输出一次）在验证集上准确度都没有提高，停止执行</p><h3 id="3、-小批量预测并计算准确率"><a href="#3、-小批量预测并计算准确率" class="headerlink" title="3、 小批量预测并计算准确率"></a>3、 小批量预测并计算准确率</h3><ul><li><p>因为需要预测<strong>测试集和验证集</strong>，这里参数指定需要的images</p><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">'''define a function to predict using batch'''</div><div class="line">batch_size_predict = 256</div><div class="line">def predict_cls(images, labels, cls_true):</div><div class="line">    num_images = len(images)</div><div class="line">    cls_pred = np.zeros(shape=num_images, dtype=np.int)</div><div class="line">    i = 0</div><div class="line">    while i &lt; num_images:</div><div class="line">        j = min(i+batch_size_predict, num_images)</div><div class="line">        feed_dict = &#123;X: images[i:j,:],</div><div class="line">                     y_true: labels[i:j,:]&#125;</div><div class="line">        cls_pred[i:j] = session.run(y_pred_cls, feed_dict=feed_dict)</div><div class="line">        i = j</div><div class="line">    correct = (cls_true==cls_pred)</div><div class="line">    return correct, cls_pred</div></pre></td></tr></table></figure></li><li><p>测试集和验证集直接调用即可</p></li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">def predict_cls_test():</div><div class="line">    return predict_cls(data.test.images, data.test.labels, data.test.cls)</div><div class="line"></div><div class="line">def predict_cls_validation():</div><div class="line">    return predict_cls(data.validation.images, data.validation.labels, data.validation.cls)</div></pre></td></tr></table></figure><ul><li>计算验证集准确率（上面optimize函数中需要用到）</li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">'''calculate the acc'''</div><div class="line">def cls_accuracy(correct):</div><div class="line">    correct_sum = correct.sum()</div><div class="line">    acc = float(correct_sum)/len(correct)</div><div class="line">    return acc, correct_sum</div><div class="line">'''define a function to calculate the validation acc'''</div><div class="line">def validation_accuracy():</div><div class="line">    correct, _ = predict_cls_validation()</div><div class="line">    return cls_accuracy(correct)</div></pre></td></tr></table></figure><ul><li>计算测试集准确率，并且输出错误的预测和confusion matrix</li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line">'''define a function to calculate test acc'''</div><div class="line">def print_test_accuracy(show_example_errors=False,</div><div class="line">                        show_confusion_matrix=False):</div><div class="line">    correct, cls_pred = predict_cls_test()</div><div class="line">    acc, num_correct = cls_accuracy(correct)</div><div class="line">    num_images = len(correct)</div><div class="line">    msg = "Accuracy on Test-Set: &#123;0:.1%&#125; (&#123;1&#125; / &#123;2&#125;)"</div><div class="line">    print(msg.format(acc, num_correct, num_images))</div><div class="line"></div><div class="line">    # Plot some examples of mis-classifications, if desired.</div><div class="line">    if show_example_errors:</div><div class="line">        print("Example errors:")</div><div class="line">        plot_example_errors(cls_pred=cls_pred, correct=correct)</div><div class="line"></div><div class="line">    # Plot the confusion matrix, if desired.</div><div class="line">    if show_confusion_matrix:</div><div class="line">        print("Confusion Matrix:")</div><div class="line">        plot_confusion_matrix(cls_pred=cls_pred)</div></pre></td></tr></table></figure><h2 id="十二：模型融合"><a href="#十二：模型融合" class="headerlink" title="十二：模型融合"></a>十二：模型融合</h2><ul><li><a href="https://github.com/lawlite19/MachineLearning_TensorFlow/blob/master/Ensemble_Learning/ensemble_learning.py" target="_blank" rel="external">全部代码</a></li><li>使用<code>MNIST</code>数据集</li><li>一些方法和之前的一致，不在给出</li><li>其中训练了多个CNN 模型，然后取预测的平均值作为最后的预测结果<h3 id="1、将测试集和验证集合并后，并重新划分"><a href="#1、将测试集和验证集合并后，并重新划分" class="headerlink" title="1、将测试集和验证集合并后，并重新划分"></a>1、将测试集和验证集合并后，并重新划分</h3></li><li>主要是希望训练时数据集有些变换，否则都是一样的数据去训练了，最后再融合意义不大<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line">'''将training set和validation set合并，并重新划分'''</div><div class="line">combine_images = np.concatenate([data.train.images, data.validation.images], axis=0)</div><div class="line">combine_labels = np.concatenate([data.train.labels, data.validation.labels], axis=0)</div><div class="line">print("合并后图片：", combine_images.shape)</div><div class="line">print("合并后label：", combine_labels.shape)</div><div class="line">combined_size = combine_labels.shape[0]</div><div class="line">train_size = int(0.8*combined_size)</div><div class="line">validation_size = combined_size - train_size</div><div class="line">'''函数：将合并后的重新随机划分'''</div><div class="line">def random_training_set():</div><div class="line">    idx = np.random.permutation(combined_size)   # 将0-combined_size数字随机排列</div><div class="line">    idx_train = idx[0:train_size]</div><div class="line">    idx_validation = idx[train_size:]</div><div class="line">    x_train = combine_images[idx_train, :]</div><div class="line">    y_train = combine_labels[idx_train, :]</div><div class="line">    </div><div class="line">    x_validation = combine_images[idx_validation, :]</div><div class="line">    y_validation = combine_images[idx_validation, :]</div><div class="line">    return x_train, y_train, x_validation, y_validation</div></pre></td></tr></table></figure></li></ul><h3 id="2、融合模型"><a href="#2、融合模型" class="headerlink" title="2、融合模型"></a>2、融合模型</h3><ul><li><p>加载训练好的模型，并输出每个模型在测试集的预测结果等</p><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">def ensemble_predictions():</div><div class="line">    pred_labels = []</div><div class="line">    test_accuracies = []</div><div class="line">    validation_accuracies = []</div><div class="line">    for i in range(num_networks):</div><div class="line">        saver.restore(sess=session, save_path=get_save_path(i))</div><div class="line">        test_acc = test_accuracy()</div><div class="line">        test_accuracies.append(test_acc)</div><div class="line">        validation_acc = validation_accuracy()</div><div class="line">        validation_accuracies.append(validation_acc)</div><div class="line">        msg = "网络：&#123;0&#125;，验证集：&#123;1:.4f&#125;，测试集&#123;2:.4f&#125;"</div><div class="line">        print(msg.format(i, validation_acc, test_acc))</div><div class="line">        pred = predict_labels(data.test.images)</div><div class="line">        pred_labels.append(pred)</div><div class="line">    return np.array(pred_labels),\</div><div class="line">           np.array(test_accuracies),\</div><div class="line">           np.array(validation_accuracies)</div></pre></td></tr></table></figure></li><li><p>调用<code>pred_labels, test_accuracies, val_accuracies = ensemble_predictions()</code></p></li><li>取均值：<code>ensemble_pred_labels = np.mean(pred_labels, axis=0)</code></li><li>融合后的真实结果：<code>ensemble_cls_pred = np.argmax(ensemble_pred_labels, axis=1)</code></li><li>其他一些信息：</li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">ensemble_correct = (ensemble_cls_pred == data.test.cls)</div><div class="line">ensemble_incorrect = np.logical_not(ensemble_correct)</div><div class="line">print(test_accuracies)</div><div class="line">best_net = np.argmax(test_accuracies)</div><div class="line">print(best_net)</div><div class="line">print(test_accuracies[best_net])</div><div class="line">best_net_pred_labels = pred_labels[best_net, :, :]</div><div class="line">best_net_cls_pred = np.argmax(best_net_pred_labels, axis=1)</div><div class="line">best_net_correct = (best_net_cls_pred == data.test.cls)</div><div class="line">best_net_incorrect = np.logical_not(best_net_correct)</div><div class="line">print("融合后预测对的：", np.sum(ensemble_correct))</div><div class="line">print("单个最好模型预测对的", np.sum(best_net_correct))</div><div class="line">ensemble_better = np.logical_and(best_net_incorrect, ensemble_correct)  # 融合之后好于单个的个数</div><div class="line">print(ensemble_better.sum())</div><div class="line">best_net_better = np.logical_and(best_net_correct, ensemble_incorrect)  # 单个好于融合之后的个数</div><div class="line">print(best_net_better.sum())</div></pre></td></tr></table></figure><h2 id="十二：Cifar-10数据集，使用variable-scope重复使用变量"><a href="#十二：Cifar-10数据集，使用variable-scope重复使用变量" class="headerlink" title="十二：Cifar-10数据集，使用variable_scope重复使用变量"></a>十二：Cifar-10数据集，使用<code>variable_scope</code>重复使用变量</h2><ul><li><a href="https://github.com/lawlite19/MachineLearning_TensorFlow/blob/master/Ensemble_Learning/CNN_for_CIFAR-10" target="_blank" rel="external">全部代码</a></li><li>使用<code>CIFAR-10</code>数据集</li><li>创建了<strong>两个网络</strong>，一个用于训练，一个用于测试，测试使用的是训练好的权重参数，所以用到<strong>参数重用</strong></li><li>网络结构</li></ul><p><img src="https://raw.githubusercontent.com/lawlite19/MachineLearning_TensorFlow/master/images/06_network_flowchart.png" alt="cifar-10结构" title="06_network_flowchart"></p><h3 id="1、数据集"><a href="#1、数据集" class="headerlink" title="1、数据集"></a>1、数据集</h3><ul><li><p>导入包：</p><ul><li>这是别人实现好的下载和处理<code>cifar-10</code>数据集的diamante<figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">import cifar10</div><div class="line">from cifar10 import img_size, num_channels, num_classes</div></pre></td></tr></table></figure></li></ul></li><li><p>输出一些数据集信息</p></li></ul><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="string">''</span><span class="string">'下载cifar10数据集, 大概163M'</span><span class="string">''</span></div><div class="line">cifar10.maybe_download_and_extract()</div><div class="line"><span class="string">''</span><span class="string">'加载数据集'</span><span class="string">''</span></div><div class="line">images_train, cls_train, labels_train = cifar10.load_training_data()</div><div class="line">images_test,  cls_test,  labels_test  = cifar10.load_test_data()</div><div class="line"></div><div class="line"><span class="string">''</span><span class="string">'打印一些信息'</span><span class="string">''</span></div><div class="line">class_names = cifar10.load_class_names()</div><div class="line"><span class="function"><span class="title">print</span><span class="params">(class_names)</span></span></div><div class="line"><span class="function"><span class="title">print</span><span class="params">(<span class="string">"Size of:"</span>)</span></span></div><div class="line"><span class="function"><span class="title">print</span><span class="params">(<span class="string">"training set:\t\t&#123;&#125;"</span>.format(len(images_train)</span></span>))</div><div class="line"><span class="function"><span class="title">print</span><span class="params">(<span class="string">"test set:\t\t\t&#123;&#125;"</span>.format(len(images_test)</span></span>))</div></pre></td></tr></table></figure><ul><li>显示9张图片函数<ul><li>相比之前的，加入了<code>smooth</code></li></ul></li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line">'''显示9张图片函数'''</div><div class="line">def plot_images(images, cls_true, cls_pred=None, smooth=True):   # smooth是否平滑显示</div><div class="line">    assert len(images) == len(cls_true) == 9</div><div class="line">    fig, axes = plt.subplots(3,3)</div><div class="line">    </div><div class="line">    for i, ax in enumerate(axes.flat):</div><div class="line">        if smooth:</div><div class="line">            interpolation = 'spline16'</div><div class="line">        else:</div><div class="line">            interpolation = 'nearest'</div><div class="line">        ax.imshow(images[i, :, :, :], interpolation=interpolation)</div><div class="line">        cls_true_name = class_names[cls_true[i]]</div><div class="line">        if cls_pred is None:</div><div class="line">            xlabel = "True:&#123;0&#125;".format(cls_true_name)</div><div class="line">        else:</div><div class="line">            cls_pred_name = class_names[cls_pred[i]]</div><div class="line">            xlabel = "True:&#123;0&#125;, Pred:&#123;1&#125;".format(cls_true_name, cls_pred_name)</div><div class="line">        ax.set_xlabel(xlabel)</div><div class="line">        ax.set_xticks([])</div><div class="line">        ax.set_yticks([])</div><div class="line">    plt.show()</div></pre></td></tr></table></figure><h3 id="2、定义placeholder"><a href="#2、定义placeholder" class="headerlink" title="2、定义placeholder"></a>2、定义<code>placeholder</code></h3><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">X = tf.placeholder(tf<span class="selector-class">.float32</span>, shape=[None, img_size, img_size, num_channels], name=<span class="string">"X"</span>)</div><div class="line">y_true = tf.placeholder(tf<span class="selector-class">.float32</span>, shape=[None, num_classes], name=<span class="string">"y"</span>)</div><div class="line">y_true_cls = tf.argmax(y_true, axis=<span class="number">1</span>)</div></pre></td></tr></table></figure><h3 id="3、图片处理"><a href="#3、图片处理" class="headerlink" title="3、图片处理"></a>3、图片处理</h3><ul><li><p>单张图片处理</p><ul><li>原图是<code>32*32</code>像素的，裁剪成<code>24*24</code>像素的</li><li>如果是训练集进行一些裁剪，翻转，饱和度等处理</li><li>如果是测试集，只进行简单的裁剪处理</li><li>这也是为什么使用<code>variable_scope</code>定义两个网络<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">'''单个图片预处理, 测试集只需要裁剪就行了'''</div><div class="line">def pre_process_image(image, training):</div><div class="line">    if training:</div><div class="line">        image = tf.random_crop(image, size=[img_size_cropped, img_size_cropped, num_channels])  # 裁剪</div><div class="line">        image = tf.image.random_flip_left_right(image)                  # 左右翻转</div><div class="line">        image = tf.image.random_hue(image, max_delta=0.05)              # 色调调整</div><div class="line">        image = tf.image.random_brightness(image, max_delta=0.2)        # 曝光</div><div class="line">        image = tf.image.random_saturation(image, lower=0.0, upper=2.0) # 饱和度</div><div class="line">        '''上面的调整可能pixel值超过[0, 1], 所以约束一下'''        </div><div class="line">        image = tf.minimum(image, 1.0)</div><div class="line">        image = tf.maximum(image, 0.0)</div><div class="line">    else:</div><div class="line">        image = tf.image.resize_image_with_crop_or_pad(image, target_height=img_size_cropped, </div><div class="line">                                              target_width=img_size_cropped)</div><div class="line">    return image</div></pre></td></tr></table></figure></li></ul></li><li><p>多张图片处理</p><ul><li>因为训练和测试是都是使用<code>batch</code>的方式</li><li>调用上面处理单张图片的函数</li><li>tf.map_fn(fn, elems)函数，前面一般是<code>lambda</code>函数，后面是所有的数据<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">'''调用上面的函数，处理多个图片images'''</div><div class="line">def pre_process(images, training):</div><div class="line">    images = tf.map_fn(lambda image: pre_process_image(image, training), images)   # tf.map_fn()使用lambda函数</div><div class="line">    return images</div></pre></td></tr></table></figure></li></ul></li></ul><h3 id="4、定义tensorflow计算图"><a href="#4、定义tensorflow计算图" class="headerlink" title="4、定义tensorflow计算图"></a>4、定义tensorflow计算图</h3><ul><li>定义主网络图<ul><li>使用<code>prettytensor</code></li><li>分为<code>training</code>和<code>test</code>两个阶段</li></ul></li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line">'''定义主网络函数'''</div><div class="line">def main_network(images, training):</div><div class="line">    x_pretty = pt.wrap(images)</div><div class="line">    if training:</div><div class="line">        phase = pt.Phase.train</div><div class="line">    else:</div><div class="line">        phase = pt.Phase.infer</div><div class="line">    with pt.defaults_scope(activation_fn=tf.nn.relu, phase=phase):</div><div class="line">        y_pred, loss = x_pretty.\</div><div class="line">        conv2d(kernel=5, depth=64, name="layer_conv1", batch_normalize=True).\</div><div class="line">        max_pool(kernel=2, stride=2).\</div><div class="line">        conv2d(kernel=5, depth=64, name="layer_conv2").\</div><div class="line">        max_pool(kernel=2, stride=2).\</div><div class="line">        flatten().\</div><div class="line">        fully_connected(size=256, name="layer_fc1").\</div><div class="line">        fully_connected(size=128, name="layer_fc2").\</div><div class="line">        softmax_classifier(num_classes, labels=y_true)</div><div class="line">    return y_pred, loss</div></pre></td></tr></table></figure><ul><li><p>创建所有网络，包含<strong>预处理图片和主网络</strong></p><ul><li>需要使用<strong>variable_scope</strong>, 测试阶段需要<code>reuse</code>训练阶段的参数<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">'''创建所有网络, 包含预处理和主网络，'''</div><div class="line">def create_network(training):</div><div class="line">    # 使用variable_scope可以重复使用定义的变量，训练时创建新的，测试时重复使用</div><div class="line">    with tf.variable_scope("network", reuse=not training):</div><div class="line">        images = X</div><div class="line">        images = pre_process(images=images, training=training)</div><div class="line">        y_pred, loss = main_network(images=images, training=training)</div><div class="line">    return y_pred, loss</div></pre></td></tr></table></figure></li></ul></li><li><p>创建训练阶段网络</p><ul><li>定义一个<code>global_step</code>记录训练的次数，下面会将其保存到<code>checkpoint</code>,<code>trainable</code>为<code>False</code>就不会训练改变<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">'''训练阶段网络创建'''</div><div class="line">global_step = tf.Variable(initial_value=0, </div><div class="line">                          name="global_step",</div><div class="line">                          trainable=False) # trainable 在训练阶段不会改变</div><div class="line">_, loss = create_network(training=True)</div><div class="line">optimizer = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(loss, global_step)</div></pre></td></tr></table></figure></li></ul></li><li><p>定义测试阶段网络</p><ul><li>同时定义准确率</li></ul></li></ul><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="string">''</span><span class="string">'测试阶段网络创建'</span><span class="string">''</span></div><div class="line">y_pred, _ = create_network(training=False)</div><div class="line">y_pred_cls = tf.argmax(y_pred, dimension=<span class="number">1</span>)</div><div class="line">correct_prediction = tf.equal(y_pred_cls, y_true_cls)</div><div class="line">accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</div></pre></td></tr></table></figure><h3 id="5、获取权重和每层的输出值信息"><a href="#5、获取权重和每层的输出值信息" class="headerlink" title="5、获取权重和每层的输出值信息"></a>5、获取权重和每层的输出值信息</h3><ul><li>获取权重变量</li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">def get_weights_variable(layer_name):</div><div class="line">    with tf.variable_scope("network/" + layer_name, reuse=True):</div><div class="line">        variable = tf.get_variable("weights")</div><div class="line">    return variable </div><div class="line">weights_conv1 = get_weights_variable("layer_conv1")</div><div class="line">weights_conv2 = get_weights_variable("layer_conv2")</div></pre></td></tr></table></figure><ul><li>获取每层的输出变量</li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">def get_layer_output(layer_name):</div><div class="line">    tensor_name = "network/" + layer_name + "/Relu:0"</div><div class="line">    tensor = tf.get_default_graph().get_tensor_by_name(tensor_name)</div><div class="line">    return tensor</div><div class="line">output_conv1 = get_layer_output("layer_conv1")</div><div class="line">output_conv2 = get_layer_output("layer_conv2")</div></pre></td></tr></table></figure><h3 id="6、保存和加载计算图参数"><a href="#6、保存和加载计算图参数" class="headerlink" title="6、保存和加载计算图参数"></a>6、保存和加载计算图参数</h3><ul><li>因为第一次不会加载，所以放到<code>try</code>中判断</li></ul><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="string">''</span><span class="string">'执行tensorflow graph'</span><span class="string">''</span></div><div class="line">session = tf.Session()</div><div class="line">save_dir = <span class="string">"checkpoints/"</span></div><div class="line"><span class="keyword">if</span> not os<span class="selector-class">.path</span><span class="selector-class">.exists</span>(save_dir):</div><div class="line">    os.makedirs(save_dir)</div><div class="line">save_path = os<span class="selector-class">.path</span><span class="selector-class">.join</span>(save_dir, <span class="string">'cifat10_cnn'</span>)</div><div class="line"></div><div class="line"><span class="string">''</span><span class="string">'尝试存储最新的checkpoint, 可能会失败，比如第一次运行checkpoint不存在等'</span><span class="string">''</span></div><div class="line">try:</div><div class="line">    print(<span class="string">"开始存储最新的存储..."</span>)</div><div class="line">    last_chk_path = tf<span class="selector-class">.train</span><span class="selector-class">.latest_checkpoint</span>(save_dir)</div><div class="line">    saver.restore(session, save_path=last_chk_path)</div><div class="line">    print(<span class="string">"存储点来自："</span>, last_chk_path)</div><div class="line">except:</div><div class="line">    print(<span class="string">"存储错误, 初始化变量"</span>)</div><div class="line">    session.run(tf.global_variables_initializer())</div></pre></td></tr></table></figure><h3 id="7、训练"><a href="#7、训练" class="headerlink" title="7、训练"></a>7、训练</h3><ul><li>获取<code>batch</code></li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">'''SGD'''</div><div class="line">train_batch_size = 64</div><div class="line">def random_batch():</div><div class="line">    num_images = len(images_train)</div><div class="line">    idx = np.random.choice(num_images, size=train_batch_size, replace=False)</div><div class="line">    x_batch = images_train[idx, :, :, :]</div><div class="line">    y_batch = labels_train[idx, :]</div><div class="line">    return x_batch, y_batch</div></pre></td></tr></table></figure><ul><li>训练网络<ul><li>每1000次保存一下<code>checkpoint</code></li><li>因为上面会<code>restored</code>已经保存训练的网络，同时也保存了训练的次数，所以可以接着训练<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">def optimize(num_iterations):</div><div class="line">    start_time = time.time()</div><div class="line">    for i in range(num_iterations):</div><div class="line">        x_batch, y_batch = random_batch()</div><div class="line">        feed_dict_train = &#123;X: x_batch, y_true: y_batch&#125;</div><div class="line">        i_global, _ = session.run([global_step, optimizer], feed_dict=feed_dict_train)</div><div class="line">        if (i_global%100==0) or (i == num_iterations-1):</div><div class="line">            batch_acc = session.run(accuracy, feed_dict=feed_dict_train)</div><div class="line">            msg = "global step: &#123;0:&gt;6&#125;, training batch accuracy: &#123;1:&gt;6.1%&#125;"</div><div class="line">            print(msg.format(i_global, batch_acc))</div><div class="line">        if(i_global%1000==0) or (i==num_iterations-1):</div><div class="line">            saver.save(session, save_path=save_path,</div><div class="line">                       global_step=global_step)</div><div class="line">            print("保存checkpoint")</div><div class="line">    end_time = time.time()</div><div class="line">    time_diff = end_time-start_time</div><div class="line">    print("耗时：", str(timedelta(seconds=int(round(time_diff)))))</div></pre></td></tr></table></figure></li></ul></li></ul><h2 id="十三、Inception-model-GoogleNet"><a href="#十三、Inception-model-GoogleNet" class="headerlink" title="十三、Inception model (GoogleNet)"></a>十三、Inception model (GoogleNet)</h2><ul><li><a href="https://github.com/lawlite19/MachineLearning_TensorFlow/blob/master/Inception_model/InceptionModel_pretrained.py" target="_blank" rel="external">全部代码</a></li><li>使用训练好的<code>inception model</code>,因为模型很复杂，一般的电脑运行不起来的。</li><li>网络结构</li></ul><p><img src="https://raw.githubusercontent.com/lawlite19/MachineLearning_TensorFlow/master/images/07_inception_flowchart.png" alt="inception model(Google Net)" title="07_inception_flowchart"></p><h3 id="1、下载和加载inception-model"><a href="#1、下载和加载inception-model" class="headerlink" title="1、下载和加载inception model"></a>1、下载和加载inception model</h3><ul><li>因为是预训练好的模型，所以无需我们定义结构了</li><li><p>导入包</p><ul><li>这里 <code>inception</code>是别人实现好的下载的代码<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">import numpy as np</div><div class="line">import tensorflow as tf</div><div class="line">from matplotlib import pyplot as plt</div><div class="line">import inception # 第三方类加载inception model</div><div class="line">import os</div></pre></td></tr></table></figure></li></ul></li><li><p>下载和加载模型</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="string">''</span><span class="string">'下载和加载inception model'</span><span class="string">''</span></div><div class="line">inception.maybe_download()</div><div class="line">model = inception.Inception()</div></pre></td></tr></table></figure></li><li><p>预测和显示图片函数</p></li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">'''预测和显示图片'''</div><div class="line">def classify(image_path):</div><div class="line">    plt.imshow(plt.imread(image_path))</div><div class="line">    plt.show()</div><div class="line">    pred = model.classify(image_path=image_path)</div><div class="line">    model.print_scores(pred=pred, k=10, only_first_name=True)</div></pre></td></tr></table></figure><ul><li>显示调整后的图片<ul><li>因为 <code>inception model</code>要求输入图片为 <code>299*299</code> 像素的，所以它会<code>resize</code>成这个大小然后作为输入</li></ul></li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">'''显示处理后图片的样式'''</div><div class="line">def plot_resized_image(image_path):</div><div class="line">    resized_image = model.get_resized_image(image_path)</div><div class="line">    plt.imshow(resized_image, interpolation='nearest')</div><div class="line">    plt.show()</div><div class="line">plot_resized_image(image_path)</div></pre></td></tr></table></figure><h2 id="十四、迁移学习-Transfer-Learning"><a href="#十四、迁移学习-Transfer-Learning" class="headerlink" title="十四、迁移学习 Transfer Learning"></a>十四、迁移学习 Transfer Learning</h2><ul><li><a href="30">全部代码</a></li><li>网络结构还是使用上一节的<code>inception model</code>, 去掉最后的全连接层，然后重新构建全连接层进行训练<ul><li>因为<code>inception model</code> 是训练好的，前面的卷积层用于捕捉<strong>特征</strong>, 而后面的全连接层可用于<strong>分类</strong>，所以我们<strong>训练全连接层</strong>即可</li></ul></li><li>因为要计算每张图片的<code>transfer values</code>,所以使用一个<code>cache</code>缓存<code>transfer-values</code>，第一次计算完成后，后面重新运行直接读取存储的结果，这样比较节省时间<ul><li><code>transfer values</code>是<code>inception model</code>在<code>Softmax</code>层前一层的值</li><li><code>cifar-10</code>数据集, 我放在实验室电脑上运行了<strong>几个小时</strong>才得到<code>transfer values</code>，还是比较慢的</li></ul></li><li>总之最后相当于训练<strong>下面</strong>的神经网络，对应的 <code>transfer-values</code>作为输入<br><img src="https://raw.githubusercontent.com/lawlite19/MachineLearning_TensorFlow/master/images/08_transfer_learning_flowchart.png" alt="transfer learning-inception model" title="08_transfer_learning_flowchart"></li></ul><h3 id="1、准备工作"><a href="#1、准备工作" class="headerlink" title="1、准备工作"></a>1、准备工作</h3><ul><li><p>导入包</p><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">import numpy as np</div><div class="line">import tensorflow as tf</div><div class="line">import prettytensor as pt</div><div class="line">from matplotlib import pyplot as plt</div><div class="line">import time</div><div class="line">from datetime import timedelta</div><div class="line">import os</div><div class="line">import inception   # 第三方下载inception model的代码</div><div class="line">from inception import transfer_values_cache  # cache</div><div class="line">import cifar10     # 也是第三方的库，下载cifar-10数据集</div><div class="line">from cifar10 import num_classes</div></pre></td></tr></table></figure></li><li><p>下载<code>cifar-10</code>数据集</p></li></ul><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="string">''</span><span class="string">'下载cifar-10数据集'</span><span class="string">''</span></div><div class="line">cifar10.maybe_download_and_extract()</div><div class="line">class_names = cifar10.load_class_names()</div><div class="line"><span class="function"><span class="title">print</span><span class="params">(<span class="string">"所有类别是："</span>,class_names)</span></span></div><div class="line"><span class="string">''</span><span class="string">'训练和测试集'</span><span class="string">''</span></div><div class="line">images_train, cls_train, labels_train = cifar10.load_training_data()</div><div class="line">images_test,  cls_test,  labels_test  = cifar10.load_test_data()</div></pre></td></tr></table></figure><ul><li>下载和加载<code>inception model</code></li></ul><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="string">''</span><span class="string">'下载inception model'</span><span class="string">''</span></div><div class="line">inception.maybe_download()</div><div class="line">model = inception.Inception()</div></pre></td></tr></table></figure><ul><li><p>计算<code>cifar-10</code>训练集和测试集在<code>inception model</code>上的<code>transfer values</code></p><ul><li>因为计算非常耗时，这里第一次运行存储到本地，以后再运行直接读取即可</li><li><code>transfer values</code>的<code>shape</code>是<code>(dataset size, 2048)</code>，因为是<code>softmax</code>层的前一层<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">'''训练和测试的cache的路径'''</div><div class="line">file_path_cache_train = os.path.join(cifar10.data_path, 'inception_cifar10_train.pkl')</div><div class="line">file_path_cache_test = os.path.join(cifar10.data_path, 'inception_cifar10_test.pkl')</div><div class="line"></div><div class="line">print('处理训练集上的transfer-values.......... ')</div><div class="line">image_scaled = images_train * 255.0  # cifar-10的pixel是0-1的, shape=(50000, 32, 32, 3)</div><div class="line">transfer_values_train = transfer_values_cache(cache_path=file_path_cache_train,</div><div class="line">                                              images=image_scaled, </div><div class="line">                                              model=model)  # shape=(50000, 2048)</div><div class="line">print('处理测试集上的transfer-values.......... ')</div><div class="line">images_scaled = images_test * 255.0</div><div class="line">transfer_values_test = transfer_values_cache(cache_path=file_path_cache_test,</div><div class="line">                                             model=model,</div><div class="line">                                             images=images_scaled)</div><div class="line">print("transfer_values_train: ",transfer_values_train.shape)</div><div class="line">print("transfer_values_test: ",transfer_values_test.shape)</div></pre></td></tr></table></figure></li></ul></li><li><p>可视化一张图片对应的<code>transfer values</code></p></li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">'''显示transfer values'''</div><div class="line">def plot_transfer_values(i):</div><div class="line">    print("输入图片：")</div><div class="line">    plt.imshow(images_test[i], interpolation='nearest')</div><div class="line">    plt.show()</div><div class="line">    print('transfer values --&gt; 此图片在inception model上')</div><div class="line">    img = transfer_values_test[i]</div><div class="line">    img = img.reshape((32, 64))</div><div class="line">    plt.imshow(img, interpolation='nearest', cmap='Reds')</div><div class="line">    plt.show()</div><div class="line">plot_transfer_values(16)</div></pre></td></tr></table></figure><h3 id="2、分析transfer-values"><a href="#2、分析transfer-values" class="headerlink" title="2、分析transfer values"></a>2、分析<code>transfer values</code></h3><h4 id="1-使用PCA主成分分析"><a href="#1-使用PCA主成分分析" class="headerlink" title="(1) 使用PCA主成分分析"></a>(1) 使用PCA主成分分析</h4><ul><li>将数据<strong>降到2维</strong>，可视化，因为<code>transfer values</code>是已经捕捉到的<strong>特征</strong>，所以可视化应该是可以隐约看到<strong>不同类别的数据是有区别的</strong></li><li>取<code>3000</code>个数据观察（因为<code>PCA</code>也是比较耗时的）</li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">'''使用PCA分析transfer values'''</div><div class="line">from sklearn.decomposition import PCA</div><div class="line">pca = PCA(n_components=2)</div><div class="line">transfer_values = transfer_values_train[0:3000]  # 取3000个，大的话计算量太大</div><div class="line">cls = cls_train[0:3000]</div><div class="line">print(transfer_values.shape)</div><div class="line">transfer_values_reduced = pca.fit_transform(transfer_values)</div><div class="line">print(transfer_values_reduced.shape)</div></pre></td></tr></table></figure><ul><li>可视化降维后的数据</li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">## 显示降维后的transfer values</div><div class="line">def plot_scatter(values, cls):</div><div class="line">    from matplotlib import cm as cm</div><div class="line">    cmap = cm.rainbow(np.linspace(0.0, 1.0, num_classes))</div><div class="line">    colors = cmap[cls]</div><div class="line">    x = values[:, 0]</div><div class="line">    y = values[:, 1]</div><div class="line">    plt.scatter(x, y, color=colors)</div><div class="line">    plt.show()</div><div class="line">plot_scatter(transfer_values_reduced, cls)</div></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/lawlite19/MachineLearning_TensorFlow/master/images/08_transfer_learning_pca_visualize.png" alt="pca 降维后可视化transfer values" title="08_transfer_learning_pca_visualize"></p><h4 id="2-使用TSNE主成分分析"><a href="#2-使用TSNE主成分分析" class="headerlink" title="(2) 使用TSNE主成分分析"></a>(2) 使用TSNE主成分分析</h4><ul><li>因为<code>t-SNE</code>运行非常慢，所以这里先用<code>PCA</code>将到<strong>50维</strong></li></ul><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">from sklearn<span class="selector-class">.manifold</span> import TSNE</div><div class="line">pca = PCA(n_components=<span class="number">50</span>)</div><div class="line">transfer_values_50d = pca.fit_transform(transfer_values)</div><div class="line">tsne = TSNE(n_components=<span class="number">2</span>)</div><div class="line">transfer_values_reduced = tsne.fit_transform(transfer_values_50d)</div><div class="line"><span class="function"><span class="title">print</span><span class="params">(<span class="string">"最终降维后："</span>, transfer_values_reduced.shape)</span></span></div><div class="line"><span class="function"><span class="title">plot_scatter</span><span class="params">(transfer_values_reduced, cls)</span></span></div></pre></td></tr></table></figure><ul><li>数据区分还是比较明显的<br><img src="https://raw.githubusercontent.com/lawlite19/MachineLearning_TensorFlow/master/images/08_transfer_learning_pca_visualize_02.png" alt="t-SNE降维后可视化transfer values" title="08_transfer_learning_pca_visualize_02"></li></ul><h3 id="3、创建我们自己的网络"><a href="#3、创建我们自己的网络" class="headerlink" title="3、创建我们自己的网络"></a>3、创建我们自己的网络</h3><ul><li>使用<code>prettytensor</code>创建一个全连接层，使用<code>softmax</code>作为分类</li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">'''创建网络'''</div><div class="line">transfer_len = model.transfer_len   # 获取transfer values的大小，这里是2048</div><div class="line">x = tf.placeholder(tf.float32, shape=[None, transfer_len], name="x")</div><div class="line">y_true = tf.placeholder(tf.float32, shape=[None, num_classes], name="y")</div><div class="line">y_true_cls = tf.argmax(y_true, axis=1)</div><div class="line">x_pretty = pt.wrap(x)</div><div class="line">with pt.defaults_scope(activation_fn=tf.nn.relu):</div><div class="line">    y_pred, loss = x_pretty.\</div><div class="line">        fully_connected(1024, name="layer_fc1").\</div><div class="line">        softmax_classifier(num_classes, labels=y_true)</div></pre></td></tr></table></figure><ul><li>优化器</li></ul><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="string">''</span><span class="string">'优化器'</span><span class="string">''</span></div><div class="line">global_step = tf.Variable(initial_value=<span class="number">0</span>, name=<span class="string">"global_step"</span>, trainable=False)</div><div class="line">optimizer = tf<span class="selector-class">.train</span><span class="selector-class">.AdamOptimizer</span>(<span class="number">0.0001</span>).minimize(loss, global_step)</div></pre></td></tr></table></figure><ul><li>准确度</li></ul><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="string">''</span><span class="string">'accuracy'</span><span class="string">''</span></div><div class="line">y_pred_cls = tf.argmax(y_pred, axis=<span class="number">1</span>)</div><div class="line">correct_prediction = tf.equal(y_pred_cls, y_true_cls)</div><div class="line">accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</div></pre></td></tr></table></figure><ul><li><code>SGD</code>训练</li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line">'''SGD 训练'''</div><div class="line">session = tf.Session()</div><div class="line">session.run(tf.initialize_all_variables())</div><div class="line">train_batch_size = 64</div><div class="line">def random_batch():</div><div class="line">    num_images = len(images_train)</div><div class="line">    idx = np.random.choice(num_images, </div><div class="line">                           size=train_batch_size,</div><div class="line">                           replace=False)</div><div class="line">    x_batch = transfer_values_train[idx]</div><div class="line">    y_batch = labels_train[idx]</div><div class="line">    return x_batch, y_batch</div><div class="line">def optimize(num_iterations):</div><div class="line">    start_time = time.time()</div><div class="line">    for i in range(num_iterations):</div><div class="line">        x_batch, y_true_batch = random_batch()</div><div class="line">        feed_dict_train = &#123;x: x_batch,</div><div class="line">                           y_true: y_true_batch&#125;</div><div class="line">        i_global, _ = session.run([global_step, optimizer], feed_dict=feed_dict_train)</div><div class="line">        if (i_global % 100 == 0) or (i==num_iterations-1):</div><div class="line">            batch_acc = session.run(accuracy, feed_dict=feed_dict_train)</div><div class="line">            msg = "Global Step: &#123;0:&gt;6&#125;, Training Batch Accuracy: &#123;1:&gt;6.1%&#125;"</div><div class="line">            print(msg.format(i_global, batch_acc))            </div><div class="line">    end_time = time.time()</div><div class="line">    time_diff = end_time - start_time</div><div class="line">    print("耗时：", str(timedelta(seconds=int(round(time_diff)))))</div></pre></td></tr></table></figure><ul><li>使用<code>batch size</code>预测测试集数据</li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">'''batch 预测'''</div><div class="line">batch_size = 256</div><div class="line">def predict_cls(transfer_values, labels, cls_true):</div><div class="line">    num_images = len(images_test)</div><div class="line">    cls_pred = np.zeros(shape=num_images, dtype=np.int)</div><div class="line">    i = 0</div><div class="line">    while i &lt; num_images:</div><div class="line">        j = min(i + batch_size, num_images)</div><div class="line">        feed_dict = &#123;x: transfer_values[i:j],</div><div class="line">                     y_true: labels[i:j]&#125;</div><div class="line">        cls_pred[i:j] = session.run(y_pred_cls, feed_dict=feed_dict)</div><div class="line">        i = j</div><div class="line">    correct = (cls_true == cls_pred)</div><div class="line">    return correct, cls_pred</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;ul&gt;
&lt;li&gt;github地址：&lt;a href=&quot;https://github.com/lawlite19/MachineLearning_TensorFlow&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://github.com/lawlite19/MachineLearning_TensorFlow&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;一、TensorFlow介绍&quot;&gt;&lt;a href=&quot;#一、TensorFlow介绍&quot; class=&quot;headerlink&quot; title=&quot;一、TensorFlow介绍&quot;&gt;&lt;/a&gt;一、TensorFlow介绍&lt;/h2&gt;
    
    </summary>
    
    
      <category term="DeepLearning" scheme="http://lawlite.me/tags/DeepLearning/"/>
    
      <category term="Tensorflow" scheme="http://lawlite.me/tags/Tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>Python科学计算</title>
    <link href="http://lawlite.me/2016/11/09/Python%E7%A7%91%E5%AD%A6%E8%AE%A1%E7%AE%97/"/>
    <id>http://lawlite.me/2016/11/09/Python科学计算/</id>
    <published>2016-11-09T14:25:43.000Z</published>
    <updated>2017-06-25T08:49:02.016Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一、Numpy"><a href="#一、Numpy" class="headerlink" title="一、Numpy"></a>一、Numpy</h2><h3 id="1、Numpy特征和导入"><a href="#1、Numpy特征和导入" class="headerlink" title="1、Numpy特征和导入"></a>1、Numpy特征和导入</h3><ul><li>（1）用于多维数组的第三方Python包</li><li>（2）更接近于底层和硬件 (高效)</li><li>（3）专注于科学计算 (方便)</li><li>（4）导入包：<code>import numpy as np</code>     </li></ul><a id="more"></a><h3 id="2、list转为数组"><a href="#2、list转为数组" class="headerlink" title="2、list转为数组"></a>2、list转为数组</h3><ul><li>（1）<code>a = np.array([0,1,2,3])</code></li><li>（2）输出为：<code>[0 1 2 3]</code></li><li>（3）数据类型：<code>&lt;type &#39;numpy.ndarray&#39;&gt;</code></li></ul><h3 id="3、一维数组"><a href="#3、一维数组" class="headerlink" title="3、一维数组"></a>3、一维数组</h3><ul><li>（1）<code>a = np.array([1,2,3,4])</code>属性<br><code>a.ndim</code>–&gt;维度为1<br><code>a.shape</code>–&gt;形状，返回<code>(4,)</code><br><code>len(a)</code>–&gt;长度，4</li><li>（2）访问数组<br><code>a[1:5:2]</code>下标1-5，下标关系+2</li><li><p>（3）逆序</p><p>  <code>a[::-1]</code></p></li></ul><h3 id="4、多维数组"><a href="#4、多维数组" class="headerlink" title="4、多维数组"></a>4、多维数组</h3><ul><li><p>（1）二维：<code>a = np.array([[0,1,2,3],[1,2,3,4]])</code><br>输出为：</p><p>  [[0 1 2 3]<br>   [1 2 3 4]]<br>a.ndm   –&gt;2<br>a.shape –&gt;(2,4)–&gt;行数，列数<br>len(a)  –&gt;2–&gt;第一维大小</p></li><li>（2）三维：<code>a = np.array([[[0],[1]],[[2],[4]]])</code><br><code>a.shape</code>–&gt;(2,2,1)</li></ul><h3 id="5、用函数创建数组"><a href="#5、用函数创建数组" class="headerlink" title="5、用函数创建数组"></a>5、用函数创建数组</h3><ul><li><p>（1）<code>np.arange()</code></p><p>  a = np.arange(0, 10)<br>b = np.arange(10)<br>c = np.arange(0,10,2)<br>输出：</p><p>  [0 1 2 3 4 5 6 7 8 9]<br>[0 1 2 3 4 5 6 7 8 9]<br>[0 2 4 6 8]</p></li><li>（2）<code>np.linspace(start, stop, num=50, endpoint=True, retstep=False, dtype=None)</code><br>等距离产生num个数</li><li>（3）<code>np.logspace(start, stop, num=50, endpoint=True, base=10.0, dtype=None)</code><br>以log函数取</li></ul><h3 id="6、常用数组"><a href="#6、常用数组" class="headerlink" title="6、常用数组"></a>6、常用数组</h3><ul><li><p>（1）<code>a = np.ones((3,3))</code><br>输出：</p><p>  [[ 1.  1.  1.]<br>[ 1.  1.  1.]<br>[ 1.  1.  1.]]</p></li><li><p>（2）<code>np.zeros((3,3))</code></p></li><li>（3）<code>np.eye(2)</code>单位矩阵</li><li>（4）<code>np.diag([1,2,3],k=0)</code>对角矩阵，k为对角线的偏移</li></ul><h3 id="7、随机数矩阵"><a href="#7、随机数矩阵" class="headerlink" title="7、随机数矩阵"></a>7、随机数矩阵</h3><ul><li>（1）<code>a = np.random.rand(4)</code><br>输出：<code>[ 0.99890402  0.41171695  0.40725671  0.42501804]</code>范围在[0,1]之间</li><li>（2）<code>a = np.random.randn(4)</code> Gaussian函数，</li><li><p>（3）生成100个0-m的随机数:  <code>[t for t in [np.random.randint(x-x, m) for x in range(100)]]</code> </p><ul><li>也可以<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">m_arr = np.arange(0,m)      # 生成0-m-1</div><div class="line">np.random.shuffle(m_arr)    # 打乱m_arr顺序</div></pre></td></tr></table></figure></li></ul><p>然后取前100个即可</p></li></ul><h3 id="8、查看数据类型"><a href="#8、查看数据类型" class="headerlink" title="8、查看数据类型"></a>8、查看数据类型</h3><ul><li>（1）<code>a.dtype</code></li></ul><h3 id="9、数组复制"><a href="#9、数组复制" class="headerlink" title="9、数组复制"></a>9、数组复制</h3><ul><li>（1）共享内存<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">a = np.array([1,2,3,4,5])</div><div class="line">b = a</div><div class="line">print np.may_share_memory(a,b)</div></pre></td></tr></table></figure></li></ul><p>输出：True<br>说明使用的同一个存储区域，修改一个数组同时另外的也会修改</p><ul><li>（2）不共享内存<br><code>b = a.copy()</code></li></ul><h3 id="10、布尔型"><a href="#10、布尔型" class="headerlink" title="10、布尔型"></a>10、布尔型</h3><ul><li>（1）<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">a = np.random.random_integers(0,20,5)</div><div class="line">print a</div><div class="line">print a%3==0</div><div class="line">print a[a % 3 == 0]</div></pre></td></tr></table></figure></li></ul><p>输出：<br>    [14  3  6 15  4]<br>    [False  True  True  True False]<br>    [ 3  6 15]</p><h3 id="11、中间数、平均值"><a href="#11、中间数、平均值" class="headerlink" title="11、中间数、平均值"></a>11、中间数、平均值</h3><ul><li>（1）中间数<code>np.median(a)</code></li><li>（2）平均值<code>np.mean(a)</code>,<ul><li>若是矩阵，不指定<code>axis</code>默认求所有元素的均值</li><li><code>axis=0</code>,求列的均值</li><li><code>axis=1</code>，求行的均值</li></ul></li></ul><h3 id="12、矩阵操作"><a href="#12、矩阵操作" class="headerlink" title="12、矩阵操作"></a>12、矩阵操作</h3><ul><li>（1）乘积<code>np.dot(a,b)</code><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">a = np.array([[1,2,3],[2,3,4]])</div><div class="line">b = np.array([[1,2],[2,3],[2,2]])</div><div class="line">print np.dot(a,b)</div></pre></td></tr></table></figure></li></ul><p>或者使用<code>np.matrix()</code>生成矩阵，相乘需要满足矩阵相乘的条件</p><ul><li><p>（2）内积<code>np.inner(a,b)</code><br>行相乘</p></li><li><p>（3）逆矩阵<code>np.linalg.inv(a)</code></p></li><li>（4）列的最大值<code>np.max(a[:,0])</code>–&gt;返回第一列的最大值</li><li>（5）每列的和<code>np.sum(a,0)</code></li><li>（6）每行的平均数<code>np.mean(a,1)</code></li><li>（7）求交集<code>p.intersect1d(a,b)</code>，返回一维数组</li><li>（8）转置：<code>np.transpose(a)</code></li><li>（9）两个矩阵对应对应元素相乘（点乘）：<code>a*b</code></li></ul><h3 id="13、文件操作"><a href="#13、文件操作" class="headerlink" title="13、文件操作"></a>13、文件操作</h3><ul><li>（1）保存：<code>tofile()</code><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">a = np.arange(10)</div><div class="line">a.shape=2,5</div><div class="line">a.tofile(&quot;test.bin&quot;)</div></pre></td></tr></table></figure></li></ul><p>读取：（需要注意指定保存的数据类型）<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">a = np.fromfile(&quot;test.bin&quot;,dtype=np.int32)</div><div class="line">print a</div></pre></td></tr></table></figure></p><ul><li>（2）保存：<code>np.save(&quot;test&quot;,a)</code>–&gt;会保存成test.npy文件<br>读取：<code>a = np.load(&quot;test&quot;)</code></li></ul><h3 id="14、组合两个数组"><a href="#14、组合两个数组" class="headerlink" title="14、组合两个数组"></a>14、组合两个数组</h3><ul><li><p>（1）垂直组合</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">a = np.array([1,2,3])</div><div class="line">b = np.array([[1,2,3],[4,5,6]])</div><div class="line"></div><div class="line">c = np.vstack((b,a))</div></pre></td></tr></table></figure></li><li><p>（2）水平组合</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">a = np.array([[1,2],[3,4]])</div><div class="line">b = np.array([[1,2,3],[4,5,6]])</div><div class="line"></div><div class="line">c = np.hstack((a,b))</div></pre></td></tr></table></figure></li></ul><h3 id="15、读声音Wave文件"><a href="#15、读声音Wave文件" class="headerlink" title="15、读声音Wave文件"></a>15、读声音Wave文件</h3><ul><li>（1）<code>wave</code>     <figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div></pre></td><td class="code"><pre><div class="line">import wave</div><div class="line">from matplotlib import pyplot as plt</div><div class="line">import numpy as np</div><div class="line"></div><div class="line"># 打开WAV文档</div><div class="line">f = wave.open(r&quot;c:\WINDOWS\Media\ding.wav&quot;, &quot;rb&quot;)</div><div class="line"></div><div class="line"># 读取格式信息</div><div class="line"># (nchannels, sampwidth, framerate, nframes, comptype, compname)</div><div class="line">params = f.getparams()</div><div class="line">nchannels, sampwidth, framerate, nframes = params[:4]</div><div class="line"></div><div class="line"># 读取波形数据</div><div class="line">str_data = f.readframes(nframes)</div><div class="line">f.close()</div><div class="line"></div><div class="line">#将波形数据转换为数组</div><div class="line">wave_data = np.fromstring(str_data, dtype=np.short)</div><div class="line">wave_data.shape = -1, 2</div><div class="line">wave_data = wave_data.T</div><div class="line">time = np.arange(0, nframes) * (1.0 / framerate)</div><div class="line"></div><div class="line"># 绘制波形</div><div class="line">plt.subplot(211) </div><div class="line">plt.plot(time, wave_data[0])</div><div class="line">plt.subplot(212) </div><div class="line">plt.plot(time, wave_data[1], c=&quot;g&quot;)</div><div class="line">plt.xlabel(&quot;time (seconds)&quot;)</div><div class="line">plt.show()</div></pre></td></tr></table></figure></li></ul><h3 id="16、where"><a href="#16、where" class="headerlink" title="16、where"></a>16、<code>where</code></h3><ul><li>（1）找到y数组中=1的位置：<code>np.where(y==1)</code></li></ul><h3 id="17、np-ravel-y"><a href="#17、np-ravel-y" class="headerlink" title="17、np.ravel(y)"></a>17、<code>np.ravel(y)</code></h3><ul><li>将二维的转化为一维的，eg:<code>(5000,1)--&gt;(5000,)</code></li></ul><h3 id="18、ndarray-flat函数"><a href="#18、ndarray-flat函数" class="headerlink" title="18、ndarray.flat函数"></a>18、ndarray.flat函数</h3><ul><li>将数据展开对应的数组，可以进行访问</li><li>应用：0/1映射<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">def dense_to_one_hot(label_dense,num_classes):</div><div class="line">    num_labels = label_dense.shape[0]</div><div class="line">    index_offset = np.arange(num_labels)*num_classes</div><div class="line">    labels_one_hot = numpy.zeros((num_labels, num_classes))</div><div class="line">    labels_one_hot.flat[index_offset + labels_dense.ravel()] = 1</div><div class="line">    return labels_one_hot</div></pre></td></tr></table></figure></li></ul><h3 id="19、数组访问"><a href="#19、数组访问" class="headerlink" title="19、数组访问"></a>19、数组访问</h3><ul><li>X = np.array([[1,2],[3,4]])<ul><li><code>X[0:1]和X[0:1,:]</code>等价，都是系那是第一行数据</li></ul></li></ul><h3 id="20、np-c"><a href="#20、np-c" class="headerlink" title="20、np.c_()"></a>20、<code>np.c_()</code></h3><ul><li>按照第二维度，即列拼接数据<ul><li><code>np.c_[np.array([[1,2,3]]), 0, 0, np.array([[4,5,6]])]</code><br>输出：<code>array([[1, 2, 3, 0, 0, 4, 5, 6]])</code></li></ul></li><li>两个列表list拼接，长度要一致<ul><li><code>np.c_[[1,2,3],[2,3,4]]</code></li><li><code>np.c_[range(1,5),range(2,6)]</code></li></ul></li></ul><h2 id="二、Matplotlib"><a href="#二、Matplotlib" class="headerlink" title="二、Matplotlib"></a>二、Matplotlib</h2><h3 id="1、关于pyplot"><a href="#1、关于pyplot" class="headerlink" title="1、关于pyplot"></a>1、关于<code>pyplot</code></h3><ul><li>（1）matplotlib的pyplot子库提供了和matlab类似的绘图API，方便用户快速绘制2D图表。</li><li>（2）导入包：<code>from matplotlib import pyplot as plt</code></li></ul><h3 id="2、绘图基础"><a href="#2、绘图基础" class="headerlink" title="2、绘图基础"></a>2、绘图基础</h3><ul><li><p>（1）sin和cos</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">x = np.linspace(-np.pi, np.pi,256,endpoint=True)</div><div class="line">C,S = np.cos(x),np.sin(x)</div><div class="line">plt.plot(x,C)</div><div class="line">plt.plot(x,S)</div><div class="line">plt.xlabel(&quot;x&quot;)</div><div class="line">plt.ylabel(&quot;y&quot;)</div><div class="line">plt.show()</div></pre></td></tr></table></figure></li><li><p>（2）指定绘图的大小</p><p>  plt.figure(figsize=(8,6), dpi=80)</p></li><li><p>（3）指定线的颜色、粗细和类型</p><p>  plt.plot(x,C,color=”blue”,linewidth=2.0,linestyle=”-“,label=”cos”)<br>  蓝色、宽度、连续、label（使用legend会显示这个label）</p></li><li><p>（4）指定x坐标轴范围</p><p>  plt.xlim(-4.0,4.0)</p></li><li>（5）设置y抽刻度间隔<br><code>plt.yticks(np.linspace(-1, 1, 15, endpoint=True))</code></li><li><p>（6）显示图例</p><p>  plt.legend(loc=”upper left”)<br>  显示在左上方</p></li><li><p>（7）一个figure上画多个图subplot方式</p><p>  plt.subplot(1, 2, 1)<br>  plt.subplot(1, 2, 2)<br>  例如：plt.subplot(m, n, p)<br>  代表图共有的m行，n列，第p个图<br>  p是指第几个图，横向数<br>  上面代表有一行，两个图</p><ul><li>[更详细解释]：<br>231,232,233表示第一行的1,2,3个位置，接着的223表示把整个矩形分成4分，所以第3个位置就是第二行的第一个位置，但是相比第一行占了1.5列<br>（每次subplot划分都是整个图重新划分）</li></ul></li><li><p>（8）一个figure上画多个图，axes方式</p><p>  plt.axes([.1, .1, .8, .8])<br>  plt.axes([.2, .2, .3, .3])</p></li><li><p>（9）填充</p><p>  plt.fill_between(x, y1, y2=0, where=None, interpolate=False, step=None, </p><pre><code>hold=None, data=None)</code></pre><p>  eg:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">plt.fill_between(X, 1, C+1, C+1&gt;1,color=&quot;red&quot;)</div><div class="line">plt.fill_between(X, 1, C+1, C+1&lt;1,color=&quot;blue&quot;)</div></pre></td></tr></table></figure></li></ul><h3 id="3、散点图"><a href="#3、散点图" class="headerlink" title="3、散点图"></a>3、散点图</h3><ul><li><p>（1）</p><p>  <code>plt.scatter(X,Y)</code></p></li></ul><h3 id="4、条形图"><a href="#4、条形图" class="headerlink" title="4、条形图"></a>4、条形图</h3><ul><li><p>（1）</p><p>  <code>plt.bar(X, Y, facecolor=&quot;red&quot;, edgecolor=&quot;blue&quot; )</code><br>  填充颜色为facecolor,边界颜色为edgecolor</p></li></ul><h3 id="5、等高线图"><a href="#5、等高线图" class="headerlink" title="5、等高线图"></a>5、等高线图</h3><ul><li>（1）只显示等高线<code>contour</code></li><li>（2）显示表面<code>contourf</code></li><li>（3）注意三维图要用到<code>meshgrid</code>转化为网格<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">def f(x,y):</div><div class="line">    return (1 - x / 2 + x**5 + y**3) * np.exp(-x**2 -y**2)</div><div class="line"></div><div class="line">n = 256</div><div class="line">x = np.linspace(-3,3,n)</div><div class="line">y = np.linspace(-3,3,n)</div><div class="line">X,Y = np.meshgrid(x,y)</div><div class="line"></div><div class="line">plt.contourf(X,Y,f(X,Y),alpha=.5)</div><div class="line">C = plt.contour(X,Y,f(X, Y),colors=&quot;black&quot;,linewidth=.5)</div><div class="line">plt.clabel(C)</div><div class="line"></div><div class="line">plt.show()</div></pre></td></tr></table></figure></li></ul><h3 id="6、显示图片imshow"><a href="#6、显示图片imshow" class="headerlink" title="6、显示图片imshow"></a>6、显示图片<code>imshow</code></h3><ul><li>（1）<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">def f(x,y):</div><div class="line">return (1 - x / 2 + x ** 5 + y ** 3 ) * np.exp(-x ** 2 - y ** 2)</div><div class="line">n = 10</div><div class="line">x = np.linspace(-3, 3, 3.5 * n)</div><div class="line">y = np.linspace(-3, 3, 3.0 * n)</div><div class="line">X, Y = np.meshgrid(x, y)</div><div class="line">z = f(X,Y)</div><div class="line">plt.imshow(z)</div><div class="line">plt.show()</div></pre></td></tr></table></figure></li></ul><h3 id="7、饼图pie"><a href="#7、饼图pie" class="headerlink" title="7、饼图pie"></a>7、饼图<code>pie</code></h3><ul><li>（1）传入一个序列<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">plt.figure(figsize=(8,8))</div><div class="line">n = 20</div><div class="line">Z = np.arange(10)</div><div class="line">plt.pie(Z)</div><div class="line">plt.show()</div></pre></td></tr></table></figure></li></ul><h3 id="8、三维表面图"><a href="#8、三维表面图" class="headerlink" title="8、三维表面图*"></a>8、三维表面图*</h3><ul><li>（1）需要导入包：<code>from mpl_toolkits.mplot3d import Axes3D</code></li><li>（2）<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">fig = plt.figure()</div><div class="line">ax = Axes3D(fig)</div><div class="line">X = np.arange(-4, 4, 0.25)</div><div class="line">Y = np.arange(-4, 4, 0.25)</div><div class="line">X, Y = np.meshgrid(X, Y)</div><div class="line">R = np.sqrt(X ** 2 + Y ** 2)</div><div class="line">Z = np.sin(R)</div><div class="line">ax.plot_surface(X, Y, Z, rstride=1, cstride=1, cmap=plt.cm.hot)</div><div class="line">ax.contourf(X, Y, Z, zdir=&apos;z&apos;, offset=-2, cmap=plt.cm.hot)</div><div class="line">ax.set_zlim(-2, 2)</div><div class="line">plt.show()</div></pre></td></tr></table></figure></li></ul><h3 id="9、legend显示问题"><a href="#9、legend显示问题" class="headerlink" title="9、legend显示问题"></a>9、legend显示问题</h3><ul><li>（1）<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">p1, = plt.plot(np.ravel(X[pos,0]),np.ravel(X[pos,1]),&apos;ro&apos;,markersize=8)</div><div class="line">p2, = plt.plot(np.ravel(X[neg,0]),np.ravel(X[neg,1]),&apos;g^&apos;,markersize=8)</div><div class="line">plt.xlabel(&quot;X1&quot;)</div><div class="line">plt.ylabel(&quot;X2&quot;)</div><div class="line">plt.legend([p1,p2],[&quot;y==1&quot;,&quot;y==0&quot;])</div></pre></td></tr></table></figure></li></ul><p><strong>注意</strong> p1后要加上<code>,</code>逗号，里面的数据要是<strong>一维</strong>的，使用<code>np.ravel()</code>转化一下</p><h3 id="10、显示网格"><a href="#10、显示网格" class="headerlink" title="10、显示网格"></a>10、显示网格</h3><ul><li>（1）<code>plt.grid(True, linestyle = &quot;-.&quot;, color = &quot;b&quot;, linewidth = &quot;1&quot;)</code>  </li></ul><h3 id="11、显示正方形的坐标区域"><a href="#11、显示正方形的坐标区域" class="headerlink" title="11、显示正方形的坐标区域"></a>11、显示正方形的坐标区域</h3><ul><li>（1）<code>plt.axis(&#39;square&#39;)</code></li></ul><h2 id="三、Scipy"><a href="#三、Scipy" class="headerlink" title="三、Scipy"></a>三、Scipy</h2><h3 id="1、-Scipy特征"><a href="#1、-Scipy特征" class="headerlink" title="1、 Scipy特征"></a>1、 Scipy特征</h3><ul><li>（1）内置了图像处理， 优化，统计等等相关问题的子模块</li><li>（2）scipy 是Python科学计算环境的核心。 它被设计为利用 numpy 数组进行高效的运行。从这个角度来讲，scipy和numpy是密不可分的。</li></ul><h3 id="2、文件操作io"><a href="#2、文件操作io" class="headerlink" title="2、文件操作io"></a>2、文件操作<code>io</code></h3><ul><li>（1）导包：<code>from scipy import io as spio</code></li><li><p>（2）保存<code>mat</code>格式文件</p><p>  <code>spio.savemat(&quot;test.mat&quot;, {&#39;a&#39;:a})</code></p></li></ul><ul><li><p>（3）加载<code>mat</code>文件</p><p>  <code>data = spio.loadmat(&quot;test.mat&quot;)</code><br>  访问值：data[‘a’]–&gt;相当于map</p></li><li>（4）读取图片文件<br>导包：<code>from scipy import misc</code><br>读取：<code>data = misc.imread(&quot;123.png&quot;)</code><br>[注1]：与matplotlib中<code>plt.imread(&#39;fname.png&#39;)</code>类似<br>[注2]：执行<code>misc.imread</code>时可能提醒不存在这个模块，那就安装<code>pillow</code>的包</li></ul><h3 id="3、线性代数操作linalg"><a href="#3、线性代数操作linalg" class="headerlink" title="3、线性代数操作linalg"></a>3、线性代数操作<code>linalg</code></h3><ul><li><p>（1）求行列式<code>det</code></p><p>  <code>res = linalg.det(a)</code></p></li><li><p>（2）求逆矩阵<code>inv</code></p><p>  <code>res = linalg.inv(a)</code><br>  若是矩阵不可逆，则会抛异常<code>LinAlgError: singular matrix</code></p></li><li>（3）奇异值分解<code>svd</code><br>  <code>u,s,v = linalg.svd(a)</code><br>  [注1]：s为a的特征值（一维），降序排列，<br>  [注2]：a = u*s*v’（需要将s转换一下才能相乘）<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">t = np.diag(s)</div><div class="line">print u.dot(t).dot(v)</div></pre></td></tr></table></figure></li></ul><h3 id="4、梯度下降优化算法"><a href="#4、梯度下降优化算法" class="headerlink" title="4、梯度下降优化算法"></a>4、梯度下降优化算法</h3><ul><li><p>（1）<code>fmin_bfgs</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">def f(x):</div><div class="line">    return x**2-2*x</div><div class="line">initial_x = 0</div><div class="line">optimize.fmin_bfgs(f,initial_x)</div></pre></td></tr></table></figure><p>  [注]：initial_x为初始点（此方法可能会得到局部最小值）</p></li><li>（2）<code>fmin()</code>、<code>fmin_cg</code>等等方法</li></ul><h3 id="5、拟合（最小二乘法）"><a href="#5、拟合（最小二乘法）" class="headerlink" title="5、拟合（最小二乘法）"></a>5、拟合（最小二乘法）</h3><ul><li>（1）<code>curve_fit</code><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line">#产生数据</div><div class="line">def f(x):</div><div class="line">    return x**2 + 10*np.sin(x)</div><div class="line">xdata = np.linspace(-10, 10, num=20)</div><div class="line">ydata = f(xdata)+np.random.randn(xdata.size)</div><div class="line">plt.scatter(xdata, ydata, linewidths=3.0, </div><div class="line">           edgecolors=&quot;red&quot;)</div><div class="line">#plt.show()</div><div class="line">#拟合</div><div class="line">def f2(x,a,b):</div><div class="line">    return a*x**2 + b*np.sin(x)</div><div class="line">guess = [2,2]</div><div class="line">params, params_covariance = optimize.curve_fit(f2, xdata, ydata, guess)</div><div class="line">#画出拟合的曲线</div><div class="line">x1 = np.linspace(-10,10,256)</div><div class="line">y1 = f2(x1,params[0],params[1])</div><div class="line">plt.plot(x1,y1)</div><div class="line">plt.show()</div></pre></td></tr></table></figure></li></ul><h3 id="6、统计检验"><a href="#6、统计检验" class="headerlink" title="6、统计检验"></a>6、统计检验</h3><ul><li>（1）T-检验<code>stats.ttest_ind</code><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">a = np.random.normal(0, 1, size=10)</div><div class="line">b = np.random.normal(1, 1, size=10)</div><div class="line">print stats.ttest_ind(a, b)</div></pre></td></tr></table></figure></li></ul><p>输出：(-2.6694785119868358, 0.015631342180817954)<br>后面的是概率p: 两个过程相同的概率。如果其值接近1，那么两个过程几乎可以确定是相同的，如果其值接近0，那么它们很可能拥有不同的均值。</p><h3 id="7、插值"><a href="#7、插值" class="headerlink" title="7、插值"></a>7、插值</h3><ul><li>（1）导入包：<code>from scipy.interpolate import interp1d</code><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">#产生一些数据</div><div class="line">x = np.linspace(0, 1, 10)</div><div class="line">y = np.sin(2 * np.pi * x)</div><div class="line">computed_time = np.linspace(0, 1, 50)</div><div class="line">#线性插值</div><div class="line">linear_interp = interp1d(x, y)</div><div class="line">linear_results = linear_interp(computed_time)</div><div class="line">#三次方插值</div><div class="line">cubic_interp = interp1d(x, y, kind=&apos;cubic&apos;)</div><div class="line">cubic_results = cubic_interp(computed_time)</div><div class="line">#作图</div><div class="line">plt.plot(x, y, &apos;o&apos;, ms=6, label=&apos;y&apos;)</div><div class="line">plt.plot(computed_time, linear_results, label=&apos;linear interp&apos;)</div><div class="line">plt.plot(computed_time, cubic_results, label=&apos;cubic interp&apos;)</div><div class="line">plt.legend()</div><div class="line">plt.show()</div></pre></td></tr></table></figure></li></ul><h3 id="8、求解非线性方程组"><a href="#8、求解非线性方程组" class="headerlink" title="8、求解非线性方程组"></a>8、求解非线性方程组</h3><ul><li>（1）<code>optimize</code>中的<code>fsolve</code><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">from scipy.optimize import fsolve</div><div class="line">def func(x):</div><div class="line">    x0,x1,x2 = x.tolist()</div><div class="line">    return [5*x1-25,5*x0*x0-x1*x2,x2*x0-27]</div><div class="line">initial_x = [1,1,1]</div><div class="line">result = fsolve(func, initial_x)</div><div class="line">print result</div></pre></td></tr></table></figure></li></ul><h2 id="四、pandas"><a href="#四、pandas" class="headerlink" title="四、pandas"></a>四、pandas</h2><h3 id="1、pandas特征与导入"><a href="#1、pandas特征与导入" class="headerlink" title="1、pandas特征与导入"></a>1、pandas特征与导入</h3><ul><li>（1）包含高级的数据结构和精巧的工具</li><li>（2）pandas建造在NumPy之上</li><li>（3）导入：<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">from pandas import Series, DataFrame</div><div class="line">import pandas as pd</div></pre></td></tr></table></figure></li></ul><h3 id="2、pandas数据结构"><a href="#2、pandas数据结构" class="headerlink" title="2、pandas数据结构"></a>2、pandas数据结构</h3><h4 id="（1）Series"><a href="#（1）Series" class="headerlink" title="（1）Series"></a>（1）Series</h4><ul><li>一维的类似的数组对象</li><li><p>包含一个数组的数据（任何NumPy的数据类型）和一个与数组关联的索引</p><ul><li><p>不指定索引：<code>a = Series([1,2,3])</code> ，输出为</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">0    1</div><div class="line">1    2</div><div class="line">2    3</div></pre></td></tr></table></figure><p>包含属性<code>a.index,a.values</code>，对应索引和值</p></li><li>指定索引：<code>a = Series([1,2,3],index=[&#39;a&#39;,&#39;b&#39;,&#39;c&#39;])</code><br>可以通过索引访问<code>a[&#39;b&#39;]</code></li></ul></li><li>判断某个索引是否存在：<code>&#39;b&#39; in a</code></li><li>通过字典建立<code>Series</code><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">dict = &#123;&apos;china&apos;:10,&apos;america&apos;:30,&apos;indian&apos;:20&#125;</div><div class="line">print Series(dict)</div></pre></td></tr></table></figure></li></ul><p>输出：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">america    30</div><div class="line">china      10</div><div class="line">indian     20</div><div class="line">dtype: int64</div></pre></td></tr></table></figure></p><ul><li>判断哪个索引值缺失：<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">dict = &#123;&apos;china&apos;:10,&apos;america&apos;:30,&apos;indian&apos;:20&#125;</div><div class="line">state = [&apos;china&apos;,&apos;america&apos;,&apos;test&apos;]</div><div class="line">a = Series(dict,state)</div><div class="line">print a.isnull()</div></pre></td></tr></table></figure></li></ul><p>输出：（test索引没有对应值）<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">china      False</div><div class="line">america    False</div><div class="line">test        True</div><div class="line">dtype: bool</div></pre></td></tr></table></figure></p><ul><li>在算术运算中它会<strong>自动对齐</strong>不同索引的数据<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">a = Series([10,20],[&apos;china&apos;,&apos;test&apos;])</div><div class="line">b = Series([10,20],[&apos;test&apos;,&apos;china&apos;])</div><div class="line">print a+b</div></pre></td></tr></table></figure></li></ul><p>输出：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">china    30</div><div class="line">test     30</div><div class="line">dtype: int64</div></pre></td></tr></table></figure></p><ul><li>指定<code>Series</code>对象的<code>name</code>和<code>index</code>的<code>name</code>属性<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">a = Series([10,20],[&apos;china&apos;,&apos;test&apos;])</div><div class="line">a.index.name = &apos;state&apos;</div><div class="line">a.name = &apos;number&apos;</div><div class="line">print a</div></pre></td></tr></table></figure></li></ul><p>输出：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">state</div><div class="line">china    10</div><div class="line">test     20</div><div class="line">Name: number, dtype: int64</div></pre></td></tr></table></figure></p><h4 id="（2）DataFrame"><a href="#（2）DataFrame" class="headerlink" title="（2）DataFrame"></a>（2）DataFrame</h4><ul><li><code>Datarame</code>表示一个表格，类似电子表格的数据结构</li><li>包含一个经过<strong>排序的列表集</strong>（按<code>列名</code>排序）</li><li>每一个都可以有不同的类型值（数字，字符串，布尔等等）</li><li><code>DataFrame</code>在内部把数据存储为一个二维数组的格式，因此你可以采用分层索引以表格格式来表示高维的数据</li><li><p>创建：</p><ul><li>通过字典<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">data = &#123;&apos;state&apos;: [&apos;a&apos;, &apos;b&apos;, &apos;c&apos;, &apos;d&apos;, &apos;d&apos;],</div><div class="line">        &apos;year&apos;: [2000, 2001, 2002, 2001, 2002],</div><div class="line">        &apos;pop&apos;: [1.5, 1.7, 3.6, 2.4, 2.9]&#125;</div><div class="line">frame = DataFrame(data)</div><div class="line">print frame</div></pre></td></tr></table></figure></li></ul><p>输出：(按照<strong>列名排好序</strong>的[若是手动分配列名，会按照你设定的]，并且索引会自动分配)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">    pop state  year</div><div class="line">0  1.5     a  2000</div><div class="line">1  1.7     b  2001</div><div class="line">2  3.6     c  2002</div><div class="line">3  2.4     d  2001</div><div class="line">4  2.9     d  2002</div></pre></td></tr></table></figure></li><li><p>访问</p><ul><li><strong>列</strong>：与<code>Series</code>一样，通过列名访问：<code>frame[&#39;state&#39;]</code>或者<code>frame.state</code></li><li><strong>行</strong>：<code>ix</code> 索引成员（field），<code>frame.ix[2]</code>，返回每一列的第3行数据</li></ul></li><li>赋值：<code>frame2[&#39;debt&#39;] = np.arange(5.)</code>，若没有<code>debt</code>列名，则会新增一列</li><li>删除某一列：<code>del frame2[&#39;eastern&#39;]</code></li><li>像Series一样， <code>values</code> 属性返回一个包含在DataFrame中的数据的二维ndarray</li><li>返回所有的列信息：<code>frame.columns</code></li><li>转置：<code>frame2.T</code></li></ul><h4 id="（3）索引对象"><a href="#（3）索引对象" class="headerlink" title="（3）索引对象"></a>（3）索引对象</h4><ul><li>pandas的索引对象用来保存坐标轴标签和其它元数据（如坐标轴名或名称）</li><li>索引对象是不可变的，因此不能由用户改变</li><li>创建<code>index = pd.Index([1,2,3])</code></li><li>常用操作<ul><li><code>append</code>–&gt;链接额外的索引对象，产生一个新的索引</li><li><code>diff</code>    –&gt;计算索引的差集</li><li><code>intersection</code>    –&gt;计算交集</li><li><code>union</code>    –&gt;计算并集</li><li><code>isin</code>    –&gt;计算出一个布尔数组表示每一个值是否包含在所传递的集合里</li><li><code>delete</code>    –&gt;计算删除位置i的元素的索引</li><li><code>drop</code>    –&gt;计算删除所传递的值后的索引</li><li><code>insert</code>    –&gt;计算在位置i插入元素后的索引</li><li><code>is_monotonic</code>    –&gt;返回True，如果每一个元素都比它前面的元素大或相等</li><li><code>is_unique</code>    –&gt;返回True，如果索引没有重复的值</li><li><code>unique</code>    –&gt;计算索引的唯一值数组</li></ul></li></ul><h3 id="3、重新索引reindex"><a href="#3、重新索引reindex" class="headerlink" title="3、重新索引reindex"></a>3、重新索引<code>reindex</code></h3><h4 id="（1）Series-1"><a href="#（1）Series-1" class="headerlink" title="（1）Series"></a>（1）Series</h4><ul><li><p>（1）重新排列</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">a = Series([2,3,1],index=[&apos;b&apos;,&apos;a&apos;,&apos;c&apos;])</div><div class="line">b = a.reindex([&apos;a&apos;,&apos;b&apos;,&apos;c&apos;])</div><div class="line">print b</div></pre></td></tr></table></figure></li><li><p>（2）重新排列，没有的索引补充为0,<code>b=a.reindex([&#39;a&#39;,&#39;b&#39;,&#39;c&#39;,&#39;d&#39;],fill_value=0)</code></p></li><li>（3）重建索引时对值进行内插或填充<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">a = Series([&apos;a&apos;,&apos;b&apos;,&apos;c&apos;],index=[0,2,4])</div><div class="line">b = a.reindex(range(6),method=&apos;ffill&apos;)</div><div class="line">print b</div></pre></td></tr></table></figure></li></ul><p>输出：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">0    a</div><div class="line">1    a</div><div class="line">2    b</div><div class="line">3    b</div><div class="line">4    c</div><div class="line">5    cdata_link</div><div class="line">dtype: object</div></pre></td></tr></table></figure></p><p><code>method</code>的参数<br>ffill或pad—-&gt;前向（或进位）填充<br>bfill或backfill—-&gt;后向（或进位）填充</p><h4 id="（3）DataFrame"><a href="#（3）DataFrame" class="headerlink" title="（3）DataFrame"></a>（3）DataFrame</h4><ul><li>与Series一样，<code>reindex</code> index</li><li>还可以reindex column列，<code>frame.reindex(columns=[&#39;a&#39;,&#39;b&#39;])</code></li></ul><h3 id="4、从一个坐标轴删除条目"><a href="#4、从一个坐标轴删除条目" class="headerlink" title="4、从一个坐标轴删除条目"></a>4、从一个坐标轴删除条目</h3><h4 id="（1）Series-2"><a href="#（1）Series-2" class="headerlink" title="（1）Series"></a>（1）Series</h4><ul><li><code>a.drop([&#39;a&#39;,&#39;b&#39;])</code> 删除a，b索引项<h4 id="（2）DataFrame-1"><a href="#（2）DataFrame-1" class="headerlink" title="（2）DataFrame"></a>（2）DataFrame</h4></li><li>索引项的删除与<code>Series</code>一样</li><li>删除column—&gt;<code>a.drop([&#39;one&#39;], axis=1)</code> 删除column名为one的一列</li></ul><h3 id="5、索引，挑选和过滤"><a href="#5、索引，挑选和过滤" class="headerlink" title="5、索引，挑选和过滤"></a>5、索引，挑选和过滤</h3><h4 id="（1）Series-3"><a href="#（1）Series-3" class="headerlink" title="（1）Series"></a>（1）Series</h4><ul><li>可以通过index值或者整数值来访问数据，eg：对于<code>a = Series(np.arange(4.), index=[&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;])</code>，<code>a[&#39;b&#39;]</code>和<code>a[1]</code>是一样的</li><li>使用标签来切片和正常的Python切片并不一样，它会把结束点也包括在内<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">a = Series(np.arange(4.), index=[&apos;a&apos;, &apos;b&apos;, &apos;c&apos;, &apos;d&apos;])</div><div class="line">print a[&apos;b&apos;:&apos;c&apos;]</div></pre></td></tr></table></figure></li></ul><p>输出包含c索引对应的值</p><h4 id="（2）DataFrame-2"><a href="#（2）DataFrame-2" class="headerlink" title="（2）DataFrame"></a>（2）DataFrame</h4><ul><li>显示前两行：<code>a[:2]</code></li><li>布尔值访问：<code>a[a[&#39;two&#39;]&gt;5]</code></li><li>索引字段 ix 的使用<ul><li>index为2，column为’one’和’two’—&gt;<code>a.ix[[2],[&#39;one&#39;,&#39;two&#39;]]</code></li><li>index为2的一行：<code>a.ix[2]</code></li></ul></li></ul><h3 id="6、DataFrame和Series运算"><a href="#6、DataFrame和Series运算" class="headerlink" title="6、DataFrame和Series运算"></a>6、DataFrame和Series运算</h3><ul><li>（1）DataFrame每一行都减去一个Series<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">a = pd.DataFrame(np.arange(16).reshape(4,4),index=[0,1,2,3],columns=[&apos;one&apos;,    &apos;two&apos;,&apos;three&apos;,&apos;four&apos;])</div><div class="line">print a</div><div class="line">b = Series([0,1,2,3],index=[&apos;one&apos;,&apos;two&apos;,&apos;three&apos;,&apos;four&apos;])</div><div class="line">print b</div><div class="line">print a-b</div></pre></td></tr></table></figure></li></ul><p>输出：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">   one  two  three  four</div><div class="line">0    0    1      2     3</div><div class="line">1    4    5      6     7</div><div class="line">2    8    9     10    11</div><div class="line">3   12   13     14    15</div><div class="line">one      0</div><div class="line">two      1</div><div class="line">three    2</div><div class="line">four     3</div><div class="line">dtype: int64</div><div class="line">   one  two  three  four</div><div class="line">0    0    0      0     0</div><div class="line">1    4    4      4     4</div><div class="line">2    8    8      8     8</div><div class="line">3   12   12     12    12</div></pre></td></tr></table></figure></p><h3 id="7、读取文件"><a href="#7、读取文件" class="headerlink" title="7、读取文件"></a>7、读取文件</h3><ul><li>（1）<code>csv</code>文件<br><code>pd.read_csv(r&quot;data/train.csv&quot;)</code>，返回的数据类型是<code>DataFrame</code>类型</li></ul><h3 id="8、查看DataFrame的信息"><a href="#8、查看DataFrame的信息" class="headerlink" title="8、查看DataFrame的信息"></a>8、查看DataFrame的信息</h3><ul><li>（1）<code>train_data.describe()</code><br>eg:<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">       PassengerId    Survived      Pclass         Age       SibSp  \</div><div class="line">count   891.000000  891.000000  891.000000  714.000000  891.000000   </div><div class="line">mean    446.000000    0.383838    2.308642   29.699118    0.523008   </div><div class="line">std     257.353842    0.486592    0.836071   14.526497    1.102743   </div><div class="line">min       1.000000    0.000000    1.000000    0.420000    0.000000   </div><div class="line">25%     223.500000    0.000000    2.000000   20.125000    0.000000   </div><div class="line">50%     446.000000    0.000000    3.000000   28.000000    0.000000   </div><div class="line">75%     668.500000    1.000000    3.000000   38.000000    1.000000   </div><div class="line">max     891.000000    1.000000    3.000000   80.000000    8.000000</div></pre></td></tr></table></figure></li></ul><h3 id="9、定位到一列并替换"><a href="#9、定位到一列并替换" class="headerlink" title="9、定位到一列并替换"></a>9、定位到一列并替换</h3><ul><li><code>df.loc[df.Age.isnull(),&#39;Age&#39;] = 23 #&#39;Age&#39;列为空的内容补上数字23</code></li></ul><h3 id="10、将分类变量转化为指示变量get-dummies"><a href="#10、将分类变量转化为指示变量get-dummies" class="headerlink" title="10、将分类变量转化为指示变量get_dummies()"></a>10、将分类变量转化为指示变量<code>get_dummies()</code></h3><ul><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">s = pd.Series(list(&apos;abca&apos;))</div><div class="line">pd.get_dummies(s)</div></pre></td></tr></table></figure></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">   a  b  c</div><div class="line">0  1  0  0</div><div class="line">1  0  1  0</div><div class="line">2  0  0  1</div><div class="line">3  1  0  0</div></pre></td></tr></table></figure><h3 id="11、list和string互相转化"><a href="#11、list和string互相转化" class="headerlink" title="11、list和string互相转化"></a>11、list和string互相转化</h3><ul><li><p>string转list</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; str = &apos;abcde&apos;</div><div class="line">&gt;&gt;&gt; list = list(str)</div><div class="line">&gt;&gt;&gt; list</div><div class="line">[&apos;a&apos;, &apos;b&apos;, &apos;c&apos;, &apos;d&apos;, &apos;e&apos;]</div></pre></td></tr></table></figure></li><li><p>list转string</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; str_convert = &apos;,&apos;.join(list)</div><div class="line">&gt;&gt;&gt; str_convert</div><div class="line">&apos;a,b,c,d,e&apos;</div></pre></td></tr></table></figure></li></ul><h3 id="12、删除原来的索引，重新从0-n索引"><a href="#12、删除原来的索引，重新从0-n索引" class="headerlink" title="12、删除原来的索引，重新从0-n索引"></a>12、删除原来的索引，重新从0-n索引</h3><ul><li><code>x = x.reset_index(drop=True)</code></li></ul><h3 id="13、apply函数"><a href="#13、apply函数" class="headerlink" title="13、apply函数"></a>13、apply函数</h3><ul><li>DataFrame.apply(func, axis=0, broadcast=False, raw=False, reduce=None, …..</li><li>df.apply(numpy.sqrt) # returns DataFrame<ul><li>等价==》df.apply(lambda x : numpy.sqrt(x))==&gt;使用更灵活</li></ul></li><li>df.apply(numpy.sum, axis=0) # equiv to df.sum(0)</li><li>df.apply(numpy.sum, axis=1) # equiv to df.sum(1)</li></ul><h3 id="13、re-search-group-函数"><a href="#13、re-search-group-函数" class="headerlink" title="13、re.search().group()函数"></a>13、re.search().group()函数</h3><ul><li>re.search(pattern, string, flags=0)</li><li>group(num=0)函数返回匹配的字符，默认num=0,可以指定<strong>多个组号</strong>，例如group(0,1)</li></ul><h3 id="14、pandas-cut-函数"><a href="#14、pandas-cut-函数" class="headerlink" title="14、pandas.cut()函数"></a>14、pandas.cut()函数</h3><ul><li>pandas.cut(x, bins, right=True, labels=None, retbins=False, precision=3, include_lowest=False)</li><li>x为以为数组</li><li>bins可以是<strong>int值</strong>或者<strong>序列</strong><ul><li>若是int值就根据x分为bins个数的<strong>区间</strong></li><li>若是序列就是自己指定的区间</li></ul></li><li>right<strong>包含</strong>最右边的区间，默认为True</li><li><p>labels <strong>数组</strong>或者<strong>一个布尔值</strong></p><ul><li>若是数组，需要与对应bins的<strong>结果</strong>一致</li><li>若是布尔值<strong>False</strong>，返回bin中的一个值</li></ul></li><li><p>eg:pd.cut(full[“FamilySize”], bins=[0,1,4,20], labels=[0,1,2])</p></li></ul><h3 id="15、添加一行数据"><a href="#15、添加一行数据" class="headerlink" title="15、添加一行数据"></a>15、添加一行数据</h3><ul><li>定义空的dataframe: <code>data_process = pd.DataFrame(columns=[&#39;route&#39;,&#39;date&#39;,&#39;1&#39;,&#39;2&#39;,&#39;3&#39;,&#39;4&#39;,&#39;5&#39;,&#39;6&#39;,&#39;7&#39;,&#39;8&#39;,&#39;9&#39;,&#39;10&#39;,&#39;11&#39;,&#39;12&#39;])</code></li><li>定义一行新的数据，<code>new = pd.DataFrame(columns=[&#39;route&#39;,&#39;date&#39;,&#39;1&#39;,&#39;2&#39;,&#39;3&#39;,&#39;4&#39;,&#39;5&#39;,&#39;6&#39;,&#39;7&#39;,&#39;8&#39;,&#39;9&#39;,&#39;10&#39;,&#39;11&#39;,&#39;12&#39;],index=[j])</code><ul><li>这里index可以随意设置，若是想指定就指定</li></ul></li><li>添加：<code>data_process = data_process.append(new, ignore_index=True)</code>，<ul><li>注意这里是<code>data_process = data_process.......</code></li></ul></li></ul><h2 id="五、scikit-learn"><a href="#五、scikit-learn" class="headerlink" title="五、scikit-learn"></a>五、scikit-learn</h2><h3 id="1、手写数字识别（SVM）"><a href="#1、手写数字识别（SVM）" class="headerlink" title="1、手写数字识别（SVM）"></a>1、手写数字识别（SVM）</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div></pre></td><td class="code"><pre><div class="line">from sklearn import datasets</div><div class="line">from sklearn import svm</div><div class="line">import numpy as np</div><div class="line">from matplotlib import pyplot as plt</div><div class="line"></div><div class="line">&apos;&apos;&apos;</div><div class="line">使用sciki-learn中的数据集，一般有data,target,DESCR等属性属性</div><div class="line">&apos;&apos;&apos;</div><div class="line"></div><div class="line">digits = datasets.load_digits()                 #加载scikit-learn中的数据集</div><div class="line"></div><div class="line">clf = svm.SVC(gamma=0.001,C=100)                    #使用支持向量机进行分类，gamma为核函数的系数</div><div class="line">clf.fit(digits.data[:-4],digits.target[:-4])        #将除最后4组的数据输入进行训练</div><div class="line"></div><div class="line">predict = clf.predict(digits.data[-4:])         #预测最后4组的数据，[-4:]表示最后4行所有数据，而[-4,:]表示倒数第4行数据</div><div class="line"></div><div class="line">print &quot;预测值为：&quot;,predict</div><div class="line">print &quot;真实值：&quot;,digits.target[-4:]</div><div class="line"></div><div class="line">#显示最后四个图像</div><div class="line">plt.subplot(2,2,1)</div><div class="line">plt.imshow(digits.data[-4,:].reshape(8,8))</div><div class="line">plt.subplot(2,2,2)</div><div class="line">plt.imshow(digits.data[-3,:].reshape(8,8))</div><div class="line">plt.subplot(2,2,3)</div><div class="line">plt.imshow(digits.data[-2,:].reshape(8,8))</div><div class="line">plt.subplot(2,2,4)</div><div class="line">plt.imshow(digits.data[-1,:].reshape(8,8))</div><div class="line">plt.show()</div></pre></td></tr></table></figure><p>svm的参数参数解释：</p><ul><li>（1）<strong>C: 目标函数的惩罚系数C，用来平衡分类间隔margin和错分样本的，default C = 1.0；</strong></li><li>（2）<strong>kernel：参数选择有RBF, Linear, Poly, Sigmoid, 默认的是”RBF”;</strong></li><li>（3）degree：if you choose ‘Poly’ in param 2, this is effective, degree决定了多项式的最高次幂；</li><li>（4）<strong>gamma：核函数的系数(‘Poly’, ‘RBF’ and ‘Sigmoid’), 默认是gamma = 1 / n_features;</strong></li><li>（5）coef0：核函数中的独立项，’RBF’ and ‘Poly’有效；</li><li>（6）probablity: 可能性估计是否使用(true or false)；</li><li>（7）shrinking：是否进行启发式；</li><li>（8）tol（default = 1e - 3）: svm结束标准的精度;</li><li>（9）cache_size: 制定训练所需要的内存（以MB为单位）；</li><li>（10）class_weight:每个类所占据的权重，不同的类设置不同的惩罚参数C,缺省的话自适应；</li><li>（11）verbose: 跟多线程有关，不大明白啥意思具体；</li><li>（12）<strong>max_iter: 最大迭代次数，default = 1000， if max_iter = -1, no limited;</strong></li><li>（13）decision_function_shape ： ‘ovo’ 一对一, ‘ovr’ 多对多  or None 无, default=None</li><li>（14）random_state ：用于概率估计的数据重排时的伪随机数生成器的种子。</li></ul><h3 id="2、保存训练过的模型"><a href="#2、保存训练过的模型" class="headerlink" title="2、保存训练过的模型"></a>2、保存训练过的模型</h3><ul><li><code>from sklearn.externals import joblib</code></li><li><code>joblib.dump(clf, &quot;digits.pkl&quot;)  #将训练的模型保存成digits.pkl文件</code></li><li>加载模型：<br><code>clf = joblib.load(&quot;digits.pkl&quot;)</code><br>其余操作数据即可，预测</li></ul><h3 id="3、鸢尾花分类（svm，分离出测试集）"><a href="#3、鸢尾花分类（svm，分离出测试集）" class="headerlink" title="3、鸢尾花分类（svm，分离出测试集）"></a>3、鸢尾花分类（svm，分离出测试集）</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line">from sklearn import datasets</div><div class="line">from sklearn.cross_validation import train_test_split</div><div class="line">from sklearn.svm import SVC</div><div class="line">import numpy as np</div><div class="line">&apos;&apos;&apos;</div><div class="line">加载scikit-learn中的鸢尾花数据集</div><div class="line">&apos;&apos;&apos;</div><div class="line"></div><div class="line">#加载鸢尾花数据集</div><div class="line">iris = datasets.load_iris()</div><div class="line">iris_data = iris.data;          #相当于X</div><div class="line">iris_target = iris.target;      #对应的label种类，相当于y</div><div class="line"></div><div class="line">x_train,x_test,y_train,y_test =     train_test_split(iris_data,iris_target,test_size=0.2)       #将数据分成训练集x_train和测试集x_test，测试集占总数据的0.2</div><div class="line"></div><div class="line">model = SVC().fit(x_train,y_train);     #使用svm在训练集上拟合</div><div class="line">predict = model.predict(x_test)         #在测试集上预测</div><div class="line">right = sum(predict == y_test)          #求预测正确的个数</div><div class="line"></div><div class="line">print (&apos;测试集准确率：%f%%&apos;%(right*100.0/predict.shape[0]))        #求在测试集上预测的正确率，shape[0]返回第一维的长度，即数据个数</div></pre></td></tr></table></figure><p><strong>[另：留一验证法]：</strong>–&gt;每次取一条数据作为测试集，其余作为训练集<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line">from sklearn import datasets</div><div class="line">from sklearn.svm import SVC</div><div class="line">import numpy as np</div><div class="line"></div><div class="line">def data_svc_test(data,target,index):</div><div class="line">    x_train = np.vstack((data[0:index],data[index+1:-1]))#除第index号之外的    数据为训练集</div><div class="line">    x_test = data[index].reshape(1,-1)                    #第index号数据为测试集，reshape(1,-1)的作用是只有一条数据时，使用reshap    e(1,-1)，否则有个过时方法的警告</div><div class="line">    y_train = np.hstack((target[0:index],target[index+1:-1]))</div><div class="line">    y_test = target[index]</div><div class="line">    model = SVC().fit(x_train,y_train)    #建立SVC模型</div><div class="line">    predict = model.predict(x_test)</div><div class="line">    </div><div class="line">    return predict == y_test        #返回结果是否预测正确</div><div class="line"></div><div class="line">#读取数据</div><div class="line">iris = datasets.load_iris()</div><div class="line">iris_data = iris.data</div><div class="line">iris_target = iris.target</div><div class="line">m = iris_target.shape[0]</div><div class="line"></div><div class="line">right = 0;</div><div class="line">for i in range(0,m):</div><div class="line">    right += data_svc_test(iris_data,iris_target,i)</div><div class="line">print (&quot;%f%%&quot;%(right*100.0/m))</div></pre></td></tr></table></figure></p><h3 id="4、房价预测-SVR–-gt-支持向量回归"><a href="#4、房价预测-SVR–-gt-支持向量回归" class="headerlink" title="4、房价预测(SVR–&gt;支持向量回归)"></a>4、房价预测(SVR–&gt;支持向量回归)</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">from sklearn import datasets</div><div class="line">from sklearn.svm import SVR     #引入支持向量回归所需的SVR模型</div><div class="line">from sklearn.cross_validation import train_test_split</div><div class="line">from sklearn.preprocessing import StandardScaler</div><div class="line">import numpy as np</div><div class="line"></div><div class="line">#加载数据</div><div class="line">house_dataset = datasets.load_boston()</div><div class="line">house_data = house_dataset.data</div><div class="line">house_price = house_dataset.target</div><div class="line">#数据预处理--&gt;归一化</div><div class="line">x_train,x_test,y_train,y_test =     train_test_split(house_data,house_price,test_size=0.2)  </div><div class="line">scaler = StandardScaler()</div><div class="line">scaler.fit(x_train)</div><div class="line">x_train = scaler.transform(x_train) #训练集</div><div class="line">x_test = scaler.transform(x_test)   #测试集</div><div class="line"></div><div class="line">#回归，预测</div><div class="line">model = SVR().fit(x_train,y_train)  #使用SVR回归拟合</div><div class="line">predict = model.predict(x_test)     #预测</div><div class="line">result = np.hstack((y_test.reshape(-1,1),predict.reshape(-1,1))) #reshape(-1,1)所有行转为1列向量</div><div class="line">print(result)</div></pre></td></tr></table></figure><h2 id="六、sk-learn模型总结"><a href="#六、sk-learn模型总结" class="headerlink" title="六、sk-learn模型总结"></a>六、sk-learn模型总结</h2><h3 id="0、数据处理"><a href="#0、数据处理" class="headerlink" title="0、数据处理"></a>0、数据处理</h3><h4 id="（1）均值归一化：from-sklearn-preprocessing-import-StandardScaler"><a href="#（1）均值归一化：from-sklearn-preprocessing-import-StandardScaler" class="headerlink" title="（1）均值归一化：from sklearn.preprocessing import StandardScaler"></a>（1）均值归一化：<code>from sklearn.preprocessing import StandardScaler</code></h4><ul><li>scaler = StandardScaler()</li><li>scaler.fit(X_train)</li><li>X_train = scaler.transform(X_train)</li></ul><h4 id="（2）分割数据：from-sklearn-cross-validation-import-train-test-split"><a href="#（2）分割数据：from-sklearn-cross-validation-import-train-test-split" class="headerlink" title="（2）分割数据：from sklearn.cross_validation import train_test_split"></a>（2）分割数据：<code>from sklearn.cross_validation import train_test_split</code></h4><ul><li><code>x_train,x_test,y_train,y_test =     train_test_split(iris_data,iris_target,test_size=0.2)</code></li></ul><h3 id="1、线性模型from-sklearn-import-linear-model"><a href="#1、线性模型from-sklearn-import-linear-model" class="headerlink" title="1、线性模型from sklearn import linear_model"></a>1、线性模型<code>from sklearn import linear_model</code></h3><h4 id="（1）逻辑回归模型"><a href="#（1）逻辑回归模型" class="headerlink" title="（1）逻辑回归模型"></a>（1）逻辑回归模型</h4><ul><li>linear_model.LogisticRegression()</li><li>重要参数<ul><li>C：正则化作用，默认值<code>1.0</code>，值越小，正则化作用<strong>越强</strong></li><li>max_iter：最大梯度下降执行次数，默认值<code>100</code></li><li>tol：停止执行的容忍度，默认值<code>1e-4</code></li></ul></li><li>重要返回值<ul><li>coef_：对应feature的<strong>系数</strong></li></ul></li></ul><h3 id="2、svm模型from-sklearn-import-svm"><a href="#2、svm模型from-sklearn-import-svm" class="headerlink" title="2、svm模型from sklearn import svm"></a>2、svm模型<code>from sklearn import svm</code></h3><h4 id="（1）分类模型"><a href="#（1）分类模型" class="headerlink" title="（1）分类模型"></a>（1）分类模型</h4><ul><li>svm.SVC()</li><li>重要参数<ul><li>kernel：使用的核函数，默认是<code>rbf</code>径向基函数，还有<code>linear，poly，sigmoid ，precomputed</code>核函数</li><li>C：正则化作用，默认值<code>1.0</code>，值越大，<code>margin</code>越大</li><li>tol：停止执行的容忍度，默认值<code>1e-4</code></li><li>gamma：为核函数的系数，值<strong>越大</strong>拟合的越好，默认是<code>1/feature的个数</code></li><li>degree：对应<code>poly</code>核函数</li></ul></li><li>重要返回值</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;一、Numpy&quot;&gt;&lt;a href=&quot;#一、Numpy&quot; class=&quot;headerlink&quot; title=&quot;一、Numpy&quot;&gt;&lt;/a&gt;一、Numpy&lt;/h2&gt;&lt;h3 id=&quot;1、Numpy特征和导入&quot;&gt;&lt;a href=&quot;#1、Numpy特征和导入&quot; class=&quot;headerlink&quot; title=&quot;1、Numpy特征和导入&quot;&gt;&lt;/a&gt;1、Numpy特征和导入&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;（1）用于多维数组的第三方Python包&lt;/li&gt;
&lt;li&gt;（2）更接近于底层和硬件 (高效)&lt;/li&gt;
&lt;li&gt;（3）专注于科学计算 (方便)&lt;/li&gt;
&lt;li&gt;（4）导入包：&lt;code&gt;import numpy as np&lt;/code&gt;     &lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="Python" scheme="http://lawlite.me/tags/Python/"/>
    
      <category term="机器学习" scheme="http://lawlite.me/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>搭建自己的VPN</title>
    <link href="http://lawlite.me/2016/11/05/%E6%90%AD%E5%BB%BA%E8%87%AA%E5%B7%B1%E7%9A%84VPN/"/>
    <id>http://lawlite.me/2016/11/05/搭建自己的VPN/</id>
    <published>2016-11-05T09:33:50.000Z</published>
    <updated>2017-06-25T08:50:27.306Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/assets/blog_images/vpn/vpn_setup_00.png" alt="google" title="vpn_setup_00"></p><h3 id="一、首先租一个服务器"><a href="#一、首先租一个服务器" class="headerlink" title="一、首先租一个服务器"></a>一、首先租一个服务器</h3><ul><li>1、租一个香港的服务器，这里我选的按量付费，如果不使用了释放就可以了，按小时收费的，不过要求你账户上要多于<code>100</code>块钱。<a id="more"></a></li><li>2、操作系统选择的64位<code>CentOS6.5</code>，<code>CentOS7</code>以上下面的命令会有所不同。<br><img src="/assets/blog_images/vpn/vpn_setup_02.png" alt="enter description here" title="vpn_setup_02.png"></li><li>3、创建成功后管理控制台会有公网和私网两个<code>ip</code>地址<br><img src="/assets/blog_images/vpn/vpn_setup_03.png" alt="enter description here" title="vpn_setup_03.png"></li></ul><h3 id="二、配置VPN"><a href="#二、配置VPN" class="headerlink" title="二、配置VPN"></a>二、配置VPN</h3><ul><li><p>1、安装<code>ppp</code>和<code>pptpd</code>:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">yum install ppp pptpd</div></pre></td></tr></table></figure></li><li><p>2、配置<code>DNS</code><br><code>/etc/ppp/options.pptpd</code>文件中<code>的ms-dns</code>配置为：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">ms-dns 8.8.8.8</div><div class="line">ms-dns 8.8.4.4</div></pre></td></tr></table></figure></li><li><p>3、配置<code>IP</code><br><code>/etc/pptpd.conf</code>文件中最后加入：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">localip 192.168.0.1</div><div class="line">remoteip 192.168.0.2-254</div></pre></td></tr></table></figure></li><li><p>4、配置<code>VPN</code>用户名和密码<br><code>/etc/ppp/chap-secrets</code>文件中加入：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">userName  pptpd  password  *</div></pre></td></tr></table></figure></li></ul><p>就是<code>userName</code>位置写上你的用户名<code>，password</code>位置写上你的密码</p><ul><li>5、配置IP转发<br><code>/etc/sysctl.conf</code>文件中<code>net.ipv4.ip_forward = 0</code>改为<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">net.ipv4.ip_forward = 1</div></pre></td></tr></table></figure></li></ul><p>然后执行：<code>sysctl -p</code>使其生效</p><h3 id="三、配置防火墙"><a href="#三、配置防火墙" class="headerlink" title="三、配置防火墙"></a>三、配置防火墙</h3><ul><li><p>1、加入防火墙规则</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">iptables -A INPUT -p TCP -i eth1 --dport  1723  --sport 1024:65534 -j ACCEPT</div><div class="line">iptables -t nat -A POSTROUTING -o eth1 -s 192.168.0.0/24 -j MASQUERADE</div><div class="line">iptables -I FORWARD -p tcp --syn -i ppp+ -j TCPMSS --set-mss 1356</div></pre></td></tr></table></figure><ul><li>注意这里指定的网卡是<code>eth1</code>，其对应外网的网卡，否则能够连上<code>VPN</code>，但是是访问不了外网的。</li><li><code>VPN</code>默认的端口是<code>1723</code> </li></ul></li><li><p>2、保存防火墙配置，启动<code>pptpd</code>，让其开机自启动</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">service iptables save</div><div class="line">service iptables restart</div><div class="line">service pptpd start </div><div class="line">chkconfig pptpd on</div></pre></td></tr></table></figure></li></ul><h3 id="四、测试"><a href="#四、测试" class="headerlink" title="四、测试"></a>四、测试</h3><p>1、<code>window</code>或手机等连接</p><ul><li><p>对应外网<code>IP</code>，设置的用户名和密码 </p></li><li><p>速度是可以的，<br><img src="/assets/blog_images/vpn/vpn_setup_04.png" alt="enter description here" title="vpn_setup_04.png"></p></li><li><p>我也测试了一下国外的服务器，速度非常慢，还不如免费的<code>VPN</code>软件，</p></li></ul><h3 id="五、shell脚本"><a href="#五、shell脚本" class="headerlink" title="五、shell脚本"></a>五、<code>shell</code>脚本</h3><ul><li><p>1、我写了一个简单的<code>shell</code>脚本放在了<code>github</code>上，<code>github</code>地址：<a href="https://github.com/lawlite19/Script" target="_blank" rel="external">https://github.com/lawlite19/Script</a></p></li><li><p>2、运行步骤如下：</p><ul><li>下载脚本：<code>wget https://raw.githubusercontent.com/lawlite19/Script/master/shell/vpn_setup.sh</code></li><li>添加执行权限：<code>chmod +x vpn_setup.sh</code></li><li>执行即可：<code>./vpn_setup.sh</code><br>3、完整代码：<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div></pre></td><td class="code"><pre><div class="line">#!/bin/bash</div><div class="line"></div><div class="line"># Author: Wang Yongzhi(bob)</div><div class="line"># Date:   2016.11.16</div><div class="line">echo -e &quot;-----------------------------------------------&quot;</div><div class="line">echo -e &quot;|                   Setup VPN...              |&quot;</div><div class="line">echo -e &quot;-----------------------------------------------\n&quot;</div><div class="line"></div><div class="line"># Step 1:install ppp and pptpd</div><div class="line"></div><div class="line">yum install -y ppp</div><div class="line">yum install -y pptpd</div><div class="line"></div><div class="line">if [ $? -eq 0 ]</div><div class="line">then</div><div class="line">    echo -e &quot;install ppp and pptpd Success!\n&quot;</div><div class="line">else</div><div class="line">    echo -e &quot;Sorry! install ppp and pptpd Failed!\n&quot;</div><div class="line">    exit 0</div><div class="line">fi</div><div class="line"></div><div class="line"># Step 2:configure pptpd DNS</div><div class="line">sed -i -e &apos;/#ms-dns 10.0.0.1/a\ms-dns 8.8.8.8&apos; /etc/ppp/options.pptpd</div><div class="line">sed -i -e &apos;/#ms-dns 10.0.0.2/a\ms-dns 8.8.4.4&apos; /etc/ppp/options.pptpd</div><div class="line"></div><div class="line">if [ $? -eq 0 ]</div><div class="line">then</div><div class="line">    echo -e &quot;Configure DNS Success!\n&quot;</div><div class="line">else</div><div class="line">    echo -e &quot;Configure DNS Failed!\n&quot;</div><div class="line">    exit 0</div><div class="line">fi</div><div class="line"></div><div class="line"></div><div class="line"># Step 3:configure pptpd IP</div><div class="line"></div><div class="line">echo  localip 192.168.0.1 &gt;&gt; /etc/pptpd.conf</div><div class="line">echo  remoteip 192.168.0.2-254 &gt;&gt; /etc/pptpd.conf</div><div class="line"></div><div class="line">if [ $? -eq 0 ]</div><div class="line">then</div><div class="line">    echo -e &quot;Configure pptpd IP Success!\n&quot;</div><div class="line">else</div><div class="line">    echo -e &quot;Configure pptpd IP Failed!\n&quot;</div><div class="line">    exit 0</div><div class="line">fi</div><div class="line"></div><div class="line"># Step 4: configure VPN userName and password</div><div class="line"></div><div class="line">while true</div><div class="line">do</div><div class="line">    read -p &quot;Please input userName:&quot; userName</div><div class="line">    read -p &quot;Please input passwd:  &quot; Passwd</div><div class="line">    echo $userNamepptpd$Passwd \* &gt;&gt; /etc/ppp/chap-secrets</div><div class="line">    read -p &quot;continue?y/N:         &quot; flag</div><div class="line">    if [ $flag = &quot;n&quot; -o $flag = &quot;N&quot; ]</div><div class="line">    then</div><div class="line">        break</div><div class="line">    fi</div><div class="line">done</div><div class="line"></div><div class="line"></div><div class="line"># Step 5: configure forwarding</div><div class="line"></div><div class="line">sed -i &apos;s/net.ipv4.ip_forward = 0/net.ipv4.ip_forward = 1/g&apos; /etc/sysctl.conf</div><div class="line"></div><div class="line">if [ $? -eq 0 ]</div><div class="line">then</div><div class="line">    echo -e &quot;Configure forwarding Success!\n&quot;</div><div class="line">else</div><div class="line">    echo -e &quot;Configure forwarding Failed\n&quot;</div><div class="line">    exit 0</div><div class="line">fi</div><div class="line"></div><div class="line">sysctl -p</div><div class="line"></div><div class="line"># Step 6: configure iptables</div><div class="line"></div><div class="line">#EXTIF=$(ifconfig | head -n 1 | grep -v lo | cut -d &apos; &apos; -f 1)</div><div class="line">iptables -A INPUT -p TCP -i eth1 --dport  1723  --sport 1024:65534 -j ACCEPT</div><div class="line">iptables -t nat -A POSTROUTING -o eth1 -s 192.168.0.0/24 -j MASQUERADE</div><div class="line">iptables -I FORWARD -p tcp --syn -i ppp+ -j TCPMSS --set-mss 1356</div><div class="line"></div><div class="line"># Step 7: configure when start server to start pptpd and iptables</div><div class="line"></div><div class="line">service iptables save</div><div class="line">service iptables restart</div><div class="line">service pptpd start </div><div class="line">chkconfig pptpd on</div><div class="line"></div><div class="line">echo -e &quot;Complete! Now you can connect the VPN throuth your computer or phone!\n&quot;</div><div class="line"></div><div class="line">echo &quot;                *****         *****&quot;</div><div class="line">echo &quot;              *********     *********&quot;</div><div class="line">echo &quot;            ************* *************&quot;</div><div class="line">echo &quot;           *****************************&quot;</div><div class="line">echo &quot;           *****************************&quot;</div><div class="line">echo &quot;           *****************************&quot;</div><div class="line">echo &quot;            ***************************&quot;</div><div class="line">echo &quot;              ***********************&quot;</div><div class="line">echo &quot;                *******************&quot;</div><div class="line">echo &quot;                  ***************&quot;</div><div class="line">echo &quot;                    ***********&quot;</div><div class="line">echo &quot;                      *******&quot;</div><div class="line">echo &quot;                        ***&quot;</div><div class="line">echo &quot;                         *&quot;</div></pre></td></tr></table></figure></li></ul></li></ul><h3 id="六、总结"><a href="#六、总结" class="headerlink" title="六、总结"></a>六、总结</h3><ul><li>最初是在租了一个国外的服务器测试的，没有问题，但是后来租用香港的服务器就出现的了错误，同样的系统、同样的配置，后来查看内网绑定的是网卡eth0,外网绑定的是网卡<code>eth1</code>，而我防火墙里设置的是内网的网卡<code>eth0</code>。而国外的那个服务器只要一个网卡，所以没有问题。另外练练<code>shell</code>脚本。 </li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;/assets/blog_images/vpn/vpn_setup_00.png&quot; alt=&quot;google&quot; title=&quot;vpn_setup_00&quot;&gt;&lt;/p&gt;
&lt;h3 id=&quot;一、首先租一个服务器&quot;&gt;&lt;a href=&quot;#一、首先租一个服务器&quot; class=&quot;headerlink&quot; title=&quot;一、首先租一个服务器&quot;&gt;&lt;/a&gt;一、首先租一个服务器&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;1、租一个香港的服务器，这里我选的按量付费，如果不使用了释放就可以了，按小时收费的，不过要求你账户上要多于&lt;code&gt;100&lt;/code&gt;块钱。
    
    </summary>
    
    
      <category term="翻墙" scheme="http://lawlite.me/tags/%E7%BF%BB%E5%A2%99/"/>
    
  </entry>
  
  <entry>
    <title>Scrapy爬虫框架模板</title>
    <link href="http://lawlite.me/2016/10/09/Python%E7%88%AC%E8%99%AB-Scrapy/"/>
    <id>http://lawlite.me/2016/10/09/Python爬虫-Scrapy/</id>
    <published>2016-10-09T05:42:56.000Z</published>
    <updated>2017-06-25T08:49:09.925Z</updated>
    
    <content type="html"><![CDATA[<h2 id="说明"><a href="#说明" class="headerlink" title="说明"></a>说明</h2><ul><li>github地址：<a href="https://github.com/lawlite19/PythonCrawler-Scrapy-Mysql-File-Template" target="_blank" rel="external">https://github.com/lawlite19/PythonCrawler-Scrapy-Mysql-File-Template</a></li><li><p>使用scrapy爬虫框架将数据保存Mysql数据库和文件中</p><a id="more"></a><h2 id="settings-py"><a href="#settings-py" class="headerlink" title="settings.py"></a>settings.py</h2><ul><li><p>修改Mysql的配置信息</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">#Mysql数据库的配置信息</div><div class="line">MYSQL_HOST = <span class="string">'127.0.0.1'</span></div><div class="line">MYSQL_DBNAME = <span class="string">'testdb'</span>         #数据库名字，请修改</div><div class="line">MYSQL_USER = <span class="string">'root'</span>             #数据库账号，请修改 </div><div class="line">MYSQL_PASSWD = <span class="string">'123456'</span>         #数据库密码，请修改</div><div class="line"></div><div class="line">MYSQL_PORT = <span class="number">3306</span>               #数据库端口，在dbhelper中使用</div></pre></td></tr></table></figure></li><li><p>指定pipelines</p></li></ul></li></ul><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">ITEM_PIPELINES = &#123;</div><div class="line">    <span class="string">'webCrawler_scrapy.pipelines.WebcrawlerScrapyPipeline'</span>: <span class="number">300</span>,#保存到mysql数据库</div><div class="line">    <span class="string">'webCrawler_scrapy.pipelines.JsonWithEncodingPipeline'</span>: <span class="number">300</span>,#保存到文件中</div><div class="line">&#125;</div></pre></td></tr></table></figure><h2 id="items-py"><a href="#items-py" class="headerlink" title="items.py"></a>items.py</h2><ul><li>声明需要格式化处理的字段</li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">class WebcrawlerScrapyItem(scrapy.Item):</div><div class="line">    '''定义需要格式化的内容（或是需要保存到数据库的字段）'''</div><div class="line">    # define the fields for your item here like:</div><div class="line">    # name = scrapy.Field()</div><div class="line">    name = scrapy.Field()   #修改你所需要的字段</div><div class="line">    url = scrapy.Field()</div></pre></td></tr></table></figure><h2 id="pipelines-py"><a href="#pipelines-py" class="headerlink" title="pipelines.py"></a>pipelines.py</h2><h3 id="一、保存到数据库的类WebcrawlerScrapyPipeline（在settings中声明）"><a href="#一、保存到数据库的类WebcrawlerScrapyPipeline（在settings中声明）" class="headerlink" title="一、保存到数据库的类WebcrawlerScrapyPipeline（在settings中声明）"></a>一、保存到数据库的类<code>WebcrawlerScrapyPipeline</code>（在settings中声明）</h3><ul><li>定义一个类方法<code>from_settings</code>，得到settings中的Mysql数据库配置信息，得到数据库连接池dbpool</li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">@classmethod</div><div class="line">def from_settings(cls,settings):</div><div class="line">    '''1、@classmethod声明一个类方法，而对于平常我们见到的则叫做实例方法。 </div><div class="line">       2、类方法的第一个参数cls（class的缩写，指这个类本身），而实例方法的第一个参数是self，表示该类的一个实例</div><div class="line">       3、可以通过类来调用，就像C.f()，相当于java中的静态方法'''</div><div class="line">    dbparams=dict(</div><div class="line">        host=settings['MYSQL_HOST'],#读取settings中的配置</div><div class="line">        db=settings['MYSQL_DBNAME'],</div><div class="line">        user=settings['MYSQL_USER'],</div><div class="line">        passwd=settings['MYSQL_PASSWD'],</div><div class="line">        charset='utf8',#编码要加上，否则可能出现中文乱码问题</div><div class="line">        cursorclass=MySQLdb.cursors.DictCursor,</div><div class="line">        use_unicode=False,</div><div class="line">    )</div><div class="line">    dbpool=adbapi.ConnectionPool('MySQLdb',**dbparams)#**表示将字典扩展为关键字参数,相当于host=xxx,db=yyy....</div><div class="line">    return cls(dbpool)#相当于dbpool付给了这个类，self中可以得到</div></pre></td></tr></table></figure><ul><li><code>__init__</code>中会得到连接池dbpool</li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">def __init__(self,dbpool):</div><div class="line">    self.dbpool=dbpool</div></pre></td></tr></table></figure><ul><li><code>process_item</code>方法是pipeline默认调用的，进行数据库操作</li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">#pipeline默认调用</div><div class="line">def process_item(self, item, spider):</div><div class="line">    query=self.dbpool.runInteraction(self._conditional_insert,item)#调用插入的方法</div><div class="line">    query.addErrback(self._handle_error,item,spider)#调用异常处理方法</div><div class="line">    return item</div></pre></td></tr></table></figure><ul><li>插入数据库方法<code>_conditional_insert</code></li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">#写入数据库中</div><div class="line">def _conditional_insert(self,tx,item):</div><div class="line">    #print item['name']</div><div class="line">    sql="insert into testpictures(name,url) values(%s,%s)"</div><div class="line">    params=(item["name"],item["url"])</div><div class="line">    tx.execute(sql,params)</div></pre></td></tr></table></figure><ul><li>错误处理方法<code>_handle_error</code></li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">#错误处理方法</div><div class="line">def _handle_error(self, failue, item, spider):</div><div class="line">    print failue</div></pre></td></tr></table></figure><h3 id="二、保存到文件中的类JsonWithEncodingPipeline（在settings中声明）"><a href="#二、保存到文件中的类JsonWithEncodingPipeline（在settings中声明）" class="headerlink" title="二、保存到文件中的类JsonWithEncodingPipeline（在settings中声明）"></a>二、保存到文件中的类<code>JsonWithEncodingPipeline</code>（在settings中声明）</h3><ul><li>保存为json格式的文件，比较简单，代码如下</li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">class JsonWithEncodingPipeline(object):</div><div class="line">    '''保存到文件中对应的class</div><div class="line">       1、在settings.py文件中配置</div><div class="line">       2、在自己实现的爬虫类中yield item,会自动执行'''    </div><div class="line">    def __init__(self):</div><div class="line">        self.file = codecs.open('info.json', 'w', encoding='utf-8')#保存为json文件</div><div class="line">    def process_item(self, item, spider):</div><div class="line">        line = json.dumps(dict(item)) + "\n"#转为json的</div><div class="line">        self.file.write(line)#写入文件中</div><div class="line">        return item</div><div class="line">    def spider_closed(self, spider):#爬虫结束时关闭文件</div><div class="line">        self.file.close()</div></pre></td></tr></table></figure><h2 id="dbhelper-py"><a href="#dbhelper-py" class="headerlink" title="dbhelper.py"></a>dbhelper.py</h2><ul><li>自己实现的操作Mysql数据库的类</li><li><strong>init</strong>方法，获取settings配置文件中的信息</li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">def __init__(self):</div><div class="line">    self.settings=get_project_settings() #获取settings配置，设置需要的信息</div><div class="line">    </div><div class="line">    self.host=self.settings['MYSQL_HOST']</div><div class="line">    self.port=self.settings['MYSQL_PORT']</div><div class="line">    self.user=self.settings['MYSQL_USER']</div><div class="line">    self.passwd=self.settings['MYSQL_PASSWD']</div><div class="line">    self.db=self.settings['MYSQL_DBNAME']</div></pre></td></tr></table></figure><ul><li>连接到Mysql</li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">#连接到mysql，不是连接到具体的数据库</div><div class="line">def connectMysql(self):</div><div class="line">    conn=MySQLdb.connect(host=self.host,</div><div class="line">                         port=self.port,</div><div class="line">                         user=self.user,</div><div class="line">                         passwd=self.passwd,</div><div class="line">                         #db=self.db,不指定数据库名</div><div class="line">                         charset='utf8') #要指定编码，否则中文可能乱码</div><div class="line">    return conn</div></pre></td></tr></table></figure><ul><li>连接到settings配置文件中的数据库名（MYSQL_DBNAME）</li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">#连接到具体的数据库（settings中设置的MYSQL_DBNAME）</div><div class="line">def connectDatabase(self):</div><div class="line">    conn=MySQLdb.connect(host=self.host,</div><div class="line">                         port=self.port,</div><div class="line">                         user=self.user,</div><div class="line">                         passwd=self.passwd,</div><div class="line">                         db=self.db,</div><div class="line">                         charset='utf8') #要指定编码，否则中文可能乱码</div><div class="line">    return conn</div></pre></td></tr></table></figure><ul><li>创建数据库（settings文件中配置的数据库名）</li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">#创建数据库</div><div class="line">def createDatabase(self):</div><div class="line">    '''因为创建数据库直接修改settings中的配置MYSQL_DBNAME即可，所以就不要传sql语句了'''</div><div class="line">    conn=self.connectMysql()#连接数据库</div><div class="line">    </div><div class="line">    sql="create database if not exists "+self.db</div><div class="line">    cur=conn.cursor()</div><div class="line">    cur.execute(sql)#执行sql语句</div><div class="line">    cur.close()</div><div class="line">    conn.close()</div></pre></td></tr></table></figure><ul><li>还有一些数据库操作方法传入sql语句和参数即可（具体看代码）</li></ul><h2 id="实现具体的爬虫-py（即模板中的pictureSpider-demo-py文件）"><a href="#实现具体的爬虫-py（即模板中的pictureSpider-demo-py文件）" class="headerlink" title="实现具体的爬虫.py（即模板中的pictureSpider_demo.py文件）"></a>实现具体的爬虫.py（即模板中的<code>pictureSpider_demo.py</code>文件）</h2><ul><li>继承<code>scrapy.spiders.Spider</code> 类</li><li>声明三个属性</li></ul><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">name=<span class="string">"webCrawler_scrapy"</span>    #定义爬虫名，要和settings中的BOT_NAME属性对应的值一致</div><div class="line"></div><div class="line">allowed_domains=[<span class="string">"desk.zol.com.cn"</span>] #搜索的域名范围，也就是爬虫的约束区域，规定爬虫只爬取这个域名下的网页</div><div class="line"></div><div class="line">start_urls=[<span class="string">"http://desk.zol.com.cn/fengjing/1920x1080/1.html"</span>]   #开始爬取的地址</div></pre></td></tr></table></figure><ul><li>实现<code>parse</code>方法，该函数名不能改变，因为Scrapy源码中默认callback函数的函数名就是parse</li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">def parse(self, response):</div></pre></td></tr></table></figure><ul><li>返回item</li></ul><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">item=WebcrawlerScrapyItem()  #实例item（具体定义的item类）,将要保存的值放到事先声明的item属性中</div><div class="line">item[<span class="string">'name'</span>]=file_name </div><div class="line">item[<span class="string">'url'</span>]=realUrl</div><div class="line">print item[<span class="string">"name"</span>],item[<span class="string">"url"</span>]    </div><div class="line">                </div><div class="line">yield item  #返回item,这时会自定解析item</div></pre></td></tr></table></figure><h2 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h2><ul><li>测试DBHelper<br>创建testdb数据库和testtable表</li></ul><p><img src="/assets/blog_images/scrapy_images/testDBHelper.gif" alt="创建testdb数据库和testtable表" title="testDBHelper.gif"></p><ul><li>测试爬虫<ul><li>在D盘建立文件夹pics; 图片自动保存到该文件夹中。</li><li><code>scrapy crawl webCrawler_scrapy</code>运行爬虫后会将爬取得图片保存到本地，并且将name和url保存到数据库中</li></ul></li></ul><p><img src="/assets/blog_images/scrapy_images/testCrawl.gif" alt="测试爬虫" title="testCrawl.gif"></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;说明&quot;&gt;&lt;a href=&quot;#说明&quot; class=&quot;headerlink&quot; title=&quot;说明&quot;&gt;&lt;/a&gt;说明&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;github地址：&lt;a href=&quot;https://github.com/lawlite19/PythonCrawler-Scrapy-Mysql-File-Template&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://github.com/lawlite19/PythonCrawler-Scrapy-Mysql-File-Template&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;使用scrapy爬虫框架将数据保存Mysql数据库和文件中&lt;/p&gt;
    
    </summary>
    
    
      <category term="Python" scheme="http://lawlite.me/tags/Python/"/>
    
      <category term="爬虫" scheme="http://lawlite.me/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>算法练习</title>
    <link href="http://lawlite.me/2016/09/09/%E7%AE%97%E6%B3%95%E7%BB%83%E4%B9%A0/"/>
    <id>http://lawlite.me/2016/09/09/算法练习/</id>
    <published>2016-09-09T05:35:45.000Z</published>
    <updated>2017-06-25T08:51:00.473Z</updated>
    
    <content type="html"><![CDATA[<h2 id="说明"><a href="#说明" class="headerlink" title="说明"></a>说明</h2><ul><li>github地址：<a href="https://github.com/lawlite19/AlgorithmExercises" target="_blank" rel="external">https://github.com/lawlite19/AlgorithmExercises</a><h2 id="一、-排序算法"><a href="#一、-排序算法" class="headerlink" title="一、 排序算法"></a>一、 排序算法</h2><h3 id="1-交换排序"><a href="#1-交换排序" class="headerlink" title="1. 交换排序"></a>1. 交换排序</h3><ul><li><a href="https://github.com/lawlite19/AlgorithmExerises/blob/master/一、排序算法/1.交换排序/冒泡排序.cpp" target="_blank" rel="external">冒泡排序</a>      </li></ul></li><li><a href="https://github.com/lawlite19/AlgorithmExercises/blob/master/一、排序算法/1.交换排序/冒泡排序改进1.cpp" target="_blank" rel="external">冒泡排序改进1</a></li><li><a href="https://github.com/lawlite19/AlgorithmExercises/blob/master/一、排序算法/1.交换排序/冒泡排序改进2.cpp" target="_blank" rel="external">冒泡排序改进2</a></li><li><a href="https://github.com/lawlite19/AlgorithmExercises/blob/master/一、排序算法/1.交换排序/冒泡排序改进3.cpp" target="_blank" rel="external">冒泡排序改进3</a><ul><li><a href="https://github.com/lawlite19/AlgorithmExercises/blob/master/一、排序算法/1.交换排序/快速排序.cpp" target="_blank" rel="external">快速排序</a>     </li></ul></li></ul><a id="more"></a><h3 id="2-插入排序"><a href="#2-插入排序" class="headerlink" title="2. 插入排序"></a>2. 插入排序</h3><ul><li><a href="https://github.com/lawlite19/AlgorithmExercises/blob/master/一、排序算法/2.插入排序/直接插入排序.cpp" target="_blank" rel="external">直接插入排序</a><ul><li><a href="https://github.com/lawlite19/AlgorithmExercises/blob/master/一、排序算法/2.插入排序/直接插入排序递归版.cpp" target="_blank" rel="external">直接插入排序递归版</a></li></ul></li><li><a href="https://github.com/lawlite19/AlgorithmExercises/blob/master/一、排序算法/2.插入排序/希尔排序.cpp" target="_blank" rel="external">希尔排序</a></li></ul><h3 id="3-选择排序"><a href="#3-选择排序" class="headerlink" title="3. 选择排序"></a>3. 选择排序</h3><ul><li><a href="https://github.com/lawlite19/AlgorithmExercises/blob/master/一、排序算法/3.选择排序/简单选择排序.cpp" target="_blank" rel="external">简单选择排序</a><ul><li><a href="https://github.com/lawlite19/AlgorithmExercises/blob/master/一、排序算法/3.选择排序/简单选择排序改进.cpp" target="_blank" rel="external">二元选择排序</a></li></ul></li><li><a href="https://github.com/lawlite19/AlgorithmExercises/blob/master/一、排序算法/3.选择排序/堆排序.cpp" target="_blank" rel="external">堆排序</a></li></ul><h3 id="4-归并排序"><a href="#4-归并排序" class="headerlink" title="4. 归并排序"></a>4. 归并排序</h3><ul><li><a href="https://github.com/lawlite19/AlgorithmExercises/blob/master/一、排序算法/4.归并排序/二路归并排序_递归版.cpp" target="_blank" rel="external">二路归并排序递归版</a></li><li><a href="https://github.com/lawlite19/AlgorithmExercises/blob/master/一、排序算法/4.归并排序/二路归并排序_非递归.cpp" target="_blank" rel="external">二路归并排序非递归版</a></li></ul><h2 id="二、-字符串"><a href="#二、-字符串" class="headerlink" title="二、 字符串"></a>二、 字符串</h2><h3 id="1-字符串旋转"><a href="#1-字符串旋转" class="headerlink" title="1. 字符串旋转"></a>1. 字符串旋转</h3><ul><li><a href="https://github.com/lawlite19/AlgorithmExercises/blob/master/二、字符串/1.字符串旋转/字符串旋转_暴力法.cpp" target="_blank" rel="external">字符串旋转_暴力法</a></li><li><a href="https://github.com/lawlite19/AlgorithmExercises/blob/master/二、字符串/1.字符串旋转/字符串旋转_三步翻转法.cpp" target="_blank" rel="external">字符串旋转_三步翻转法</a></li></ul><h3 id="2-字符串包含"><a href="#2-字符串包含" class="headerlink" title="2. 字符串包含"></a>2. 字符串包含</h3><ul><li><a href="https://github.com/lawlite19/AlgorithmExercises/blob/master/二、字符串/2.字符串包含判断/字符串包含判断_暴力法.cpp" target="_blank" rel="external">字符串包含判断_遍历</a></li><li><a href="https://github.com/lawlite19/AlgorithmExercises/blob/master/二、字符串/2.字符串包含判断/字符串包含_排序法.cpp" target="_blank" rel="external">字符串包含判断_排序</a></li><li><a href="https://github.com/lawlite19/AlgorithmExercises/blob/master/二、字符串/2.字符串包含判断/字符串包含判断_素数.cpp" target="_blank" rel="external">字符串包含判断_素数乘积</a></li><li><a href="https://github.com/lawlite19/AlgorithmExercises/blob/master/二、字符串/2.字符串包含判断/字符串包含判断_哈希.cpp" target="_blank" rel="external">字符串包含判断_哈希</a>   ★★★    </li></ul><h3 id="3-回文"><a href="#3-回文" class="headerlink" title="3. 回文"></a>3. 回文</h3><ul><li><a href="https://github.com/lawlite19/AlgorithmExercises/blob/master/二、字符串/3.回文判断/回文判断.cpp" target="_blank" rel="external">回文判断</a></li></ul><h3 id="4-最长回文子串长度"><a href="#4-最长回文子串长度" class="headerlink" title="4. 最长回文子串长度"></a>4. 最长回文子串长度</h3><ul><li><a href="https://github.com/lawlite19/AlgorithmExercises/blob/master/二、字符串/4.最长回文子串/最长回文子串_一般解法.cpp" target="_blank" rel="external">最长回文子串长度_一般解法</a></li><li><a href="https://github.com/lawlite19/AlgorithmExercises/blob/master/二、字符串/4.最长回文子串/最长回文子串_Manacher.cpp" target="_blank" rel="external">最长回文子串长度_Manacher</a>  ★★★   <ul><li><a href="https://github.com/lawlite19/AlgorithmExercises/blob/master/零、算法说明/Manacher.md" target="_blank" rel="external">算法说明</a>    </li></ul></li></ul><h3 id="5-全排列"><a href="#5-全排列" class="headerlink" title="5. 全排列"></a>5. 全排列</h3><ul><li><a href="https://github.com/lawlite19/AlgorithmExercises/blob/master/二、字符串/5.全排列/全排列_递归.cpp" target="_blank" rel="external">全排列_递归</a> </li><li><a href="https://github.com/lawlite19/AlgorithmExercises/blob/master/二、字符串/5.全排列/全排列_字典序排列.cpp" target="_blank" rel="external">全排列_字典序排列</a></li><li><a href="https://github.com/lawlite19/AlgorithmExercises/blob/master/二、字符串/5.全排列/字典序全排列.cpp" target="_blank" rel="external">字典序全排列</a></li></ul><h3 id="6-变形词"><a href="#6-变形词" class="headerlink" title="6. 变形词"></a>6. 变形词</h3><ul><li><a href="https://github.com/lawlite19/AlgorithmExercises/blob/master/二、字符串/6.变形词/变形词判断.cpp" target="_blank" rel="external">变形词判断</a></li></ul><h3 id="7-字符串中数字串之和"><a href="#7-字符串中数字串之和" class="headerlink" title="7. 字符串中数字串之和"></a>7. 字符串中数字串之和</h3><ul><li><a href="https://github.com/lawlite19/AlgorithmExercises/blob/master/二、字符串/7.字符串中数字串求和/字符串中数字串求和.cpp" target="_blank" rel="external">字符串中数字串之和</a> ★  </li></ul><h3 id="8-去除字符串中连续K个0串"><a href="#8-去除字符串中连续K个0串" class="headerlink" title="8. 去除字符串中连续K个0串"></a>8. 去除字符串中连续K个0串</h3><ul><li><a href="https://github.com/lawlite19/AlgorithmExercises/blob/master/二、字符串/8.去除字符串中连续K个0子串/去除字符串中连续K个0子串.cpp" target="_blank" rel="external">去除字符串中连续K个0串</a></li></ul><h3 id="9-整数字符串转整数值"><a href="#9-整数字符串转整数值" class="headerlink" title="9. 整数字符串转整数值"></a>9. 整数字符串转整数值</h3><ul><li><a href="https://github.com/lawlite19/AlgorithmExercises/blob/master/二、字符串/9.整数字符串转为整数值/整数字符串转为整数值.cpp" target="_blank" rel="external">整数字符串转整数值</a> ★★   </li></ul><h3 id="10-字符串匹配问题"><a href="#10-字符串匹配问题" class="headerlink" title="10. 字符串匹配问题"></a>10. 字符串匹配问题</h3><ul><li><a href="https://github.com/lawlite19/AlgorithmExercises/blob/master/二、字符串/10.字符串匹配问题/字符串匹配_KMP.cpp" target="_blank" rel="external">字符串匹配_KMP</a>  ★★★★★<ul><li><a href="https://github.com/lawlite19/AlgorithmExercises/blob/master/零、算法说明/KMP.md" target="_blank" rel="external">算法说明</a></li></ul></li></ul><h2 id="三、-数组和矩阵"><a href="#三、-数组和矩阵" class="headerlink" title="三、 数组和矩阵"></a>三、 数组和矩阵</h2><h3 id="1-二维数组查找"><a href="#1-二维数组查找" class="headerlink" title="1. 二维数组查找"></a>1. 二维数组查找</h3><ul><li><a href="https://github.com/lawlite19/AlgorithmExercises/blob/master/三、数组和矩阵/1.查找/二维数组的查找.cpp" target="_blank" rel="external">二维数组查找</a></li></ul><h3 id="2-矩阵相关操作"><a href="#2-矩阵相关操作" class="headerlink" title="2. 矩阵相关操作"></a>2. 矩阵相关操作</h3><ul><li><a href="https://github.com/lawlite19/AlgorithmExercises/blob/master/三、数组和矩阵/2.矩阵操作/转圈打印矩阵.cpp" target="_blank" rel="external">转圈打印矩阵</a></li></ul><h3 id="3-最小的k个元素"><a href="#3-最小的k个元素" class="headerlink" title="3. 最小的k个元素"></a>3. 最小的k个元素</h3><ul><li><a href="https://github.com/lawlite19/AlgorithmExercises/blob/master/三、数组和矩阵/3.最小的K个数/最小的K个数_堆.cpp" target="_blank" rel="external">最小的k个元素_堆</a></li><li><a href="https://github.com/lawlite19/AlgorithmExercises/blob/master/三、数组和矩阵/3.最小的K个数/最小的K个数_BFPRT算法.cpp" target="_blank" rel="external">最小的k个元素_BFPRT</a>  ★★★★★<ul><li><a href="https://github.com/lawlite19/AlgorithmExercises/blob/master/零、算法说明/BFPRT.md" target="_blank" rel="external">算法说明</a></li></ul></li></ul><h3 id="4-中间数"><a href="#4-中间数" class="headerlink" title="4.中间数"></a>4.中间数</h3><ul><li><a href="https://github.com/lawlite19/AlgorithmExercises/blob/master/三、数组和矩阵/4.中间数/中间数_辅助数组.cpp" target="_blank" rel="external">中间数_辅助数组</a>  ★ </li></ul><h3 id="5-非负数组和为K的最长子数组"><a href="#5-非负数组和为K的最长子数组" class="headerlink" title="5.非负数组和为K的最长子数组"></a>5.非负数组和为K的最长子数组</h3><ul><li><a href="https://github.com/lawlite19/AlgorithmExercises/blob/master/三、数组和矩阵/5.非负数组和为K的最长子数组/非负数组和为K的最长子数组_双指针.cpp" target="_blank" rel="external">非负数组和为K的最长子数组_双指针</a>  ★★★</li></ul><h3 id="8-次数出现大于N-K的数"><a href="#8-次数出现大于N-K的数" class="headerlink" title="8.次数出现大于N/K的数"></a>8.次数出现大于N/K的数</h3><ul><li><a href="https://github.com/lawlite19/AlgorithmExercises/blob/master/三、数组和矩阵/8.数组中出现次数大于N除以K的数/出现次数大于一半的数.cpp" target="_blank" rel="external">次数出现大于N/2的数</a>  ★ </li><li><a href="https://github.com/lawlite19/AlgorithmExercises/blob/master/三、数组和矩阵/8.数组中出现次数大于N除以K的数/出现次数大于N除以K的数.cpp" target="_blank" rel="external">次数出现大于N/K的数</a>  ★★★</li></ul><h3 id="9-逆序对"><a href="#9-逆序对" class="headerlink" title="9.逆序对"></a>9.逆序对</h3><ul><li><a href="https://github.com/lawlite19/AlgorithmExercises/blob/master/三、数组和矩阵/9.逆序对/逆序对_分治(归并).cpp" target="_blank" rel="external">逆序对数_分治归并</a>★ </li></ul><h3 id="10-两个有序数组的中位数"><a href="#10-两个有序数组的中位数" class="headerlink" title="10.两个有序数组的中位数"></a>10.两个有序数组的中位数</h3><ul><li><a href="https://github.com/lawlite19/AlgorithmExercises/blob/master/三、数组和矩阵/10.两个有序数组的中位数/两个有序数组的中位数_分治.cpp" target="_blank" rel="external">两个有序数组的中位数_分治</a>★★★★<ul><li><a href="https://github.com/lawlite19/AlgorithmExercises/blob/master/零、算法说明/MedianOfTwoSortedArrays.md" target="_blank" rel="external">算法说明</a></li></ul></li></ul><h2 id="四、-递归和动态规划"><a href="#四、-递归和动态规划" class="headerlink" title="四、 递归和动态规划"></a>四、 递归和动态规划</h2><h3 id="1-斐波那契问题"><a href="#1-斐波那契问题" class="headerlink" title="1. 斐波那契问题"></a>1. 斐波那契问题</h3><ul><li><a href="https://github.com/lawlite19/AlgorithmExercises/blob/master/四、递归和动态规划/1.斐波那契问题/矩形覆盖_递归.cpp" target="_blank" rel="external">矩形覆盖_递归</a></li><li><a href="https://github.com/lawlite19/AlgorithmExercises/blob/master/四、递归和动态规划/1.斐波那契问题/矩阵覆盖_dp.cpp" target="_blank" rel="external">矩形覆盖_dp</a>  ★ </li><li><a href="https://github.com/lawlite19/AlgorithmExercises/blob/master/四、递归和动态规划/1.斐波那契问题/矩阵覆盖_矩阵转化_class.cpp" target="_blank" rel="external">矩阵覆盖_矩阵转化_class实现</a>  ★★★</li><li><a href="https://github.com/lawlite19/AlgorithmExercises/blob/master/四、递归和动态规划/1.斐波那契问题/矩阵覆盖_矩阵转化_vector.cpp" target="_blank" rel="external">矩阵覆盖_矩阵转化_vector实现</a>  ★★★<ul><li><a href="https://github.com/lawlite19/AlgorithmExercises/blob/master/零、算法说明/Fibonacci.md" target="_blank" rel="external">算法说明</a></li></ul></li><li><a href="https://github.com/lawlite19/AlgorithmExercises/blob/master/四、递归和动态规划/1.斐波那契问题/爬楼梯问题_递归.cpp" target="_blank" rel="external">爬楼梯_递归</a></li><li><p><a href="https://github.com/lawlite19/AlgorithmExercises/blob/master/四、递归和动态规划/1.斐波那契问题/爬楼梯问题_dp.cpp" target="_blank" rel="external">爬楼梯_dp</a>  ★ </p></li><li><p><a href="https://github.com/lawlite19/AlgorithmExercises/blob/master/四、递归和动态规划/1.斐波那契问题/变态跳台阶_递归.cpp" target="_blank" rel="external">变态跳台阶_递归</a></p></li><li><a href="https://github.com/lawlite19/AlgorithmExercises/blob/master/四、递归和动态规划/1.斐波那契问题/变态跳台阶_计算.cpp" target="_blank" rel="external">变态跳台阶_直接计算</a>  ★ </li></ul><h3 id="2-最大子数组和相关问题"><a href="#2-最大子数组和相关问题" class="headerlink" title="2. 最大子数组和相关问题"></a>2. 最大子数组和相关问题</h3><ul><li><a href="https://github.com/lawlite19/AlgorithmExercises/blob/master/四、递归和动态规划/2.最大子数组和相关问题/最大子数组和.cpp" target="_blank" rel="external">最大子数组和_dp</a>  ★ </li><li><a href="https://github.com/lawlite19/AlgorithmExercises/blob/master/四、递归和动态规划/2.最大子数组和相关问题/两个子数组最大和.cpp" target="_blank" rel="external">两个不相容子数组最大和_辅助数组</a> ★★   </li></ul><h3 id="3-最长递增子序列相关问题"><a href="#3-最长递增子序列相关问题" class="headerlink" title="3. 最长递增子序列相关问题"></a>3. 最长递增子序列相关问题</h3><ul><li><a href="https://github.com/lawlite19/AlgorithmExercises/blob/master/四、递归和动态规划/3.最长递增子序列相关问题/最长递增子序列_一般dp.cpp" target="_blank" rel="external">最长递增子序列_一般dp</a></li><li><a href="https://github.com/lawlite19/AlgorithmExercises/blob/master/四、递归和动态规划/3.最长递增子序列相关问题/最长递增子序列_dp优化.cpp" target="_blank" rel="external">最长递增子序列_dp优化</a> ★★   </li><li><a href="https://github.com/lawlite19/AlgorithmExercises/blob/master/四、递归和动态规划/3.最长递增子序列相关问题/摞数组问题_自定义class.cpp" target="_blank" rel="external">摞数组问题(俄国沙皇问题)_纯代码实现</a> ★★★★</li><li><a href="https://github.com/lawlite19/AlgorithmExercises/blob/master/四、递归和动态规划/3.最长递增子序列相关问题/摞数组问题_借助stl.cpp" target="_blank" rel="external">摞数组问题（俄国沙皇问题）_借助stl</a> ★★★★</li></ul><h2 id="五、-栈和队列"><a href="#五、-栈和队列" class="headerlink" title="五、 栈和队列"></a>五、 栈和队列</h2><h3 id="1-getMin功能栈"><a href="#1-getMin功能栈" class="headerlink" title="1. getMin功能栈"></a>1. getMin功能栈</h3><ul><li><a href="https://github.com/lawlite19/AlgorithmExercises/blob/master/五、栈和队列/1.getMin功能栈/getMin功能栈_方案1.cpp" target="_blank" rel="external">getMin功能栈_方案1</a></li><li><a href="https://github.com/lawlite19/AlgorithmExercises/blob/master/五、栈和队列/1.getMin功能栈/getMin功能栈_方案2.cpp" target="_blank" rel="external">getMin功能栈_方案2</a></li></ul><h3 id="2-两个栈实现队列功能"><a href="#2-两个栈实现队列功能" class="headerlink" title="2. 两个栈实现队列功能"></a>2. 两个栈实现队列功能</h3><ul><li><a href="https://github.com/lawlite19/AlgorithmExercises/blob/master/五、栈和队列/2.两个栈实现队列功能/两个栈实现队列.cpp" target="_blank" rel="external">两个栈实现队列</a></li></ul><h2 id="七、二叉树"><a href="#七、二叉树" class="headerlink" title="七、二叉树"></a>七、二叉树</h2><h3 id="1-遍历"><a href="#1-遍历" class="headerlink" title="1. 遍历"></a>1. 遍历</h3><ul><li><a href="https://github.com/lawlite19/AlgorithmExercises/blob/master/七、二叉树/1.二叉树的遍历/二叉树先中后序遍历_递归.cpp" target="_blank" rel="external">先、中、后序遍历_递归</a></li><li><a href="https://github.com/lawlite19/AlgorithmExercises/blob/master/七、二叉树/1.二叉树的遍历/二叉树先中后序遍历_非递归.cpp" target="_blank" rel="external">先、中、后序遍历_非递归</a> ★★   </li></ul><h2 id="八、位运算"><a href="#八、位运算" class="headerlink" title="八、位运算"></a>八、位运算</h2><ul><li><a href="https://github.com/lawlite19/AlgorithmExercises/blob/master/八、位运算/1.出现奇数次的数/出现奇数次的数.cpp" target="_blank" rel="external">出现奇数次的数</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;说明&quot;&gt;&lt;a href=&quot;#说明&quot; class=&quot;headerlink&quot; title=&quot;说明&quot;&gt;&lt;/a&gt;说明&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;github地址：&lt;a href=&quot;https://github.com/lawlite19/AlgorithmExercises&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://github.com/lawlite19/AlgorithmExercises&lt;/a&gt;&lt;h2 id=&quot;一、-排序算法&quot;&gt;&lt;a href=&quot;#一、-排序算法&quot; class=&quot;headerlink&quot; title=&quot;一、 排序算法&quot;&gt;&lt;/a&gt;一、 排序算法&lt;/h2&gt;&lt;h3 id=&quot;1-交换排序&quot;&gt;&lt;a href=&quot;#1-交换排序&quot; class=&quot;headerlink&quot; title=&quot;1. 交换排序&quot;&gt;&lt;/a&gt;1. 交换排序&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/lawlite19/AlgorithmExerises/blob/master/一、排序算法/1.交换排序/冒泡排序.cpp&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;冒泡排序&lt;/a&gt;      &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/lawlite19/AlgorithmExercises/blob/master/一、排序算法/1.交换排序/冒泡排序改进1.cpp&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;冒泡排序改进1&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/lawlite19/AlgorithmExercises/blob/master/一、排序算法/1.交换排序/冒泡排序改进2.cpp&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;冒泡排序改进2&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/lawlite19/AlgorithmExercises/blob/master/一、排序算法/1.交换排序/冒泡排序改进3.cpp&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;冒泡排序改进3&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/lawlite19/AlgorithmExercises/blob/master/一、排序算法/1.交换排序/快速排序.cpp&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;快速排序&lt;/a&gt;     &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="算法" scheme="http://lawlite.me/tags/%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
</feed>
