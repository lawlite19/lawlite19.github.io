<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Lawlite的博客</title>
  <icon>https://www.gravatar.com/avatar/af9133ab432a8ab359eccaf9cdcbfa55</icon>
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://lawlite.cn/"/>
  <updated>2022-09-26T15:53:53.541Z</updated>
  <id>http://lawlite.cn/</id>
  
  <author>
    <name>Lawlite</name>
    <email>lawlitewang@gmail.com</email>
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>高并发网络线程模型</title>
    <link href="http://lawlite.cn/2022/08/27/%E9%AB%98%E5%B9%B6%E5%8F%91%E7%BD%91%E7%BB%9C%E7%BA%BF%E7%A8%8B%E6%A8%A1%E5%9E%8B/"/>
    <id>http://lawlite.cn/2022/08/27/高并发网络线程模型/</id>
    <published>2022-08-27T03:38:15.000Z</published>
    <updated>2022-09-26T15:53:53.541Z</updated>
    
    <content type="html"><![CDATA[<h1 id="基础"><a href="#基础" class="headerlink" title="基础"></a>基础</h1><h2 id="服务器端处理一次网络请求流程"><a href="#服务器端处理一次网络请求流程" class="headerlink" title="服务器端处理一次网络请求流程"></a>服务器端处理一次网络请求流程</h2><ul><li>获取请求的数据<ul><li>客户端与服务器建立连接发出请求，服务器接受请求</li></ul></li><li>构建响应数据<ul><li>当服务器接收完请求，并在用户空间处理客户端的请求，直到构建响应完成</li></ul></li><li>返回数据<ul><li>服务器将已构建好的响应再通过内核空间的网络 I/O 发还给客户端</li></ul></li></ul><p><img src="/assets/blog_images/io_model/1569484-20190320120750866-113550043.png" alt="图片"></p><a id="more"></a><h2 id="内核缓冲区和用户进程缓冲区"><a href="#内核缓冲区和用户进程缓冲区" class="headerlink" title="内核缓冲区和用户进程缓冲区"></a>内核缓冲区和用户进程缓冲区</h2><ul><li><p>说明</p><blockquote><p>在 Linux 系统中，所有的系统资源管理都是在内核空间中完成的。比如读写磁盘文件、分配回收内存、从网络接口读写数据等等。应用程序是无法直接进行这样的操作的。但是可以通过内核提供的接口来完成这样的任务。</p><p>比如应用程序要读取磁盘上的一个文件，它可以向内核发起一个 “系统调用” 告诉内核：”我要读取磁盘上的某某文件”。其实就是通过一个特殊的指令让进程从用户态进入到内核态，在内核空间中，CPU 可以执行任何的指令，当然也包括从磁盘上读取数据。具体过程是先把数据读取到内核空间中，然后再把数据拷贝到用户空间并从内核态切换到用户态。此时应用程序已经从系统调用中返回并且拿到了想要的数据，然后往下继续执行</p></blockquote></li><li><p>用户进程的IO读写会用到底层的read、write两大系统调用</p><ul><li>read系统调用并不是直接从物理设备把数据读取到应用内存中，而是把数据从内核缓冲区复制到用户进程缓冲区。write也是类似。</li><li>使用缓冲区的好处就是等待缓冲区达到一定数量的时候，进行IO设备的中断处理，集中执行物理设备实际的IO操作</li></ul></li><li><p>Linux系统中，操作系统内核只有一个内核缓冲区，每个用户进程都有自己的独立缓冲区。</p></li><li><p>再以上图为例，Java客户端和服务端完成一次socket请求和相应的数据交换流程</p><ul><li>客户端发送请求：<ul><li>Java客户端通过write系统调用将数据复制到内核缓冲区，Linux将内核缓冲区的请求数据通过客户端机器的网卡发送出去。</li><li>在服务端请求数据会从接收网卡中的数据读取到服务端机器的内核缓冲区</li></ul></li><li>服务端获取请求：<ul><li>Java服务端通过read系统调用将内核缓冲区的数据读取到用户进程缓冲区进行处理</li></ul></li><li>服务器端处理请求<ul><li>具体的业务逻辑</li></ul></li><li>服务器端返回数据<ul><li>服务器端处理完请求，通过write系统调用将用户进程数据写入内核缓冲区</li></ul></li><li>发送给客户端<ul><li>Linux将内核缓冲区的数据写入到网卡，网卡通过底层的通信协议将数据发送给目标客户端</li></ul></li></ul></li></ul><h2 id="文件描述符"><a href="#文件描述符" class="headerlink" title="文件描述符"></a>文件描述符</h2><ul><li><p><strong>文件描述符（File descriptor）</strong>是计算机科学中的一个术语，是一个用于表述指向文件的引用的抽象化概念。</p></li><li><p>文件描述符在形式上是一个非负整数。实际上，它是一个索引值，指向内核为每一个进程所维护的该进程打开文件的记录表。当程序打开一个现有文件或者创建一个新文件时，内核向进程返回一个文件描述符。在程序设计中，一些涉及底层的程序编写往往会围绕着文件描述符展开。但是文件描述符这一概念往往只适用于UNIX、Linux这样的操作系统。</p></li></ul><h1 id="IO模型"><a href="#IO模型" class="headerlink" title="IO模型"></a>IO模型</h1><h2 id="同步阻塞IO（BIO）"><a href="#同步阻塞IO（BIO）" class="headerlink" title="同步阻塞IO（BIO）"></a>同步阻塞IO（BIO）</h2><h3 id="说明"><a href="#说明" class="headerlink" title="说明"></a>说明</h3><h4 id="阻塞IO和非阻塞IO"><a href="#阻塞IO和非阻塞IO" class="headerlink" title="阻塞IO和非阻塞IO"></a>阻塞IO和非阻塞IO</h4><ul><li>阻塞IO是指需要内核IO操作彻底完成之后才返回到用户空间执行用户程序；</li><li>非阻塞IO是指用户空间的程序不需要等待内核IO操作彻底完成，可以立即返回用户空间去执行后续的指令。</li></ul><h4 id="同步IO和异步IO"><a href="#同步IO和异步IO" class="headerlink" title="同步IO和异步IO"></a>同步IO和异步IO</h4><ul><li>同步IO是指用户程序是主动发起IO请求的一方，系统内核是被动接收的一方；</li><li>异步IO是指系统内核是主动发起IO请求的一方，用户程序是被动接收方。</li></ul><h3 id="时序图"><a href="#时序图" class="headerlink" title="时序图"></a>时序图</h3><ul><li>用户进程A想要读socket1的数据，这时它发起系统调用，向kernel要数据，但是由于数据还没准备好，所以它发起读取请求后，就<strong>阻塞</strong>住了，直到数据准备好了，然后kernel把数据从socket1缓冲区拷贝到进程A的缓冲区后，才给进程A做出响应，然后进程A才能继续做后续操作。</li></ul><p><img src="/assets/blog_images/io_model/bio_时序.png" alt="图片"></p><h3 id="BIO代码示例"><a href="#BIO代码示例" class="headerlink" title="BIO代码示例"></a>BIO代码示例</h3><ul><li><p>Java中默认创建的socker都是阻塞IO模型</p></li><li><p>如下示例<code>serverSocket.accept()</code>会程序阻塞，并且读取数据时程序也会阻塞，一次只能处理一个请求</p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SocketServerV1</span> </span>&#123;</div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException </span>&#123;</div><div class="line">        ServerSocket serverSocket = <span class="keyword">new</span> ServerSocket(<span class="number">8080</span>);</div><div class="line">        <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</div><div class="line">            System.out.println(<span class="string">"等待连接..."</span>);</div><div class="line">            <span class="comment">// 程序阻塞，等待客户端连接</span></div><div class="line">            Socket client = serverSocket.accept();</div><div class="line">            System.out.println(<span class="string">"有客户端连接服务端..."</span>);</div><div class="line">            handle(client);</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">handle</span><span class="params">(Socket client)</span> </span>&#123;</div><div class="line">        <span class="keyword">try</span> (InputStream inputStream = client.getInputStream();) &#123;</div><div class="line">            <span class="keyword">byte</span>[] bytes = <span class="keyword">new</span> <span class="keyword">byte</span>[<span class="number">1024</span>];</div><div class="line">            <span class="comment">// 程序阻塞，没有数据可读时就阻塞</span></div><div class="line">            <span class="keyword">int</span> read = inputStream.read(bytes);</div><div class="line">            <span class="keyword">if</span> (read != -<span class="number">1</span>) &#123;</div><div class="line">                System.out.println(<span class="string">"读取客户端数据："</span> + <span class="keyword">new</span> String(bytes, StandardCharsets.UTF_8));</div><div class="line">            &#125;</div><div class="line">        &#125; <span class="keyword">catch</span> (IOException e) &#123;</div><div class="line">            e.printStackTrace();</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></li><li><p>优化一下，可以将处理socket连接的程序放到一个线程中处理（或者线程池）</p><ul><li>每来一个请求都需要一个线程进行处理，开销大</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SocketServerV2</span> </span>&#123;</div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException </span>&#123;</div><div class="line">        ServerSocket serverSocket = <span class="keyword">new</span> ServerSocket(<span class="number">8080</span>);</div><div class="line">        <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</div><div class="line">            System.out.println(<span class="string">"等待连接..."</span>);</div><div class="line">            <span class="comment">// 程序阻塞，等待客户端连接</span></div><div class="line">            <span class="keyword">final</span> Socket client = serverSocket.accept();</div><div class="line">            System.out.println(<span class="string">"有客户端连接服务端..."</span>);</div><div class="line"></div><div class="line">            <span class="comment">// 使用线程处理请求</span></div><div class="line">            <span class="keyword">new</span> Thread(<span class="keyword">new</span> Runnable() &#123;</div><div class="line">                <span class="meta">@Override</span></div><div class="line">                <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</div><div class="line">                    handle(client);</div><div class="line">                &#125;</div><div class="line">            &#125;).start();</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">handle</span><span class="params">(Socket client)</span> </span>&#123;</div><div class="line">        <span class="keyword">try</span> (InputStream inputStream = client.getInputStream();) &#123;</div><div class="line">            <span class="keyword">byte</span>[] bytes = <span class="keyword">new</span> <span class="keyword">byte</span>[<span class="number">1024</span>];</div><div class="line">            <span class="comment">// 程序阻塞，没有数据可读时就阻塞</span></div><div class="line">            <span class="keyword">int</span> read = inputStream.read(bytes);</div><div class="line">            <span class="keyword">if</span> (read != -<span class="number">1</span>) &#123;</div><div class="line">                System.out.println(<span class="string">"读取客户端数据："</span> + <span class="keyword">new</span> String(bytes, StandardCharsets.UTF_8));</div><div class="line">            &#125;</div><div class="line">        &#125; <span class="keyword">catch</span> (IOException e) &#123;</div><div class="line">            e.printStackTrace();</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></li><li><p>调用流程图示</p></li></ul><p><img src="/assets/blog_images/io_model/bio_流程.png" alt="图片"></p><h2 id="同步非阻塞IO（NIO）"><a href="#同步非阻塞IO（NIO）" class="headerlink" title="同步非阻塞IO（NIO）"></a>同步非阻塞IO（NIO）</h2><h3 id="说明-1"><a href="#说明-1" class="headerlink" title="说明"></a>说明</h3><ul><li><p>同步非阻塞IO是指用户进程主动发起，不需要等待内核IO操作彻底完成就立即返回用户空间的IO操作，在IO操作过程中，发起IO请求的用户进程或线程处理非阻塞的状态。</p></li><li><p>非阻塞和阻塞的区别是</p><ul><li>阻塞是指用户进程或线程一直在等待，不能做别的事情</li><li>非阻塞是指用户进程或线程获得内核返回的状态值就返回自己的空间，可以去做其他的事情。</li></ul></li></ul><h3 id="时序图-1"><a href="#时序图-1" class="headerlink" title="时序图"></a>时序图</h3><ul><li>与阻塞IO模型不同的是，当数据未准备好的时候，kernel会先返回给进程A一个消息，告诉它还没准备好，这时进程A就会搞一个while循环，<strong>一直在这里不停的问kernel数据好了没（在每次询问之间进程A还是可以做一点其他事情的，而不是像阻塞IO那样啥也不能做，就一直等着）</strong>，直到数据准备好了，然后开始阻塞住，等待数据拷贝完成，kernel再回复进程A，进程A就可以处理数据了。</li><li><strong>这里的非阻塞是指发出要数据的请求后，kernel直接会给出一个数据还没准备好的答复，而不像阻塞IO那样，一个回音都没有。但是后面数据准备好了，在数据拷贝的过程中，进程A依然是阻塞的，要等到kernel拷贝好数据后给出答复再进行数据处理。</strong></li></ul><p><img src="/assets/blog_images/io_model/nio.png" alt="图片"></p><h3 id="NIO代码示例"><a href="#NIO代码示例" class="headerlink" title="NIO代码示例"></a>NIO代码示例</h3><ul><li><p>Jdk1.4版本开始支持NIO API</p></li><li><p>基础的NIO代码如下</p><ul><li>channelList用于存储socket连接，每次有连接进来就放到集合中，然后遍历集合进行数据的读取</li><li>遍历channelList会有性能的损耗，下面多路复用就是解决这个问题</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">NioServerV1</span> </span>&#123;</div><div class="line">    <span class="keyword">static</span> List&lt;SocketChannel&gt; channelList = <span class="keyword">new</span> ArrayList&lt;&gt;();</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException </span>&#123;</div><div class="line">        ServerSocketChannel serverSocketChannel = ServerSocketChannel.open();</div><div class="line">        serverSocketChannel.socket().bind(<span class="keyword">new</span> InetSocketAddress(<span class="number">8080</span>));</div><div class="line">        <span class="comment">// 设置ServerSocketChannel为非阻塞</span></div><div class="line">        serverSocketChannel.configureBlocking(<span class="keyword">false</span>);</div><div class="line">        System.out.println(<span class="string">"服务启动成功"</span>);</div><div class="line"></div><div class="line">        <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</div><div class="line">            <span class="comment">// 非阻塞模式accept方法不会阻塞</span></div><div class="line">            <span class="comment">// NIO的非阻塞是由操作系统内部实现的，底层会调用linux内核的accept函数</span></div><div class="line">            SocketChannel socketChannel = serverSocketChannel.accept();</div><div class="line">            <span class="keyword">if</span> (socketChannel != <span class="keyword">null</span>) &#123;</div><div class="line">                System.out.println(<span class="string">"客户端连接成功"</span>);</div><div class="line">                <span class="comment">// 设置SocketChannel为非阻塞</span></div><div class="line">                socketChannel.configureBlocking(<span class="keyword">false</span>);</div><div class="line">                channelList.add(socketChannel);</div><div class="line">            &#125;</div><div class="line">            <span class="comment">// 遍历连接进行数据的读取</span></div><div class="line">            Iterator&lt;SocketChannel&gt; iterator = channelList.iterator();</div><div class="line">            <span class="keyword">while</span> (iterator.hasNext()) &#123;</div><div class="line">                SocketChannel sc = iterator.next();</div><div class="line">                ByteBuffer byteBuffer = ByteBuffer.allocate(<span class="number">256</span>);</div><div class="line">                <span class="keyword">int</span> read = sc.read(byteBuffer);</div><div class="line">                <span class="keyword">if</span> (read &gt; <span class="number">0</span>) &#123;</div><div class="line">                    System.out.println(<span class="string">"读取客户端数据: "</span> + <span class="keyword">new</span> String(byteBuffer.array(), StandardCharsets.UTF_8));</div><div class="line">                &#125; <span class="keyword">else</span> <span class="keyword">if</span> (read == -<span class="number">1</span>) &#123;</div><div class="line">                    iterator.remove();</div><div class="line">                    System.out.println(<span class="string">"客户端断开连接"</span>);</div><div class="line">                &#125;</div><div class="line">            &#125;</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></li></ul><h2 id="IO多路复用"><a href="#IO多路复用" class="headerlink" title="IO多路复用"></a>IO多路复用</h2><h3 id="说明-2"><a href="#说明-2" class="headerlink" title="说明"></a>说明</h3><ul><li>为了提高性能，操作系统引入一种新的系统调用，专门用于查询IO文件描述符的就绪状态，通过该系统调用，一个用户进程可以监听多个文件描述符，一旦某个描述符就绪（一般是内核缓冲区可读/可写），内核就能够将文件描述符的就绪状态返回给用户进程，用户空间可以根据文件描述符的就绪状态进行相应的IO系统调用。<ul><li>通俗理解就是<ul><li><strong>多路是指</strong>： 多个业务方（句柄）并发下来的 IO </li><li><strong>复用是指</strong>：复用一个后台处理程序</li></ul></li></ul></li></ul><h3 id="时序图-2"><a href="#时序图-2" class="headerlink" title="时序图"></a>时序图</h3><ul><li><p>select</p><p><img src="/Users/lawlite/Library/Application Support/typora-user-images/image-20220703155615706.png" alt="image-20220703155615706"></p></li><li><p>epoll</p><p>  <img src="/assets/blog_images/io_model/image-20220619221532446.png" alt="image-20220619221532446"></p></li></ul><h3 id="IO多路复用代码示例"><a href="#IO多路复用代码示例" class="headerlink" title="IO多路复用代码示例"></a>IO多路复用代码示例</h3><ul><li><p>代码流程解析</p><p><img src="/assets/blog_images/io_model/image-20220529225247909.png" alt="image-20220529225247909"></p></li><li><p>只处理真正有数据收发的socketchannel</p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">NioServerV2</span> </span>&#123;</div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException </span>&#123;</div><div class="line">        ServerSocketChannel serverSocketChannel = ServerSocketChannel.open();</div><div class="line">        serverSocketChannel.socket().bind(<span class="keyword">new</span> InetSocketAddress(<span class="number">8080</span>));</div><div class="line">        <span class="comment">// 设置ServerSocketChannel为非阻塞</span></div><div class="line">        serverSocketChannel.configureBlocking(<span class="keyword">false</span>);</div><div class="line">        <span class="comment">// 打开Selector处理Channel，也就是使用epoll</span></div><div class="line">        Selector selector = Selector.open();</div><div class="line">        <span class="comment">// 把ServerSocketChannel注册到Selector上</span></div><div class="line">        serverSocketChannel.register(selector, SelectionKey.OP_ACCEPT);</div><div class="line">        System.out.println(<span class="string">"服务启动成功"</span>);</div><div class="line"></div><div class="line">        <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</div><div class="line">            <span class="comment">// 阻塞等待需要处理的事件发生</span></div><div class="line">            selector.select();</div><div class="line"></div><div class="line">            <span class="comment">// 获取selector中注册的全部时间的SelectKey实例</span></div><div class="line">            Set&lt;SelectionKey&gt; selectionKeys = selector.selectedKeys();</div><div class="line">            Iterator&lt;SelectionKey&gt; iterator = selectionKeys.iterator();</div><div class="line">            <span class="keyword">while</span> (iterator.hasNext()) &#123;</div><div class="line">                SelectionKey selectionKey = iterator.next();</div><div class="line">                <span class="keyword">if</span> (selectionKey.isAcceptable()) &#123;</div><div class="line">                    <span class="comment">// 如果是OP_ACCEPT事件，则进行连接和事件注册</span></div><div class="line">                    ServerSocketChannel server = (ServerSocketChannel) selectionKey.channel();</div><div class="line">                    SocketChannel socketChannel = server.accept();</div><div class="line">                    socketChannel.configureBlocking(<span class="keyword">false</span>);</div><div class="line">                    <span class="comment">// 客户端SocketChannel同样注册到Selector中，服务端监听读事件</span></div><div class="line">                    socketChannel.register(selector, SelectionKey.OP_READ);</div><div class="line">                    System.out.println(<span class="string">"客户端连接成功"</span>);</div><div class="line">                &#125; <span class="keyword">else</span> <span class="keyword">if</span> (selectionKey.isReadable()) &#123;</div><div class="line">                    <span class="comment">// 如果是OP_READ事件，则进行数据读取和打印</span></div><div class="line">                    SocketChannel socketChannel = (SocketChannel) selectionKey.channel();</div><div class="line">                    ByteBuffer byteBuffer = ByteBuffer.allocate(<span class="number">256</span>);</div><div class="line">                    <span class="keyword">int</span> len = socketChannel.read(byteBuffer);</div><div class="line">                    <span class="keyword">if</span> (len &gt; <span class="number">0</span>) &#123;</div><div class="line">                        System.out.println(<span class="string">"读取客户端数据: "</span> + <span class="keyword">new</span> String(byteBuffer.array(), StandardCharsets.UTF_8));</div><div class="line">                    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (len == -<span class="number">1</span>) &#123;</div><div class="line">                        <span class="comment">// 客户端断开连接，从连接中移除。</span></div><div class="line">                        System.out.println(<span class="string">"客户端断开连接"</span>);</div><div class="line">                        socketChannel.close();</div><div class="line">                    &#125;</div><div class="line">                    <span class="comment">// 从事件集合中删除本次处理的key, 防止限次select重复处理</span></div><div class="line">                    iterator.remove();</div><div class="line">                &#125;</div><div class="line">            &#125;</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></li></ul><h3 id="Selector"><a href="#Selector" class="headerlink" title="Selector"></a>Selector</h3><h4 id="select"><a href="#select" class="headerlink" title="select"></a>select</h4><ul><li><p>API</p><figure class="highlight c"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">int</span> <span class="title">select</span><span class="params">(<span class="keyword">int</span> nfds, fd_set *readfds, fd_set *writefds, fd_set *exceptfds, struct timeval *timeout)</span></span></div></pre></td></tr></table></figure><ul><li>nfds: 最大文件描述符+1</li><li>rset: 读事件集合/位图</li><li>wset: 写事件集合/位图</li><li>eset: 异常事件集合/位图</li><li>time: 等待I/o的最长时间，NULL表示一直等待</li></ul></li><li><p>代码示例</p><figure class="highlight c"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// 读事件集合，使用位图表示，默认大小是1024，所以最多处理1024个连接，可修改</span></div><div class="line">fd_set rset;</div><div class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span> ** argv)</span></span>&#123;</div><div class="line">...变量声明、socket端口绑定等省略</div><div class="line">  <span class="comment">// 准备5个文件描述符接收客户端的连接</span></div><div class="line">  <span class="keyword">for</span> (i=<span class="number">0</span>;i&lt;<span class="number">5</span>;i++) &#123;</div><div class="line"><span class="built_in">memset</span>(&amp;client, <span class="number">0</span>, <span class="keyword">sizeof</span> (client));</div><div class="line">addrlen = <span class="keyword">sizeof</span>(client);</div><div class="line">fds[i] = accept(sockfd,(struct sockaddr*)&amp;client, &amp;addrlen);</div><div class="line"><span class="keyword">if</span>(fds[i] &gt; max)&#123;</div><div class="line">      max = fds[i];</div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"><span class="keyword">while</span>(<span class="number">1</span>)&#123;</div><div class="line">    <span class="comment">// rset会被内核修改，每次循环进行需要重新初始化</span></div><div class="line">FD_ZERO(&amp;rset);</div><div class="line"><span class="keyword">for</span> (i = <span class="number">0</span>; i&lt; <span class="number">5</span>; i++ ) &#123;</div><div class="line">FD_SET(fds[i],&amp;rset);</div><div class="line">&#125;</div><div class="line"></div><div class="line">    <span class="comment">// select函数，传入读事件集合进行监听处理，如果没有数据到来会阻塞，如果有数据来，会对fd对应的位图置位</span></div><div class="line">select(max+<span class="number">1</span>,&amp;rset,<span class="literal">NULL</span>,<span class="literal">NULL</span>,<span class="literal">NULL</span>);</div><div class="line">    <span class="comment">// 遍历所有的fd,判断哪个fd被置位了，表示有数据到来，进行数据读取</span></div><div class="line"><span class="keyword">for</span>(i=<span class="number">0</span>;i&lt;<span class="number">5</span>;i++)&#123;</div><div class="line"><span class="keyword">if</span>(FD_ISSET(fds[i],&amp;rset))&#123;</div><div class="line"><span class="built_in">memset</span>(buffer,<span class="number">0</span>,MAXBUF);</div><div class="line">read(fds[i], buffer, MAXBUF);</div><div class="line"><span class="built_in">puts</span>(buffer);</div><div class="line">&#125;</div><div class="line">&#125;</div><div class="line">&#125;</div><div class="line"><span class="keyword">return</span> <span class="number">0</span>;</div><div class="line">&#125;</div></pre></td></tr></table></figure></li><li><p>缺点：</p><ul><li>fd_set默认是1024个；</li><li>fd_set rset不可重用，每次循环需要重新初始化；</li><li>fd_set会从用户态copy到内核态，由内核判断fd是否有数据，虽然是整体的拷贝，但还是存在用户态到内核态的切换；</li><li>数据到来后，fd_set rset需要再次遍历一遍来判断哪个fd有数据，需要O(n)的复杂度</li></ul></li></ul><h4 id="poll"><a href="#poll" class="headerlink" title="poll"></a>poll</h4><ul><li><p>API</p><figure class="highlight c"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">int</span> <span class="title">poll</span><span class="params">(struct pollfd *fds, <span class="keyword">nfds_t</span> nfds, <span class="keyword">int</span> timeout)</span></span>;</div></pre></td></tr></table></figure><ul><li>fds: pollfd接口体集合</li><li>nfds: 监听多少个文件描述符</li><li>timeout：超时时间</li></ul><p>pollfd结构体：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">struct</span> <span class="title">pollfd</span> &#123;</span></div><div class="line">  <span class="keyword">int</span>   fd;         <span class="comment">/* file descriptor */</span></div><div class="line">  <span class="keyword">short</span> events;     <span class="comment">/* requested events */</span></div><div class="line">  <span class="keyword">short</span> revents;    <span class="comment">/* returned events */</span></div><div class="line">&#125;;</div></pre></td></tr></table></figure></li><li><p>代码示例</p><ul><li>poll和select类似，只是没有采用位图，而是声明了一个结构体pollfd</li></ul><figure class="highlight c"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span> ** argv)</span></span>&#123;</div><div class="line">...变量声明、socket端口绑定等省略</div><div class="line">  <span class="class"><span class="keyword">struct</span> <span class="title">pollfd</span> <span class="title">pollfds</span>[5];</span></div><div class="line"><span class="keyword">for</span> (i=<span class="number">0</span>;i&lt;<span class="number">5</span>;i++)&#123;</div><div class="line"><span class="built_in">memset</span>(&amp;client, <span class="number">0</span>, <span class="keyword">sizeof</span> (client));</div><div class="line">addrlen = <span class="keyword">sizeof</span>(client);</div><div class="line">    <span class="comment">// 文件描述符为连接的文件</span></div><div class="line">pollfds[i].fd = accept(sockfd,(struct sockaddr*)&amp;client, &amp;addrlen);</div><div class="line">    <span class="comment">// 接收读事件</span></div><div class="line">pollfds[i].events = POLLIN;</div><div class="line">&#125;</div><div class="line">sleep(<span class="number">1</span>);</div><div class="line"><span class="keyword">while</span>(<span class="number">1</span>)&#123;</div><div class="line">    <span class="comment">// poll函数，当有数据到来时，会将对应fd的revents置位POLLIN</span></div><div class="line">poll(pollfds, <span class="number">5</span>, <span class="number">50000</span>);</div><div class="line"></div><div class="line"><span class="keyword">for</span>(i=<span class="number">0</span>;i&lt;<span class="number">5</span>;i++) &#123;</div><div class="line"><span class="keyword">if</span> (pollfds[i].revents &amp; POLLIN)&#123;</div><div class="line">        <span class="comment">// 恢复到之前的状态，然后读取数据</span></div><div class="line">pollfds[i].revents = <span class="number">0</span>;</div><div class="line"><span class="built_in">memset</span>(buffer,<span class="number">0</span>,MAXBUF);</div><div class="line">read(pollfds[i].fd, buffer, MAXBUF);</div><div class="line"><span class="built_in">puts</span>(buffer);</div><div class="line">&#125;</div><div class="line">&#125;</div><div class="line">&#125;</div><div class="line"><span class="keyword">return</span> <span class="number">0</span>;</div><div class="line">&#125;</div></pre></td></tr></table></figure></li><li><p>poll解决了select的1、2两个缺点，后面两个缺点还是存在，</p></li></ul><h4 id="epoll"><a href="#epoll" class="headerlink" title="epoll"></a>epoll</h4><ul><li><p>epoll API主要提供三个系统调用</p><ul><li><p>int epoll_create(int size);</p><p>创建一个 epoll 对象</p><ul><li><p>创建一个epoll的句柄，size用来告诉内核这个监听的数目一共有多大，这个参数不同于select()中的第一个参数，给出最大监听的fd+1的值，参数size并不是限制了epoll所能监听的描述符最大个数，只是对内核初始分配内部数据结构的一个建议。</p></li><li><p>当创建好epoll句柄后，它就会占用一个fd值，在linux下如果查看/proc/进程id/fd/，是能够看到这个fd的，所以在使用完epoll后，必须调用close()关闭，否则可能导致fd被耗尽。</p></li><li><p>Linux 内核会创建一个 eventpoll 结构体，这个结构体中有两个成员与epoll的使用方式密切相关：</p><ul><li><strong>wq：</strong> 等待队列链表。软中断数据就绪的时候会通过 wq 来找到阻塞在 epoll 对象上的用户进程。</li><li><strong>rbr：</strong> 一棵红黑树。为了支持对海量连接的高效查找、插入和删除，eventpoll 内部使用了一棵红黑树。通过这棵树来管理用户进程下添加进来的所有 socket 连接。</li><li><strong>rdllist：</strong> 就绪的描述符的链表。当有的连接就绪的时候，内核会把就绪的连接放到 rdllist 链表里。这样应用进程只需要判断链表就能找出就绪进程，而不用去遍历整棵树。</li></ul><figure class="highlight c"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"></div><div class="line"><span class="class"><span class="keyword">struct</span> <span class="title">eventpoll</span> &#123;</span></div><div class="line">　　...</div><div class="line">    <span class="comment">/*等待队列链表*/</span></div><div class="line">    <span class="keyword">wait_queue_head_t</span> wq;</div><div class="line">　　<span class="comment">/*红黑树的根节点，这棵树中存储着所有添加到epoll中的事件，</span></div><div class="line">　　也就是这个epoll监控的事件*/</div><div class="line">　　<span class="class"><span class="keyword">struct</span> <span class="title">rb_root</span> <span class="title">rbr</span>;</span></div><div class="line">　　<span class="comment">/*双向链表rdllist保存着将要通过epoll_wait返回给用户的、满足条件的事件*/</span></div><div class="line">　　<span class="class"><span class="keyword">struct</span> <span class="title">list_head</span> <span class="title">rdllist</span>;</span></div><div class="line">　　...</div><div class="line">&#125;;</div></pre></td></tr></table></figure></li></ul></li></ul></li></ul><pre><code>- 我们在调用 epoll_create 时，内核除了帮我们在 epoll 文件系统里建了个 file 结点，在内核 cache 里建了个红黑树用于存储以后 epoll_ctl 传来的 socket 外，还会再建立一个 rdllist 双向链表，用于存储准备就绪的事件，当 epoll_wait 调用时，仅仅观察这个 rdllist 双向链表里有没有数据即可。有数据就返回，没有数据就 sleep，等到 timeout 时间到后即使链表没数据也返回。</code></pre><ul><li><p>int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event)；<br>向 epoll 对象中添加要管理的连接</p><ul><li><p>epfd：是epoll_create()的返回值。</p></li><li><p>op：表示op操作，用三个宏来表示：添加EPOLL_CTL_ADD，删除EPOLL_CTL_DEL，修改EPOLL_CTL_MOD。分别添加、删除和修改对fd的监听事件。</p></li><li><p>fd：是需要监听的fd（文件描述符）</p></li><li><p>epoll_event：是告诉内核需要监听什么事，struct epoll_event结构如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">struct epoll_event &#123;</div><div class="line"> __uint32_t events; /* Epoll events */</div><div class="line"> epoll_data_t data; /* User data variable */</div><div class="line">&#125;;</div></pre></td></tr></table></figure><p>//events可以是以下几个宏的集合：<br>EPOLLIN ：表示对应的文件描述符可以读（包括对端SOCKET正常关闭）；<br>EPOLLOUT：表示对应的文件描述符可以写；<br>EPOLLPRI：表示对应的文件描述符有紧急的数据可读（这里应该表示有带外数据到来）；<br>EPOLLERR：表示对应的文件描述符发生错误；<br>EPOLLHUP：表示对应的文件描述符被挂断；<br>EPOLLET： 将EPOLL设为边缘触发(Edge Triggered)模式，这是相对于水平触发(Level Triggered)来说的。<br>EPOLLONESHOT：只监听一次事件，当监听完这次事件之后，如果还需要继续监听这个socket的话，需要再次把这个socket加入到EPOLL队列里</p></li><li><p>如果增加 socket 句柄，则检查在红黑树中是否存在，存在立即返回，不存在则添加到树干上，然后向内核注册回调函数，用于当中断事件来临时向准备就绪链表中插入数据；</p><ul><li>软中断：是执行中断指令产生的，无外部施加中断请求信号，因此中断的发生不是随机的而是程序安排好的，比如编程异常（1/0），系统调用就是典型的软中断。</li><li>硬中断：是由与系统相连的外部设备产生的，比如磁盘、网卡、键盘、鼠标、时钟等，因此具有随机性和突发性，每个设备都有它自己的IRQ（中断请求）。基于IRQ，CPU可以将相应的请求分发到对应的硬件驱动（中断处理程序）上。</li></ul></li></ul></li></ul><ul><li><p>int epoll_wait(int epfd, struct epoll_event * events, int maxevents, int timeout);</p><p> 等待其管理的连接上的 IO 事件，就是负责打盹的，让出 CPU 调度，但是只要有“事”，立马会从这里唤醒；</p><ul><li>等待epfd上的io事件，最多返回maxevents个事件。</li><li>参数events用来从内核得到事件的集合，maxevents告之内核这个events有多大，这个maxevents的值不能大于创建epoll_create()时的size，</li><li>参数timeout是超时时间（毫秒，0会立即返回，-1将不确定，也有说法说是永久阻塞）。</li><li>该函数返回需要处理的事件数目，如返回0表示已超时。</li></ul></li></ul><ul><li><p>调用图示</p><p><img src="/assets/blog_images/io_model/epoll工作流程.png" alt="img"></p></li><li><p>epoll的两种触发模式</p><ul><li><p>LT（水平触发）模式下，只要这个文件描述符还有数据可读，每次 epoll_wait都会返回它的事件，提醒用户程序去操作；</p></li><li><p>ET（边缘触发）模式下，在它检测到有 I/O 事件时，通过 epoll_wait 调用会得到有事件通知的文件描述符，对于每一个被通知的文件描述符，如可读，则必须将该文件描述符一直读到空，让 errno 返回 EAGAIN 为止，否则下次的 epoll_wait 不会返回余下的数据，会丢掉事件。</p><p><img src="/assets/blog_images/io_model/epoll_et_lt.png" alt="图片"></p><ul><li>如果采用 EPOLLLT 模式的话，系统中一旦有大量你不需要读写的就绪文件描述符，它们每次调用epoll_wait都会返回，这样会大大降低处理程序检索自己关心的就绪文件描述符的效率.。而采用EPOLLET这种边缘触发模式的话，当被监控的文件描述符上有可读写事件发生时，epoll_wait()会通知处理程序去读写。如果这次没有把数据全部读写完(如读写缓冲区太小)，那么下次调用epoll_wait()时，它不会通知你，也就是它只会通知你一次，直到该文件描述符上出现第二次可读写事件才会通知你！！！这种模式比水平触发效率高，系统不会充斥大量你不关心的就绪文件描述符。</li></ul></li><li><p>代码示例</p><figure class="highlight c"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span> ** argv)</span> </span>&#123;</div><div class="line">...变量声明、socket端口绑定等省略</div><div class="line"><span class="keyword">int</span> epfd, epct, i;</div><div class="line"><span class="comment">// 定义epoll事件</span></div><div class="line"><span class="class"><span class="keyword">struct</span> <span class="title">epoll_event</span> <span class="title">event</span>;</span></div><div class="line"><span class="comment">// 定义epoll事件集合</span></div><div class="line"><span class="class"><span class="keyword">struct</span> <span class="title">epoll_event</span> <span class="title">events</span>[20];</span></div><div class="line"><span class="built_in">memset</span>(events, <span class="number">0</span>, <span class="number">20</span> * <span class="keyword">sizeof</span>(struct epoll_event));</div><div class="line"><span class="comment">// 创建epoll的fd，红黑树</span></div><div class="line">epfd = epoll_create(<span class="number">1</span>);</div><div class="line"></div><div class="line">event.data.fd = serverFd;</div><div class="line"><span class="comment">// 填充事件类型，监听读事件</span></div><div class="line">event.events = EPOLLIN;</div><div class="line"><span class="comment">// 把serverFd(监听FD)和事件添加到红黑树上</span></div><div class="line">epoll_ctl(epfd, EPOLL_CTL_ADD, serverFd, &amp;event);</div><div class="line"></div><div class="line"><span class="keyword">while</span>(<span class="number">1</span>) &#123;</div><div class="line"><span class="comment">// 等待时间到来，阻塞模式，同时监听20个事件，返回就绪的事件个数</span></div><div class="line">epct = epoll_wait(epfd, events, <span class="number">20</span>, <span class="number">-1</span>);</div><div class="line"><span class="comment">// 根据epoll返回的值查询事件</span></div><div class="line"><span class="keyword">for</span> (i = <span class="number">0</span>; i &lt; epct; i++) &#123;</div><div class="line"><span class="keyword">if</span> (events[i].data.fd == serverFd) &#123;</div><div class="line"><span class="keyword">socklen_t</span> length = <span class="keyword">sizeof</span>(clientAddr);</div><div class="line">clientFd = accept(events[i].data.fd, (struct sockaddr *)&amp;clientAddr, &amp;length);</div><div class="line"><span class="built_in">printf</span>(<span class="string">"new fd=%d ip %s\n"</span>, clientFd, inet_ntoa(clientAddr.sin_addr));</div><div class="line">event.data.fd = clientFd;</div><div class="line">event.events = EPOLLIN | EPOLLET;</div><div class="line"><span class="comment">// event.events = EPOLLIN;</span></div><div class="line">epoll_ctl(epfd, EPOLL_CTL_ADD, clientFd, &amp;event);</div><div class="line">&#125; <span class="keyword">else</span> &#123;</div><div class="line"><span class="built_in">printf</span>(<span class="string">"new data arrive\n"</span>);</div><div class="line"><span class="comment">// 如果不是serverFd, 就是clientFd的读事件</span></div><div class="line"><span class="built_in">memset</span>(buf, <span class="number">0</span>, BUFLEN);</div><div class="line">rlen = read(events[i].data.fd, buf, BUFLEN);</div><div class="line"><span class="keyword">if</span> (rlen &lt;= <span class="number">0</span>) &#123;</div><div class="line"><span class="comment">// 客户端断开</span></div><div class="line"><span class="built_in">printf</span>(<span class="string">"fd %d disconnected\n"</span>, events[i].data.fd);</div><div class="line">close(events[i].data.fd);</div><div class="line">epoll_ctl(epfd, EPOLL_CTL_DEL, events[i].data.fd, &amp;event);</div><div class="line"><span class="keyword">continue</span>;</div><div class="line">&#125;</div><div class="line"><span class="built_in">printf</span>(<span class="string">"fd: %d data: %s\n"</span>, events[i].data.fd, buf);</div><div class="line">&#125;</div><div class="line">&#125;</div><div class="line">&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></li></ul></li><li><p>解决了select和poll的缺点</p></li></ul><h4 id="三种对比"><a href="#三种对比" class="headerlink" title="三种对比"></a>三种对比</h4><table><thead><tr><th></th><th>select</th><th>poll</th><th>epoll</th></tr></thead><tbody><tr><td>底层数据结构</td><td>数组存储文件描述符</td><td>链表存储文件描述符</td><td>红黑树存储监控的文件描述符，双链表存储就绪的文件描述符</td></tr><tr><td>如何从fd数据中获取就绪的fd</td><td>遍历fd_set</td><td>遍历链表</td><td>回调</td></tr><tr><td>时间复杂度</td><td>获得就绪的文件描述符需要遍历fd数组，O(n)</td><td>获得就绪的文件描述符需要遍历fd链表，O(n)</td><td>当有就绪事件时，系统注册的回调函数就会被调用，将就绪的fd放入到就绪链表中。O(1)</td></tr><tr><td>FD数据拷贝</td><td>每次调用select，需要将fd数据从用户空间拷贝到内核空间</td><td>每次调用poll，需要将fd数据从用户空间拷贝到内核空间</td><td>使用内存映射(mmap)，不需要从用户空间频繁拷贝fd数据到内核空间</td></tr><tr><td>最大连接数</td><td>有限制，一般为1024</td><td>无限制</td><td>无限制</td></tr></tbody></table><h3 id="Reactor设计模式"><a href="#Reactor设计模式" class="headerlink" title="Reactor设计模式"></a>Reactor设计模式</h3><ul><li><p>wikipedia的描述</p><blockquote><p>The reactor design pattern is an event handling pattern for handling service requests delivered concurrently by one or more inputs. The service handler then demultiplexes the incoming requests and dispatches them synchronously to associated request handlers.</p></blockquote></li><li><p>Reactor模式是处理并发I/O常见的一种模式，用于同步I/O，其中心思想是将所有要处理的I/O事件注册到一个中心I/O多路复用器上，同时主线程阻塞在多路复用器上，一旦有I/O事件到来或是准备就绪，多路复用器将返回并将相应<code>I/O</code>事件分发到对应的处理器中。</p><ul><li>Reactor是一种事件驱动机制，和普通函数调用不同的是应用程序不是主动的调用某个API来完成处理，恰恰相反的是Reactor逆置了事件处理流程，应用程序需提供相应的接口并注册到Reactor上，如果有相应的事件发生，Reactor将主动调用应用程序注册的接口（回调函数）</li></ul><p><img src="/assets/blog_images/io_model/reactor.png" alt="img"></p></li></ul><h4 id="Reactor模式结构"><a href="#Reactor模式结构" class="headerlink" title="Reactor模式结构"></a>Reactor模式结构</h4><ul><li>Handle 句柄；用来标识socket连接或是打开文件；</li><li>Synchronous Event Demultiplexer：同步事件多路分解器：由操作系统内核实现的一个函数；用于阻塞等待发生在句柄集合上的一个或多个事件；（如select/epoll；）</li><li>Event Handler：事件处理接口</li><li>Concrete Event HandlerA：实现应用程序所提供的特定事件处理逻辑；</li><li>Reactor：反应器，定义一个接口，实现以下功能：  1）供应用程序注册和删除关注的事件句柄；  2）运行事件循环；  3）有就绪事件到来时，分发事件到之前注册的回调函数上处理；</li></ul><p><img src="/assets/blog_images/io_model/reactor_class.png" alt="Reactor"></p><h4 id="Reactor时序图"><a href="#Reactor时序图" class="headerlink" title="Reactor时序图"></a>Reactor时序图</h4><ul><li><p>应用启动，将关注的事件handle注册到Reactor中；</p></li><li><p>调用Reactor，进入无限事件循环，等待注册的事件到来；</p></li><li><p>事件到来，select返回，Reactor将事件分发到之前注册的回调函数中处理；</p></li></ul><p><img src="/assets/blog_images/io_model/reactor_时序.png" alt="seq_Reactor"></p><h4 id="单线程"><a href="#单线程" class="headerlink" title="单线程"></a>单线程</h4><ul><li><p>单线程的<code>Reactor</code>模式对于客户端的所有请求使用一个专门的线程去处理，这个线程无限循环地监听是否有客户端的请求抵达，一旦收到客户端的请求，就将其分发给响应处理程序进行处理。</p><ul><li><code>Reactor</code> 负责响应IO事件，当检测到一个新的事件会将其发送给相应的处理程序去处理。</li><li><code>Handler</code> 负责处理非阻塞的行为，标识系统管理的资源，同时将处理程序与事件绑定。</li></ul></li><li><p>单线程的Reactor与NIO流程类似，只是将消息相关处理独立到<code>Handler</code>中。虽然NIO中一个线程可以支持所有的IO处理，但瓶颈也是显而易见的。如果某个客户端多次进行请求时在<code>Handler</code>中的处理速度较慢，那么后续的客户端请求都会被积压，导致响应变慢。所以需要引入Reactor多线程模型。</p><p><img src="/assets/blog_images/io_model/image-20220712001415441.png" alt="image-20220712001415441"></p></li></ul><h4 id="多线程"><a href="#多线程" class="headerlink" title="多线程"></a>多线程</h4><ul><li>考虑到工作线程的复用，可以将工作线程设计线程池。将处理器的执行放入线程池，并使用多线程处理业务逻辑，Reactor仍然是单个线程。</li><li>Reactor读线程模型是将Handler中的IO操作和非IO操作分开，操作IO的线程称为IO线程，非IO操作的线程称为工作线程。客户端的请求会被直接丢到线程池中，因此不会发生堵塞。</li><li>多线程的Reactor的特点是一个Reactor线程和多个处理线程，将业务处理即<code>process</code>交给线程池进行了分离，Reactor线程只关注事件分发和字节的发送和读取。需要注意的是，实际的发送和读取还是由Reactor来处理。当在高并发环境下，有可能会出现连接来不及接收。<ul><li>当用户进一步增加时Reactor也会出现瓶颈，因为Reactor既要处理IO操作请求也要响应连接请求。为了分担Reactor的负担，可以引入主从Reactor模型。</li></ul></li></ul><p>​    <img src="/assets/blog_images/io_model/image-20220703160015365.png" alt="image-20220703160015365"></p><h4 id="主从多线程"><a href="#主从多线程" class="headerlink" title="主从多线程"></a>主从多线程</h4><ul><li>对于多个CPU的机器，为了充分利用系统资源会将Reactor拆分为两部分。<ul><li>Main Reactor 负责监听连接，将<code>accept</code>连接交给<code>Sub Reactor</code>处理，主Reactor用于响应连接请求。</li><li>Sub Reactor 处理<code>accept</code>连接，从Reactor用于处理IO操作请求。</li></ul></li><li>主从Reactor的特点是使用 一个<code>Selector</code>池，通常有一个主<code>Reactor</code>用于处理接收连接事件，多个从<code>Reactor</code>处理实际的IO。整体来看，分工合作，分而治之，非常高效。</li><li>为什么需要单独拆分一个Reactor来处理监听呢？<ul><li>因为像TCP这样需要经过3次握手才能建立连接，这个建立的过程也是需要消耗时间和资源的，单独拆分一个Reactor来处理，可以提高性能。</li></ul></li></ul><p><img src="/assets/blog_images/io_model/image-20220703160033862.png" alt="image-20220703160033862"></p><h5 id="netty模型图"><a href="#netty模型图" class="headerlink" title="netty模型图"></a>netty模型图</h5><ul><li>Netty 抽象出两组线程池：BossGroup 和 WorkerGroup，<ul><li>BossGroup 中的线程专门负责和客户端建立连接，</li><li>WorkerGroup中的线程专门负责处理连接上的读写。</li></ul></li></ul><p><img src="/assets/blog_images/io_model/netty模型图.png" alt="image-20210605210313208"></p><h4 id="工作流程（Epoll为例）"><a href="#工作流程（Epoll为例）" class="headerlink" title="工作流程（Epoll为例）"></a>工作流程（Epoll为例）</h4><ol><li><p>主线程向<code>epoll</code>内核事件表中注册<code>socket</code>上的读就绪事件</p></li><li><p>主线程调用<code>epoll_wait</code>等待<code>socket</code>上有数据可读</p></li><li><p>当<code>socket</code>上有数据可读时，<code>epoll_wait</code>通知主线程，主线程将<code>socket</code>可读事件放入请求队列。</p></li><li><p>休眠在请求队列上的某个工作线程被唤醒，从<code>socket</code>中读取数据并处理客户端请求，然后向<code>epoll</code>内核事件表中注册该<code>socket</code>上的写就绪事件。</p></li><li><p>主线程调用<code>epoll_wait</code>等待<code>socket</code>可写</p></li><li><p>当<code>socket</code>可写时<code>epoll_wait</code>通知主线程，主线程将<code>socket</code>可写事件放入请求队列。</p></li><li><p>休眠在请求队列上的某个工作线程被唤醒，向<code>socket</code>上写入服务器处理客户请求的结果。</p><p><img src="/assets/blog_images/io_model/epoll_工作流程.png" alt="img"></p></li></ol><h4 id="Reactor优缺点"><a href="#Reactor优缺点" class="headerlink" title="Reactor优缺点"></a>Reactor优缺点</h4><ul><li><p>优点</p><ul><li><p>响应快，不为单个同步时间所阻塞，虽然Reactor自身依然是同步的。</p></li><li><p>编程相对简单，可以最大程度的避免复杂的多线程以及同步问题和多线程以及多进程的切换开销。</p></li><li><p>可扩展性，可以方便的通过增加Reactor实例个数来充分利用CPU资源。</p></li><li><p>可复用性， Reactor框架本身与具体事件处理逻辑无关，具有很高的复用性。</p></li></ul></li><li><p>缺点</p><ul><li><p>Reactor增加了一定的复杂性，因而具有一定的门槛，并且不易于调试。</p></li><li><p>Reactor模式需要底层的<code>Synchronous Event Demultiplexer</code>支持，比如Java中的Selector支持，操作系统的select系统调用支持。</p></li><li><p>Reactor模式在IO读写数据时会在同一线程中实现，即使使用多个Reactor机制的情况下，那些共享一个Reactor的Channel如果出现一个长时间的数据读写，会影响这个Reactor中其他Channel的相应时间。例如在大文件传输时，IO操作会影响其他客户端的时间，因而对于这种操作，使用传统的<code>Thread-Per-Connection</code>或许是一个更好的选择，或者采用<code>Proactor</code>模式。</p></li></ul></li></ul><h5 id="单线程Reactor代码示例"><a href="#单线程Reactor代码示例" class="headerlink" title="单线程Reactor代码示例"></a>单线程Reactor代码示例</h5><ul><li><p>Reactor负责响应IO事件，当检测到一个新的事件会将其发送给相应的处理程序去处理</p><ul><li><p>首次连接进来，调用dispatch方法，获取到Acceptor实例处理连接</p></li><li><p>客户端发送数据，获取到BasicHandler实例进行处理数据</p></li></ul></li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Reactor</span> <span class="keyword">implements</span> <span class="title">Runnable</span> </span>&#123;</div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</div><div class="line"><span class="keyword">try</span> &#123;</div><div class="line">Thread th = <span class="keyword">new</span> Thread(<span class="keyword">new</span> Reactor(<span class="number">8080</span>));</div><div class="line">th.setName(<span class="string">"Reactor"</span>);</div><div class="line">th.start();</div><div class="line">th.join();</div><div class="line">&#125; <span class="keyword">catch</span> (Exception e) &#123;</div><div class="line">e.printStackTrace();</div><div class="line">&#125;</div><div class="line">&#125;</div><div class="line"><span class="comment">// 选择器，通知通道就绪的事件</span></div><div class="line"><span class="keyword">final</span> Selector selector;</div><div class="line"><span class="keyword">final</span> ServerSocketChannel serverSocket;</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">public</span> <span class="title">Reactor</span><span class="params">(<span class="keyword">int</span> port)</span> <span class="keyword">throws</span> IOException </span>&#123;</div><div class="line">selector = Selector.open();</div><div class="line">serverSocket = ServerSocketChannel.open();</div><div class="line"><span class="comment">// 绑定端口</span></div><div class="line">serverSocket.socket().bind(<span class="keyword">new</span> InetSocketAddress(port));</div><div class="line"><span class="comment">// 设置成非阻塞模式</span></div><div class="line">serverSocket.configureBlocking(<span class="keyword">false</span>);</div><div class="line"><span class="comment">// 注册并关注一个 IO 事件</span></div><div class="line">SelectionKey sk = serverSocket.register(selector, SelectionKey.OP_ACCEPT);</div><div class="line"><span class="comment">// 关联事件的处理程序</span></div><div class="line">sk.attach(<span class="keyword">new</span> Acceptor());</div><div class="line"></div><div class="line">System.out.println(<span class="string">"Listening on port "</span> + port);</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="meta">@Override</span></div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</div><div class="line"><span class="keyword">try</span> &#123;</div><div class="line"><span class="comment">// 死循环</span></div><div class="line"><span class="keyword">while</span> (!Thread.interrupted()) &#123;</div><div class="line"><span class="comment">// 阻塞，直到有通道事件就绪</span></div><div class="line">selector.select();</div><div class="line"><span class="comment">// 拿到就绪通道 SelectionKey 的集合</span></div><div class="line">Set&lt;SelectionKey&gt; selected = selector.selectedKeys();</div><div class="line">Iterator&lt;SelectionKey&gt; it = selected.iterator();</div><div class="line"><span class="keyword">while</span> (it.hasNext()) &#123;</div><div class="line">SelectionKey skTmp = it.next();</div><div class="line"><span class="comment">// 分发</span></div><div class="line">dispatch(skTmp);</div><div class="line">&#125;</div><div class="line"><span class="comment">// 清空就绪通道的 key</span></div><div class="line">selected.clear();</div><div class="line">&#125;</div><div class="line">&#125; <span class="keyword">catch</span> (IOException ex) &#123;</div><div class="line">ex.printStackTrace();</div><div class="line">&#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">void</span> <span class="title">dispatch</span><span class="params">(SelectionKey k)</span> </span>&#123;</div><div class="line"><span class="comment">// 获取key关联的处理器</span></div><div class="line">Runnable r = (Runnable) (k.attachment());</div><div class="line"><span class="comment">// 执行处理</span></div><div class="line"><span class="keyword">if</span> (r != <span class="keyword">null</span>) &#123;</div><div class="line">r.run();</div><div class="line">&#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">/**</span></div><div class="line"> * 处理连接建立事件</div><div class="line"> */</div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Acceptor</span> <span class="keyword">implements</span> <span class="title">Runnable</span> </span>&#123;</div><div class="line"><span class="meta">@Override</span></div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</div><div class="line"><span class="keyword">try</span> &#123;</div><div class="line"><span class="comment">// 接收连接，非阻塞模式下，没有连接直接返回 null</span></div><div class="line">SocketChannel sc = serverSocket.accept();</div><div class="line"><span class="keyword">if</span> (sc != <span class="keyword">null</span>) &#123;</div><div class="line"><span class="comment">// 把提示发到界面</span></div><div class="line">sc.write(ByteBuffer.wrap(<span class="string">"Implementation of Reactor Design Partten \r\nreactor&gt; "</span>.getBytes()));</div><div class="line">System.out.println(<span class="string">"Accept and handler - "</span> + sc.socket().getLocalSocketAddress());</div><div class="line"><span class="keyword">new</span> BasicHandler(selector, sc); <span class="comment">// 单线程处理连接</span></div><div class="line">&#125;</div><div class="line">&#125; <span class="keyword">catch</span> (IOException ex) &#123;</div><div class="line">ex.printStackTrace();</div><div class="line">&#125;</div><div class="line">&#125;</div><div class="line">&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure><h2 id="信号量驱动"><a href="#信号量驱动" class="headerlink" title="信号量驱动"></a>信号量驱动</h2><h3 id="说明-3"><a href="#说明-3" class="headerlink" title="说明"></a>说明</h3><ul><li><p>在信号驱动式 I/O 模型中，应用程序使用套接口进行信号驱动 I/O，并安装一个信号处理函数，进程继续运行并不阻塞。</p></li><li><p>当数据准备好时，进程会收到一个 SIGIO 信号，可以在信号处理函数中调用 I/O 操作函数处理数据。</p><ul><li><strong>比喻：</strong>鱼竿上系了个铃铛，当铃铛响，就知道鱼上钩，然后可以做别的事情。</li></ul></li></ul><h3 id="时序图-3"><a href="#时序图-3" class="headerlink" title="时序图"></a>时序图</h3><ul><li>开启套接字信号驱动IO功能</li><li>系统调用Sigaction执行信号处理函数（非阻塞，立刻返回）</li><li>数据就绪，生成Sigio信号，通过信号回调通知应用来读取数据<ul><li>此种IO方式存在的一个很大的问题：Linux中信号队列是有限制的，如果超过这个数字问题就无法读取数据</li></ul></li></ul><p><img src="/assets/blog_images/io_model/信号量驱动_时序.png" alt="图片"></p><h3 id="优缺点"><a href="#优缺点" class="headerlink" title="优缺点"></a>优缺点</h3><ul><li>信号驱动式IO对于TCP套接字产生的作用不大。因为该信号在TCP套接字中产生的过于频繁。以下条件均会导致对一个TCP套接字产生SIGIO信号：<ul><li>监听套接字上某个连接请求已经完成；</li><li>某个断连请求已经发起；</li><li>某个断连请求已经完成；</li><li>某个连接之半已经关闭；</li><li>数据到达套接字；</li><li>数据已经从套接字发送走；</li><li>发生某个异步错误。</li></ul></li><li>UDP套接字中，只有以下两个条件会产生SIGIO信号：<ul><li>数据报到达套接字；</li><li>套接字上发生异步错误。</li></ul></li></ul><h2 id="异步IO"><a href="#异步IO" class="headerlink" title="异步IO"></a>异步IO</h2><h3 id="说明-4"><a href="#说明-4" class="headerlink" title="说明"></a>说明</h3><ul><li><p>异步IO模型的基本流程是：</p><ul><li>用户线程通过系统调用向内核注册某个IO操作</li><li><p>内核在整个IO操作（包括数据准备、数据复制）完成后通知用户程序，用户执行后续的业务操作</p></li><li><p>在整个内核的数据准备过程中，用户程序都不需要阻塞</p></li></ul></li><li><p>与NIO不同，当进行读写操作时，只需要直接调用API的read或write方法即可，这两种方法均为异步的，对于读操作而言，当有流可读时，操作系统会将可读的流传入read方法的缓冲区，对于写操作而言，当操作系统将write方法传入的流写入完毕是，操作系统会主动通知应用程序</p><ul><li>read和write方法都是异步的，完成后主动调用回调函数。</li></ul></li></ul><h3 id="时序图-4"><a href="#时序图-4" class="headerlink" title="时序图"></a>时序图</h3><p><img src="/assets/blog_images/io_model/aio_时序.png" alt="图片"></p><h3 id="异步IO代码示例"><a href="#异步IO代码示例" class="headerlink" title="异步IO代码示例"></a>异步IO代码示例</h3><table><thead><tr><th>BIO</th><th>NIO</th><th>AIO</th></tr></thead><tbody><tr><td>Socket</td><td>SocketChannel</td><td>AsynchronousSocketChannel</td></tr><tr><td>ServerSocket</td><td>ServerSocketChannel</td><td>AsynchronousServerSocketChannel</td></tr></tbody></table><ul><li><p>Server</p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">AioServer</span> </span>&#123;</div><div class="line">    <span class="keyword">static</span> <span class="keyword">int</span> PORT = <span class="number">8080</span>;</div><div class="line">    <span class="keyword">static</span> String CHARSET = <span class="string">"utf-8"</span>; <span class="comment">//默认编码</span></div><div class="line">    <span class="keyword">static</span> CharsetDecoder decoder = Charset.forName(CHARSET).newDecoder(); <span class="comment">//解码</span></div><div class="line"></div><div class="line">    <span class="keyword">int</span> port;</div><div class="line">    AsynchronousServerSocketChannel serverChannel;</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="title">AioServer</span><span class="params">(<span class="keyword">int</span> port)</span> <span class="keyword">throws</span> IOException </span>&#123;</div><div class="line">        <span class="keyword">this</span>.port = port;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">listen</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</div><div class="line">        <span class="comment">//打开一个服务通道</span></div><div class="line">        <span class="comment">//绑定服务端口</span></div><div class="line">        <span class="keyword">this</span>.serverChannel = AsynchronousServerSocketChannel.open().bind(<span class="keyword">new</span> InetSocketAddress(port), <span class="number">100</span>);</div><div class="line">        <span class="keyword">this</span>.serverChannel.accept(<span class="keyword">this</span>, <span class="keyword">new</span> AcceptHandler());</div><div class="line"></div><div class="line">        Thread t = <span class="keyword">new</span> Thread(() -&gt; &#123;</div><div class="line">            <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</div><div class="line">                System.out.println(<span class="string">"运行中..."</span>);</div><div class="line">                <span class="keyword">try</span> &#123;</div><div class="line">                    Thread.sleep(<span class="number">2000</span>);</div><div class="line">                &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</div><div class="line">                    e.printStackTrace();</div><div class="line">                &#125;</div><div class="line">            &#125;</div><div class="line">        &#125;);</div><div class="line">        t.start();</div><div class="line">    &#125;</div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">(AsynchronousSocketChannel client)</span> </span>&#123;</div><div class="line">        <span class="keyword">try</span> &#123;</div><div class="line">            client.close();</div><div class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</div><div class="line">            e.printStackTrace();</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</div><div class="line">        <span class="keyword">try</span> &#123;</div><div class="line">            System.out.println(<span class="string">"正在启动服务..."</span>);</div><div class="line">            AioServer server = <span class="keyword">new</span> AioServer(PORT);</div><div class="line">            server.listen();</div><div class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</div><div class="line">            e.printStackTrace();</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">&#125;</div></pre></td></tr></table></figure></li><li><p>AcceptHandler</p><ul><li>接收请求处理，如果有连接请求进来，读取客户端数据</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">AcceptHandler</span> <span class="keyword">implements</span> <span class="title">CompletionHandler</span>&lt;<span class="title">AsynchronousSocketChannel</span>, <span class="title">AioServer</span>&gt; </span>&#123;</div><div class="line">    <span class="keyword">static</span> <span class="keyword">int</span> BUFFER_SIZE = <span class="number">1024</span>;</div><div class="line">    <span class="meta">@Override</span></div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">completed</span><span class="params">(<span class="keyword">final</span> AsynchronousSocketChannel client, AioServer attachment)</span> </span>&#123;</div><div class="line">        <span class="keyword">try</span> &#123;</div><div class="line">            System.out.println(<span class="string">"远程地址："</span> + client.getRemoteAddress());</div><div class="line">            <span class="comment">//tcp各项参数</span></div><div class="line">            client.setOption(StandardSocketOptions.TCP_NODELAY, <span class="keyword">true</span>);</div><div class="line">            client.setOption(StandardSocketOptions.SO_SNDBUF, <span class="number">1024</span>);</div><div class="line">            client.setOption(StandardSocketOptions.SO_RCVBUF, <span class="number">1024</span>);</div><div class="line"></div><div class="line">            <span class="keyword">if</span> (client.isOpen()) &#123;</div><div class="line">                System.out.println(<span class="string">"client.isOpen："</span> + client.getRemoteAddress());</div><div class="line">                <span class="keyword">final</span> ByteBuffer buffer = ByteBuffer.allocate(BUFFER_SIZE);</div><div class="line">                buffer.clear();</div><div class="line">                client.read(buffer, client, <span class="keyword">new</span> ReadHandler(buffer));</div><div class="line">            &#125;</div><div class="line"></div><div class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</div><div class="line">            e.printStackTrace();</div><div class="line">        &#125; <span class="keyword">finally</span> &#123;</div><div class="line">            attachment.serverChannel.accept(attachment, <span class="keyword">this</span>);<span class="comment">// 监听新的请求，递归调用。</span></div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="meta">@Override</span></div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">failed</span><span class="params">(Throwable exc, AioServer attachment)</span> </span>&#123;</div><div class="line">        <span class="keyword">try</span> &#123;</div><div class="line">            exc.printStackTrace();</div><div class="line">        &#125; <span class="keyword">finally</span> &#123;</div><div class="line">            attachment.serverChannel.accept(attachment, <span class="keyword">this</span>);<span class="comment">// 监听新的请求，递归调用。</span></div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></li><li><p>ReadHandler</p><ul><li>读取客户端数据，处理请求完之后相应客户端</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ReadHandler</span> <span class="keyword">implements</span> <span class="title">CompletionHandler</span>&lt;<span class="title">Integer</span>, <span class="title">AsynchronousSocketChannel</span>&gt; </span>&#123;</div><div class="line">    <span class="keyword">private</span> ByteBuffer buffer;</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="title">ReadHandler</span><span class="params">(ByteBuffer buffer)</span> </span>&#123;</div><div class="line">        <span class="keyword">this</span>.buffer = buffer;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="meta">@Override</span></div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">completed</span><span class="params">(Integer result, AsynchronousSocketChannel attachment)</span> </span>&#123;</div><div class="line">        <span class="keyword">try</span> &#123;</div><div class="line">            <span class="keyword">if</span> (result &lt; <span class="number">0</span>) &#123;<span class="comment">// 客户端关闭了连接</span></div><div class="line">                AioServer.close(attachment);</div><div class="line">            &#125; <span class="keyword">else</span> <span class="keyword">if</span> (result == <span class="number">0</span>) &#123;</div><div class="line">                System.out.println(<span class="string">"空数据"</span>); <span class="comment">// 处理空数据</span></div><div class="line">            &#125; <span class="keyword">else</span> &#123;</div><div class="line">                <span class="comment">// 读取请求，处理客户端发送的数据</span></div><div class="line">                buffer.flip();</div><div class="line">                CharBuffer charBuffer = AioServer.decoder.decode(buffer);</div><div class="line">                System.out.println(charBuffer.toString()); <span class="comment">//接收请求</span></div><div class="line"></div><div class="line">                <span class="comment">//响应操作，服务器响应结果</span></div><div class="line">                buffer.clear();</div><div class="line">                String res = <span class="string">"HTTP/1.1 200 OK"</span> + <span class="string">"\r\n\r\n"</span> + <span class="string">"hellworld"</span>;</div><div class="line">                buffer = ByteBuffer.wrap(res.getBytes());</div><div class="line">                attachment.write(buffer, attachment, <span class="keyword">new</span> WriteHandler(buffer));<span class="comment">//Response：响应。</span></div><div class="line">            &#125;</div><div class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</div><div class="line">            e.printStackTrace();</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="meta">@Override</span></div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">failed</span><span class="params">(Throwable exc, AsynchronousSocketChannel attachment)</span> </span>&#123;</div><div class="line">        exc.printStackTrace();</div><div class="line">        AioServer.close(attachment);</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></li><li><p>WriteHandler</p><ul><li>处理完相应，服务端关闭连接</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WriteHandler</span> <span class="keyword">implements</span> <span class="title">CompletionHandler</span>&lt;<span class="title">Integer</span>, <span class="title">AsynchronousSocketChannel</span>&gt;</span>&#123;</div><div class="line">    <span class="keyword">private</span> ByteBuffer buffer;</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="title">WriteHandler</span><span class="params">(ByteBuffer buffer)</span> </span>&#123;</div><div class="line">        <span class="keyword">this</span>.buffer = buffer;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="meta">@Override</span></div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">completed</span><span class="params">(Integer result, AsynchronousSocketChannel attachment)</span> </span>&#123;</div><div class="line">        buffer.clear();</div><div class="line">        AioServer.close(attachment);</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="meta">@Override</span></div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">failed</span><span class="params">(Throwable exc, AsynchronousSocketChannel attachment)</span> </span>&#123;</div><div class="line">        exc.printStackTrace();</div><div class="line">        AioServer.close(attachment);</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></li></ul><h3 id="Proactor设计模式"><a href="#Proactor设计模式" class="headerlink" title="Proactor设计模式"></a>Proactor设计模式</h3><h4 id="Proactor模式结构"><a href="#Proactor模式结构" class="headerlink" title="Proactor模式结构"></a>Proactor模式结构</h4><ul><li>Handle 句柄；用来标识socket连接或是打开文件；</li><li>Asynchronous Operation Processor：异步操作处理器；负责执行异步操作，一般由操作系统内核实现；</li><li>Asynchronous Operation：异步操作</li><li>Completion Event Queue：完成事件队列；异步操作完成的结果放到队列中等待后续使用</li><li>Proactor：主动器；为应用程序进程提供事件循环；从完成事件队列中取出异步操作的结果，分发调用相应的后续处理逻辑；</li><li>Completion Handler：完成事件接口；一般是由回调函数组成的接口；</li><li>Concrete Completion Handler：完成事件处理逻辑；实现接口定义特定的应用处理逻辑；</li></ul><p><img src="/assets/blog_images/io_model/proactor_class.png" alt="Proactor"></p><h4 id="Proactor时序图"><a href="#Proactor时序图" class="headerlink" title="Proactor时序图"></a>Proactor时序图</h4><ul><li>应用程序启动，调用异步操作处理器提供的异步操作接口函数，调用之后应用程序和异步操作处理就独立运行；应用程序可以调用新的异步操作，而其它操作可以并发进行；</li><li>应用程序启动Proactor主动器，进行无限的事件循环，等待完成事件到来；</li><li>异步操作处理器执行异步操作，完成后将结果放入到完成事件队列；</li><li>主动器从完成事件队列中取出结果，分发到相应的完成事件回调函数处理逻辑中；</li></ul><p><img src="/assets/blog_images/io_model/proactor_时序png.png" alt="seq_Proactor"></p><h4 id="和Reactor差别"><a href="#和Reactor差别" class="headerlink" title="和Reactor差别"></a>和Reactor差别</h4><h5 id="主动和被动"><a href="#主动和被动" class="headerlink" title="主动和被动"></a>主动和被动</h5><p>以主动写为例： </p><ul><li><p>Reactor将handle放到select()，等待可写就绪，然后调用write()写入数据；写完处理后续逻辑； </p></li><li><p>Proactor调用aoi_write后立刻返回，由内核负责写操作，写完后调用相应的回调函数处理后续逻辑；</p></li></ul><p>可以看出，Reactor被动的等待指示事件的到来并做出反应；它有一个等待的过程，做什么都要先放入到监听事件集合中等待handler可用时再进行操作；<br>Proactor直接调用异步读写操作，调用完后立刻返回；</p><h5 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h5><ul><li><p>Reactor实现了一个被动的事件分离和分发模型，服务等待请求事件的到来，再通过不受间断的同步处理事件，从而做出反应；</p></li><li><p>Proactor实现了一个主动的事件分离和分发模型；这种设计允许多个任务并发的执行，从而提高吞吐量；并可执行耗时长的任务（各个任务间互不影响）</p></li></ul><h2 id="5种IO模型比较"><a href="#5种IO模型比较" class="headerlink" title="5种IO模型比较"></a>5种IO模型比较</h2><p><img src="/assets/blog_images/io_model/IO模型比较.png" alt="wxmp"></p><h1 id="IO模型使用的例子"><a href="#IO模型使用的例子" class="headerlink" title="IO模型使用的例子"></a>IO模型使用的例子</h1><h2 id="Redis-6-0之前-gt-单线程Reactor"><a href="#Redis-6-0之前-gt-单线程Reactor" class="headerlink" title="Redis(6.0之前) =&gt; 单线程Reactor"></a>Redis(6.0之前) =&gt; 单线程Reactor</h2><ul><li><p>Redis6.0之前都是单线程，Redis6.0以及之后的版本引入多线程</p></li><li><p>这里单线程指的是网络请求模块使用一个线程来处理，即一个线程处理所有网络请求，其他模块仍用了多个线程，也就是单线程Reactor模式，为什么之前都是单线程官方给出的解释是：</p><blockquote><p>It’s not very frequent that CPU becomes your bottleneck with Redis, as usually Redis is either memory or network bound. For instance, using pipelining Redis running on an average Linux system can deliver even 1 million requests per second, so if your application mainly uses O(N) or O(log(N)) commands, it is hardly going to use too much CPU.</p></blockquote><p>意思CPU 通常不会是瓶颈，因为大多数请求不会是 CPU 密集型的，而是 I/O 密集型。具体到 Redis 的话，如果不考虑 RDB/AOF 等持久化方案，Redis 是完全的纯内存操作，执行速度是非常快的，因此这部分操作通常不会是性能瓶颈，Redis 真正的性能瓶颈在于网络 I/O，也就是客户端和服务端之间的网络传输延迟，因此 Redis 选择了单线程的 I/O 多路复用来实现它的核心网络模型。</p><p><img src="/assets/blog_images/io_model/redis_1.png" alt="图片"></p></li><li><p>client: 客户端对象</p><ul><li>客户端通过 <strong>socket</strong> 与服务端建立网络通道然后发送请求命令，服务端执行请求的命令并回复。Redis 使用结构体 client 存储客户端的所有相关信息，包括但不限于封装的套接字连接 – *conn，当前选择的读入缓冲区 – querybuf，写出缓冲区 – buf，写出数据链表 – reply等。</li></ul></li></ul><ul><li><strong>aeApiPoll</strong>：I/O 多路复用 API，是基于 epoll_wait/select/kevent 等系统调用的封装，监听等待读写事件触发，然后处理，它是事件循环（Event Loop）中的核心函数，是事件驱动得以运行的基础。</li><li><strong>acceptTcpHandler</strong>：连接应答处理器，底层使用系统调用 accept 接受来自客户端的新连接，并为新连接注册绑定命令读取处理器，以备后续处理新的客户端 TCP 连接</li><li><strong>readQueryFromClient</strong>：命令读取处理器，解析并执行客户端的请求命令。</li><li><strong>beforeSleep</strong>：事件循环中进入 aeApiPoll 等待事件到来之前会执行的函数，其中包含一些日常的任务，比如把 client-&gt;buf 或者 client-&gt;reply （后面会解释为什么这里需要两个缓冲区）中的响应写回到客户端，持久化 AOF 缓冲区的数据到磁盘等，相对应的还有一个 afterSleep 函数，在 aeApiPoll 之后执行。</li><li><strong>sendReplyToClient</strong>：命令回复处理器，当一次事件循环之后写出缓冲区中还有数据残留，则这个处理器会被注册绑定到相应的连接上，等连接触发写就绪事件时，它会将写出缓冲区剩余的数据回写到客户端。</li></ul><p><img src="/assets/blog_images/io_model/redis_单线程.png" alt="图片"></p><h2 id="Nginx-gt-master-worker多进程模型"><a href="#Nginx-gt-master-worker多进程模型" class="headerlink" title="Nginx =&gt; master-worker多进程模型"></a>Nginx =&gt; master-worker多进程模型</h2><ul><li><p>master进程先建好需要监听的socket后，再fork出多个worker进程，这样每个worker进程都可以去接收这个socket。</p></li><li><p>当一个client连接到来时，所有的worker进程都会收到通知，但只有一个可以accept成功。</p><ul><li>这里Nginx提供了一个共享锁accept_mutex，虽然所有的worker都会收到通知，但只有一个进程抢到锁，其它失败，成功的worker进程接收请求。</li><li>当一个worker进程在accept这个连接之后，就开始读取请求，解析请求，处理请求，产生数据后，再返回给客户端，最后才断开连接。</li></ul></li><li><p>当运行过程中，如果worker进程出现异常，master会对worker进行重启。重启时会先启动新的worker进程，然后向老的worker发送信号。新的worker启动后，就开始接收新的请求；而老的worker在收到信号后不再接收请求，将当前进程中所有未处理完的请求处理完成后，再退出。</p></li><li><p>nginx是基于事件模型，适合于IO密集型任务，比如反向代理，IO模型采用epoll实现</p></li></ul><p><img src="/assets/blog_images/io_model/nginx_master_worker.png" alt="图片"></p><h2 id="Mysql-gt-一个连接一个线程或线程池"><a href="#Mysql-gt-一个连接一个线程或线程池" class="headerlink" title="Mysql =&gt; 一个连接一个线程或线程池"></a>Mysql =&gt; 一个连接一个线程或线程池</h2><ul><li><p>在 MySQL 5.6出现以前，MySQL 处理连接的方式是 <code>One-Connection-Per-Thread</code>,即对于每一个数据库连接，MySQL-Server都会创建一个独立的线程服务，请求结束后，销毁线程。</p><ul><li>这种方式在高并发情况下，会<strong>导致线程的频繁创建和释放</strong>。</li></ul></li><li><p>对于 <code>One-Thread-Per-Connection</code> 方式，一个线程对应一个连接，<code>Thread-Pool</code> 实现方式中，线程处理的最小单位是statement(语句)，一个线程可以处理多个连接的请求。这样，在保证充分利用硬件资源情况下(合理设置线程池大小)，可以避免瞬间连接数暴增导致的服务器抖动。</p></li><li><p>MySQL-Server 同时支持3种连接管理方式，包括<code>No-Threads</code>，<code>One-Thread-Per-Connection</code> 和 <code>Pool-Threads</code>。</p><ol><li>No-Threads 表示处理连接使用主线程处理，不额外创建线程，这种方式主要用于调试；</li><li>One-Thread-Per-Connection 是线程池出现以前最常用的方式，为每一个连接创建一个线程服务；</li><li>Pool-Threads 则是本文所讨论的线程池方式。Mysql-Server通过一组函数指针来同时支持3种连接管理方式，对于特定的方式，将函数指针设置成特定的回调函数，连接管理方式通过thread_handling参数控制，代码如下：</li></ol><figure class="highlight c"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">if</span> (thread_handling &lt;= SCHEDULER_ONE_THREAD_PER_CONNECTION)   </div><div class="line">   one_thread_per_connection_scheduler(thread_scheduler,&amp;max_connections, &amp;connection_count);</div><div class="line"><span class="keyword">else</span> <span class="keyword">if</span> (thread_handling == SCHEDULER_NO_THREADS)</div><div class="line">     one_thread_scheduler(thread_scheduler);</div><div class="line"><span class="keyword">else</span>                                 </div><div class="line">    pool_of_threads_scheduler(thread_scheduler, &amp;max_connections,&amp;connection_count);</div></pre></td></tr></table></figure></li><li><p>通过poll监听mysql端口的连接请求 收到连接后，调用accept接口，创建通信socket</p></li><li><p>mysql线程池实现框架</p><ul><li>每一个绿色的方框代表一个group，group数目由thread_pool_size参数决定。</li><li>每个group包含一个优先队列和普通队列，包含一个listener线程和若干个工作线程，listener线程和worker线程可以动态转换，worker线程数目由工作负载决定，同时受到thread_pool_oversubscribe设置影响。</li><li>此外，整个线程池有一个timer线程监控group，防止group“停滞”。</li></ul><p><img src="/assets/blog_images/io_model/mysql_线程框架.png" alt="图片"></p></li><li><p>连接池与线程池</p><ul><li>连接池通常实现在 Client 端，是指应用(客户端)创建预先创建一定的连接，利用这些连接服务于客户端所有的DB请求。如果某一个时刻，空闲的连接数小于DB的请求数，则需要将请求排队，等待空闲连接处理。通过连接池可以复用连接，避免连接的频繁创建和释放，从而减少请求的平均响应时间，并且在请求繁忙时，通过请求排队，可以缓冲应用对DB的冲击。</li><li>线程池实现在server端，通过创建一定数量的线程服务DB请求，相对于 <code>one-conection-per-thread</code> 的一个线程服务一个连接的方式，线程池服务的最小单位是语句，即一个线程可以对应多个活跃的连接。通过线程池，可以将 server 端的服务线程数控制在一定的范围，减少了系统资源的竞争和线程上下文切换带来的消耗，同时也避免出现高连接数导致的高并发问题。</li><li>如下图：每个web-server端维护了3个连接的连接池，对于连接池的每个连接实际不是独占db-server的一个worker，而是可能与其他连接共享。这里假设db-server只有3个group，每个group只有一个worker，每个worker处理了2个连接的请求。</li></ul><p><img src="/assets/blog_images/io_model/mysql_连接池和线程池.png" alt="图片"></p></li><li><p>mysql为什么使用连接池而不使用IO多路复用，网上有一些解答：</p><ul><li><a href="https://developer.aliyun.com/article/866904" target="_blank" rel="external">https://developer.aliyun.com/article/866904</a></li></ul></li></ul><h2 id="Memcache-》-单listener-固定worker线程"><a href="#Memcache-》-单listener-固定worker线程" class="headerlink" title="Memcache =》 单listener+固定worker线程"></a>Memcache =》 单listener+固定worker线程</h2><ul><li>memcached不同于Redis的单进程单线程，是采用多线程的工作方式。</li><li>有一个主线程，同时维护了一个线程池（工作线程）。worker thread工作线程和main thread主线程之间主要通过pipe来进行通信。</li></ul><p><img src="/assets/blog_images/io_model/memcache网络模型.png" alt="img"></p><h2 id="Zookeeper"><a href="#Zookeeper" class="headerlink" title="Zookeeper"></a>Zookeeper</h2><ul><li>在服务端有一个线程(AccpectThread)，该线程负责处理客户端建立连接建立，并在连接建立后将该连接放入到一个队列(AcceptedQueue)。</li><li>紧接着线程(SelectThread)负责从队列取出连接进行处理，只要IO操作准备就绪(可读、可写)，该连接会被扔到工作线程中进行处理。</li><li>类似主从Reactor</li></ul><p><img src="/assets/blog_images/io_model/zookeeper.png" alt="在这里插入图片描述"></p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul><li><a href="https://mp.weixin.qq.com/s/O9E6ceTWVLGSarbbHR0qqA" target="_blank" rel="external">https://mp.weixin.qq.com/s/O9E6ceTWVLGSarbbHR0qqA</a></li><li><a href="https://mp.weixin.qq.com/s/EDzFOo3gcivOe_RgipkTkQ" target="_blank" rel="external">https://mp.weixin.qq.com/s/EDzFOo3gcivOe_RgipkTkQ</a></li><li><a href="http://gee.cs.oswego.edu/dl/cpjslides/nio.pdf" target="_blank" rel="external">http://gee.cs.oswego.edu/dl/cpjslides/nio.pdf</a></li><li><a href="https://mp.weixin.qq.com/s/-d8E56sBa7X-6_20vQJ45A" target="_blank" rel="external">https://mp.weixin.qq.com/s/-d8E56sBa7X-6_20vQJ45A</a></li><li><a href="https://blog.csdn.net/stromcruise/article/details/117606997" target="_blank" rel="external">https://blog.csdn.net/stromcruise/article/details/117606997</a></li><li><a href="https://mp.weixin.qq.com/s/mKJrOMo8c1IYOZSvY4Rp1A" target="_blank" rel="external">https://mp.weixin.qq.com/s/mKJrOMo8c1IYOZSvY4Rp1A</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;基础&quot;&gt;&lt;a href=&quot;#基础&quot; class=&quot;headerlink&quot; title=&quot;基础&quot;&gt;&lt;/a&gt;基础&lt;/h1&gt;&lt;h2 id=&quot;服务器端处理一次网络请求流程&quot;&gt;&lt;a href=&quot;#服务器端处理一次网络请求流程&quot; class=&quot;headerlink&quot; title=&quot;服务器端处理一次网络请求流程&quot;&gt;&lt;/a&gt;服务器端处理一次网络请求流程&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;获取请求的数据&lt;ul&gt;
&lt;li&gt;客户端与服务器建立连接发出请求，服务器接受请求&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;构建响应数据&lt;ul&gt;
&lt;li&gt;当服务器接收完请求，并在用户空间处理客户端的请求，直到构建响应完成&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;返回数据&lt;ul&gt;
&lt;li&gt;服务器将已构建好的响应再通过内核空间的网络 I/O 发还给客户端&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&quot;/assets/blog_images/io_model/1569484-20190320120750866-113550043.png&quot; alt=&quot;图片&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="io" scheme="http://lawlite.cn/tags/io/"/>
    
  </entry>
  
  <entry>
    <title>R树索引</title>
    <link href="http://lawlite.cn/2022/03/27/R%E6%A0%91%E7%B4%A2%E5%BC%95/"/>
    <id>http://lawlite.cn/2022/03/27/R树索引/</id>
    <published>2022-03-27T03:38:15.000Z</published>
    <updated>2022-03-27T08:23:30.840Z</updated>
    
    <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><h2 id="B树"><a href="#B树" class="headerlink" title="B树"></a>B树</h2><ul><li><p>B树是一棵平衡树，它是把一维直线分为若干段线段，当我们查找满足某个要求的点的时候，只要去查找它所属的线段即可。这种思想其实就是先找一个大的空间，再逐步缩小所要查找的空间，最终在一个自己设定的最小不可分空间内找出满足要求的解。</p><p><img src="/assets/blog_images/RTree/1051369-20170404121903503-367470976.png" alt="img"></p><p><img src="/assets/blog_images/RTree/btree-example.png" alt="btree-example"></p></li></ul><a id="more"></a><h1 id="R树介绍"><a href="#R树介绍" class="headerlink" title="R树介绍"></a>R树介绍</h1><ul><li>二维 R 树索引不同于传统的等级（一维）B 树索引。空间数据是二维的，所以无法使用 B 树索引组织空间数据。</li><li>同样，使用 R 树索引不能表示非空间数据。R 树是一种用矩形表示结点的树状结构来组织数据的访问方法。</li></ul><h1 id="R树数据结构"><a href="#R树数据结构" class="headerlink" title="R树数据结构"></a>R树数据结构</h1><ul><li>R树是B树在高维空间的扩展，是一棵平衡树。</li><li>每个R树的叶子结点包含了多个指向不同数据的指针，这些数据可以是存放在硬盘中的，也可以是存在内存中。</li></ul><p><img src="/assets/blog_images/RTree/700px-R-tree.svg.png" alt="File:R-tree.svg"></p><ul><li>M 为一个节点中的最大条目数。取 m &lt;= M/2，约定此 m 为一个节点中的最小条目数。此时，R 树具有如下性质：<ul><li>若叶节点不是根节点，则每个叶节点所包含的索引记录个数介于 m 与 M 之间；</li><li>对于叶节点中的索引记录（I，tuple-identifier），I 是其最小外包矩形；</li><li>若非叶节点不是根节点，则其中包含的子节点个数介于 m 与 M 之间；</li><li>对于非叶节点中的条目（I，child-pointer），I 表示能够覆盖其所有子节点的外包矩形的外包矩形。</li><li>根节点若非叶节点，则其至少有两个子节点。</li><li>所有叶子节点都在同一层。</li></ul></li></ul><h1 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h1><h2 id="GeoTools实现"><a href="#GeoTools实现" class="headerlink" title="GeoTools实现"></a>GeoTools实现</h2><h3 id="R树构建"><a href="#R树构建" class="headerlink" title="R树构建"></a>R树构建</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line">UIManager.setLookAndFeel(UIManager.getCrossPlatformLookAndFeelClassName());</div><div class="line"><span class="comment">// 显示打开shp文件的窗口</span></div><div class="line">File file = JFileDataStoreChooser.showOpenFile(<span class="string">"shp"</span>, <span class="keyword">null</span>);</div><div class="line"><span class="keyword">if</span> (file == <span class="keyword">null</span>) &#123;</div><div class="line">  <span class="keyword">return</span>;</div><div class="line">&#125;</div><div class="line"></div><div class="line">STRtree stRtree = <span class="keyword">new</span> STRtree();</div><div class="line"></div><div class="line">FileDataStore store = FileDataStoreFinder.getDataStore(file);</div><div class="line">SimpleFeatureSource featureSource = store.getFeatureSource();</div><div class="line">SimpleFeatureCollection features = featureSource.getFeatures();</div><div class="line">SimpleFeatureIterator featureIterator = features.features();</div><div class="line"><span class="keyword">while</span> (featureIterator.hasNext()) &#123;</div><div class="line">  SimpleFeature simpleFeature = featureIterator.next();</div><div class="line">  Geometry geometry = (Geometry) simpleFeature.getDefaultGeometry();</div><div class="line">  stRtree.insert(geometry.getEnvelopeInternal(), simpleFeature);</div><div class="line">&#125;</div><div class="line"><span class="comment">// 构建RTree</span></div><div class="line">stRtree.build();</div></pre></td></tr></table></figure><h3 id="查询"><a href="#查询" class="headerlink" title="查询"></a>查询</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// RTree查询</span></div><div class="line">Envelope envelope = <span class="keyword">new</span> Envelope(<span class="keyword">new</span> Coordinate(<span class="number">10</span>, <span class="number">20</span>));</div><div class="line">List&lt;?&gt; result = stRtree.query(envelope);</div><div class="line">System.out.println(<span class="string">"query size: "</span> + result.size());</div><div class="line">result.forEach(System.out::println);</div></pre></td></tr></table></figure><h2 id="GeoTools实现说明"><a href="#GeoTools实现说明" class="headerlink" title="GeoTools实现说明"></a>GeoTools实现说明</h2><h3 id="STR算法"><a href="#STR算法" class="headerlink" title="STR算法"></a>STR算法</h3><ul><li>geotools中R树的实现使用的是STR算法，<ul><li>对矩形的分组只考虑每个矩形的中心点，STR的基本思想是将所有的矩形分配到$\left \lceil r/n \right \rceil $（取上界）个分组中；</li><li>首先，对矩形按x坐标排序，然后划分成 $\sqrt{r/n}$ 个slice；</li><li>然后，对slice内的矩形按y坐标排序，进一步划分成 $\sqrt{r/n}$ 份；</li><li>最后递归的向上构建，直到根节点。</li></ul></li></ul><p><img src="/assets/blog_images/RTree/STR_algorithm.png" alt="image-20220327151016622"></p><h3 id="Insert"><a href="#Insert" class="headerlink" title="Insert"></a>Insert</h3><p><code>stRtree.insert(Envelope itemEnv, Object item)</code>时序调用如下</p><ul><li>插入的一条记录为<code>ItemBoundable</code> 对象，包含一个<code>Envelope</code>和插入的数据</li><li><code>insert</code>的数据实际就是叶子节点，<code>RTree build</code>的时候根据叶子节点向上构建。</li></ul><p><img src="/assets/blog_images/RTree/STRtree_insert.png" alt="STRtree_insert"></p><h3 id="Build"><a href="#Build" class="headerlink" title="Build"></a>Build</h3><ul><li><p><code>stRtree.build();</code>真正构建<code>R</code>树</p><ul><li><p><code>createHigherLevels</code>递归构建<code>R</code>树，递归结束的条件就是父节点只有一个，即为根节点</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">private AbstractNode createHigherLevels(List boundablesOfALevel, int level) &#123;</div><div class="line">  Assert.isTrue(!boundablesOfALevel.isEmpty());</div><div class="line">  List parentBoundables = createParentBoundables(boundablesOfALevel, level + 1);</div><div class="line">  if (parentBoundables.size() == 1) &#123;</div><div class="line">    return (AbstractNode) parentBoundables.get(0);</div><div class="line">  &#125;</div><div class="line">  return createHigherLevels(parentBoundables, level + 1);</div><div class="line">&#125;</div></pre></td></tr></table></figure></li><li><p><code>createParentBoundables</code>，根据给定的子节点信息，获取父节点的信息</p><ul><li><code>GeoTool</code>中<code>STRtree</code>对象每个节点的子节点默认大小为<code>10</code>个</li><li>将子节点根据<code>Envelope</code>中心点按<code>x</code>轴排序，然后划分成 $\sqrt{r/n}$ 个slice，比如r=288, n=10，即划分为6个slice，每个slice包含288/6=48个节点</li><li></li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">protected</span> List <span class="title">createParentBoundables</span><span class="params">(List childBoundables, <span class="keyword">int</span> newLevel)</span> </span>&#123;</div><div class="line">  Assert.isTrue(!childBoundables.isEmpty());</div><div class="line">  <span class="keyword">int</span> minLeafCount = (<span class="keyword">int</span>) Math.ceil((childBoundables.size() / (<span class="keyword">double</span>) getNodeCapacity()));</div><div class="line">  ArrayList sortedChildBoundables = <span class="keyword">new</span> ArrayList(childBoundables);</div><div class="line">  Collections.sort(sortedChildBoundables, xComparator);</div><div class="line">  List[] verticalSlices = verticalSlices(sortedChildBoundables,</div><div class="line">      (<span class="keyword">int</span>) Math.ceil(Math.sqrt(minLeafCount)));</div><div class="line">  <span class="keyword">return</span> createParentBoundablesFromVerticalSlices(verticalSlices, newLevel);</div><div class="line">&#125;</div><div class="line">    </div><div class="line"><span class="function"><span class="keyword">private</span> List <span class="title">createParentBoundablesFromVerticalSlices</span><span class="params">(List[] verticalSlices, <span class="keyword">int</span> newLevel)</span> </span>&#123;</div><div class="line">  Assert.isTrue(verticalSlices.length &gt; <span class="number">0</span>);</div><div class="line">  List parentBoundables = <span class="keyword">new</span> ArrayList();</div><div class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; verticalSlices.length; i++) &#123;</div><div class="line">    parentBoundables.addAll(</div><div class="line">          createParentBoundablesFromVerticalSlice(verticalSlices[i], newLevel));</div><div class="line">  &#125;</div><div class="line">  <span class="keyword">return</span> parentBoundables;</div><div class="line">&#125;</div><div class="line">    </div><div class="line"><span class="function"><span class="keyword">protected</span> List <span class="title">createParentBoundablesFromVerticalSlice</span><span class="params">(List childBoundables, <span class="keyword">int</span> newLevel)</span> </span>&#123;</div><div class="line">  <span class="keyword">return</span> <span class="keyword">super</span>.createParentBoundables(childBoundables, newLevel);</div><div class="line">&#125;</div></pre></td></tr></table></figure><ul><li><p>其中<code>super.createParentBoundables</code>针对每个slice，按照y坐标排序，每10个节点划分到一个group中</p><ul><li>其中父节点使用的是createNode(newLevel)创建，创建的为<code>AbstractNode</code>对象，</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">protected</span> List <span class="title">createParentBoundables</span><span class="params">(List childBoundables, <span class="keyword">int</span> newLevel)</span> </span>&#123;</div><div class="line">  Assert.isTrue(!childBoundables.isEmpty());</div><div class="line">  ArrayList parentBoundables = <span class="keyword">new</span> ArrayList();</div><div class="line">  parentBoundables.add(createNode(newLevel));</div><div class="line">  ArrayList sortedChildBoundables = <span class="keyword">new</span> ArrayList(childBoundables);</div><div class="line">  Collections.sort(sortedChildBoundables, getComparator());</div><div class="line">  <span class="keyword">for</span> (Iterator i = sortedChildBoundables.iterator(); i.hasNext(); ) &#123;</div><div class="line">    Boundable childBoundable = (Boundable) i.next();</div><div class="line">    <span class="keyword">if</span> (lastNode(parentBoundables).getChildBoundables().size() == getNodeCapacity()) &#123;</div><div class="line">      parentBoundables.add(createNode(newLevel));</div><div class="line">    &#125;</div><div class="line">    lastNode(parentBoundables).addChildBoundable(childBoundable);</div><div class="line">  &#125;</div><div class="line">  <span class="keyword">return</span> parentBoundables;</div><div class="line">&#125;</div><div class="line">      </div><div class="line"><span class="function"><span class="keyword">protected</span> AbstractNode <span class="title">lastNode</span><span class="params">(List nodes)</span> </span>&#123;</div><div class="line">  <span class="keyword">return</span> (AbstractNode) nodes.get(nodes.size() - <span class="number">1</span>);</div><div class="line">&#125;</div></pre></td></tr></table></figure><ul><li><p>父节点的<code>Envelope</code>对象计算方式如下，即遍历父节点对应的子节点，找到能将子节点包围住的矩形框</p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">protected</span> Object <span class="title">computeBounds</span><span class="params">()</span> </span>&#123;</div><div class="line">  Envelope bounds = <span class="keyword">null</span>;</div><div class="line">  <span class="keyword">for</span> (Iterator i = getChildBoundables().iterator(); i.hasNext(); ) &#123;</div><div class="line">    Boundable childBoundable = (Boundable) i.next();</div><div class="line">    <span class="keyword">if</span> (bounds == <span class="keyword">null</span>) &#123;</div><div class="line">      bounds = <span class="keyword">new</span> Envelope((Envelope)childBoundable.getBounds());</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">else</span> &#123;</div><div class="line">      bounds.expandToInclude((Envelope)childBoundable.getBounds());</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">  <span class="keyword">return</span> bounds;</div><div class="line">&#125;</div><div class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">expandToInclude</span><span class="params">(Envelope other)</span> </span>&#123;</div><div class="line">    <span class="keyword">if</span> (other.isNull()) &#123;</div><div class="line">      <span class="keyword">return</span>;</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">if</span> (isNull()) &#123;</div><div class="line">      minx = other.getMinX();</div><div class="line">      maxx = other.getMaxX();</div><div class="line">      miny = other.getMinY();</div><div class="line">      maxy = other.getMaxY();</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">else</span> &#123;</div><div class="line">      <span class="keyword">if</span> (other.minx &lt; minx) &#123;</div><div class="line">        minx = other.minx;</div><div class="line">      &#125;</div><div class="line">      <span class="keyword">if</span> (other.maxx &gt; maxx) &#123;</div><div class="line">        maxx = other.maxx;</div><div class="line">      &#125;</div><div class="line">      <span class="keyword">if</span> (other.miny &lt; miny) &#123;</div><div class="line">        miny = other.miny;</div><div class="line">      &#125;</div><div class="line">      <span class="keyword">if</span> (other.maxy &gt; maxy) &#123;</div><div class="line">        maxy = other.maxy;</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">  &#125;</div></pre></td></tr></table></figure></li></ul></li></ul></li></ul></li></ul><p>  <img src="/assets/blog_images/RTree/AbstractSTRtree_build.png" alt="image-20220327145347438"></p><h3 id="Query"><a href="#Query" class="headerlink" title="Query"></a>Query</h3><ul><li><p>先判断根节点是否是否包含</p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">protected</span> List <span class="title">query</span><span class="params">(Object searchBounds)</span> </span>&#123;</div><div class="line">  build();</div><div class="line">  ArrayList matches = <span class="keyword">new</span> ArrayList();</div><div class="line">  <span class="keyword">if</span> (isEmpty()) &#123;</div><div class="line">    <span class="comment">//Assert.isTrue(root.getBounds() == null);</span></div><div class="line">    <span class="keyword">return</span> matches;</div><div class="line">  &#125;</div><div class="line">  <span class="keyword">if</span> (getIntersectsOp().intersects(root.getBounds(), searchBounds)) &#123;</div><div class="line">    queryInternal(searchBounds, root, matches);</div><div class="line">  &#125;</div><div class="line">  <span class="keyword">return</span> matches;</div><div class="line">&#125;</div></pre></td></tr></table></figure></li><li><p>然后递归查找子节点</p><ul><li>如上我们说的，insert的叶子节点是ItemBoundable对象，父节点是AbstractNode对象，根据此可以判断是否到达叶子节点</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line">  </div><div class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">queryInternal</span><span class="params">(Object searchBounds, AbstractNode node, List matches)</span> </span>&#123;</div><div class="line">  List childBoundables = node.getChildBoundables();</div><div class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; childBoundables.size(); i++) &#123;</div><div class="line">    Boundable childBoundable = (Boundable) childBoundables.get(i);</div><div class="line">    <span class="keyword">if</span> (! getIntersectsOp().intersects(childBoundable.getBounds(), searchBounds)) &#123;</div><div class="line">      <span class="keyword">continue</span>;</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">if</span> (childBoundable <span class="keyword">instanceof</span> AbstractNode) &#123;</div><div class="line">      queryInternal(searchBounds, (AbstractNode) childBoundable, matches);</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">else</span> <span class="keyword">if</span> (childBoundable <span class="keyword">instanceof</span> ItemBoundable) &#123;</div><div class="line">      matches.add(((ItemBoundable)childBoundable).getItem());</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">else</span> &#123;</div><div class="line">      Assert.shouldNeverReachHere();</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></li></ul><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul><li><a href="https://www.codedump.info/post/20200609-btree-1/" target="_blank" rel="external">https://www.codedump.info/post/20200609-btree-1/</a></li><li><a href="https://desktop.arcgis.com/zh-cn/arcmap/10.3/manage-data/using-sql-with-gdbs/the-rtree-index.htm" target="_blank" rel="external">https://desktop.arcgis.com/zh-cn/arcmap/10.3/manage-data/using-sql-with-gdbs/the-rtree-index.htm</a></li><li><a href="http://www-db.deis.unibo.it/courses/SI-LS/papers/Gut84.pdf" target="_blank" rel="external">Guttman, A.; “R-trees: a dynamic index structure for spatial searching,” ACM, 1984, 14</a></li><li><a href="https://ieeexplore.ieee.org/document/582015" target="_blank" rel="external">STR: a simple and efficient algorithm for R-tree packing</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h1&gt;&lt;h2 id=&quot;B树&quot;&gt;&lt;a href=&quot;#B树&quot; class=&quot;headerlink&quot; title=&quot;B树&quot;&gt;&lt;/a&gt;B树&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;B树是一棵平衡树，它是把一维直线分为若干段线段，当我们查找满足某个要求的点的时候，只要去查找它所属的线段即可。这种思想其实就是先找一个大的空间，再逐步缩小所要查找的空间，最终在一个自己设定的最小不可分空间内找出满足要求的解。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/assets/blog_images/RTree/1051369-20170404121903503-367470976.png&quot; alt=&quot;img&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/assets/blog_images/RTree/btree-example.png&quot; alt=&quot;btree-example&quot;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Ubuntu16.04安装Nvidia驱动cuda,cudnn和tensorflow-gpu</title>
    <link href="http://lawlite.cn/2018/10/25/Ubuntu16.04%E5%AE%89%E8%A3%85Nvidia%E9%A9%B1%E5%8A%A8cuda,%20cudnn%E5%92%8Ctensorflow-gpu/"/>
    <id>http://lawlite.cn/2018/10/25/Ubuntu16.04安装Nvidia驱动cuda, cudnn和tensorflow-gpu/</id>
    <published>2018-10-25T07:25:54.000Z</published>
    <updated>2019-05-04T04:41:26.032Z</updated>
    
    <content type="html"><![CDATA[<ul><li>之前有在阿里云GPU服务器上弄过： <a href="http://lawlite.me/2017/12/25/%E9%98%BF%E9%87%8C%E4%BA%91GPU%E6%9C%8D%E5%8A%A1%E5%99%A8%E4%B8%8ATorch%E5%AE%89%E8%A3%85%E4%B8%8E%E4%BD%BF%E7%94%A8/" target="_blank" rel="external">点击查看</a>， 这里从装<code>Nvidia</code>开始<h2 id="一、-安装Nvidia驱动"><a href="#一、-安装Nvidia驱动" class="headerlink" title="一、 安装Nvidia驱动"></a>一、 安装Nvidia驱动</h2></li></ul><h3 id="1-1-查找需要安装的Nvidia版本"><a href="#1-1-查找需要安装的Nvidia版本" class="headerlink" title="1.1 查找需要安装的Nvidia版本"></a>1.1 查找需要安装的Nvidia版本</h3><h4 id="1-1-1-官网"><a href="#1-1-1-官网" class="headerlink" title="1.1.1 官网"></a>1.1.1 官网</h4><ul><li>官网上查找： <a href="https://www.nvidia.com/Download/index.aspx?lang=en-us" target="_blank" rel="external">https://www.nvidia.com/Download/index.aspx?lang=en-us</a><ul><li>这里是 <code>GeForce GTX 1080 TI</code></li><li>如下图，推荐 <code>410</code> 版本的</li></ul></li></ul><a id="more"></a><p><img src="/assets/blog_images/GPU/15_GPU%E5%AF%B9%E5%BA%94nvidia%E7%89%88%E6%9C%AC.jpg" alt="GPU对应nvidia版本" title="15_GPU对应nvidia版本"></p><p><img src="/assets/blog_images/GPU/16_GPU%E5%AF%B9%E5%BA%94nvidia%E7%89%88%E6%9C%AC.jpg" alt="GPU对应驱动版本" title="16_GPU对应nvidia版本"></p><h4 id="1-1-2-命令行查看推荐驱动"><a href="#1-1-2-命令行查看推荐驱动" class="headerlink" title="1.1.2 命令行查看推荐驱动"></a>1.1.2 命令行查看推荐驱动</h4><ul><li><p>查看驱动：<code>ubuntu-drivers devices</code>， 如下图</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">ubuntu@ubuntu-System-Product-Name:~$ ubuntu-drivers devices</div><div class="line">== cpu-microcode<span class="selector-class">.py</span> ==</div><div class="line">driver   : intel-microcode - distro free</div><div class="line"></div><div class="line">== /sys/devices/pci0000:<span class="number">00</span>/<span class="number">0000</span>:<span class="number">00</span>:<span class="number">01.0</span>/<span class="number">0000</span>:<span class="number">01</span>:<span class="number">00.0</span> ==</div><div class="line">vendor   : NVIDIA Corporation</div><div class="line">modalias : pci:v000010DEd00001B06sv00001458sd0000374Dbc03sc00i00</div><div class="line">driver   : nvidia-<span class="number">410</span> - third-party free recommended</div><div class="line">driver   : nvidia-<span class="number">384</span> - distro non-free</div><div class="line">driver   : xserver-xorg-video-nouveau - distro free builtin</div><div class="line">driver   : nvidia-<span class="number">390</span> - third-party free</div><div class="line">driver   : nvidia-<span class="number">396</span> - third-party free</div></pre></td></tr></table></figure></li><li><p>注意这里添加了<code>ppa</code>, 若是没有，可能最新的只有<code>nvidia-384</code>， 但是若想安装<code>cuda-9.0</code> 需要大于<code>384.81</code>, 不然后面安装<code>tensorflow-gpu</code> 之后也会报错</p><ul><li>图片对应网址：<a href="https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html" target="_blank" rel="external">https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html</a></li></ul></li></ul><p><img src="/assets/blog_images/GPU/14_cuda%E7%89%88%E6%9C%AC%E5%AF%B9%E5%BA%94nvidia%E7%89%88%E6%9C%AC.jpg" alt="cuda版本对应nvidia版本" title="14_cuda版本对应nvidia版本"></p><ul><li>添加 <code>ppa</code>: <ul><li><code>sudo add-apt-repository ppa:graphics-drivers/ppa</code>   （注意联网，去掉代理）</li><li><code>sudo apt update</code></li></ul></li><li>然后执行<code>ubuntu-drivers devices</code>就可以看到如上的结果</li><li>安装：<ul><li>可能需要的依赖：<code>sudo apt install dkms build-essential linux-headers-generic</code> </li><li>有些可能需要禁用<code>nouveau</code>模块，查看：<a href="https://blog.csdn.net/u012235003/article/details/54575758" target="_blank" rel="external">https://blog.csdn.net/u012235003/article/details/54575758</a></li><li><code>sudo apt-get install linux-headers-$(uname -r)</code></li><li><code>sudo apt install nvidia-410</code></li><li>重启机器</li></ul></li><li>查看：<ul><li><code>nvidia-smi</code>  </li><li>显示如下结果</li></ul></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line">(wangyongzhi_ml) ubuntu@ubuntu-System-Product-Name:/usr/<span class="built_in">local</span>/cuda-10.0/bin$ nvidia-smi</div><div class="line">Thu Oct 25 15:49:46 2018</div><div class="line">+-----------------------------------------------------------------------------+</div><div class="line">| NVIDIA-SMI 410.66       Driver Version: 410.66       CUDA Version: 10.0     |</div><div class="line">|-------------------------------+----------------------+----------------------+</div><div class="line">| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |</div><div class="line">| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |</div><div class="line">|===============================+======================+======================|</div><div class="line">|   0  GeForce GTX 108...  Off  | 00000000:01:00.0  On |                  N/A |</div><div class="line">|  0%   44C    P8    20W / 250W |     42MiB / 11174MiB |      0%      Default |</div><div class="line">+-------------------------------+----------------------+----------------------+</div><div class="line">|   1  GeForce GTX 108...  Off  | 00000000:02:00.0 Off |                  N/A |</div><div class="line">|  0%   50C    P8    20W / 250W |      2MiB / 11178MiB |      0%      Default |</div><div class="line">+-------------------------------+----------------------+----------------------+</div><div class="line"></div><div class="line">+-----------------------------------------------------------------------------+</div><div class="line">| Processes:                                                       GPU Memory |</div><div class="line">|  GPU       PID   Type   Process name                             Usage      |</div><div class="line">|=============================================================================|</div><div class="line">|    0       949      G   /usr/lib/xorg/Xorg                            39MiB |</div><div class="line">+-----------------------------------------------------------------------------+</div></pre></td></tr></table></figure><ul><li>跑个程序的使用情况</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line">ubuntu@ubuntu-System-Product-Name:~$ nvidia-smi</div><div class="line">Thu Oct 25 21:20:00 2018</div><div class="line">+-----------------------------------------------------------------------------+</div><div class="line">| NVIDIA-SMI 410.66       Driver Version: 410.66       CUDA Version: 10.0     |</div><div class="line">|-------------------------------+----------------------+----------------------+</div><div class="line">| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |</div><div class="line">| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |</div><div class="line">|===============================+======================+======================|</div><div class="line">|   0  GeForce GTX 108...  Off  | 00000000:01:00.0  On |                  N/A |</div><div class="line">|  0%   53C    P2   128W / 250W |  10776MiB / 11174MiB |     44%      Default |</div><div class="line">+-------------------------------+----------------------+----------------------+</div><div class="line">|   1  GeForce GTX 108...  Off  | 00000000:02:00.0 Off |                  N/A |</div><div class="line">|  0%   52C    P8    21W / 250W |  10631MiB / 11178MiB |      0%      Default |</div><div class="line">+-------------------------------+----------------------+----------------------+</div><div class="line"></div><div class="line">+-----------------------------------------------------------------------------+</div><div class="line">| Processes:                                                       GPU Memory |</div><div class="line">|  GPU       PID   Type   Process name                             Usage      |</div><div class="line">|=============================================================================|</div><div class="line">|    0       949      G   /usr/lib/xorg/Xorg                            39MiB |</div><div class="line">|    0      3009      C   python                                     10725MiB |</div><div class="line">|    1      3009      C   python                                     10619MiB |</div><div class="line">+-----------------------------------------------------------------------------+</div></pre></td></tr></table></figure><h2 id="二、安装cuda"><a href="#二、安装cuda" class="headerlink" title="二、安装cuda"></a>二、安装cuda</h2><ul><li>官网： <a href="https://developer.nvidia.com/cuda-toolkit-archive" target="_blank" rel="external">https://developer.nvidia.com/cuda-toolkit-archive</a></li><li>选择想要安装的版本，这里选择的是<code>cuda-9.0</code>, 下载</li><li>安装<ul><li><code>chmod +x cuda_9.0.176_384.81_linux-run</code></li><li><code>sudo ./cuda_9.0.176_384.81_linux-run</code></li><li>根据提示安装选择即可</li><li>添加环境变量<ul><li><code>vim ~/.bashrc</code></li><li>加入环境变量</li></ul></li></ul></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># cuda9.0</span></div><div class="line"><span class="built_in">export</span> PATH=/usr/<span class="built_in">local</span>/cuda-9.0/bin/:<span class="variable">$PATH</span>;</div><div class="line"><span class="built_in">export</span> LD_LIBRARY_PATH=/usr/<span class="built_in">local</span>/cuda-9.0/lib64/:<span class="variable">$LD_LIBRARY_PATH</span>;</div></pre></td></tr></table></figure><ul><li><p>测试1</p><ul><li><code>nvcc -V</code></li><li>如下图，版本为<code>V9.0.176</code><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">(wangyongzhi_ml) ubuntu@ubuntu-System-Product-Name:~/wangyongzhi/software$ nvcc -V</div><div class="line">nvcc: NVIDIA (R) Cuda compiler driver</div><div class="line">Copyright (c) 2005-2017 NVIDIA Corporation</div><div class="line">Built on Fri_Sep__1_21:08:03_CDT_2017</div><div class="line">Cuda compilation tools, release 9.0, V9.0.176</div></pre></td></tr></table></figure></li></ul></li><li><p>测试2</p><ul><li>如果上面安装过程中选择了安装<code>Examples</code>, 会在 <code>~</code> 文件夹下生成测试<code>NVIDIA_CUDA-9.0_Samples</code> 的文件</li><li>进入： <code>cd NVIDIA_CUDA-9.0_Samples</code></li><li><code>make</code></li><li>进入 <code>NVIDIA_CUDA-9.0_Samples/bin/x86_64/linux/release</code> 文件夹<ul><li>执行： <code>./deviceQuery</code>, 可以看到类似如下信息</li></ul></li></ul></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line">./deviceQuery Starting...</div><div class="line"></div><div class="line"> CUDA Device Query (Runtime API) version (CUDART static linking)</div><div class="line"></div><div class="line">Detected 2 CUDA Capable device(s)</div><div class="line"></div><div class="line">Device 0: &quot;GeForce GTX 1080 Ti&quot;</div><div class="line">  CUDA Driver Version / Runtime Version          10.0 / 9.0</div><div class="line">  CUDA Capability Major/Minor version number:    6.1</div><div class="line">  Total amount of global memory:                 11174 MBytes (11717181440 bytes)</div><div class="line">  (28) Multiprocessors, (128) CUDA Cores/MP:     3584 CUDA Cores</div><div class="line">  GPU Max Clock rate:                            1683 MHz (1.68 GHz)</div><div class="line">  Memory Clock rate:                             5505 Mhz</div><div class="line">  Memory Bus Width:                              352-bit</div><div class="line">  L2 Cache Size:                                 2883584 bytes</div><div class="line">  Maximum Texture Dimension Size (x,y,z)         1D=(131072), 2D=(131072, 65536), 3D=(16384, 16384, 16384)</div><div class="line">  Maximum Layered 1D Texture Size, (num) layers  1D=(32768), 2048 layers</div><div class="line">  Maximum Layered 2D Texture Size, (num) layers  2D=(32768, 32768), 2048 layers</div><div class="line">  Total amount of constant memory:               65536 bytes</div><div class="line">  Total amount of shared memory per block:       49152 bytes</div><div class="line">  Total number of registers available per block: 65536</div><div class="line">  Warp size:                                     32</div><div class="line">  Maximum number of threads per multiprocessor:  2048</div><div class="line">  Maximum number of threads per block:           1024</div></pre></td></tr></table></figure><h2 id="三、安装cudnn"><a href="#三、安装cudnn" class="headerlink" title="三、安装cudnn"></a>三、安装cudnn</h2><ul><li>官网：<a href="https://developer.nvidia.com/rdp/cudnn-download" target="_blank" rel="external">https://developer.nvidia.com/rdp/cudnn-download</a></li><li>选择<code>cuda</code>对应的版本, 我的选择如下图</li></ul><p><img src="/assets/blog_images/GPU/17_cudnn.jpg" alt="cudnn版本" title="17_cudnn"></p><ul><li>安装<ul><li><code>tar -zxvf cudnn-9.0-linux-x64-v7.3.1.20.tgz</code></li><li>将解压得到的<code>cuda</code> 文件夹下的内容拷贝到对应的 <code>/usr/local/cuda-9.0</code>文件夹下即可</li></ul></li></ul><h2 id="四、安装Anaconda和tensorflow-gpu"><a href="#四、安装Anaconda和tensorflow-gpu" class="headerlink" title="四、安装Anaconda和tensorflow-gpu"></a>四、安装Anaconda和tensorflow-gpu</h2><ul><li>官网： <a href="https://www.anaconda.com/download/#linux" target="_blank" rel="external">https://www.anaconda.com/download/#linux</a></li><li>下载安装即可，我这里选择的是 <code>python3.7</code> 版本</li><li>安装之后添加到环境变量：</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"># anaconda3</div><div class="line">export PATH=/home/ubuntu/anaconda3/bin:$PATH</div></pre></td></tr></table></figure><ul><li><p>创建虚拟环境，防止污染他人使用环境</p><ul><li><code>conda create -n xxx python-3.6</code></li><li><code>conda install tensorflow-gpu</code></li></ul></li><li><p>测试</p></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line">sess = tf.Session(config=tf.ConfigProto(log_device_placement=<span class="keyword">True</span>))</div></pre></td></tr></table></figure><ul><li>打印如下信息：</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line">2018-10-25 16:25:35.683507: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1405] Found device 0 with properties:</div><div class="line">name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.683</div><div class="line">pciBusID: 0000:01:00.0</div><div class="line">totalMemory: 10.91GiB freeMemory: 10.72GiB</div><div class="line">2018-10-25 16:25:35.783459: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:897] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero</div><div class="line">2018-10-25 16:25:35.783843: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1405] Found device 1 with properties:</div><div class="line">name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.683</div><div class="line">pciBusID: 0000:02:00.0</div><div class="line">totalMemory: 10.92GiB freeMemory: 10.76GiB</div><div class="line">2018-10-25 16:25:35.784321: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1484] Adding visible gpu devices: 0, 1</div><div class="line">2018-10-25 16:25:36.069610: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Device interconnect StreamExecutor with strength 1 edge matrix:</div><div class="line">2018-10-25 16:25:36.069634: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971]      0 1</div><div class="line">2018-10-25 16:25:36.069637: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 0:   N Y</div><div class="line">2018-10-25 16:25:36.069639: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 1:   Y N</div><div class="line">2018-10-25 16:25:36.069852: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1097] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10367 MB memory) -&gt; physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)</div><div class="line">2018-10-25 16:25:36.101498: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1097] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 10409 MB memory) -&gt; physical GPU (device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:02:00.0, compute capability: 6.1)</div><div class="line">Device mapping:</div><div class="line">/job:localhost/replica:0/task:0/device:GPU:0 -&gt; device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1</div><div class="line">/job:localhost/replica:0/task:0/device:GPU:1 -&gt; device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:02:00.0, compute capability: 6.1</div><div class="line">2018-10-25 16:25:36.134430: I tensorflow/core/common_runtime/direct_session.cc:288] Device mapping:</div><div class="line">/job:localhost/replica:0/task:0/device:GPU:0 -&gt; device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1</div><div class="line">/job:localhost/replica:0/task:0/device:GPU:1 -&gt; device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:02:00.0, compute capability: 6.1</div></pre></td></tr></table></figure><h2 id="五、-多个cuda版本切换"><a href="#五、-多个cuda版本切换" class="headerlink" title="五、 多个cuda版本切换"></a>五、 多个cuda版本切换</h2><ul><li><p>安装<code>cuda-9.0</code> 会在 <code>/usr/local/</code> 目录下</p><ul><li>如下图，它会创建一个软连接指向了 <code>/usr/local/cuda-9.0/</code><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">(wangyongzhi_ml) ubuntu@ubuntu-System-Product-Name:/usr/local$ ll</div><div class="line">总用量 48</div><div class="line">drwxr-xr-x 12 root root 4096 10月 25 14:51 ./</div><div class="line">drwxr-xr-x 13 root root 4096 10月 25 09:39 ../</div><div class="line">drwxr-xr-x  2 root root 4096 4月  21  2016 bin/</div><div class="line">lrwxrwxrwx  1 root root   19 10月 25 00:41 cuda -&gt; /usr/local/cuda-9.0/</div><div class="line">drwxr-xr-x 19 root root 4096 10月 25 14:52 cuda-10.0/</div><div class="line">drwxr-xr-x 18 root root 4096 10月 25 00:41 cuda-9.0/</div><div class="line">drwxr-xr-x  2 root root 4096 4月  21  2016 etc/</div><div class="line">drwxr-xr-x  2 root root 4096 4月  21  2016 games/</div><div class="line">drwxr-xr-x  2 root root 4096 4月  21  2016 include/</div><div class="line">drwxr-xr-x  4 root root 4096 4月  21  2016 lib/</div><div class="line">lrwxrwxrwx  1 root root    9 10月 24 14:52 man -&gt; share/man/</div><div class="line">drwxr-xr-x  2 root root 4096 4月  21  2016 sbin/</div><div class="line">drwxr-xr-x  8 root root 4096 4月  21  2016 share/</div><div class="line">drwxr-xr-x  2 root root 4096 4月  21  2016 src/</div></pre></td></tr></table></figure></li></ul></li><li><p>所以正常安装<code>cuda</code> 其他版本，然后创建软连接指向对应的版本即可</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">sudo rm -rf cuda</div><div class="line">sudo ln <span class="_">-s</span> /usr/<span class="built_in">local</span>/cuda-10.0 /usr/<span class="built_in">local</span>/cuda</div></pre></td></tr></table></figure></li></ul><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li><a href="https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html" target="_blank" rel="external">https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html</a></li><li><a href="https://blog.csdn.net/u012235003/article/details/54575758" target="_blank" rel="external">https://blog.csdn.net/u012235003/article/details/54575758</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;ul&gt;
&lt;li&gt;之前有在阿里云GPU服务器上弄过： &lt;a href=&quot;http://lawlite.me/2017/12/25/%E9%98%BF%E9%87%8C%E4%BA%91GPU%E6%9C%8D%E5%8A%A1%E5%99%A8%E4%B8%8ATorch%E5%AE%89%E8%A3%85%E4%B8%8E%E4%BD%BF%E7%94%A8/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;点击查看&lt;/a&gt;， 这里从装&lt;code&gt;Nvidia&lt;/code&gt;开始&lt;h2 id=&quot;一、-安装Nvidia驱动&quot;&gt;&lt;a href=&quot;#一、-安装Nvidia驱动&quot; class=&quot;headerlink&quot; title=&quot;一、 安装Nvidia驱动&quot;&gt;&lt;/a&gt;一、 安装Nvidia驱动&lt;/h2&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;1-1-查找需要安装的Nvidia版本&quot;&gt;&lt;a href=&quot;#1-1-查找需要安装的Nvidia版本&quot; class=&quot;headerlink&quot; title=&quot;1.1 查找需要安装的Nvidia版本&quot;&gt;&lt;/a&gt;1.1 查找需要安装的Nvidia版本&lt;/h3&gt;&lt;h4 id=&quot;1-1-1-官网&quot;&gt;&lt;a href=&quot;#1-1-1-官网&quot; class=&quot;headerlink&quot; title=&quot;1.1.1 官网&quot;&gt;&lt;/a&gt;1.1.1 官网&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;官网上查找： &lt;a href=&quot;https://www.nvidia.com/Download/index.aspx?lang=en-us&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://www.nvidia.com/Download/index.aspx?lang=en-us&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;这里是 &lt;code&gt;GeForce GTX 1080 TI&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;如下图，推荐 &lt;code&gt;410&lt;/code&gt; 版本的&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="Tensorflow" scheme="http://lawlite.cn/tags/Tensorflow/"/>
    
      <category term="Cudnn" scheme="http://lawlite.cn/tags/Cudnn/"/>
    
  </entry>
  
  <entry>
    <title>Triplet-Loss原理及其实现</title>
    <link href="http://lawlite.cn/2018/10/16/Triplet-Loss%E5%8E%9F%E7%90%86%E5%8F%8A%E5%85%B6%E5%AE%9E%E7%8E%B0/"/>
    <id>http://lawlite.cn/2018/10/16/Triplet-Loss原理及其实现/</id>
    <published>2018-10-16T06:32:35.000Z</published>
    <updated>2019-05-04T04:44:04.826Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一、-Triplet-loss"><a href="#一、-Triplet-loss" class="headerlink" title="一、 Triplet loss"></a>一、 Triplet loss</h2><h3 id="1、介绍"><a href="#1、介绍" class="headerlink" title="1、介绍"></a>1、介绍</h3><ul><li><code>Triplet loss</code>最初是在 <a href="https://arxiv.org/abs/1503.03832" target="_blank" rel="external">FaceNet: A Unified Embedding for Face Recognition and Clustering</a> 论文中提出的，可以学到较好的人脸的<code>embedding</code></li><li>为什么不适用 <code>softmax</code>函数呢，<code>softmax</code>最终的类别数是确定的，而<code>Triplet loss</code>学到的是一个好的<code>embedding</code>，相似的图像在<code>embedding</code>空间里是相近的，可以判断是否是同一个人脸。<h3 id="2、原理"><a href="#2、原理" class="headerlink" title="2、原理"></a>2、原理</h3></li><li><p>输入是一个三元组 <code>&lt;a, p, n&gt;</code></p><ul><li><code>a： anchor</code></li><li><code>p： positive</code>, 与 <code>a</code> 是同一类别的样本</li><li><code>n： negative</code>, 与 <code>a</code> 是不同类别的样本</li></ul><a id="more"></a></li></ul><p><img src="/assets/blog_images/Triplet-Loss/01_triplet_loss%E7%A4%BA%E6%84%8F.png" alt="triplet loss示意" title="01_triplet_loss示意"></p><ul><li>公式是： $$L = max(d(a, p) - d(a, n) + margin, 0)$$<ul><li>所以最终的优化目标是拉近 <code>a, p</code> 的距离， 拉远 <code>a, n</code> 的距离</li><li><code>easy triplets</code>:  $L = 0$ 即 $d(a, p) +margin &lt; d(a, n)$，这种情况不需要优化，天然<code>a, p</code>的距离很近， <code>a, n</code>的距离远</li><li><code>hard triplets</code>:  $d(a, n) &lt; d(a, p)$,  即<code>a, p</code>的距离远</li><li><code>semi-hard triplets</code>: $d(a, p) &lt; d(a, n) &lt; d(a, p) + margin$, 即<code>a, n</code>的距离靠的很近，但是有一个<code>margin</code></li></ul></li></ul><p><img src="/assets/blog_images/Triplet-Loss/02_triplets.png" alt="三种triplets 情况" title="02_triplets"></p><ul><li><code>FaceNet</code> 中是随机选取<code>semi-hard triplets</code> 进行训练的, （也可以选择 <code>hard triplets</code>  或者两者一起进行训练）</li></ul><h3 id="3、训练方法"><a href="#3、训练方法" class="headerlink" title="3、训练方法"></a>3、训练方法</h3><h4 id="3-1-offline"><a href="#3-1-offline" class="headerlink" title="3.1 offline"></a>3.1 <code>offline</code></h4><ul><li>训练集<strong>所有数据</strong>经过计算得到对应的 <code>embeddings</code>, 可以得到 很多<code>&lt;i, j, k&gt;</code> 的三元组，然后再计算 <code>triplet loss</code></li><li>效率不高，因为需要过一遍所有的数据得到三元组，然后训练反向更新网络<h4 id="3-2-online"><a href="#3-2-online" class="headerlink" title="3.2 online"></a>3.2 <code>online</code></h4></li><li>从训练集中抽取<code>B</code>个样本，然后计算 <code>B</code> 个<code>embeddings</code>，可以产生 $B^3$ 个 <code>triplets</code> （当然其中有不合法的，因为需要的是<code>&lt;a, p, n&gt;</code>）</li></ul><p><img src="/assets/blog_images/Triplet-Loss/03_online_triplet_loss.png" alt="online triplet loss" title="03_online_triplet_loss"></p><ul><li>实际使用中采用此方法，又分为两种策略 （是在一篇<strong>行人重识别</strong>的论文中提到的 <a href="https://arxiv.org/abs/1703.07737" target="_blank" rel="external">In Defense of the Triplet Loss for Person Re-Identification</a>），假设 $B = PK$, 其中<code>P</code>个身份的人，每个身份的人<code>K</code>张图片（一般<code>K</code> 取 <code>4</code>）<ul><li><code>Batch All</code>:  计算<code>batch_size</code>中所有<code>valid</code>的的<code>hard triplet</code> 和 <code>semi-hard triplet</code>， 然后取平均得到<code>Loss</code><ul><li>注意因为很多 <code>easy triplets</code>的情况，所以平均会导致<code>Loss</code>很小，所以是对所有 <strong>valid</strong> 的所有求平均 （下面代码中会介绍）</li><li>可以产生 $PK(K-1)(PK-K)$个 <code>triplets</code><ul><li><code>PK</code>个 <code>anchor</code></li><li><code>K-1</code> 个 <code>positive</code></li><li><code>PK-K</code> 个 <code>negative</code></li></ul></li></ul></li><li><code>Batch Hard</code>:  对于每一个<code>anchor</code>， 选择距离最大的<code>d(a, p)</code>  和 距离最大的 <code>d(a, n)</code><ul><li>所以公有 $PK$ 个 三元组<code>triplets</code></li></ul></li></ul></li></ul><h2 id="二、-Tensorflow-中的实现"><a href="#二、-Tensorflow-中的实现" class="headerlink" title="二、 Tensorflow 中的实现"></a>二、 Tensorflow 中的实现</h2><ul><li><a href="https://github.com/lawlite19/Blog-Back-Up/tree/master/code/triplet-loss" target="_blank" rel="external">全部代码</a></li><li><code>Tensorflow</code> 中有实现好的<code>triplet loss</code> 接口，这里自己实现，（实现起来还是有点绕的, 有一些小细节问题）</li><li>使用<code>numpy</code>也仿照实现了，便于调试查看中间的结果, <a href="https://github.com/lawlite19/Blog-Back-Up/blob/master/code/triplet-loss/triplet_loss_np.py" target="_blank" rel="external">全部代码</a><h3 id="1、Batch-All"><a href="#1、Batch-All" class="headerlink" title="1、Batch All"></a>1、Batch All</h3><h4 id="1-1-计算两两embeddings的距离"><a href="#1-1-计算两两embeddings的距离" class="headerlink" title="1.1 计算两两embeddings的距离"></a>1.1 计算两两<code>embeddings</code>的距离</h4></li><li><code>numpy</code> 中的实现，便于调试理解， <a href="https://github.com/lawlite19/Blog-Back-Up/blob/master/code/triplet-loss/triplet_loss_np.py#L9" target="_blank" rel="external">点击查看</a></li><li>输入大小是<code>（batch_size, vector_size）</code>大小的 <code>embeddings</code> 向量</li><li>因为 $(a-b)^2 = a^2 -2ab + b^2$, 矩阵相乘 $embeddings \times embeddings^T$ 中包含<code>a*b</code>的值，对象线上是向量平方的值，所以可以直接使用矩阵计算</li><li>如果不使用平方，就开根号，<ul><li>注意根号下不能为<code>0</code>，<code>0</code>开根号是没有问题的，但是<code>Tensorflow</code>梯度反向传播是就会导致无穷大，所以加上一个平滑项<code>1e-16</code>，最后再修改回来。</li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">_pairwise_distance</span><span class="params">(embeddings, squared=False)</span>:</span></div><div class="line">    <span class="string">'''</span></div><div class="line">       计算两两embedding的距离</div><div class="line">       ------------------------------------------</div><div class="line">       Args：</div><div class="line">          embedding: 特征向量， 大小（batch_size, vector_size）</div><div class="line">          squared:   是否距离的平方，即欧式距离</div><div class="line">    </div><div class="line">       Returns：</div><div class="line">          distances: 两两embeddings的距离矩阵，大小 （batch_size, batch_size）</div><div class="line">    '''    </div><div class="line">    <span class="comment"># 矩阵相乘,得到（batch_size, batch_size），因为计算欧式距离|a-b|^2 = a^2 -2ab + b^2, </span></div><div class="line">    <span class="comment"># 其中 ab 可以用矩阵乘表示</span></div><div class="line">    dot_product = tf.matmul(embeddings, tf.transpose(embeddings))   </div><div class="line">    <span class="comment"># dot_product对角线部分就是 每个embedding的平方</span></div><div class="line">    square_norm = tf.diag_part(dot_product)</div><div class="line">    <span class="comment"># |a-b|^2 = a^2 - 2ab + b^2</span></div><div class="line">    <span class="comment"># tf.expand_dims(square_norm, axis=1)是（batch_size, 1）大小的矩阵，减去 （batch_size, batch_size）大小的矩阵，相当于每一列操作</span></div><div class="line">    distances = tf.expand_dims(square_norm, axis=<span class="number">1</span>) - <span class="number">2.0</span> * dot_product + tf.expand_dims(square_norm, axis=<span class="number">0</span>)</div><div class="line">    distances = tf.maximum(distances, <span class="number">0.0</span>)   <span class="comment"># 小于0的距离置为0</span></div><div class="line">    <span class="keyword">if</span> <span class="keyword">not</span> squared:          <span class="comment"># 如果不平方，就开根号，但是注意有0元素，所以0的位置加上 1e*-16</span></div><div class="line">        distances = distances + mask * <span class="number">1e-16</span></div><div class="line">        distances = tf.sqrt(distances)</div><div class="line">        distances = distances * (<span class="number">1.0</span> - mask)    <span class="comment"># 0的部分仍然置为0</span></div><div class="line">    <span class="keyword">return</span> distances</div></pre></td></tr></table></figure><h4 id="1-2-计算valid-mask"><a href="#1-2-计算valid-mask" class="headerlink" title="1.2 计算valid mask"></a>1.2 计算valid mask</h4><ul><li><code>numpy</code> 中的实现， <a href="https://github.com/lawlite19/Blog-Back-Up/blob/master/code/triplet-loss/triplet_loss_np.py#L27" target="_blank" rel="external">点击查看</a></li><li>上面得到了 <code>(batch_size, batch_size)</code> 大小的距离矩阵，然后就可以计算所有 <code>embeddings</code> 组成的三元组<code>&lt;i, j, k&gt;</code>损失</li><li>但是不是所有的三元组都是 <code>valid</code> 的, 要是<code>&lt;a, p, n&gt;</code>的形式，所以计算一个<code>3D</code>的<code>mask</code>，然后乘上得到的 <code>(batch_size, batch_size, batch_size)</code>的所有三元组的损失即可，如何得到<code>mask</code>呢</li><li><code>&lt;i, j, k&gt;</code>要满足<ul><li><code>i, j, k</code>不相等</li><li><code>labels[i] == labels[j] and labels[i] != labels[k]</code></li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">_get_triplet_mask</span><span class="params">(labels)</span>:</span></div><div class="line">    <span class="string">'''</span></div><div class="line">       得到一个3D的mask [a, p, n], 对应triplet（a, p, n）是valid的位置是True</div><div class="line">       ----------------------------------</div><div class="line">       Args:</div><div class="line">          labels: 对应训练数据的labels, shape = (batch_size,)</div><div class="line">       </div><div class="line">       Returns:</div><div class="line">          mask: 3D,shape = (batch_size, batch_size, batch_size)</div><div class="line">    </div><div class="line">    '''</div><div class="line">    </div><div class="line">    <span class="comment"># 初始化一个二维矩阵，坐标(i, j)不相等置为1，得到indices_not_equal</span></div><div class="line">    indices_equal = tf.cast(tf.eye(tf.shape(labels)[<span class="number">0</span>]), tf.bool)</div><div class="line">    indices_not_equal = tf.logical_not(indices_equal)</div><div class="line">    <span class="comment"># 因为最后得到一个3D的mask矩阵(i, j, k)，增加一个维度，则 i_not_equal_j 在第三个维度增加一个即，(batch_size, batch_size, 1), 其他同理</span></div><div class="line">    i_not_equal_j = tf.expand_dims(indices_not_equal, <span class="number">2</span>) </div><div class="line">    i_not_equal_k = tf.expand_dims(indices_not_equal, <span class="number">1</span>)</div><div class="line">    j_not_equal_k = tf.expand_dims(indices_not_equal, <span class="number">0</span>)</div><div class="line">    <span class="comment"># 想得到i!=j!=k, 三个不等取and即可, 最后可以得到当下标（i, j, k）不相等时才取True</span></div><div class="line">    distinct_indices = tf.logical_and(tf.logical_and(i_not_equal_j, i_not_equal_k), j_not_equal_k)</div><div class="line"></div><div class="line">    <span class="comment"># 同样根据labels得到对应i=j, i!=k</span></div><div class="line">    label_equal = tf.equal(tf.expand_dims(labels, <span class="number">0</span>), tf.expand_dims(labels, <span class="number">1</span>))</div><div class="line">    i_equal_j = tf.expand_dims(label_equal, <span class="number">2</span>)</div><div class="line">    i_equal_k = tf.expand_dims(label_equal, <span class="number">1</span>)</div><div class="line">    valid_labels = tf.logical_and(i_equal_j, tf.logical_not(i_equal_k))</div><div class="line">    <span class="comment"># mask即为满足上面两个约束，所以两个3D取and</span></div><div class="line">    mask = tf.logical_and(distinct_indices, valid_labels)</div><div class="line">    <span class="keyword">return</span> mask</div></pre></td></tr></table></figure><h4 id="1-3-计算triplet-loss"><a href="#1-3-计算triplet-loss" class="headerlink" title="1.3 计算triplet loss"></a>1.3 计算triplet loss</h4><ul><li><code>numpy</code> 中的实现， <a href="https://github.com/lawlite19/Blog-Back-Up/blob/master/code/triplet-loss/triplet_loss_np.py#L66" target="_blank" rel="external">点击查看</a></li><li><code>1.1</code> 中计算得到了两两<code>embeddings</code>的距离，大小 <code>（batch_size, batch_size）</code>, 需要得到所有三元组的<code>triplet loss</code>， 即<code>（batch_size, batch_size, batch_size)</code>大小</li><li>为什么<code>triplet_loss = anchor_positive_dist - anchor_negative_dist + margin</code> 可以得到所有<code>(i, j, k)</code>的<code>triplet loss</code>， <ul><li>如下图，<code>x0y</code>平面的是<code>anchor_positive_dist</code>的距离矩阵（其实是<code>3D</code>的, 想象一下）</li><li><code>x0z</code>平面是<code>anchor_negative_dist</code>的距离矩阵（也是<code>3D</code>的）</li><li>两个相减, 比如<code>0-0 = 0</code>就相当于<code>i=0, j=0</code>的距离，减去 <code>j=0, k=0</code>的距离</li><li>以此类推，得到所有三元组的<code>loss</code></li></ul></li></ul><p><img src="/assets/blog_images/Triplet-Loss/04_triplet_loss%E4%BE%8B%E5%AD%90.png" alt="triplet loss 例子" title="04_triplet_loss例子"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">batch_all_triplet_loss</span><span class="params">(labels, embeddings, margin, squared=False)</span>:</span></div><div class="line">    <span class="string">'''</span></div><div class="line">       triplet loss of a batch</div><div class="line">       -------------------------------</div><div class="line">       Args:</div><div class="line">          labels:     标签数据，shape = （batch_size,）</div><div class="line">          embeddings: 提取的特征向量， shape = (batch_size, vector_size)</div><div class="line">          margin:     margin大小， scalar</div><div class="line">          </div><div class="line">       Returns:</div><div class="line">          triplet_loss: scalar, 一个batch的损失值</div><div class="line">          fraction_postive_triplets : valid的triplets占的比例</div><div class="line">    '''</div><div class="line">    </div><div class="line">    <span class="comment"># 得到每两两embeddings的距离，然后增加一个维度，一维需要得到（batch_size, batch_size, batch_size）大小的3D矩阵</span></div><div class="line">    <span class="comment"># 然后再点乘上valid 的 mask即可</span></div><div class="line">    pairwise_dis = _pairwise_distance(embeddings, squared=squared)</div><div class="line">    anchor_positive_dist = tf.expand_dims(pairwise_dis, <span class="number">2</span>)</div><div class="line">    <span class="keyword">assert</span> anchor_positive_dist.shape[<span class="number">2</span>] == <span class="number">1</span>, <span class="string">"&#123;&#125;"</span>.format(anchor_positive_dist.shape)</div><div class="line">    anchor_negative_dist = tf.expand_dims(pairwise_dis, <span class="number">1</span>)</div><div class="line">    <span class="keyword">assert</span> anchor_negative_dist.shape[<span class="number">1</span>] == <span class="number">1</span>, <span class="string">"&#123;&#125;"</span>.format(anchor_negative_dist.shape)</div><div class="line">    triplet_loss = anchor_positive_dist - anchor_negative_dist + margin</div><div class="line">    </div><div class="line">    mask = _get_triplet_mask(labels)</div><div class="line">    mask = tf.to_float(mask)</div><div class="line">    triplet_loss = tf.multiply(mask, triplet_loss)</div><div class="line">    triplet_loss = tf.maximum(triplet_loss, <span class="number">0.0</span>)</div><div class="line">    </div><div class="line">    <span class="comment"># 计算valid的triplet的个数，然后对所有的triplet loss求平均</span></div><div class="line">    valid_triplets = tf.to_float(tf.greater(triplet_loss, <span class="number">1e-16</span>))</div><div class="line">    num_positive_triplets = tf.reduce_sum(valid_triplets)</div><div class="line">    num_valid_triplets = tf.reduce_sum(mask)</div><div class="line">    fraction_postive_triplets = num_positive_triplets / (num_valid_triplets + <span class="number">1e-16</span>)</div><div class="line">    </div><div class="line">    triplet_loss = tf.reduce_sum(triplet_loss) / (num_positive_triplets + <span class="number">1e-16</span>)</div><div class="line">    <span class="keyword">return</span> triplet_loss, fraction_postive_triplets</div></pre></td></tr></table></figure><h3 id="2、Batch-Hard"><a href="#2、Batch-Hard" class="headerlink" title="2、Batch Hard"></a>2、Batch Hard</h3><ul><li><code>numpy</code> 中的实现，<a href="https://github.com/lawlite19/Blog-Back-Up/blob/master/code/triplet-loss/triplet_loss_np.py#L103" target="_blank" rel="external">点击查看</a></li><li>因为最后只有$PK$个<code>triplet</code>, 从 <code>positive</code> 中选择距离最大的，从 <code>negative</code> 中选择距离最小的即可<h4 id="2-1-计算positive-mask"><a href="#2-1-计算positive-mask" class="headerlink" title="2.1 计算positive mask"></a>2.1 计算positive mask</h4></li><li>满足 <code>a!=p and  a, p label一致即可</code></li><li>之后用<code>mask</code> 乘上计算的<code>pairwice_distances</code>， 然后取每行最大值即为每个样本对应 <code>positive</code> 的最大距离<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">_get_anchor_positive_triplet_mask</span><span class="params">(labels)</span>:</span></div><div class="line">    <span class="string">''' </span></div><div class="line">       得到合法的positive的mask， 即2D的矩阵，[a, p], a!=p and a和p相同labels</div><div class="line">       ------------------------------------------------</div><div class="line">       Args:</div><div class="line">          labels: 标签数据，shape = (batch_size, )</div><div class="line">          </div><div class="line">       Returns:</div><div class="line">          mask: 合法的positive mask, shape = (batch_size, batch_size)</div><div class="line">    '''</div><div class="line">    indices_equal = tf.cast(tf.eye(tf.shape(labels)[<span class="number">0</span>]), tf.bool)</div><div class="line">    indices_not_equal = tf.logical_not(indices_equal)                 <span class="comment"># （i, j）不相等</span></div><div class="line">    labels_equal = tf.equal(tf.expand_dims(labels, <span class="number">0</span>), tf.expand_dims(labels, <span class="number">1</span>))  <span class="comment"># labels相等，</span></div><div class="line">    mask = tf.logical_and(indices_not_equal, labels_equal)            <span class="comment"># 取and即可</span></div><div class="line">    <span class="keyword">return</span> mask</div></pre></td></tr></table></figure></li></ul><h4 id="2-2-计算negative-mask"><a href="#2-2-计算negative-mask" class="headerlink" title="2.2 计算negative mask"></a>2.2 计算negative mask</h4><ul><li>只需 <code>[a, n]</code> 对应的 <code>labels</code> 不一致即可<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">_get_anchor_negative_triplet_mask</span><span class="params">(labels)</span>:</span></div><div class="line">    <span class="string">'''</span></div><div class="line">       得到negative的2D mask, [a, n] 只需a, n不同且有不同的labels</div><div class="line">       ------------------------------------------------</div><div class="line">       Args:</div><div class="line">          labels: 标签数据，shape = (batch_size, )</div><div class="line">          </div><div class="line">       Returns:</div><div class="line">          mask: negative mask, shape = (batch_size, batch_size)</div><div class="line">    '''</div><div class="line">    labels_equal = tf.equal(tf.expand_dims(labels, <span class="number">0</span>), tf.expand_dims(labels, <span class="number">1</span>))</div><div class="line">    mask = tf.logical_not(labels_equal)</div><div class="line">    <span class="keyword">return</span> mask</div></pre></td></tr></table></figure></li></ul><h4 id="2-3-batch-hard-loss"><a href="#2-3-batch-hard-loss" class="headerlink" title="2.3 batch hard loss"></a>2.3 batch hard loss</h4><ul><li>计算最大 <code>positive</code> 距离时直接取 <code>valid</code> 的每一行的最大值即可</li><li>计算最小<code>negative</code> 距离时不能直接取每一行的最小值，因为 <code>invalid</code> 位置的值为 <code>0</code>，所以可以在 <code>invalid</code> 位置加上每一行的最大值，然后就可以取每一行的最小值了<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">batch_hard_triplet_loss</span><span class="params">(labels, embeddings, margin, squared=False)</span>:</span></div><div class="line">    <span class="string">'''</span></div><div class="line">       batch hard triplet loss of a batch， 每个样本最大的positive距离 - 对应样本最小的negative距离</div><div class="line">       ------------------------------------</div><div class="line">       Args:</div><div class="line">          labels:     标签数据，shape = （batch_size,）</div><div class="line">          embeddings: 提取的特征向量， shape = (batch_size, vector_size)</div><div class="line">          margin:     margin大小， scalar</div><div class="line">          </div><div class="line">       Returns:</div><div class="line">          triplet_loss: scalar, 一个batch的损失值</div><div class="line">    '''</div><div class="line">    pairwise_distances = _pairwise_distance(embeddings)</div><div class="line">    mask_anchor_positive = _get_anchor_positive_triplet_mask(labels)</div><div class="line">    mask_anchor_positive = tf.to_float(mask_anchor_positive)</div><div class="line">    anchor_positive_dist = tf.multiply(mask_anchor_positive, pairwise_distances)</div><div class="line">    hardest_positive_dist = tf.reduce_max(anchor_positive_dist, axis=<span class="number">1</span>, keepdims=<span class="keyword">True</span>)  <span class="comment"># 取每一行最大的值即为最大positive距离</span></div><div class="line">    tf.summary.scalar(<span class="string">"hardest_positive_dis"</span>, tf.reduce_mean(hardest_positive_dist))</div><div class="line">    </div><div class="line">    <span class="string">'''取每一行最小值得时候，因为invalid [a, n]置为了0， 所以不能直接取，这里对应invalid位置加上每一行的最大值即可，然后再取最小的值'''</span></div><div class="line">    mask_anchor_negative = _get_anchor_negative_triplet_mask(labels)</div><div class="line">    mask_anchor_negative = tf.to_float(mask_anchor_negative)</div><div class="line">    max_anchor_negative_dist = tf.reduce_max(pairwise_distances, axis=<span class="number">1</span>, keepdims=<span class="keyword">True</span>)   <span class="comment"># 每一样最大值</span></div><div class="line">    anchor_negative_dist = pairwise_distances + max_anchor_negative_dist * (<span class="number">1.0</span> - mask_anchor_negative)  <span class="comment"># (1.0 - mask_anchor_negative)即为invalid位置</span></div><div class="line">    hardest_negative_dist = tf.reduce_min(anchor_negative_dist, axis=<span class="number">1</span>, keepdims=<span class="keyword">True</span>)</div><div class="line">    tf.summary.scalar(<span class="string">"hardest_negative_dist"</span>, tf.reduce_mean(hardest_negative_dist))</div><div class="line">    </div><div class="line">    triplet_loss = tf.maximum(hardest_positive_dist - hardest_negative_dist + margin, <span class="number">0.0</span>)</div><div class="line">    triplet_loss = tf.reduce_mean(triplet_loss)</div><div class="line">    <span class="keyword">return</span> triplet_loss</div></pre></td></tr></table></figure></li></ul><h2 id="三、具体使用"><a href="#三、具体使用" class="headerlink" title="三、具体使用"></a>三、具体使用</h2><ul><li>使用 <code>mnist</code> 数据集和 <code>triplet loss</code> 训练，最后得到的 <code>embeddings</code>应该是同一类别的靠在一起</li><li>因为只有 <code>10</code> 个类别，所以直接随机取 <code>batch</code> 大小的数据，这里<code>batch_size=64</code>,<ul><li>注意如果类别很多时，就不能随机构建<code>batch</code> 了， 需要选 <code>P</code> 个类别，然后每个类别选 <code>K</code> 张图 </li></ul></li></ul><h3 id="3-1-构建模型"><a href="#3-1-构建模型" class="headerlink" title="3.1 构建模型"></a>3.1 构建模型</h3><ul><li>上一篇介绍了 <a href="http://lawlite.me/2018/05/31/Tensorflow%E9%AB%98%E7%BA%A7API/" target="_blank" rel="external">tensorflow的高级API</a>, 这里使用 <code>Estimator</code> 构建模型</li><li>全部代码：<a href="https://github.com/lawlite19/Blog-Back-Up/blob/master/code/triplet-loss/train_with_triplet_loss.py" target="_blank" rel="external">点击查看</a><h4 id="3-1-1-使用Estimator"><a href="#3-1-1-使用Estimator" class="headerlink" title="3.1.1 使用Estimator"></a>3.1.1 使用Estimator</h4></li><li><code>params</code>  指定超参数， 这里保存为<code>json</code> 格式的文件，<ul><li>配置为：</li></ul></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line">&#123;</div><div class="line">    &quot;learning_rate&quot;: 1e-3,</div><div class="line">    &quot;batch_size&quot;: 64,</div><div class="line">    &quot;num_epochs&quot;: 20,</div><div class="line"></div><div class="line">    &quot;num_channels&quot;: 32,</div><div class="line">    &quot;use_batch_norm&quot;: false,</div><div class="line">    &quot;bn_momentum&quot;: 0.9,</div><div class="line">    &quot;margin&quot;: 0.5,</div><div class="line">    &quot;embedding_size&quot;: 64,</div><div class="line">    &quot;triplet_strategy&quot;: &quot;batch_all&quot;,</div><div class="line">    &quot;squared&quot;: false,</div><div class="line"></div><div class="line">    &quot;image_size&quot;: 28,</div><div class="line">    &quot;num_labels&quot;: 10,</div><div class="line">    &quot;train_size&quot;: 50000,</div><div class="line">    &quot;eval_size&quot;: 10000,</div><div class="line"></div><div class="line">    &quot;num_parallel_calls&quot;: 4,</div><div class="line">    &quot;save_summary_steps&quot;: 50</div><div class="line">&#125;</div></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">(argv)</span>:</span></div><div class="line">    args = parser.parse_args(argv[<span class="number">1</span>:])</div><div class="line">    tf.logging.info(<span class="string">"创建模型...."</span>)</div><div class="line">    <span class="keyword">with</span> open(args.model_config) <span class="keyword">as</span> f:</div><div class="line">        params = json.load(f)</div><div class="line">    config = tf.estimator.RunConfig(model_dir=args.model_dir, tf_random_seed=<span class="number">100</span>)  <span class="comment"># config</span></div><div class="line">    cls = tf.estimator.Estimator(model_fn=my_model, config=config, params=params)  <span class="comment"># 建立模型</span></div><div class="line">    tf.logging.info(<span class="string">"开始训练模型,共&#123;&#125; epochs...."</span>.format(params[<span class="string">'num_epochs'</span>]))</div><div class="line">    cls.train(input_fn = <span class="keyword">lambda</span>: train_input_fn(args.data_dir, params))            <span class="comment"># 训练模型，指定输入</span></div><div class="line">    </div><div class="line">    tf.logging.info(<span class="string">"测试集评价模型...."</span>)</div><div class="line">    res = cls.evaluate(input_fn = <span class="keyword">lambda</span>: test_input_fn(args.data_dir, params))    <span class="comment"># 测试模型，指定输入</span></div><div class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> res:</div><div class="line">        print(<span class="string">"评价---&#123;&#125; : &#123;&#125;"</span>.format(key, res[key]))</div></pre></td></tr></table></figure><h4 id="3-1-2-model-fn函数"><a href="#3-1-2-model-fn函数" class="headerlink" title="3.1.2 model_fn函数"></a>3.1.2 model_fn函数</h4><ul><li>下面都有对应注释</li><li>计算 <code>embedding_mean_norm</code> 中每一行 <code>embeding</code> 公式为： $||A||_F = [\sum_{i,j} abs(a_{i,j})^2]^{1/2}$ , 然后再取均值</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">my_model</span><span class="params">(features, labels, mode, params)</span>:</span></div><div class="line">    <span class="string">'''</span></div><div class="line">       model_fn指定函数，构建模型，训练等</div><div class="line">       ---------------------------------</div><div class="line">       Args:</div><div class="line">          features: 输入，shape = (batch_size, 784)</div><div class="line">          labels:   输出，shape = (batch_size, )</div><div class="line">          mode:     str, 阶段</div><div class="line">          params:   dict, 超参数</div><div class="line">    '''</div><div class="line">    is_training = (mode == tf.estimator.ModeKeys.TRAIN)</div><div class="line">    images = features</div><div class="line">    images = tf.reshape(images, shape=[<span class="number">-1</span>, params[<span class="string">'image_size'</span>], params[<span class="string">'image_size'</span>], <span class="number">1</span>])  <span class="comment"># reshape (batch_size, img_size, img_size, 1)</span></div><div class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"model"</span>):</div><div class="line">        embeddings = build_model(is_training, images, params)  <span class="comment"># 简历模型</span></div><div class="line">    </div><div class="line">    <span class="keyword">if</span> mode == tf.estimator.ModeKeys.PREDICT:     <span class="comment"># 如果是预测阶段，直接返回得到embeddings</span></div><div class="line">        predictions = &#123;<span class="string">'embeddings'</span>: embeddings&#125;</div><div class="line">        <span class="keyword">return</span> tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)</div><div class="line">    </div><div class="line">    <span class="string">'''调用对应的triplet loss'''</span></div><div class="line">    labels = tf.cast(labels, tf.int64)</div><div class="line">    <span class="keyword">if</span> params[<span class="string">'triplet_strategy'</span>] == <span class="string">'batch_all'</span>:</div><div class="line">        loss, fraction = batch_all_triplet_loss(labels, embeddings, margin=params[<span class="string">'margin'</span>], squared=params[<span class="string">'squared'</span>])</div><div class="line">    <span class="keyword">elif</span> params[<span class="string">'triplet_strategy'</span>] == <span class="string">'batch_hard'</span>:</div><div class="line">        loss = batch_hard_triplet_loss(labels, embeddings, margin=params[<span class="string">'margin'</span>], squared=params[<span class="string">'squared'</span>])</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        <span class="keyword">raise</span> ValueError(<span class="string">"triplet_strategy 配置不正确: &#123;&#125;"</span>.format(params[<span class="string">'triplet_strategy'</span>]))</div><div class="line">    </div><div class="line">    embedding_mean_norm = tf.reduce_mean(tf.norm(embeddings, axis=<span class="number">1</span>))     <span class="comment"># 这里计算了embeddings的二范数的均值 </span></div><div class="line">    tf.summary.scalar(<span class="string">"embedding_mean_norm"</span>, embedding_mean_norm)</div><div class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"metrics"</span>):</div><div class="line">        eval_metric_ops = &#123;<span class="string">'embedding_mean_norm'</span>: tf.metrics.mean(embedding_mean_norm)&#125;</div><div class="line">        <span class="keyword">if</span> params[<span class="string">'triplet_strategy'</span>] == <span class="string">'batch_all'</span>:</div><div class="line">            eval_metric_ops[<span class="string">'fraction_positive_triplets'</span>] = tf.metrics.mean(fraction)</div><div class="line">    <span class="keyword">if</span> mode == tf.estimator.ModeKeys.EVAL:</div><div class="line">        <span class="keyword">return</span> tf.estimator.EstimatorSpec(mode, loss=loss, eval_metric_ops=eval_metric_ops)</div><div class="line">    </div><div class="line">    tf.summary.scalar(<span class="string">'loss'</span>, loss)</div><div class="line">    <span class="keyword">if</span> params[<span class="string">'triplet_strategy'</span>] == <span class="string">"batch_all"</span>:</div><div class="line">        tf.summary.scalar(<span class="string">'fraction_positive_triplets'</span>, fraction)</div><div class="line">    tf.summary.image(<span class="string">'train_image'</span>, images, max_outputs=<span class="number">1</span>)   <span class="comment"># 1代表1个channel</span></div><div class="line">    </div><div class="line">    optimizer = tf.train.AdamOptimizer(learning_rate=params[<span class="string">'learning_rate'</span>])</div><div class="line">    global_step = tf.train.get_global_step()</div><div class="line">    <span class="keyword">if</span> params[<span class="string">'use_batch_norm'</span>]:</div><div class="line">        <span class="string">'''如果使用BN，需要估计batch上的均值和方差，tf.get_collection(tf.GraphKeys.UPDATE_OPS)就可以得到</span></div><div class="line">        tf.control_dependencies计算完之后再进行里面的操作</div><div class="line">        '''</div><div class="line">        <span class="keyword">with</span> tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):</div><div class="line">            train_op = optimizer.minimize(loss, global_step=global_step)</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        train_op = optimizer.minimize(loss, global_step=global_step)</div><div class="line">    <span class="keyword">return</span> tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)</div></pre></td></tr></table></figure><h4 id="3-1-3-构建模型，得到embeddings"><a href="#3-1-3-构建模型，得到embeddings" class="headerlink" title="3.1.3 构建模型，得到embeddings"></a>3.1.3 构建模型，得到embeddings</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_model</span><span class="params">(is_training, images, params)</span>:</span></div><div class="line">    <span class="string">'''</span></div><div class="line">       建立模型</div><div class="line">       ----------------------------</div><div class="line">       Args：</div><div class="line">          is_training: bool, 是否是训练阶段，可以从mode中判断</div><div class="line">          images：     (batch_size, 28*28*1), 输入mnist数据</div><div class="line">          params:      dict, 一些超参数</div><div class="line">          </div><div class="line">       Returns:</div><div class="line">          out: 输出的embeddings, shape = (batch_size, 64)</div><div class="line">    '''</div><div class="line">    num_channel = params[<span class="string">'num_channels'</span>]</div><div class="line">    bn_momentum = params[<span class="string">'bn_momentum'</span>]</div><div class="line">    channels = [num_channel, num_channel * <span class="number">2</span>]</div><div class="line">    out = images</div><div class="line">    <span class="keyword">for</span> i, c <span class="keyword">in</span> enumerate(channels):</div><div class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">"block_&#123;&#125;"</span>.format(i)):</div><div class="line">            out = tf.layers.conv2d(out, c, <span class="number">3</span>, padding=<span class="string">'same'</span>)</div><div class="line">            <span class="keyword">if</span> params[<span class="string">'use_batch_norm'</span>]:</div><div class="line">                out = tf.layers.batch_normalization(out, momentum=bn_momentum, training=is_training)</div><div class="line">            out = tf.nn.relu(out)</div><div class="line">            out = tf.layers.max_pooling2d(out, <span class="number">2</span>, <span class="number">2</span>)</div><div class="line">    <span class="keyword">assert</span> out.shape[<span class="number">1</span>:] == [<span class="number">7</span>, <span class="number">7</span>, num_channel * <span class="number">2</span>]</div><div class="line">    out = tf.reshape(out, [<span class="number">-1</span>, <span class="number">7</span>*<span class="number">7</span>*num_channel*<span class="number">2</span>])</div><div class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"fc_1"</span>):</div><div class="line">        out = tf.layers.dense(out, params[<span class="string">'embedding_size'</span>])</div><div class="line">    <span class="keyword">return</span> out</div></pre></td></tr></table></figure><h3 id="3-2-训练结果"><a href="#3-2-训练结果" class="headerlink" title="3.2 训练结果"></a>3.2 训练结果</h3><h4 id="3-2-1-batch-all"><a href="#3-2-1-batch-all" class="headerlink" title="3.2.1 batch all"></a>3.2.1 batch all</h4><ul><li><code>python  train_with_triplet_loss.py</code></li><li>可以在 <code>tensorboard</code> 中查看<ul><li><code>tensorboard --logdir experiment/model/</code></li></ul></li><li><p>embeddings_mean_norm</p><ul><li>[ 可以看到是上升的，因为我们要学到可分性好的 <code>embeddings</code>， 那么其方差应该是偏大的，均值应该是变大的 ]<br><img src="/assets/blog_images/Triplet-Loss/05_batch_all_embeddings_mean_norm.jpg" alt="embeddings_mean_norm" title="05_batch_all_embeddings_mean_norm"></li></ul></li><li><p>fraction positive</p><ul><li>这个是收敛的，因为随着优化占的比例是越来越少<br><img src="/assets/blog_images/Triplet-Loss/06_batch_all_fraction_positive.jpg" alt="fraction positive" title="06_batch_all_fraction_positive"></li></ul></li><li><p>loss</p><ul><li>注意这里的 <code>loss</code> 一般不是收敛的，因为是计算的 <code>semi-hard</code> 和 <code>hard</code> 的距离均值，因为<strong>每次是先选择出</strong> <code>semi-hard</code> 和 <code>hard</code> 的 <code>triplet</code>, 那么上次优化后的可能就选择不到了，所以 <code>loss</code> 并不会收敛，<code>但是fraction_postive_triplets</code> 是收敛的，因为随着优化占的比例是越来越少的</li></ul></li></ul><p><img src="/assets/blog_images/Triplet-Loss/07_batch_all_loss.jpg" alt="loss" title="07_batch_all_loss"></p><h4 id="3-2-2-batch-hard"><a href="#3-2-2-batch-hard" class="headerlink" title="3.2.2 batch hard"></a>3.2.2 batch hard</h4><ul><li>embeddings mean norm</li></ul><p><img src="/assets/blog_images/Triplet-Loss/09_batch_hard_embeddings_mean_norm.jpg" alt="embeddings mean norm" title="09_batch_hard_embeddings_mean_norm"></p><ul><li>positive and negative distance<ul><li>这里我原以为应该是 <code>negative</code> 应该是增大的，<code>positive</code> 应该是减小的，但实际结果是 <code>positive</code> 也是增大的，因为我们计算 <code>loss</code> 是<code>triplet_loss = tf.maximum(hardest_positive_dist - hardest_negative_dist + margin, 0.0)</code>， 只要 <code>negative</code> 的距离大于 <code>positive + margin</code> 就是 <code>0</code> 了，所以只要满足就行， 用<code>BN</code> 训练的效果可能更好一点。（有什么其他看法的可以交流一下）</li></ul></li></ul><p><img src="/assets/blog_images/Triplet-Loss/10_batch_hard_positive_negative_dis.jpg" alt="positive and negative distance" title="10_batch_hard_positive_negative_dis"></p><ul><li>loss<ul><li><code>batch hard</code> 的 <code>loss</code> 就应该是收敛的了</li></ul></li></ul><p><img src="/assets/blog_images/Triplet-Loss/12_pca.jpg" alt="loss" title="12_pca"></p><h3 id="3-3-可视化embedding"><a href="#3-3-可视化embedding" class="headerlink" title="3.3 可视化embedding"></a>3.3 可视化embedding</h3><ul><li>全部代码： <a href="https://github.com/lawlite19/Blog-Back-Up/blob/master/code/triplet-loss/visualize_embeddings.py" target="_blank" rel="external">点击查看</a></li><li>之前在 <code>tensorflow</code> 工具中使用过： <a href="http://lawlite.me/2017/06/24/Tensorflow%E5%AD%A6%E4%B9%A0-%E5%B7%A5%E5%85%B7%E7%9B%B8%E5%85%B3/#3%E3%80%81%E5%8F%AF%E8%A7%86%E5%8C%96embedding" target="_blank" rel="external">点击查看</a></li><li>这里将可视化 <code>embeddings</code> 的训练数据都放在 <code>experiment/log</code>文件夹下<ul><li><strong>另外</strong>我使用 <code>tensorflow 1.11</code> 出现问题，这里使用的版本是 <code>tensorflow 1.10</code></li></ul></li><li>加载训练的模型，预测得到embeddings</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">args = parser.parse_args(argv[<span class="number">1</span>:])</div><div class="line"><span class="keyword">with</span> open(args.model_config) <span class="keyword">as</span> f:</div><div class="line">    params = json.load(f)</div><div class="line">tf.logging.info(<span class="string">"创建模型...."</span>)</div><div class="line">config = tf.estimator.RunConfig(model_dir=args.model_dir, tf_random_seed=<span class="number">100</span>)  <span class="comment"># config</span></div><div class="line">cls = tf.estimator.Estimator(model_fn=my_model, config=config, params=params)  <span class="comment"># 建立模型</span></div><div class="line"></div><div class="line">tf.logging.info(<span class="string">"预测...."</span>)</div><div class="line"></div><div class="line">predictions = cls.predict(input_fn=<span class="keyword">lambda</span>: test_input_fn(args.data_dir, params))</div><div class="line">embeddings = np.zeros((<span class="number">10000</span>, params[<span class="string">'embedding_size'</span>]))</div><div class="line"><span class="keyword">for</span> i, p <span class="keyword">in</span> enumerate(predictions):</div><div class="line">    embeddings[i] = p[<span class="string">'embeddings'</span>]</div><div class="line">tf.logging.info(<span class="string">"embeddings shape: &#123;&#125;"</span>.format(embeddings.shape))</div></pre></td></tr></table></figure><ul><li>获得label数据，保存为 <code>metadata.tsv</code>文件</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</div><div class="line">    <span class="comment"># Obtain the test labels</span></div><div class="line">    dataset = mnist_dataset.test(args.data_dir)</div><div class="line">    dataset = dataset.map(<span class="keyword">lambda</span> img, lab: lab)</div><div class="line">    dataset = dataset.batch(<span class="number">10000</span>)</div><div class="line">    labels_tensor = dataset.make_one_shot_iterator().get_next()</div><div class="line">    labels = sess.run(labels_tensor)   </div><div class="line"></div><div class="line">np.savetxt(os.path.join(args.log_dir, <span class="string">'metadata.tsv'</span>), labels, fmt=<span class="string">'%d'</span>)</div></pre></td></tr></table></figure><ul><li>可视化<code>embedding</code></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line">shutil.copy(args.sprite_filename, args.log_dir)</div><div class="line"><span class="string">'''可视化embeddings'''</span></div><div class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</div><div class="line">    <span class="comment"># 1. Variable</span></div><div class="line">    embedding_var = tf.Variable(embeddings, name=<span class="string">"mnist_embeddings"</span>)</div><div class="line">    <span class="comment">#tf.global_variables_initializer().run()  # 不需要</span></div><div class="line">    </div><div class="line">    <span class="comment"># 2. 保存到文件中，embeddings.ckpt</span></div><div class="line">    saver = tf.train.Saver()</div><div class="line">    sess.run(embedding_var.initializer)</div><div class="line">    saver.save(sess, os.path.join(args.log_dir, <span class="string">'embeddings.ckpt'</span>))</div><div class="line">    </div><div class="line">    <span class="comment"># 3. 关联metadata.tsv, 和mnist_10k_sprite.png</span></div><div class="line">    summary_writer = tf.summary.FileWriter(args.log_dir)</div><div class="line">    config = projector.ProjectorConfig()</div><div class="line">    embedding = config.embeddings.add()</div><div class="line">    embedding.tensor_name = embedding_var.name</div><div class="line">    embedding.metadata_path = <span class="string">'metadata.tsv'</span></div><div class="line">    embedding.sprite.image_path = <span class="string">'mnist_10k_sprite.png'</span></div><div class="line">    embedding.sprite.single_image_dim.extend([<span class="number">28</span>, <span class="number">28</span>])</div><div class="line">    projector.visualize_embeddings(summary_writer, config)</div></pre></td></tr></table></figure><h4 id="3-3-1-batch-all"><a href="#3-3-1-batch-all" class="headerlink" title="3.3.1 batch all"></a>3.3.1 batch all</h4><ul><li><code>PCA</code> 结果</li></ul><p><img src="/assets/blog_images/Triplet-Loss/08_batch_all_embeddings_pca.jpg" alt="batch all embeddings pca" title="08_batch_all_embeddings_pca"></p><h4 id="3-3-2-batch-hard"><a href="#3-3-2-batch-hard" class="headerlink" title="3.3.2 batch hard"></a>3.3.2 batch hard</h4><p><img src="/assets/blog_images/Triplet-Loss/11_batch_hard_embedings_pca.jpg" alt="batch hard embeddings pca" title="11_batch_hard_embedings_pca"></p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li><a href="https://omoindrot.github.io/triplet-loss#batch-hard-strategy" target="_blank" rel="external">https://omoindrot.github.io/triplet-loss#batch-hard-strategy</a></li><li><a href="https://github.com/omoindrot/tensorflow-triplet-loss" target="_blank" rel="external">https://github.com/omoindrot/tensorflow-triplet-loss</a></li><li><a href="https://github.com/lawlite19/Blog-Back-Up" target="_blank" rel="external">https://github.com/lawlite19/Blog-Back-Up</a></li><li><a href="https://github.com/omoindrot/tensorflow-triplet-loss/issues/6" target="_blank" rel="external">https://github.com/omoindrot/tensorflow-triplet-loss/issues/6</a></li><li>FaceNet：<a href="https://arxiv.org/pdf/1503.03832.pdf" target="_blank" rel="external">https://arxiv.org/pdf/1503.03832.pdf</a></li><li>Person Re-Identification： <a href="https://arxiv.org/pdf/1703.07737.pdf" target="_blank" rel="external">https://arxiv.org/pdf/1703.07737.pdf</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;一、-Triplet-loss&quot;&gt;&lt;a href=&quot;#一、-Triplet-loss&quot; class=&quot;headerlink&quot; title=&quot;一、 Triplet loss&quot;&gt;&lt;/a&gt;一、 Triplet loss&lt;/h2&gt;&lt;h3 id=&quot;1、介绍&quot;&gt;&lt;a href=&quot;#1、介绍&quot; class=&quot;headerlink&quot; title=&quot;1、介绍&quot;&gt;&lt;/a&gt;1、介绍&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;code&gt;Triplet loss&lt;/code&gt;最初是在 &lt;a href=&quot;https://arxiv.org/abs/1503.03832&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;FaceNet: A Unified Embedding for Face Recognition and Clustering&lt;/a&gt; 论文中提出的，可以学到较好的人脸的&lt;code&gt;embedding&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;为什么不适用 &lt;code&gt;softmax&lt;/code&gt;函数呢，&lt;code&gt;softmax&lt;/code&gt;最终的类别数是确定的，而&lt;code&gt;Triplet loss&lt;/code&gt;学到的是一个好的&lt;code&gt;embedding&lt;/code&gt;，相似的图像在&lt;code&gt;embedding&lt;/code&gt;空间里是相近的，可以判断是否是同一个人脸。&lt;h3 id=&quot;2、原理&quot;&gt;&lt;a href=&quot;#2、原理&quot; class=&quot;headerlink&quot; title=&quot;2、原理&quot;&gt;&lt;/a&gt;2、原理&lt;/h3&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;输入是一个三元组 &lt;code&gt;&amp;lt;a, p, n&amp;gt;&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;a： anchor&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;p： positive&lt;/code&gt;, 与 &lt;code&gt;a&lt;/code&gt; 是同一类别的样本&lt;/li&gt;
&lt;li&gt;&lt;code&gt;n： negative&lt;/code&gt;, 与 &lt;code&gt;a&lt;/code&gt; 是不同类别的样本&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="DeepLearning" scheme="http://lawlite.cn/tags/DeepLearning/"/>
    
      <category term="Tensorflow" scheme="http://lawlite.cn/tags/Tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>Tensorflow高级API</title>
    <link href="http://lawlite.cn/2018/05/31/Tensorflow%E9%AB%98%E7%BA%A7API/"/>
    <id>http://lawlite.cn/2018/05/31/Tensorflow高级API/</id>
    <published>2018-05-31T02:43:08.000Z</published>
    <updated>2018-10-16T06:15:36.747Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一、Estimator"><a href="#一、Estimator" class="headerlink" title="一、Estimator"></a>一、Estimator</h2><h3 id="1、介绍"><a href="#1、介绍" class="headerlink" title="1、介绍"></a>1、介绍</h3><ul><li>编程堆栈</li></ul><p><img src="/assets/blog_images/Tensorflow-api/tensorflow_programming_environment.png" alt="编程堆栈" title="architecture"></p><a id="more"></a><ul><li><code>Estimator</code>：代表一个完整的模型。<code>Estimator API</code> 提供一些方法来训练模型、判断模型的准确率并生成预测。</li><li>数据集：构建数据输入管道。<code>Dataset API</code> 提供一些方法来加载和操作数据，并将数据馈送到您的模型中。<code>Dataset API</code> 与 <code>Estimator API</code> 合作无间</li></ul><h3 id="2、鸢尾花进行分类"><a href="#2、鸢尾花进行分类" class="headerlink" title="2、鸢尾花进行分类"></a>2、鸢尾花进行分类</h3><ul><li>数据集介绍：4个属性，分为3类：</li></ul><table><thead><tr><th>花萼长度</th><th>花萼宽度</th><th>花瓣长度</th><th>花瓣宽度</th><th>品种（标签）</th></tr></thead><tbody><tr><td>5.1</td><td>3.3</td><td>1.7</td><td>0.5</td><td>0（山鸢尾）</td></tr><tr><td>5.0</td><td>2.3</td><td>3.3</td><td>1.0</td><td>1（变色鸢尾）</td></tr><tr><td>6.4</td><td>2.8</td><td>5.6</td><td>2.2</td><td>2（维吉尼亚鸢尾）</td></tr></tbody></table><ul><li>网络模型</li></ul><p><img src="/assets/blog_images/Tensorflow-api/full_network.png" alt="网络模型" title="full_network"></p><h3 id="3、实现"><a href="#3、实现" class="headerlink" title="3、实现"></a>3、实现</h3><ul><li><code>Estimator</code> 是 <code>TensorFlow</code> 对完整模型的高级表示。它会处理初始化、日志记录、保存和恢复等细节部分，并具有很多其他功能，以便您可以专注于模型。</li></ul><h4 id="3-1-预创建模型"><a href="#3-1-预创建模型" class="headerlink" title="3.1 预创建模型"></a>3.1 预创建模型</h4><ul><li>完整代码：<a href="https://github.com/lawlite19/Blog-Back-Up/blob/master/code/tensorflow-high-api/premade_estimator.py" target="_blank" rel="external">点击查看</a></li><li>导入包和参数配置</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line"><span class="keyword">import</span> argparse</div><div class="line"><span class="keyword">import</span> iris_data</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># 超参数</span></div><div class="line">parser = argparse.ArgumentParser()</div><div class="line">parser.add_argument(<span class="string">'--batch_size'</span>, default=<span class="number">100</span>, type=int, help=<span class="string">"batch size"</span>)</div><div class="line">parser.add_argument(<span class="string">'--train_steps'</span>, default=<span class="number">1000</span>, type=int, help=<span class="string">"number of training steps"</span>)</div></pre></td></tr></table></figure><ul><li>构建模型<ul><li>特征列：<code>feature_column</code>:特征列是一个对象，用于说明模型应该如何使用特征字典中的原始输入数据。在构建 <code>Estimator</code> 模型时，您会向其传递一个特征列的列表，其中包含您希望模型使用的每个特征。<code>tf.feature_column</code> 模块提供很多用于向模型表示数据的选项。<ul><li>对于鸢尾花问题，4 个原始特征是数值，因此我们会构建一个特征列的列表，以告知 <code>Estimator</code> 模型将这 4 个特征都表示为 32 位浮点值。</li></ul></li><li>实例化 <code>Estimator</code>: 使用的是预创建模型 <code>cls = tf.estimator.DNNClassifier()</code>模型</li><li>训练模型 <code>cls.train(input_fn, hooks=None, steps=None, max_steps=None, saving_listeners=None)</code>：<ul><li><code>input_fn</code>指定输入的函数，包含 <code>(features, labels)</code> 的 <code>tf.data.Dataset</code> 类型的数据</li><li><code>steps</code> 参数告知方法在训练多少步后停止训练。</li></ul></li><li>评估经过训练的模型：<code>eval_res = cls.evaluate(input_fn, steps=None, hooks=None, checkpoint_path=None, name=None)</code><ul><li>输入和训练数据一致</li><li>返回的有<code>{&#39;accuracy&#39;: 1.0, &#39;loss&#39;: 3.936471, &#39;average_loss&#39;: 0.1312157, &#39;global_step&#39;: 100}</code></li></ul></li><li>预测: <code>predictions = cls.predict(input_fn, predict_keys=None, hooks=None, checkpoint_path=None, yield_single_examples=True)</code><ul><li>输入数据为 <code>batch_size</code> 的测试数据，不包含 <code>label</code>，返回生成器结果</li></ul></li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">(argv)</span>:</span></div><div class="line">    args = parser.parse_args(argv[<span class="number">1</span>:])</div><div class="line">    <span class="comment"># 加载数据， pandas类型</span></div><div class="line">    (train_x, train_y), (test_x, test_y) = iris_data.load_data()</div><div class="line">    <span class="comment"># feature columns描述如何使用输入数据</span></div><div class="line">    my_feature_columns = []</div><div class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> train_x.keys():</div><div class="line">        my_feature_columns.append(tf.feature_column.numeric_column(key = key))</div><div class="line">    <span class="comment"># 建立模型</span></div><div class="line">    cls = tf.estimator.DNNClassifier(hidden_units=[<span class="number">10</span>,<span class="number">10</span>], feature_columns=my_feature_columns, </div><div class="line">                                    n_classes=<span class="number">3</span>)</div><div class="line">    <span class="comment"># 训练模型</span></div><div class="line">    cls.train(input_fn=<span class="keyword">lambda</span>:iris_data.train_input_fn(train_x, train_y, args.batch_size),</div><div class="line">              steps=args.train_steps)</div><div class="line">    <span class="comment"># 评价模型</span></div><div class="line">    eval_res = cls.evaluate(input_fn=<span class="keyword">lambda</span>:iris_data.eval_input_fn(test_x, test_y, args.batch_size))</div><div class="line">    print(<span class="string">"\n Test Set accuracy: &#123;:0.3f&#125;\n"</span>.format(eval_res[<span class="string">'accuracy'</span>]))</div><div class="line">    </div><div class="line">    <span class="comment"># 预测</span></div><div class="line">    expected = [<span class="string">'Setosa'</span>, <span class="string">'Versicolor'</span>, <span class="string">'Virginica'</span>]</div><div class="line">    predict_x = &#123;</div><div class="line">        <span class="string">'SepalLength'</span>: [<span class="number">5.1</span>, <span class="number">5.9</span>, <span class="number">6.9</span>],</div><div class="line">        <span class="string">'SepalWidth'</span>:  [<span class="number">3.3</span>, <span class="number">3.0</span>, <span class="number">3.1</span>],</div><div class="line">        <span class="string">'PetalLength'</span>: [<span class="number">1.7</span>, <span class="number">4.2</span>, <span class="number">5.4</span>],</div><div class="line">        <span class="string">'PetalWidth'</span>:  [<span class="number">0.5</span>, <span class="number">1.5</span>, <span class="number">2.1</span>],        </div><div class="line">    &#125;</div><div class="line">    </div><div class="line">    predictions = cls.predict(input_fn=<span class="keyword">lambda</span>:iris_data.eval_input_fn(predict_x, </div><div class="line">                                                                      labels=<span class="keyword">None</span>,</div><div class="line">                                                                      batch_size=args.batch_size))</div><div class="line">    template = (<span class="string">'\n Prediction is "&#123;&#125;" (&#123;:.1f&#125;%), expected "&#123;&#125;"'</span> )</div><div class="line">    <span class="keyword">for</span> pred_dict, expec <span class="keyword">in</span> zip(predictions, expected):</div><div class="line">        class_id = pred_dict[<span class="string">'class_ids'</span>][<span class="number">0</span>]</div><div class="line">        prob = pred_dict[<span class="string">'probabilities'</span>][class_id]</div><div class="line">        print(template.format(iris_data.SPECIES[class_id], <span class="number">100</span>*prob, expec))</div></pre></td></tr></table></figure><ul><li><p>运行函数</p><ul><li><code>tf.app.run(main=main)</code>会先解析命令行参数,然后执行<code>main</code>函数<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</div><div class="line">    tf.logging.set_verbosity(tf.logging.INFO)</div><div class="line">    tf.app.run(main=main)</div></pre></td></tr></table></figure></li></ul></li><li><p>保存和加载模型</p><ul><li>指定模型地址即可：<code>model_dir</code>,在第一次训练时会保存模型<br><img src="/assets/blog_images/Tensorflow-api/first_train_calls.png" alt="first train call" title="first_train_calls"><ul><li>如果未在 <code>Estimator</code> 的构造函数中指定 <code>model_dir</code>，则 <code>Estimator</code> 会将检查点文件写入由 <code>Python</code> 的 <code>tempfile.mkdtemp</code> 函数选择的临时目录中,可以<code>print(classifier.model_dir)</code>查看</li></ul></li><li>检查点频率：<ul><li>默认<ul><li>每 <code>10</code> 分钟（<code>600</code> 秒）写入一个检查点。</li><li>在 <code>train</code> 方法开始（第一次迭代）和完成（最后一次迭代）时写入一个检查点。</li><li>只在目录中保留 <code>5</code> 个最近写入的检查点。</li></ul></li><li>自己配置：<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">my_checkpoint_config = tf.estimator.RunConfig(save_checkpoints_secs = <span class="number">20</span>*<span class="number">60</span>,   <span class="comment"># 每20分钟保存一次</span></div><div class="line">                                              keep_checkpoint_max = <span class="number">10</span>)        <span class="comment"># 保存10个最近的检查点</span></div><div class="line">cls = tf.estimator.DNNClassifier(hidden_units=[<span class="number">10</span>,<span class="number">10</span>], feature_columns=my_feature_columns, </div><div class="line">                                n_classes=<span class="number">3</span>,</div><div class="line">                                model_dir=<span class="string">'model/'</span>,</div><div class="line">                                config=my_checkpoint_config)</div></pre></td></tr></table></figure></li></ul></li></ul></li><li><ul><li>加载模型<ul><li>不需要改动，一旦存在检查点，<code>TensorFlow</code> 就会在您每次调用 <code>train()</code>、<code>evaluate()</code> 或 <code>predict()</code> 时重建模型。<br><img src="/assets/blog_images/Tensorflow-api/subsequent_calls.png" alt="subsequent_calls" title="subsequent_calls"></li></ul></li></ul></li></ul><h4 id="3-2-自定义模型"><a href="#3-2-自定义模型" class="headerlink" title="3.2 自定义模型"></a>3.2 自定义模型</h4><ul><li>完整代码：<a href="https://github.com/lawlite19/Blog-Back-Up/blob/master/code/tensorflow-high-api/custom_estimator.py" target="_blank" rel="external">点击查看</a></li><li>预创建的 <code>Estimator</code> 是 <code>tf.estimator.Estimator</code> 基类的子类，而自定义 <code>Estimator</code> 是 <code>tf.estimator.Estimator</code> 的实例<br><img src="/assets/blog_images/Tensorflow-api/estimator_types.png" alt="estimator types" title="estimator_types"></li><li><p>创建模型</p><ul><li>模型函数（即 <code>model_fn</code>）会实现机器学习算法</li><li><code>params</code> 参数会传递给自己实现的模型<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">cls = tf.estimator.Estimator(model_fn=my_model, </div><div class="line">                             params=&#123;</div><div class="line">                                <span class="string">'feature_columns'</span>: my_feature_columns,</div><div class="line">                                <span class="string">'hidden_units'</span>: [<span class="number">10</span>, <span class="number">10</span>],</div><div class="line">                                <span class="string">'num_classes'</span>: <span class="number">3</span></div><div class="line">                                &#125;)</div></pre></td></tr></table></figure></li></ul></li><li><p>自定义<code>my_model</code>函数：</p><ul><li>输入层指定输入的数据和对应的<code>feature columns</code></li><li>隐藏层通过<code>tf.layers.dense()</code>创建</li><li>通过<code>mode</code>来判断是训练、评价还是预测操作，返回必须是<code>tf.estimator.EstimatorSpec</code> 对象<br><img src="/assets/blog_images/Tensorflow-api/input_layer.png" alt="input layer" title="input_layer"></li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">my_model</span><span class="params">(features, labels, mode, params)</span>:</span></div><div class="line">    <span class="string">'''自定义模型</span></div><div class="line">       ---------------------------------------------</div><div class="line">       features: 输入数据</div><div class="line">       labels  : 标签数据</div><div class="line">       mode    : 指示是训练、评价还是预测</div><div class="line">       params  : 构建模型的参数</div><div class="line">    </div><div class="line">    '''</div><div class="line">    net = tf.feature_column.input_layer(features=features, </div><div class="line">                                        feature_columns=params[<span class="string">'feature_columns'</span>])   <span class="comment"># 输入层</span></div><div class="line">    <span class="keyword">for</span> units <span class="keyword">in</span> params[<span class="string">'hidden_units'</span>]:                                             <span class="comment"># 隐藏层，遍历参数配置</span></div><div class="line">        net = tf.layers.dense(inputs=net, units=units, activation=tf.nn.relu)</div><div class="line">    </div><div class="line">    logits = tf.layers.dense(net, params[<span class="string">'num_classes'</span>], activation=<span class="keyword">None</span>)</div><div class="line">    pred = tf.argmax(logits, <span class="number">1</span>)    <span class="comment"># 预测结果</span></div><div class="line">    <span class="keyword">if</span> mode == tf.estimator.ModeKeys.PREDICT:</div><div class="line">        predictions = &#123;</div><div class="line">            <span class="string">'class_ids'</span>: pred[:, tf.newaxis],</div><div class="line">            <span class="string">'probabilities'</span>: tf.nn.softmax(logits),</div><div class="line">            <span class="string">'logits'</span>: logits,</div><div class="line">        &#125;</div><div class="line">        <span class="keyword">return</span> tf.estimator.EstimatorSpec(mode, predictions=predictions)</div><div class="line"></div><div class="line">    <span class="comment"># 计算loss</span></div><div class="line">    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)</div><div class="line">    <span class="comment"># 计算评价信息</span></div><div class="line">    accuracy = tf.metrics.accuracy(labels=labels, predictions=pred, </div><div class="line">                                  name=<span class="string">'acc_op'</span>)</div><div class="line">    metrics = &#123;<span class="string">'accuracy'</span>: accuracy&#125;</div><div class="line">    tf.summary.scalar(name=<span class="string">'accuracy'</span>, tensor=accuracy[<span class="number">1</span>])</div><div class="line">    <span class="keyword">if</span> mode == tf.estimator.ModeKeys.EVAL:</div><div class="line">        <span class="keyword">return</span> tf.estimator.EstimatorSpec(mode, loss=loss, eval_metric_ops=metrics)</div><div class="line">    </div><div class="line">    <span class="comment"># 训练操作</span></div><div class="line">    <span class="keyword">assert</span> mode == tf.estimator.ModeKeys.TRAIN</div><div class="line">    </div><div class="line">    optimizer = tf.train.AdagradOptimizer(learning_rate=<span class="number">0.1</span>)</div><div class="line">    train_op = optimizer.minimize(loss=loss, global_step=tf.train.get_global_step())</div><div class="line">    <span class="keyword">return</span> tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)</div></pre></td></tr></table></figure><ul><li>在 <code>TensorBoard</code> 中查看自定义 <code>Estimator</code> 的训练结果。（预定义的模型结果展示更丰富一些）<ul><li><code>tensorboard --logdir=PATH</code></li><li>global_step/sec：这是一个性能指标，显示我们在进行模型训练时每秒处理的批次数（梯度更新）。<br><img src="/assets/blog_images/Tensorflow-api/global_step.png" alt="global step" title="global_step"></li><li>loss：所报告的损失。<br><img src="/assets/blog_images/Tensorflow-api/loss.png" alt="loss" title="loss"></li><li>accuracy：准确率由下列两行记录：<ul><li>eval_metric_ops={‘my_accuracy’: accuracy})（评估期间）。</li><li>tf.summary.scalar(‘accuracy’, accuracy<a href="/assets/blog_images/Tensorflow-api/tensorflow_programming_environment.png" title="architecture">1</a>)（训练期间）。<br><img src="/assets/blog_images/Tensorflow-api/accuracy.png" alt="accuracy" title="accuracy"></li></ul></li></ul></li></ul><h2 id="二、Dataset"><a href="#二、Dataset" class="headerlink" title="二、Dataset"></a>二、Dataset</h2><ul><li><code>tf.data</code> 模块包含一系列类，可让轻松地加载数据、操作数据并通过管道将数据传送到模型中。</li></ul><h3 id="1、基本输入"><a href="#1、基本输入" class="headerlink" title="1、基本输入"></a>1、基本输入</h3><ul><li><p>从数组中提取接片，上面用到的代码</p><ul><li><code>feature</code>：特征数据，为<code>feature-name: array</code>的字典或者<code>DataFrame</code></li><li><code>labels</code>: 标签数组</li><li><code>from_tensor_slices</code> 会按第一个维度进行切片，比如输入为<code>[6000, 28, 28]</code>维度的数据，切片后返回<code>6000</code>个<code>28， 28</code>的<code>Dataset</code> 对象</li><li><p><code>shuffle</code> 方法使用一个固定大小的缓冲区，在条目经过时随机化处理条目。在这种情况下，<code>buffer_size</code> 大于 <code>Dataset</code> 中样本的数量，确保数据完全被随机化处理。</p></li><li><p><code>repeat</code> 方法会在结束时重启 <code>Dataset</code>。要限制周期数量，请设置 <code>count</code> 参数。</p></li><li><p><code>batch</code> 方法会收集大量样本并将它们堆叠起来以创建批次。这为批次的形状增加了一个维度。新的维度将添加为第一个维度。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_input_fn</span><span class="params">(features, labels, batch_size)</span>:</span></div><div class="line">    <span class="string">"""训练集输入函数"""</span></div><div class="line">    dataset = tf.data.Dataset.from_tensor_slices((dict(features,), labels))   <span class="comment"># 转化为Dataset</span></div><div class="line">    </div><div class="line">    dataset = dataset.shuffle(buffer_size=<span class="number">1000</span>).repeat().batch(batch_size)    <span class="comment"># Shuffle, batch</span></div><div class="line">    </div><div class="line">    <span class="keyword">return</span> dataset</div></pre></td></tr></table></figure></li></ul></li></ul><h3 id="2、读取CSV文件"><a href="#2、读取CSV文件" class="headerlink" title="2、读取CSV文件"></a>2、读取CSV文件</h3><ul><li><a href="https://github.com/lawlite19/Blog-Back-Up/blob/master/code/tensorflow-high-api/iris_data.py#L52" target="_blank" rel="external">代码</a></li><li><p>处理一行数据，<code>line: tf.string</code>类型</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">CSV_TYPES = [[<span class="number">0.0</span>], [<span class="number">0.0</span>], [<span class="number">0.0</span>], [<span class="number">0.0</span>], [<span class="number">0</span>]]</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">_parse_line</span><span class="params">(line)</span>:</span></div><div class="line">    <span class="string">'''解析一行数据'''</span></div><div class="line">    field = tf.decode_csv(line, record_defaults=CSV_TYPES)</div><div class="line">    features = dict(zip(CSV_COLUMN_NAMES, field))</div><div class="line">    labels = features.pop(<span class="string">"Species"</span>)</div><div class="line">    <span class="keyword">return</span> features, labels</div></pre></td></tr></table></figure></li><li><p>处理<code>text</code> 文件，得到<code>dataset</code></p><ul><li>读取文本类型为：<code>&lt;SkipDataset shapes: (), types: tf.string&gt;</code></li><li>然后使用<code>map</code> 函数，每个对象处理<br><img src="/assets/blog_images/Tensorflow-api/map.png" alt="map函数示意" title="map"><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">csv_input_fn</span><span class="params">(csv_path, batch_size)</span>:</span></div><div class="line">    <span class="string">'''csv文件输入函数'''</span></div><div class="line">    dataset = tf.data.TextLineDataset(csv_path).skip(<span class="number">1</span>)   <span class="comment"># 跳过第一行</span></div><div class="line">    dataset = dataset.map(_parse_line)        <span class="comment"># 应用map函数处理dataset中的每一个元素</span></div><div class="line">    dataset = dataset.shuffle(<span class="number">1000</span>).repeat().batch(batch_size)</div><div class="line">    <span class="keyword">return</span> dataset</div></pre></td></tr></table></figure></li></ul></li></ul><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li><a href="https://tensorflow.google.cn/get_started/get_started_for_beginners?hl=zh-cn" target="_blank" rel="external">https://tensorflow.google.cn/get_started/get_started_for_beginners?hl=zh-cn</a></li><li><a href="https://tensorflow.google.cn/get_started/premade_estimators?hl=zh-cn" target="_blank" rel="external">https://tensorflow.google.cn/get_started/premade_estimators?hl=zh-cn</a></li><li><a href="https://github.com/tensorflow/models/blob/master/samples/core/get_started/iris_data.py" target="_blank" rel="external">https://github.com/tensorflow/models/blob/master/samples/core/get_started/iris_data.py</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;一、Estimator&quot;&gt;&lt;a href=&quot;#一、Estimator&quot; class=&quot;headerlink&quot; title=&quot;一、Estimator&quot;&gt;&lt;/a&gt;一、Estimator&lt;/h2&gt;&lt;h3 id=&quot;1、介绍&quot;&gt;&lt;a href=&quot;#1、介绍&quot; class=&quot;headerlink&quot; title=&quot;1、介绍&quot;&gt;&lt;/a&gt;1、介绍&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;编程堆栈&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&quot;/assets/blog_images/Tensorflow-api/tensorflow_programming_environment.png&quot; alt=&quot;编程堆栈&quot; title=&quot;architecture&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Tensorflow" scheme="http://lawlite.cn/tags/Tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>风格迁移 Style transfer</title>
    <link href="http://lawlite.cn/2018/02/28/%E9%A3%8E%E6%A0%BC%E8%BF%81%E7%A7%BBStyle-transfer/"/>
    <id>http://lawlite.cn/2018/02/28/风格迁移Style-transfer/</id>
    <published>2018-02-28T11:30:05.000Z</published>
    <updated>2018-03-03T07:07:34.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一、介绍"><a href="#一、介绍" class="headerlink" title="一、介绍"></a>一、介绍</h2><ul><li>将一张图片的艺术风格应用在另外一张图片上</li></ul><p><img src="/assets/blog_images/Style_transfer/01_style-transfer.png" alt="风格迁移结果" title="01_style-transfer"></p><ul><li>使用深度卷积网络CNN提取一张图片的<strong>内容</strong>和提取一张图片的<strong>风格</strong>， 然后将两者结合起来得到最后的结果</li></ul><a id="more"></a><h2 id="二、-方法"><a href="#二、-方法" class="headerlink" title="二、 方法"></a>二、 方法</h2><p><img src="/assets/blog_images/Style_transfer/03_representation.png" alt="提取图片内容和风格并重构" title="02_representation"></p><ul><li>我们知道 <code>CNN</code> 可以捕捉图像的高层次特征，如上图所示，内容图片经过<code>CNN</code>可以得到对应的图像表述(<code>representation</code>, 就是经过卷积操作的<code>feature map</code>)，然后经过重构可以得到近似原图的效果<ul><li>特别是前面几层经过重构得到的结果和原图更接近，也说明前几层保留的图片细节会更多，因为后面还有<code>pooling</code>层，自然会丢弃调一些信息</li><li>这里的网络使用的是<code>VGG-16</code> (如下图)，包含 <code>13</code> 个卷积层，<code>3</code> 个全连接层<br><img src="/assets/blog_images/Style_transfer/04_vgg16.png" alt="vgg-16结构" title="04_vgg16"></li></ul></li></ul><h3 id="1、内容损失"><a href="#1、内容损失" class="headerlink" title="1、内容损失"></a>1、内容损失</h3><ul><li>假设一个卷积层包含 ${N_l}$ 个过滤器 <code>filters</code>，则可以得到 ${N_l}$ 个<code>feature maps</code>，假设 <code>feature map</code>的大小是 $M_l$ (长乘宽)，则可以通过一个矩阵来存储 <code>l</code> 层的数据 $$F^l \in R^{N_l \times M_l} $$<ul><li>$F^l_{i,j}$ 表示第 <code>l</code> 层的第 <code>i</code> 个<code>filter</code> 在 <code>j</code> 位置上的激活值</li></ul></li><li>所以现在一张内容图片$\overrightarrow p$，一张生成图片$\overrightarrow x$(初始值为高斯分布), 经过一层<strong>卷积层l</strong>可以得到其对应的特征表示：$P^l$ 和 $F_l$, 则对应的损失采用<strong>均方误差</strong>: $$L_{content}(\overrightarrow p, \overrightarrow x, l) = {1 \over 2} \sum_{ij}(F^l_{ij}-P^l_{ij})^2$$<ul><li>$F$ 和 $P$是两个矩阵，大小是$N_l \times M_l$，即<code>l</code>层过滤器的个数 和 <code>feature map</code> 的长乘宽的值</li></ul></li></ul><h3 id="2、风格损失"><a href="#2、风格损失" class="headerlink" title="2、风格损失"></a>2、风格损失</h3><ul><li><strong>风格的表示</strong>这里采用<strong>格拉姆矩阵</strong>(<code>Gram Matrix</code>): $G^l \in R^{N_l \times N_l}$  $$G^l_{ij} = {\sum_k F^l_{ik}F^l_{jk}}$$<ul><li>格拉姆矩阵计算的是<strong>两两特征的相关性</strong> , 即哪两个特征是同时出现的，哪两个特征是此消彼长的等，能够<strong>保留图像的风格</strong></li><li>( 比如一幅画中有人和树，它们可以出现在任意位置，格拉姆矩阵可以衡量它们之间的关系，可以认为是这幅画的风格信息 )</li></ul></li><li><p>假设$\overrightarrow a$是风格图像，$\overrightarrow x$是生成图像，$A^l$ 和 $G^l$ 表示在 $l$ 层的格拉姆矩阵，则这一层的损失为：$$E_l = {1 \over 4N^2_lM^2_l}{\sum_{i,j} (G^l_{ij}-A^l_{ij})^2}$$</p></li><li><p>提取风格信息是我们会使用多个卷积层的输出，所以总损失为：$$L_{style}(\overrightarrow a, \overrightarrow x) = {\sum^L_lw_lE_l}$$</p><ul><li>这里$w_l$是每一层损失的权重</li></ul></li></ul><h3 id="3、总损失函数"><a href="#3、总损失函数" class="headerlink" title="3、总损失函数"></a>3、总损失函数</h3><ul><li>通过<strong>白噪声初始化</strong>(就是高斯分布)一个输出的结果，然后通过网络对这个结果进行风格和内容两方面的约束进行修正<br>$$L_{total}(\overrightarrow p,\overrightarrow a,\overrightarrow x)=\alpha L_{content}(\overrightarrow p, \overrightarrow x) +\beta L_{style}(\overrightarrow a, \overrightarrow x)$$</li></ul><h2 id="三、代码实现"><a href="#三、代码实现" class="headerlink" title="三、代码实现"></a>三、代码实现</h2><h3 id="1、说明"><a href="#1、说明" class="headerlink" title="1、说明"></a>1、说明</h3><ul><li>全部代码：<a href="https://github.com/lawlite19/Blog-Back-Up/blob/master/code/style-transfer/style_transfer.py" target="_blank" rel="external">点击查看</a></li><li>图像使用一张建筑图和梵高的星空<br><img src="/assets/blog_images/Style_transfer/05_buildings.jpg" alt="建筑" title="05_buildings"><br><img src="/assets/blog_images/Style_transfer/06_starry-sky.jpg" alt="星空" title="06_starry-sky"></li></ul><h3 id="2、加载并预处理图片和初始化输出图片"><a href="#2、加载并预处理图片和初始化输出图片" class="headerlink" title="2、加载并预处理图片和初始化输出图片"></a>2、加载并预处理图片和初始化输出图片</h3><ul><li>输出图片采用高斯分布初始化</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> backend <span class="keyword">as</span> K</div><div class="line"><span class="keyword">from</span> keras.applications.vgg16 <span class="keyword">import</span> preprocess_input</div><div class="line"><span class="keyword">from</span> keras.preprocessing.image <span class="keyword">import</span> load_img, img_to_array</div><div class="line"></div><div class="line"><span class="keyword">from</span> keras.applications <span class="keyword">import</span> VGG16</div><div class="line"><span class="keyword">from</span> scipy.optimize <span class="keyword">import</span> fmin_l_bfgs_b</div><div class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</div><div class="line"></div><div class="line"><span class="string">'''图片路径'''</span></div><div class="line">content_image_path = <span class="string">'./data/buildings.jpg'</span></div><div class="line">style_image_path = <span class="string">'./data/starry-sky.jpg'</span></div><div class="line">generate_image_path = <span class="string">'./data/output.jpg'</span></div><div class="line"></div><div class="line"></div><div class="line"><span class="string">'''加载图片并初始化输出图片'''</span></div><div class="line">target_height = <span class="number">512</span></div><div class="line">target_width = <span class="number">512</span></div><div class="line">target_size = (target_height, target_width)</div><div class="line"></div><div class="line">content_image = load_img(content_image_path, target_size=target_size)</div><div class="line">content_image_array = img_to_array(content_image)</div><div class="line">content_image_array = K.variable(preprocess_input(np.expand_dims(content_image_array, <span class="number">0</span>)), dtype=<span class="string">'float32'</span>)</div><div class="line"></div><div class="line">style_image = load_img(style_image_path, target_size=target_size)</div><div class="line">style_image_array = img_to_array(style_image)</div><div class="line">style_image_array = K.variable(preprocess_input(np.expand_dims(style_image_array, <span class="number">0</span>)), dtype=<span class="string">'float32'</span>)</div><div class="line"></div><div class="line">generate_image = np.random.randint(<span class="number">256</span>, size=(target_width, target_height, <span class="number">3</span>)).astype(<span class="string">'float64'</span>)</div><div class="line">generate_image = preprocess_input(np.expand_dims(generate_image, <span class="number">0</span>))</div><div class="line">generate_image_placeholder = K.placeholder(shape=(<span class="number">1</span>, target_width, target_height, <span class="number">3</span>))</div></pre></td></tr></table></figure><h3 id="3、获取网络中对应层的输出"><a href="#3、获取网络中对应层的输出" class="headerlink" title="3、获取网络中对应层的输出"></a>3、获取网络中对应层的输出</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_feature_represent</span><span class="params">(x, layer_names, model)</span>:</span></div><div class="line">    <span class="string">'''图片的特征图表示</span></div><div class="line">    </div><div class="line">    参数</div><div class="line">    ----------------------------------------------</div><div class="line">    x : 输入，</div><div class="line">        这里并没有使用，可以看作一个输入的标识</div><div class="line">    layer_names : list</div><div class="line">        CNN网络层的名字</div><div class="line">    model : CNN模型</div><div class="line">    </div><div class="line">    返回值</div><div class="line">    ----------------------------------------------</div><div class="line">    feature_matrices : list</div><div class="line">        经过CNN卷积层的特征表示，这里大小是(filter个数, feature map的长*宽)</div><div class="line">    </div><div class="line">    '''</div><div class="line">    feature_matrices = []</div><div class="line">    <span class="keyword">for</span> ln <span class="keyword">in</span> layer_names:</div><div class="line">        select_layer = model.get_layer(ln)</div><div class="line">        feature_raw = select_layer.output</div><div class="line">        feature_raw_shape = K.shape(feature_raw).eval(session=tf_session)</div><div class="line">        N_l = feature_raw_shape[<span class="number">-1</span>]</div><div class="line">        M_l = feature_raw_shape[<span class="number">1</span>]*feature_raw_shape[<span class="number">2</span>]</div><div class="line">        feature_matrix = K.reshape(feature_raw, (M_l, N_l))</div><div class="line">        feature_matrix = K.transpose(feature_matrix)</div><div class="line">        feature_matrices.append(feature_matrix)</div><div class="line">    <span class="keyword">return</span> feature_matrices</div></pre></td></tr></table></figure><h3 id="4、内容损失函数"><a href="#4、内容损失函数" class="headerlink" title="4、内容损失函数"></a>4、内容损失函数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_content_loss</span><span class="params">(F, P)</span>:</span></div><div class="line">    <span class="string">'''计算内容损失</span></div><div class="line">    </div><div class="line">    参数</div><div class="line">    ---------------------------------------</div><div class="line">    F : tensor, float32</div><div class="line">        生成图片特征图矩阵</div><div class="line">    P : tensor, float32</div><div class="line">        内容图片特征图矩阵</div><div class="line">    </div><div class="line">    返回值</div><div class="line">    ---------------------------------------</div><div class="line">    content_loss : tensor, float32</div><div class="line">        内容损失</div><div class="line">    '''</div><div class="line">    content_loss = <span class="number">0.5</span>*K.sum(K.square(F-P))</div><div class="line">    <span class="keyword">return</span> content_loss</div></pre></td></tr></table></figure><h3 id="5、Gram矩阵和风格损失"><a href="#5、Gram矩阵和风格损失" class="headerlink" title="5、Gram矩阵和风格损失"></a>5、Gram矩阵和风格损失</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_gram_matrix</span><span class="params">(F)</span>:</span></div><div class="line">    <span class="string">'''计算gram矩阵'''</span></div><div class="line">    G = K.dot(F, K.transpose(F))</div><div class="line">    <span class="keyword">return</span> G</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_style_loss</span><span class="params">(ws, Gs, As)</span>:</span></div><div class="line">    <span class="string">'''计算风格损失</span></div><div class="line">    </div><div class="line">    参数</div><div class="line">    ---------------------------------------</div><div class="line">    ws : array</div><div class="line">         每一层layer的权重</div><div class="line">    Gs : list</div><div class="line">         生成图片每一层得到的特征表示组成的list</div><div class="line">    As : list</div><div class="line">         风格图片每一层得到的特征表示组成的list</div><div class="line">    '''</div><div class="line">    style_loss = K.variable(<span class="number">0.</span>)</div><div class="line">    <span class="keyword">for</span> w, G, A <span class="keyword">in</span> zip(ws, Gs, As):</div><div class="line">        M_l = K.int_shape(G)[<span class="number">1</span>]</div><div class="line">        N_l = K.int_shape(G)[<span class="number">0</span>]</div><div class="line">        G_gram = get_gram_matrix(G)</div><div class="line">        A_gram = get_gram_matrix(A)</div><div class="line">        style_loss += w*<span class="number">0.25</span>*K.sum(K.square(G_gram-A_gram))/(N_l**<span class="number">2</span>*M_l**<span class="number">2</span>)</div><div class="line">    <span class="keyword">return</span> style_loss</div></pre></td></tr></table></figure><h3 id="6、总损失"><a href="#6、总损失" class="headerlink" title="6、总损失"></a>6、总损失</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_total_loss</span><span class="params">(generate_image_placeholder, alpha=<span class="number">1.0</span>, beta=<span class="number">10000.0</span>)</span>:</span></div><div class="line">    <span class="string">'''总损失</span></div><div class="line">    '''</div><div class="line">    F = get_feature_represent(generate_image_placeholder, layer_names=[content_layer_name], model=gModel)[<span class="number">0</span>]</div><div class="line">    Gs = get_feature_represent(generate_image_placeholder, layer_names=style_layer_names, model=gModel)</div><div class="line">    content_loss = get_content_loss(F, P)</div><div class="line">    style_loss = get_style_loss(ws, Gs, As)</div><div class="line">    total_loss = alpha*content_loss + beta*style_loss</div><div class="line">    <span class="keyword">return</span> total_loss</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">calculate_loss</span><span class="params">(gen_image_array)</span>:</span></div><div class="line">    <span class="string">'''调用总损失函数，计算得到总损失数值'''</span></div><div class="line">    <span class="keyword">if</span> gen_image_array != (<span class="number">1</span>, target_width, target_height, <span class="number">3</span>):</div><div class="line">        gen_image_array = gen_image_array.reshape((<span class="number">1</span>, target_width, target_height, <span class="number">3</span>))</div><div class="line">    loss_fn = K.function(inputs=[gModel.input], outputs=[get_total_loss(gModel.input)])</div><div class="line">    <span class="keyword">return</span> loss_fn([gen_image_array])[<span class="number">0</span>].astype(<span class="string">'float64'</span>)</div></pre></td></tr></table></figure><h3 id="7、损失函数梯度"><a href="#7、损失函数梯度" class="headerlink" title="7、损失函数梯度"></a>7、损失函数梯度</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_grad</span><span class="params">(gen_image_array)</span>:</span></div><div class="line">    <span class="string">'''计算损失函数的梯度'''</span></div><div class="line">    <span class="keyword">if</span> gen_image_array != (<span class="number">1</span>, target_width, target_height, <span class="number">3</span>):</div><div class="line">        gen_image_array = gen_image_array.reshape((<span class="number">1</span>, target_width, target_height, <span class="number">3</span>))</div><div class="line">    grad_fn = K.function([gModel.input], K.gradients(get_total_loss(gModel.input), [gModel.input]))</div><div class="line">    grad = grad_fn([gen_image_array])[<span class="number">0</span>].flatten().astype(<span class="string">'float64'</span>)</div><div class="line">    <span class="keyword">return</span> grad</div></pre></td></tr></table></figure><h3 id="8、生成结果后处理"><a href="#8、生成结果后处理" class="headerlink" title="8、生成结果后处理"></a>8、生成结果后处理</h3><ul><li>因为之<code>前preprocess_input</code>函数中做了处理，这里进行逆处理还原<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">postprocess_array</span><span class="params">(x)</span>:</span></div><div class="line">    <span class="string">'''生成图片后处理，因为之前preprocess_input函数中做了处理，这里进行逆处理还原</span></div><div class="line">    </div><div class="line">    '''</div><div class="line">    <span class="keyword">if</span> x.shape != (target_width, target_height, <span class="number">3</span>):</div><div class="line">        x = x.reshape((target_width, target_height, <span class="number">3</span>))</div><div class="line">    x[..., <span class="number">0</span>] += <span class="number">103.939</span></div><div class="line">    x[..., <span class="number">1</span>] += <span class="number">116.779</span></div><div class="line">    x[..., <span class="number">2</span>] += <span class="number">123.68</span></div><div class="line">    </div><div class="line">    x = x[..., ::<span class="number">-1</span>]  <span class="comment"># BGR--&gt;RGB</span></div><div class="line">    x = np.clip(x, <span class="number">0</span>, <span class="number">255</span>)</div><div class="line">    x = x.astype(<span class="string">'uint8'</span>)</div><div class="line">    <span class="keyword">return</span> x</div></pre></td></tr></table></figure></li></ul><h3 id="9、定义模型并优化"><a href="#9、定义模型并优化" class="headerlink" title="9、定义模型并优化"></a>9、定义模型并优化</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line"><span class="string">'''定义VGG模型'''</span></div><div class="line">tf_session = K.get_session()</div><div class="line">cModel = VGG16(include_top=<span class="keyword">False</span>, input_tensor=content_image_array)</div><div class="line">sModel = VGG16(include_top=<span class="keyword">False</span>, input_tensor=style_image_array)</div><div class="line">gModel = VGG16(include_top=<span class="keyword">False</span>, input_tensor=generate_image_placeholder)</div><div class="line">content_layer_name = <span class="string">'block4_conv2'</span></div><div class="line">style_layer_names = [</div><div class="line">    <span class="string">'block1_conv1'</span>,</div><div class="line">    <span class="string">'block2_conv1'</span>,</div><div class="line">    <span class="string">'block3_conv1'</span>,</div><div class="line">    <span class="string">'block4_conv1'</span></div><div class="line">]</div><div class="line"></div><div class="line"><span class="string">'''得到对应的representation矩阵'''</span></div><div class="line">P = get_feature_represent(x=content_image_array, layer_names=[content_layer_name], model=cModel)[<span class="number">0</span>]</div><div class="line">As = get_feature_represent(x=style_image_array, layer_names=style_layer_names, model=sModel)</div><div class="line">ws = np.ones(len(style_layer_names))/float(len(style_layer_names))</div><div class="line"></div><div class="line"><span class="string">'''使用fmin_l_bfgs_b进行损失函数优化'''</span></div><div class="line">iterations = <span class="number">600</span></div><div class="line">x_val = generate_image.flatten()</div><div class="line">xopt, f_val, info = fmin_l_bfgs_b(func=calculate_loss, x0=x_val, fprime=get_grad, maxiter=iterations, disp=<span class="keyword">True</span>)</div><div class="line"></div><div class="line">x_out = postprocess_array(xopt)</div></pre></td></tr></table></figure><h3 id="10、输出结果"><a href="#10、输出结果" class="headerlink" title="10、输出结果"></a>10、输出结果</h3><ul><li>初始化输出图片</li></ul><p><img src="/assets/blog_images/Style_transfer/07_initialize.jpg" alt="初始化输出图片" title="07_initialize"></p><ul><li>迭代200次，${\beta \over \alpha} = 10^3$<br><img src="/assets/blog_images/Style_transfer/09_final-result100.jpg" alt="迭代200轮结果" title="09_final-result100"></li></ul><ul><li>迭代<code>500</code>轮，${\beta \over \alpha} = 10^4$<br><img src="/assets/blog_images/Style_transfer/08_final-result.jpg" alt="迭代500轮结果" title="08_final-result"></li></ul><h2 id="四、总结"><a href="#四、总结" class="headerlink" title="四、总结"></a>四、总结</h2><ul><li><code>style tranfer</code>通过<strong>白噪声初始化</strong>(就是高斯分布)一个输出的结果，然后通过优化损失对这个结果进行<strong>风格</strong>和<strong>内容</strong>两方面的约束修正</li><li>图片的风格信息使用的是 <strong>Gram矩阵</strong>来表示</li><li>其中超参数风格损失的权重<code>ws</code>、内容损失和风格损失的权重$\alpha$, $\beta$可以进行调整查看结果<ul><li>论文给出的${\beta \over \alpha} = 10^3或10^4$结果较好，可以自己适当增加看看最后的结果</li></ul></li></ul><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li>Paper: <a href="https://arxiv.org/pdf/1508.06576.pdf" target="_blank" rel="external">https://arxiv.org/pdf/1508.06576.pdf</a></li><li><a href="https://www.cs.toronto.edu/~frossard/post/vgg16/" target="_blank" rel="external">https://www.cs.toronto.edu/~frossard/post/vgg16/</a></li><li><a href="https://zhuanlan.zhihu.com/p/33910138" target="_blank" rel="external">https://zhuanlan.zhihu.com/p/33910138</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;一、介绍&quot;&gt;&lt;a href=&quot;#一、介绍&quot; class=&quot;headerlink&quot; title=&quot;一、介绍&quot;&gt;&lt;/a&gt;一、介绍&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;将一张图片的艺术风格应用在另外一张图片上&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&quot;/assets/blog_images/Style_transfer/01_style-transfer.png&quot; alt=&quot;风格迁移结果&quot; title=&quot;01_style-transfer&quot;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;使用深度卷积网络CNN提取一张图片的&lt;strong&gt;内容&lt;/strong&gt;和提取一张图片的&lt;strong&gt;风格&lt;/strong&gt;， 然后将两者结合起来得到最后的结果&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="Style transfer" scheme="http://lawlite.cn/tags/Style-transfer/"/>
    
      <category term="Keras" scheme="http://lawlite.cn/tags/Keras/"/>
    
  </entry>
  
  <entry>
    <title>阿里云GPU服务器上Torch安装与使用</title>
    <link href="http://lawlite.cn/2017/12/25/%E9%98%BF%E9%87%8C%E4%BA%91GPU%E6%9C%8D%E5%8A%A1%E5%99%A8%E4%B8%8ATorch%E5%AE%89%E8%A3%85%E4%B8%8E%E4%BD%BF%E7%94%A8/"/>
    <id>http://lawlite.cn/2017/12/25/阿里云GPU服务器上Torch安装与使用/</id>
    <published>2017-12-25T01:35:30.000Z</published>
    <updated>2017-12-28T09:12:19.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一、介绍"><a href="#一、介绍" class="headerlink" title="一、介绍"></a>一、介绍</h2><ul><li>阿里云的<code>GPU</code>也有了竞价服务，每小时大概1块多，还是可以接受的</li><li>主要想跑<code>github</code>上的一个<a href="https://github.com/ShuangLI59/Person-Search-with-Natural-Language-Description" target="_blank" rel="external">论文代码</a>，使用的<code>GPU</code>,(奈何实验室没有<code>GPU</code>)， 本来我已经改成<code>CPU</code>版本的了，但是他训练好的模型是基于<code>GPU</code>的，所以还需要重新训练，结果非常的慢…</li><li>包含以下内容：<ul><li>购买竞价<code>GPU</code></li><li>通过<code>SSH</code>连接云服务器</li><li>安装<code>Torch、hdf5、cjson、loadcaffe</code></li><li>安装<code>cuda、cudnn、cunn</code></li></ul></li></ul><a id="more"></a><h2 id="二、购买GPU服务器"><a href="#二、购买GPU服务器" class="headerlink" title="二、购买GPU服务器"></a>二、购买GPU服务器</h2><ul><li>进入阿里云<code>GPU</code>介绍页，<a href="https://www.aliyun.com/product/ecs/gpu?spm=5176.8142029.388261.231.6a1d38418YDpK6" target="_blank" rel="external">点击访问</a>，界面如下，我选择的是<code>GN5(P100)</code></li></ul><p><img src="/assets/blog_images/GPU/0_GPU%E4%BB%8B%E7%BB%8D%E9%A1%B5.png" alt="GPU介绍页" title="0_GPU介绍页"></p><ul><li>选择竞价实例</li></ul><p><img src="/assets/blog_images/GPU/1_%E7%AB%9E%E4%BB%B7%E5%AE%9E%E4%BE%8B.png" alt="选择竞价实例" title="1_竞价实例"></p><ul><li>选择<code>GPU</code></li></ul><p><img src="/assets/blog_images/GPU/2_%E9%80%89%E6%8B%A9GPU.jpg" alt="选择GPU" title="2_选择GPU"></p><ul><li>选择<code>Ubuntu</code>版本和带宽<ul><li>这里按使用流量，所以带宽设置大点没有影响</li></ul></li></ul><p><img src="/assets/blog_images/GPU/3_%E9%80%89%E6%8B%A9Ubuntu%E7%89%88%E6%9C%AC%E5%92%8C%E5%B8%A6%E5%AE%BD.png" alt="选择系统和带宽" title="3_选择Ubuntu版本和带宽"></p><ul><li>在控制台可以看到服务器信息，下面需要使用公网<code>IP</code>连接</li></ul><p><img src="/assets/blog_images/GPU/13_%E6%8E%A7%E5%88%B6%E5%8F%B0.png" alt="控制台" title="13_控制台"></p><h2 id="三、连接GPU服务器以及软件的安装"><a href="#三、连接GPU服务器以及软件的安装" class="headerlink" title="三、连接GPU服务器以及软件的安装"></a>三、连接GPU服务器以及软件的安装</h2><h3 id="1、使用SecureCRT连接服务器"><a href="#1、使用SecureCRT连接服务器" class="headerlink" title="1、使用SecureCRT连接服务器"></a>1、使用<code>SecureCRT</code>连接服务器</h3><p><img src="/assets/blog_images/GPU/4_secureCRT.png" alt="连接服务器" title="4_secureCRT"></p><h3 id="2、安装前准备工作"><a href="#2、安装前准备工作" class="headerlink" title="2、安装前准备工作"></a>2、安装前准备工作</h3><ul><li><code>apt clean</code></li><li><code>apt update</code></li><li>安装<code>git</code>命令行：<code>apt install git</code></li><li>生成<code>ssh-key</code> : <code>ssh-keygen -t rsa -C &quot;youremail@example.com&quot;</code><ul><li>将<code>/root/.ssh/id_rsa.pub</code>中内容加入到<code>github</code></li></ul></li></ul><h3 id="3、安装Torch"><a href="#3、安装Torch" class="headerlink" title="3、安装Torch"></a>3、安装<code>Torch</code></h3><ul><li>网址：<a href="http://torch.ch/docs/getting-started.html" target="_blank" rel="external">http://torch.ch/docs/getting-started.html</a></li><li><code>git clone https://github.com/torch/distro.git ~/torch --recursive</code></li><li><code>cd ~/torch</code></li><li><code>bash install-deps</code></li><li><code>./install.sh</code></li><li><code>source ~/.bashrc</code></li><li>输入<code>th</code>查看安装是否成功</li></ul><p><img src="/assets/blog_images/GPU/9_torch.png" alt="torch" title="9_torch"></p><h3 id="4、-安装hdf5"><a href="#4、-安装hdf5" class="headerlink" title="4、 安装hdf5"></a>4、 安装<code>hdf5</code></h3><ul><li>地址: <a href="https://github.com/deepmind/torch-hdf5/blob/master/doc/usage.md" target="_blank" rel="external">https://github.com/deepmind/torch-hdf5/blob/master/doc/usage.md</a></li><li><code>apt-get install libhdf5-serial-dev hdf5-tools</code></li><li><code>git clone https://github.com/deepmind/torch-hdf5</code></li><li><code>cd torch-hdf5</code></li><li><code>luarocks make hdf5-0-0.rockspec LIBHDF5_LIBDIR=&quot;/usr/lib/x86_64-linux-gnu/&quot;</code><ul><li>注意这里 <code>luarocks</code> 是 <code>Torch</code> 里的，在 <code>/root/torch/install/bin</code> 目录下</li></ul></li></ul><h3 id="5、-安装-cjson-和-loadcaffe"><a href="#5、-安装-cjson-和-loadcaffe" class="headerlink" title="5、 安装 cjson 和 loadcaffe"></a>5、 安装 <code>cjson</code> 和 <code>loadcaffe</code></h3><ul><li><code>luarocks install lua-cjson</code></li><li><code>apt-get install libprotobuf-dev protobuf-compiler</code></li><li><code>luarocks install loadcaffe</code></li></ul><h3 id="6、安装Cuda"><a href="#6、安装Cuda" class="headerlink" title="6、安装Cuda"></a>6、安装<code>Cuda</code></h3><ul><li>网址：<a href="https://developer.nvidia.com/cuda-downloads?target_os=Linux&amp;target_arch=x86_64&amp;target_distro=Ubuntu&amp;target_version=1604&amp;target_type=deblocal" target="_blank" rel="external">点击查看</a></li><li>选择对应的<code>cuda</code>版本</li></ul><p><img src="/assets/blog_images/GPU/5_cuda%E9%80%89%E6%8B%A9.png" alt="cuda" title="5_cuda选择"></p><ul><li><code>sudo dpkg -i cuda-repo-ubuntu1604-9-1-local_9.1.85-1_amd64.deb</code></li><li><code>sudo apt-key add /var/cuda-repo-&lt;version&gt;/7fa2af80.pub</code></li><li><code>sudo apt-get update</code></li><li><code>sudo apt-get install cuda</code></li><li>安装完成后会在<code>/usr/local/</code>目录下出现<code>cuda-9.1</code>的目录</li><li>加入到环境变量<ul><li><code>echo &quot;export PATH=/usr/local/cuda-9.1/bin/:\$PATH; export LD_LIBRARY_PATH=/usr/local/cuda-9.1/lib64/:\$LD_LIBRARY_PATH; &quot; &gt;&gt;~/.bashrc &amp;&amp; source ~/.bashrc</code></li></ul></li><li>此时<code>cuda</code>已经安装成功，可以通过<code>nvcc -V</code>测试是否安装成功<ul><li><code>nvidia-smi</code>命令查看<code>GPU</code>使用情况</li></ul></li></ul><p><img src="/assets/blog_images/GPU/6_cuda%E5%AE%89%E8%A3%85%E6%B5%8B%E8%AF%95.png" alt="cuda安装测试" title="6_cuda安装测试"></p><ul><li>有时可能需要重启一下</li></ul><h3 id="7、安装cudnn"><a href="#7、安装cudnn" class="headerlink" title="7、安装cudnn"></a>7、安装<code>cudnn</code></h3><ul><li>网址1：<a href="https://github.com/facebookarchive/fbcunn/blob/master/INSTALL.md#install-cuda" target="_blank" rel="external">点击查看</a></li><li>网址2：<a href="https://developer.nvidia.com/rdp/cudnn-download" target="_blank" rel="external">下载cudnn</a><ul><li>需要先注册登录才能下载</li></ul></li><li>注意这里下载的版本，我这里使用的是<code>5.1</code>版本（尝试了最新的7.x版本，有问题）</li></ul><p><img src="/assets/blog_images/GPU/7_cudnn%E4%B8%8B%E8%BD%BD.png" alt="cudnn版本" title="7_cudnn下载"></p><ul><li>直接<code>luarocks install cudnn</code>是可以成功安装的，但是有问题</li><li>下载的是<strong>压缩包</strong>,里面有两个文件夹</li></ul><p><img src="/assets/blog_images/GPU/8_cudnn%E5%8E%8B%E7%BC%A9%E5%8C%85.png" alt="cudnn压缩包" title="8_cudnn压缩包"></p><ul><li>将<code>include</code>下的<code>cudnn.h</code>文件拷贝到<code>/usr/local/cuda-9.1/include/</code>文件夹下</li><li>将<code>lib64</code>下的<code>libcudnn.so.5.1.10</code>文件拷贝到<code>/usr/local/cuda-9.1/lib64/</code>文件夹下<ul><li>并且创建软连接: <code>ln -s libcudnn.so.5.1.10 libcudnn.so.5</code></li></ul></li><li>添加环境变量：<code>export CUDNN_PATH=&quot;/usr/local/cuda-9.1/lib64/libcudnn.so.5&quot;</code></li></ul><p><img src="/assets/blog_images/GPU/12_cudnn.png" alt="cudnn5.x" title="12_cudnn"></p><h2 id="四、测试"><a href="#四、测试" class="headerlink" title="四、测试"></a>四、测试</h2><ul><li>下面是我跑的一个程序</li></ul><p><img src="/assets/blog_images/GPU/10_GPU%E4%BD%BF%E7%94%A8%E6%83%85%E5%86%B5.png" alt="GPU使用情况" title="10_GPU使用情况"></p><h2 id="五、其他一些说明"><a href="#五、其他一些说明" class="headerlink" title="五、其他一些说明"></a>五、其他一些说明</h2><h3 id="1、rz-sz文件传输"><a href="#1、rz-sz文件传输" class="headerlink" title="1、rz/sz文件传输"></a>1、<code>rz/sz</code>文件传输</h3><ul><li><code>wget https://raw.githubusercontent.com/lawlite19/LinuxSoftware/master/rz-sz/lrzsz-0.12.20.tar.gz</code></li><li><code>tar zxvf lrzsz-0.12.20.tar.gz</code></li><li><code>cd lrzsz-0.12.20</code></li><li><code>./configure &amp;&amp; make &amp;&amp; make install</code></li><li><code>cd /usr/local/bin</code></li><li><code>ln -s lrz rz</code></li><li><code>ln -s lsz sz</code><h3 id="2、使用xftp等工具传输文件"><a href="#2、使用xftp等工具传输文件" class="headerlink" title="2、使用xftp等工具传输文件"></a>2、使用<code>xftp</code>等工具传输文件</h3></li><li>服务器上需要安装<code>ftp</code>服务<ul><li><code>apt install vsftpd</code></li></ul></li><li>配置文件；<code>vim /etc/vstfpd.conf</code></li></ul><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">write_enable=YES</div><div class="line">userlist_deny=NO</div><div class="line">userlist_enable=YES</div><div class="line">userlist_file=/etc/vsftpd<span class="selector-class">.user_list</span></div><div class="line">seccomp_sandbox=NO</div></pre></td></tr></table></figure><ul><li>加入允许连接的用户 <code>vim /etc/vstfpd.user_list</code></li></ul><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">root</div><div class="line">lawlite</div></pre></td></tr></table></figure><ul><li>删除禁止连接的用户：<code>vim /etc/ftpusers</code></li><li>使用<code>xftp、FileZilla</code>等工具连接即可<ul><li>注意端口<code>22</code></li><li>协议<code>sftp</code></li></ul></li></ul><h3 id="3、wget-下载百度云盘文件"><a href="#3、wget-下载百度云盘文件" class="headerlink" title="3、wget 下载百度云盘文件"></a>3、<code>wget</code> 下载百度云盘文件</h3><ul><li><code>wget -c ----referer=百度云盘分享地址 -O 要保存的文件名 &quot;百度云文件真实地址&quot;</code></li><li>文件的真实地址获取<ul><li>浏览器按<code>F12</code>, 点击下载找到<code>download?</code>的信息</li><li><code>dlink</code>为真实地址，注意去除转义字符<code>\</code></li></ul></li></ul><p><img src="/assets/blog_images/GPU/11_%E8%8E%B7%E5%8F%96%E7%99%BE%E5%BA%A6%E4%BA%91%E5%9C%B0%E5%9D%80.png" alt="获取百度云真实地址" title="11_获取百度云地址"></p><ul><li>比如： <code>wget -c --referer=https://pan.baidu.com/s/1kV7Xo7H -O lstm1_rnn512_bestACC.zip &quot;https://d.pcs.baidu.com/file/4e4cd12ad77d7ac60d2cfcb8e009bf1c?fid=3174489928-250528-212189063946307&amp;time=1514127189&amp;rt=pr&amp;sign=FDTAERVCY-DCb740ccc5511e5e8fedcff06b081203-LWe3VIBsW3foAEVnTUqSROJQ46s%3D&amp;expires=8h&amp;chkv=1&amp;chkbd=1&amp;chkpc=et&amp;dp-logid=8301954057401711855&amp;dp-callid=0&amp;r=884079691&quot;</code></li></ul><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li>Cuda: <ul><li><a href="https://developer.nvidia.com/cuda-downloads?target_os=Linux&amp;target_arch=x86_64&amp;target_distro=Ubuntu&amp;target_version=1604&amp;target_type=deblocal" target="_blank" rel="external">https://developer.nvidia.com/cuda-downloads?target_os=Linux&amp;target_arch=x86_64&amp;target_distro=Ubuntu&amp;target_version=1604&amp;target_type=deblocal</a></li><li><a href="http://blog.csdn.net/u012235003/article/details/54575758" target="_blank" rel="external">http://blog.csdn.net/u012235003/article/details/54575758</a></li><li><a href="http://blog.csdn.net/hungryof/article/details/51557666" target="_blank" rel="external">http://blog.csdn.net/hungryof/article/details/51557666</a></li><li><a href="https://github.com/facebookarchive/fbcunn/blob/master/INSTALL.md#install-cuda" target="_blank" rel="external">https://github.com/facebookarchive/fbcunn/blob/master/INSTALL.md#install-cuda</a></li></ul></li><li><code>Wget</code>下载百度云：<ul><li><a href="http://blog.csdn.net/zhongdajiajiao/article/details/51917886" target="_blank" rel="external">http://blog.csdn.net/zhongdajiajiao/article/details/51917886</a></li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;一、介绍&quot;&gt;&lt;a href=&quot;#一、介绍&quot; class=&quot;headerlink&quot; title=&quot;一、介绍&quot;&gt;&lt;/a&gt;一、介绍&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;阿里云的&lt;code&gt;GPU&lt;/code&gt;也有了竞价服务，每小时大概1块多，还是可以接受的&lt;/li&gt;
&lt;li&gt;主要想跑&lt;code&gt;github&lt;/code&gt;上的一个&lt;a href=&quot;https://github.com/ShuangLI59/Person-Search-with-Natural-Language-Description&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;论文代码&lt;/a&gt;，使用的&lt;code&gt;GPU&lt;/code&gt;,(奈何实验室没有&lt;code&gt;GPU&lt;/code&gt;)， 本来我已经改成&lt;code&gt;CPU&lt;/code&gt;版本的了，但是他训练好的模型是基于&lt;code&gt;GPU&lt;/code&gt;的，所以还需要重新训练，结果非常的慢…&lt;/li&gt;
&lt;li&gt;包含以下内容：&lt;ul&gt;
&lt;li&gt;购买竞价&lt;code&gt;GPU&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;通过&lt;code&gt;SSH&lt;/code&gt;连接云服务器&lt;/li&gt;
&lt;li&gt;安装&lt;code&gt;Torch、hdf5、cjson、loadcaffe&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;安装&lt;code&gt;cuda、cudnn、cunn&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="Cudnn" scheme="http://lawlite.cn/tags/Cudnn/"/>
    
      <category term="GPU" scheme="http://lawlite.cn/tags/GPU/"/>
    
      <category term="Torch" scheme="http://lawlite.cn/tags/Torch/"/>
    
      <category term="阿里云" scheme="http://lawlite.cn/tags/%E9%98%BF%E9%87%8C%E4%BA%91/"/>
    
  </entry>
  
  <entry>
    <title>论文记录_U-Net:Convolutional Networks for Biomedical Image Segmentation</title>
    <link href="http://lawlite.cn/2017/10/18/%E8%AE%BA%E6%96%87%E8%AE%B0%E5%BD%95-U-Net-Convolutional-Networks-for-Biomedical-Image-Segmentation/"/>
    <id>http://lawlite.cn/2017/10/18/论文记录-U-Net-Convolutional-Networks-for-Biomedical-Image-Segmentation/</id>
    <published>2017-10-18T11:30:05.000Z</published>
    <updated>2017-10-19T09:28:32.000Z</updated>
    
    <content type="html"><![CDATA[<ul><li><code>Keras</code> 中的实现（可基于<code>Theano</code>和<code>Tensorflow</code>）：<a href="https://github.com/ZFTurbo/ZF_UNET_224_Pretrained_Model" target="_blank" rel="external">点击查看</a></li><li><code>Tensorflow</code> 中的实现：<a href="https://github.com/jakeret/tf_unet" target="_blank" rel="external">点击查看</a></li></ul><h2 id="1、概述"><a href="#1、概述" class="headerlink" title="1、概述"></a>1、概述</h2><ul><li><code>2015</code>年提出的用于<strong>生物图像分割</strong>的网络，并在当时取得了冠军<ul><li>[实际也可用于<strong>其他领域</strong>，比如一些<code>Kaggle</code>图像分割比赛中也有使用，并且取得了很好的成绩]</li></ul></li><li>一般的 <code>CNN</code> 是预测一张图片的类别，而图像分割需要<strong>预测每个像素属于的类别</strong>，使用是<strong>全卷积网络</strong> <code>FCN</code> (<code>fully convolutional network</code>)</li><li>网络结构包括<strong>收缩</strong> [就是 <code>CNN</code> 的卷积操作]和对应的<strong>扩展</strong> [可以进行<strong>反卷积</strong>，这里采用<strong>上采样</strong>(<code>upsample</code>)]<ul><li>收缩主要用于<strong>捕捉上下文特征</strong></li><li>扩展用于<strong>定位</strong></li></ul></li><li>需要很少的训练集即可完成训练，很容易收敛</li></ul><a id="more"></a><h2 id="2、网络结构"><a href="#2、网络结构" class="headerlink" title="2、网络结构"></a>2、网络结构</h2><p><img src="/assets/blog_images/Image_Segmentation/01-U-net_architecture.png" alt="U-net architecture" title="U-net_architecture"></p><ul><li>左半部分收缩就是平常的<code>CNN</code>卷积操作<ul><li>经过卷积、<code>Relu</code> 和 <code>2x2 max pooling</code></li></ul></li><li><p>右半部分扩展采用的上采样（<code>upsampling</code>）操作，<strong>同时拼接上对应的左半部分</strong>的<code>feature maps</code></p><ul><li>因为前面几层的卷积层<strong>分辨率比较高</strong>，<strong>定位</strong>比较准确</li><li>后面的几层卷积层<strong>分辨率比较低</strong>，<strong>分类</strong>比较准确</li><li>所以把低分辨率和高分辨率的<code>feature map</code> 拼接起来，得到更好的结果</li></ul></li><li><p>如果预测时输入图片大小有问题，可以使用镜像拼接方式，同时也可以调整输入的大小是偶数，方便进行 <code>2x2 max pooling</code></p><ul><li>图中黄色部分待预测，需要蓝色部分作为输入，对称的方式生成周围的部分。</li></ul></li></ul><p><img src="/assets/blog_images/Image_Segmentation/02-Overlap-tile-strategy.png" alt="输入调整" title="02-Overlap-tile-strategy"></p><h2 id="3、训练"><a href="#3、训练" class="headerlink" title="3、训练"></a>3、训练</h2><ul><li>损失函数使用<strong>逐像素</strong>的 <code>softmax</code> 函数和<strong>交叉熵损失函数</strong>的结合</li><li><code>Softmax</code>函数：$${p_k(x)} =  {e^{a_k(x)} \over \sum_{k’=1}^K e^{a_{k’(x)}}}$$<ul><li>$a_k(x)$表示在<code>feature maps</code>中的的<code>channel=k</code>的<code>feature map</code>像素位置为<code>x</code>的激活值</li><li>$K$是类别数</li><li>[就是单个<code>feature map</code>上每个像素的类别概率]</li></ul></li><li>训练需要标注对应的<code>mask</code>,就是类别的区域标记</li></ul><p><img src="/assets/blog_images/Image_Segmentation//03_train.png" alt="example" title="03_train"></p><ul><li>因为使用了 <code>Relu</code> 激励函数，对应的权重初始化方法为<strong>标准差</strong>为$\sqrt{2/N}$的高斯分布，具体关于不同激励函数对应的输出花方法可以<a href="http://lawlite.me/2017/01/09/%E8%AE%BA%E6%96%87%E8%AE%B0%E5%BD%95-Relu%E6%BF%80%E5%8A%B1%E5%87%BD%E6%95%B0%E6%9D%83%E9%87%8D%E5%88%9D%E5%A7%8B%E5%8C%96/" target="_blank" rel="external">看这里</a></li><li>训练集小的话可以做<strong>数据增强</strong></li></ul><h2 id="4、Keras-中的实现"><a href="#4、Keras-中的实现" class="headerlink" title="4、Keras 中的实现"></a>4、<code>Keras</code> 中的实现</h2><ul><li>借鉴<code>github</code>上实现好的：<a href="https://github.com/ZFTurbo/ZF_UNET_224_Pretrained_Model" target="_blank" rel="external">点击查看</a>，版本：<code>Keras (2.0.8)</code>，<code>tensorflow (1.3.0)</code></li><li><p>两层卷积操作函数</p><ul><li>判断是使用<code>theano</code>还是<code>tensorflow</code>作为backend， 因为他们对应的数据维度不同</li><li>可以使用<code>BN</code>和<code>Dropout</code>操作</li><li>两层卷积也就对应了上面<code>U-net</code>结构图的两个卷积操作<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">def double_conv_layer(x, size, dropout, batch_norm):</div><div class="line">    if K.image_dim_ordering() == &apos;th&apos;:</div><div class="line">        axis = 1</div><div class="line">    else:</div><div class="line">        axis = 3</div><div class="line">    conv = Conv2D(size, (3, 3), padding=&apos;same&apos;)(x)</div><div class="line">    if batch_norm is True:</div><div class="line">        conv = BatchNormalization(axis=axis)(conv)</div><div class="line">    conv = Activation(&apos;relu&apos;)(conv)</div><div class="line">    conv = Conv2D(size, (3, 3), padding=&apos;same&apos;)(conv)</div><div class="line">    if batch_norm is True:</div><div class="line">        conv = BatchNormalization(axis=axis)(conv)</div><div class="line">    conv = Activation(&apos;relu&apos;)(conv)</div><div class="line">    if dropout &gt; 0:</div><div class="line">        conv = Dropout(dropout)(conv)</div><div class="line">    return conv</div></pre></td></tr></table></figure></li></ul></li><li><p>构建网络</p><ul><li>最后是<code>1x1</code>的卷积，使用的<code>Sigmoid</code>函数作为最后的输出概率<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div></pre></td><td class="code"><pre><div class="line">def ZF_UNET_224(dropout_val=0.0, batch_norm=True):</div><div class="line">    print(&quot;con&quot;)</div><div class="line">    if K.image_dim_ordering() == &apos;th&apos;:</div><div class="line">        inputs = Input((INPUT_CHANNELS, 224, 224))</div><div class="line">        axis = 1</div><div class="line">    else:</div><div class="line">        inputs = Input((224, 224, INPUT_CHANNELS))</div><div class="line">        axis = 3</div><div class="line">    filters = 32</div><div class="line"></div><div class="line">    conv_224 = double_conv_layer(inputs, filters, dropout_val, batch_norm)</div><div class="line">    pool_112 = MaxPooling2D(pool_size=(2, 2))(conv_224)</div><div class="line"></div><div class="line">    conv_112 = double_conv_layer(pool_112, 2*filters, dropout_val, batch_norm)</div><div class="line">    pool_56 = MaxPooling2D(pool_size=(2, 2))(conv_112)</div><div class="line"></div><div class="line">    conv_56 = double_conv_layer(pool_56, 4*filters, dropout_val, batch_norm)</div><div class="line">    pool_28 = MaxPooling2D(pool_size=(2, 2))(conv_56)</div><div class="line"></div><div class="line">    conv_28 = double_conv_layer(pool_28, 8*filters, dropout_val, batch_norm)</div><div class="line">    pool_14 = MaxPooling2D(pool_size=(2, 2))(conv_28)</div><div class="line"></div><div class="line">    conv_14 = double_conv_layer(pool_14, 16*filters, dropout_val, batch_norm)</div><div class="line">    pool_7 = MaxPooling2D(pool_size=(2, 2))(conv_14)</div><div class="line"></div><div class="line">    conv_7 = double_conv_layer(pool_7, 32*filters, dropout_val, batch_norm)</div><div class="line"></div><div class="line">    up_14 = concatenate([UpSampling2D(size=(2, 2))(conv_7), conv_14], axis=axis)</div><div class="line">    up_conv_14 = double_conv_layer(up_14, 16*filters, dropout_val, batch_norm)</div><div class="line"></div><div class="line">    up_28 = concatenate([UpSampling2D(size=(2, 2))(up_conv_14), conv_28], axis=axis)</div><div class="line">    up_conv_28 = double_conv_layer(up_28, 8*filters, dropout_val, batch_norm)</div><div class="line"></div><div class="line">    up_56 = concatenate([UpSampling2D(size=(2, 2))(up_conv_28), conv_56], axis=axis)</div><div class="line">    up_conv_56 = double_conv_layer(up_56, 4*filters, dropout_val, batch_norm)</div><div class="line"></div><div class="line">    up_112 = concatenate([UpSampling2D(size=(2, 2))(up_conv_56), conv_112], axis=axis)</div><div class="line">    up_conv_112 = double_conv_layer(up_112, 2*filters, dropout_val, batch_norm)</div><div class="line"></div><div class="line">    up_224 = concatenate([UpSampling2D(size=(2, 2))(up_conv_112), conv_224], axis=axis)</div><div class="line">    up_conv_224 = double_conv_layer(up_224, filters, 0, batch_norm)</div><div class="line"></div><div class="line">    conv_final = Conv2D(OUTPUT_MASK_CHANNELS, (1, 1))(up_conv_224)</div><div class="line">    conv_final = BatchNormalization(axis=axis)(conv_final)</div><div class="line">    conv_final = Activation(&apos;sigmoid&apos;)(conv_final)</div><div class="line"></div><div class="line">    model = Model(inputs, conv_final, name=&quot;ZF_UNET_224&quot;)</div><div class="line">    return model</div></pre></td></tr></table></figure></li></ul></li></ul><h2 id="5、总结"><a href="#5、总结" class="headerlink" title="5、总结"></a>5、总结</h2><ul><li>图像分割能够获得很好的结果，要求的训练集比较小</li><li>包括了收缩和扩展两部分，扩展部分<strong>拼接</strong>了对应的收缩部分的<code>feature maps</code></li></ul><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li><a href="https://arxiv.org/abs/1505.04597#" target="_blank" rel="external">https://arxiv.org/abs/1505.04597#</a></li><li><a href="https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/" target="_blank" rel="external">https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/</a></li><li><a href="https://github.com/ZFTurbo/ZF_UNET_224_Pretrained_Model" target="_blank" rel="external">https://github.com/ZFTurbo/ZF_UNET_224_Pretrained_Model</a></li><li><a href="https://github.com/jakeret/tf_unet" target="_blank" rel="external">https://github.com/jakeret/tf_unet</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;ul&gt;
&lt;li&gt;&lt;code&gt;Keras&lt;/code&gt; 中的实现（可基于&lt;code&gt;Theano&lt;/code&gt;和&lt;code&gt;Tensorflow&lt;/code&gt;）：&lt;a href=&quot;https://github.com/ZFTurbo/ZF_UNET_224_Pretrained_Model&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;点击查看&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Tensorflow&lt;/code&gt; 中的实现：&lt;a href=&quot;https://github.com/jakeret/tf_unet&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;点击查看&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;1、概述&quot;&gt;&lt;a href=&quot;#1、概述&quot; class=&quot;headerlink&quot; title=&quot;1、概述&quot;&gt;&lt;/a&gt;1、概述&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;code&gt;2015&lt;/code&gt;年提出的用于&lt;strong&gt;生物图像分割&lt;/strong&gt;的网络，并在当时取得了冠军&lt;ul&gt;
&lt;li&gt;[实际也可用于&lt;strong&gt;其他领域&lt;/strong&gt;，比如一些&lt;code&gt;Kaggle&lt;/code&gt;图像分割比赛中也有使用，并且取得了很好的成绩]&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;一般的 &lt;code&gt;CNN&lt;/code&gt; 是预测一张图片的类别，而图像分割需要&lt;strong&gt;预测每个像素属于的类别&lt;/strong&gt;，使用是&lt;strong&gt;全卷积网络&lt;/strong&gt; &lt;code&gt;FCN&lt;/code&gt; (&lt;code&gt;fully convolutional network&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;网络结构包括&lt;strong&gt;收缩&lt;/strong&gt; [就是 &lt;code&gt;CNN&lt;/code&gt; 的卷积操作]和对应的&lt;strong&gt;扩展&lt;/strong&gt; [可以进行&lt;strong&gt;反卷积&lt;/strong&gt;，这里采用&lt;strong&gt;上采样&lt;/strong&gt;(&lt;code&gt;upsample&lt;/code&gt;)]&lt;ul&gt;
&lt;li&gt;收缩主要用于&lt;strong&gt;捕捉上下文特征&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;扩展用于&lt;strong&gt;定位&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;需要很少的训练集即可完成训练，很容易收敛&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="论文记录" scheme="http://lawlite.cn/tags/%E8%AE%BA%E6%96%87%E8%AE%B0%E5%BD%95/"/>
    
      <category term="图像分割" scheme="http://lawlite.cn/tags/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/"/>
    
  </entry>
  
  <entry>
    <title>论文记录_MobileNets Efficient Convolutional Neural Networks for Mobile Vision Application</title>
    <link href="http://lawlite.cn/2017/09/12/%E8%AE%BA%E6%96%87%E8%AE%B0%E5%BD%95-MobileNets-Efficient-Convolutional-Neural-Networks-for-Mobile-Vision-Application/"/>
    <id>http://lawlite.cn/2017/09/12/论文记录-MobileNets-Efficient-Convolutional-Neural-Networks-for-Mobile-Vision-Application/</id>
    <published>2017-09-12T03:34:17.000Z</published>
    <updated>2017-09-13T11:50:19.000Z</updated>
    
    <content type="html"><![CDATA[<ul><li><code>Tensorflow</code> 中的实现：<a href="https://github.com/tensorflow/models/blob/master/slim/nets/mobilenet_v1.md" target="_blank" rel="external">点击查看</a></li><li><code>Caffe</code> 中的实现：<a href="https://github.com/shicai/MobileNet-Caffe" target="_blank" rel="external">点击查看</a></li></ul><h2 id="1、概述"><a href="#1、概述" class="headerlink" title="1、概述"></a>1、概述</h2><ul><li><code>Google</code>在<code>2017</code>年提出的适用于手机端的神经网络模型</li><li>主要使用了<strong>深度可分离卷积</strong><code>Depthwise Separable Convolution</code> 将卷积核进行分解计算来减少计算量</li><li>引入了<strong>两个超参数</strong>减少参数量和计算量<ul><li>宽度乘数（<code>Width Multiplier</code>）: [减少输入和输出的 <code>channels</code> ]</li><li>分辨率乘数（<code>Resolution Multiplier</code>）：[减少输入输出的 <code>feature maps</code> 的大小]</li></ul></li></ul><a id="more"></a><h2 id="2、深度可分离卷积（Depthwise-Separable-Convolution）"><a href="#2、深度可分离卷积（Depthwise-Separable-Convolution）" class="headerlink" title="2、深度可分离卷积（Depthwise Separable Convolution）"></a>2、深度可分离卷积（<code>Depthwise Separable Convolution</code>）</h2><ul><li>可以将一个标准卷积核<strong>分成</strong>一个深度卷积<code>depthwise convolution</code> 和 一个<code>1X1</code>的卷积（叫作逐点卷积<code>pointwise convolution</code>）。如下图所示</li></ul><p><img src="/assets/blog_images/ModelCompression/13_depthwise-separable-convolution.png" alt="depthwise separable convolution" title="13_depthwise-separable-convolution"></p><h3 id="2-1-标准卷积"><a href="#2-1-标准卷积" class="headerlink" title="2.1 标准卷积"></a>2.1 标准卷积</h3><ul><li>标准的卷积层是将维度为$D_F \times D_F \times M$的输入层转化为维度为$D_G \times D_G \times N$ [ <a href="http://lawlite.me/2017/09/11/%E8%AE%BA%E6%96%87%E8%AE%B0%E5%BD%95-PruningFiltersForEfficientConvNets/#2-1-%E5%9F%BA%E7%A1%80%EF%BC%88CNN%E7%9B%B8%E5%85%B3%E5%86%85%E5%AE%B9%EF%BC%89" target="_blank" rel="external">上篇论文</a>中也有提到]<ul><li>$D_F$ 是输入<code>feature map</code>的长和宽，<strong>M</strong> 是输入的通道数（<code>channels</code>）</li><li>$D_G$ 是输出<code>feature map</code>的长和宽，<strong>N</strong> 是输出的通道数</li></ul></li><li>假设卷积核<code>filter</code>的大小是$D_k \times D_k$，则标准卷积的计算量是$$D_k \cdot D_k \cdot M \cdot N \cdot D_F \cdot D_F$$<ul><li>引用上篇论文中的图, 只看<code>kernel matrix</code> 部分，$D_k \cdot D_k$就是一个方格的大小，然后乘上输入和输出的<code>channels</code>个数，然后作用在<strong>input feature maps</strong><br><img src="/assets/blog_images/ModelCompression/07_feature-map.png" alt="kernel matrix" title="07_feature-map"></li></ul></li><li>标准卷积是这样的, 即不管当前<code>pixel</code>有多少<code>channels</code>，卷积之后就是一个<code>channel</code></li></ul><p><img src="/assets/blog_images/ModelCompression/14_RegularConvolution.png" alt="regular conv" title="14_RegularConvolution"></p><h3 id="2-2-Depthwise-Separable-Convolution"><a href="#2-2-Depthwise-Separable-Convolution" class="headerlink" title="2.2 Depthwise Separable Convolution"></a>2.2 Depthwise Separable Convolution</h3><ul><li>分为两个步骤<ul><li>第一步<strong>深度卷积</strong>：卷积核的大小是$D_k \times D_k \times 1 \times M$，所以总的计算量是：$$D_k \cdot D_k \cdot M \cdot D_F \cdot D_F$$</li><li>第二步<strong>逐点卷积</strong>：卷积核大小是$1 \times 1 \times M \times N$，所以总的计算量是：$$M \cdot N \cdot D_F \cdot D_F$$</li></ul></li><li>所以和标准的卷积相比<strong>计算量比率</strong>为：  $${D_k \cdot D_k \cdot M \cdot D_F \cdot D_F + M \cdot N \cdot D_F \cdot D_F \over D_k \cdot D_k \cdot M \cdot N \cdot D_F \cdot D_F} = {1 \over N} + {1 \over D_k^2}$$<ul><li><code>MobileNet</code>使用的是<code>3x3</code>的卷积核，所以计算量可以减少<strong>8-9倍</strong> (因为比率是<code>1/N+1/9</code>)</li></ul></li><li>第一步深度卷积操作是在每一个<code>channel</code>上进行的卷积操作</li></ul><p><img src="/assets/blog_images/ModelCompression/15_DepthwiseConvolution.png" alt="dethwise conv" title="15_DepthwiseConvolution"></p><ul><li>第二步逐点卷积才是结合起来</li></ul><p><img src="/assets/blog_images/ModelCompression/16_PointwiseConvolution.png" alt="PointwiseConvolution" title="16_PointwiseConvolution"></p><h2 id="3-神经网络结构"><a href="#3-神经网络结构" class="headerlink" title="3. 神经网络结构"></a>3. 神经网络结构</h2><ul><li><code>MobileNet</code>共有<code>28</code>层（深度卷积和逐点卷积分开来算）</li><li>之前标准的结构是卷积层之后跟上<code>Batch Normalization</code>层和<code>Relu</code>激活函数，这里引入<code>Depthwise separable convolution</code>之后的结构如下图<ul><li>每一层都跟上了<strong>BN层</strong>和激活函数</li></ul></li><li>总的结构</li></ul><p><img src="/assets/blog_images/ModelCompression/18_structure.png" alt="mobileNet structure" title="18_structure"></p><p><img src="/assets/blog_images/ModelCompression/17_structure_of_mobileNet.png" alt="structure of mobileNet" title="17_structure_of_mobileNet"></p><h2 id="4-宽度乘数（Width-Multiplier）"><a href="#4-宽度乘数（Width-Multiplier）" class="headerlink" title="4. 宽度乘数（Width Multiplier）"></a>4. 宽度乘数（Width Multiplier）</h2><ul><li>引入<strong>超参数</strong>$\alpha$, 目的是使模型<strong>变瘦</strong>,</li><li>即输入层的<code>channels</code>个数<strong>M</strong>，变成$\alpha M$，输出层的<code>channels</code>个数<strong>N</strong>变成了$\alpha N$</li><li>所以引入宽度乘数后的总的计算量是$$D_k \cdot D_k \cdot \alpha M \cdot D_F \cdot D_F + \alpha M \cdot \alpha N \cdot D_F \cdot D_F$$<ul><li>一般$\alpha \in (0,1]$，常取的值是<code>1, 0.75, 0.5, 0.25,</code></li><li><strong>大约</strong>可以减少参数量和计算量的$\alpha ^2$</li></ul></li></ul><h2 id="5-分辨率乘数-（Resolution-Multiplier）"><a href="#5-分辨率乘数-（Resolution-Multiplier）" class="headerlink" title="5. 分辨率乘数 （Resolution Multiplier）"></a>5. 分辨率乘数 （Resolution Multiplier）</h2><ul><li>引入<strong>超参数</strong>$\rho$，目的是降低图片的分辨率</li><li>即作用在输入的<code>feature map</code>上</li><li>所以再引入分辨率乘数后总的计算量是：$$D_k \cdot D_k \cdot \alpha M \cdot  \rho D_F \cdot \rho D_F + \alpha M \cdot \alpha N \cdot \rho D_F \cdot \rho D_F$$<ul><li>一般输入图片的分辨率是<code>224, 192, 160 or 128</code></li><li>大约可以减少计算量的$\rho ^2$</li></ul></li></ul><h2 id="6-实验结果"><a href="#6-实验结果" class="headerlink" title="6. 实验结果"></a>6. 实验结果</h2><ul><li>关于超参数的选择，下图可以看出<strong>准确度和参数量和参数运算量的关系</strong>，之间有个<code>trade off</code>，合理选择参数即可</li></ul><p><img src="/assets/blog_images/ModelCompression/19_accuracy_with_computation.png" alt="准确度和参数运算量的关系" title="19_accuracy_with_computation"></p><p><img src="/assets/blog_images/ModelCompression/20_accuracy_with_parameters.png" alt="准确度和参数量的关系" title="20_accuracy_with_parameters"></p><ul><li>还在<strong>细粒度的识别</strong>，<strong>大规模地理位置识别</strong>，<strong>人脸属性提取</strong>，<strong>目标检测</strong>和<strong>人脸识别</strong>等任务上进行了测试，效果也很好</li></ul><h2 id="7-总结"><a href="#7-总结" class="headerlink" title="7. 总结"></a>7. 总结</h2><ul><li>主要是基于<strong>depthwise separable convolution</strong></li><li>引入了两个超参数<ul><li>[ 第一个宽度乘数就是<strong>减少</strong><code>feature map</code>，以此来降低模型厚度 ]</li><li>[ 第二个分辨率乘数就是<strong>缩小</strong><code>feature map</code>的大小，来减少计算量]</li></ul></li><li>[ 超参数的选择是有个<code>trade off</code>的 ]</li></ul><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li><a href="https://arxiv.org/abs/1704.04861" target="_blank" rel="external">https://arxiv.org/abs/1704.04861</a></li><li><a href="https://github.com/tensorflow/models/blob/master/slim/nets/mobilenet_v1.md" target="_blank" rel="external">https://github.com/tensorflow/models/blob/master/slim/nets/mobilenet_v1.md</a></li><li><a href="https://github.com/shicai/MobileNet-Caffe" target="_blank" rel="external">https://github.com/shicai/MobileNet-Caffe</a></li><li><a href="http://machinethink.net/blog/googles-mobile-net-architecture-on-iphone/" target="_blank" rel="external">http://machinethink.net/blog/googles-mobile-net-architecture-on-iphone/</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;ul&gt;
&lt;li&gt;&lt;code&gt;Tensorflow&lt;/code&gt; 中的实现：&lt;a href=&quot;https://github.com/tensorflow/models/blob/master/slim/nets/mobilenet_v1.md&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;点击查看&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Caffe&lt;/code&gt; 中的实现：&lt;a href=&quot;https://github.com/shicai/MobileNet-Caffe&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;点击查看&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;1、概述&quot;&gt;&lt;a href=&quot;#1、概述&quot; class=&quot;headerlink&quot; title=&quot;1、概述&quot;&gt;&lt;/a&gt;1、概述&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;code&gt;Google&lt;/code&gt;在&lt;code&gt;2017&lt;/code&gt;年提出的适用于手机端的神经网络模型&lt;/li&gt;
&lt;li&gt;主要使用了&lt;strong&gt;深度可分离卷积&lt;/strong&gt;&lt;code&gt;Depthwise Separable Convolution&lt;/code&gt; 将卷积核进行分解计算来减少计算量&lt;/li&gt;
&lt;li&gt;引入了&lt;strong&gt;两个超参数&lt;/strong&gt;减少参数量和计算量&lt;ul&gt;
&lt;li&gt;宽度乘数（&lt;code&gt;Width Multiplier&lt;/code&gt;）: [减少输入和输出的 &lt;code&gt;channels&lt;/code&gt; ]&lt;/li&gt;
&lt;li&gt;分辨率乘数（&lt;code&gt;Resolution Multiplier&lt;/code&gt;）：[减少输入输出的 &lt;code&gt;feature maps&lt;/code&gt; 的大小]&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="Paper阅读记录" scheme="http://lawlite.cn/tags/Paper%E9%98%85%E8%AF%BB%E8%AE%B0%E5%BD%95/"/>
    
      <category term="ModelCompression" scheme="http://lawlite.cn/tags/ModelCompression/"/>
    
  </entry>
  
  <entry>
    <title>论文记录-Pruning Filters For Efficient ConvNets</title>
    <link href="http://lawlite.cn/2017/09/10/%E8%AE%BA%E6%96%87%E8%AE%B0%E5%BD%95-PruningFiltersForEfficientConvNets/"/>
    <id>http://lawlite.cn/2017/09/10/论文记录-PruningFiltersForEfficientConvNets/</id>
    <published>2017-09-10T02:32:12.000Z</published>
    <updated>2017-09-12T03:38:29.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1、概述"><a href="#1、概述" class="headerlink" title="1、概述"></a>1、概述</h2><ul><li>一些剪枝的操作主要是<strong>减少了全连接层的参数</strong>，全连接层的参数量占比最多（比如<code>VGG-16</code>中全连接层操作占了<code>90%</code>，计算量只占了不到<code>1%</code>）, 但是主要的<strong>计算量集中在卷层操作</strong></li><li>论文就是提出了<strong>对卷积层进行剪枝操作</strong>，然后进行<code>retrain</code>，不会造成<strong>稀疏连接</strong>（像<a href="http://lawlite.me/2017/09/07/%E8%AE%BA%E6%96%87%E8%AE%B0%E5%BD%95-DeepCompression-CompressingDeepNeuralNetworksWithPruning-TrainedQuantizationAndHuffmanCoding/" target="_blank" rel="external">上篇论文</a>一样，稀疏矩阵操作需要特殊的库等来处理）</li><li>全连接层可以使用<strong>平均池化层</strong>来代替以减少参数量</li></ul><a id="more"></a><h2 id="2、对Filters进行剪枝，以及Feature-maps"><a href="#2、对Filters进行剪枝，以及Feature-maps" class="headerlink" title="2、对Filters进行剪枝，以及Feature maps"></a>2、对<code>Filters</code>进行剪枝，以及<code>Feature maps</code></h2><h3 id="2-1-基础（CNN相关内容）"><a href="#2-1-基础（CNN相关内容）" class="headerlink" title="2.1 基础（CNN相关内容）"></a>2.1 基础（<code>CNN</code>相关内容）</h3><ul><li>设第 <code>i</code> 层的卷积层的输入 <code>channel</code> 有 $n_i$ , $h_i$ 和 $w_i$ 表示输入的特征图<code>feature map</code>的高和宽</li><li>使用$n_{i+1}$ 个<code>3D filters</code> $F_{i,j} \in R^{n_i \times k \times k}$， 则卷积操作可以将输入的<code>feature maps</code> $x_i \in R^{n_i \times h_i \times w_i}$ 转化为 $x_{i+1} \in R^{n_{i+1} \times h_{i+1} \times w_{i+1}}$<ul><li>关于<code>CNN</code>的基础不了解的可以<a href="http://blog.csdn.net/u013082989/article/details/53673602" target="_blank" rel="external">查看这里</a></li></ul></li><li>卷积操作的运算数量是：$n_{i+1}n_ik^2h_{i+1}w_{i+1}$ (对应到下图的<code>kernel matrix</code>)</li><li>所以如下图所示，取出一个<code>feature map</code>可以直接减少$n_ik^2h_{i+1}w_{i+1}$个运算<ul><li>同时接下来的<code>feature map</code>也就没有了，附加移除$n_{i+2}k^2h_{i+2}w_{i+2}$个运算</li></ul></li><li>所以减少<code>m</code>个 <code>featuremaps</code> 可以减少 $m/n_{i+1}$ 的计算量<ul><li>下图的<code>kernel matrix</code>，一个 <code>feature map</code> 对应一列，所以是$m/n_{i+1}$<br><img src="/assets/blog_images/ModelCompression/07_feature-map.png" alt="filters" title="07_feature-map"></li></ul></li></ul><h3 id="2-2-去除哪些filters-在单层中"><a href="#2-2-去除哪些filters-在单层中" class="headerlink" title="2.2 去除哪些filters (在单层中)"></a>2.2 去除哪些<code>filters</code> (在单层中)</h3><ul><li>向<a href="http://lawlite.me/2017/09/07/%E8%AE%BA%E6%96%87%E8%AE%B0%E5%BD%95-DeepCompression-CompressingDeepNeuralNetworksWithPruning-TrainedQuantizationAndHuffmanCoding/" target="_blank" rel="external">之前的论文</a>介绍的，权重的绝对值越小，则权重的作用也就越小<ul><li>[ 假设权重值都在<code>0</code>附近，进行乘积得到的值很小，所以对结果造成的影响也很小 ]</li><li>[ 删除一些冗余的值还有可能防止过拟合 ]</li></ul></li><li>本文使用的是<code>filter</code>的绝对值的和来衡量这个<code>filter</code>的作用，即 $\sum |F_{i,j}|$ , ($l_1$范数)<ul><li>选择前<code>m</code>个<strong>最小的</strong>绝对值删除</li><li>文章和<strong>随机选择</strong>相同数量<code>的filters</code>和选择<strong>最大值</strong>的结果比较，此方法最好</li></ul></li><li><code>VGG-16</code>在<code>Cifar-10</code>数据集上训练得到的卷积层的权重分布情况，可以看出每一卷积层的分布变化还是很大的</li></ul><p><img src="/assets/blog_images/ModelCompression/08_conv_abs_distribution.png" alt="each layer distribution of abs value " title="08_conv_abs_distribution"></p><h3 id="2-3-剪枝的敏感度-Sensitivity"><a href="#2-3-剪枝的敏感度-Sensitivity" class="headerlink" title="2.3 剪枝的敏感度(Sensitivity)"></a>2.3 剪枝的敏感度(Sensitivity)</h3><ul><li>就是每一卷积层进行<strong>单独剪枝</strong>，查看在<code>validation set</code>上准确度的变化</li><li>对于<code>VGG-16</code>, 一些卷积层的<code>filter</code>数量是一样的，所以对于差不多 <code>Sensitivity</code> 的卷积层，使用相同的比例进行剪枝，而对于 <code>Sensitivity</code> 比较大的，选择最小的比例进行剪枝或者不进行剪枝</li></ul><p><img src="/assets/blog_images/ModelCompression/09_accuracy_of_prune.png" alt="accuracy" title="09_accuracy_of_prune"></p><h3 id="2-4-多层剪枝的策略"><a href="#2-4-多层剪枝的策略" class="headerlink" title="2.4 多层剪枝的策略"></a>2.4 多层剪枝的策略</h3><ul><li>之前的一些剪枝策略是逐层剪枝，然后进行<code>retraining</code>，但是这样是非常耗时的</li><li>两种策略<ul><li>独立剪枝：就是每一层是独立的，然后进行剪枝</li><li>贪心剪枝：就是考虑到上一层被剪掉的情况</li></ul></li><li>如下图，第一种方法就是不考虑已经前面已经移除的<code>filters</code>（蓝色的），就是黄色的<code>kernel</code>仍然参与计算<ul><li>而对于贪心剪枝就不用计算黄色的<code>kernel</code>了</li></ul></li></ul><p><img src="/assets/blog_images/ModelCompression/10_two_strategies_of_pruning.png" alt="two strategies of pruning" title="10_two_strategies_of_pruning"></p><h2 id="3、-Retraining"><a href="#3、-Retraining" class="headerlink" title="3、 Retraining"></a>3、 Retraining</h2><ul><li>剪枝之后，应该<code>retraining</code>，（和迁移学习很像，有些<code>fine-tune</code>的意思）</li><li>也是两种策略：<ul><li>一次性剪枝然后 <code>retrain</code></li><li>逐层剪枝进行 <code>retrain</code></li></ul></li><li>第二种策略结果可能会更好，但是需要更多的<code>epochs</code></li></ul><h2 id="4、实验结果"><a href="#4、实验结果" class="headerlink" title="4、实验结果"></a>4、实验结果</h2><ul><li>剪枝之后进行<code>retrain</code>，在原来的基础之上得到的结果要比完全重新训练得到的结果好</li></ul><p><img src="/assets/blog_images/ModelCompression/11_experiment_result.png" alt="result" title="11_experiment_result"></p><ul><li>和随机剪枝、减去最大值$l_1$范数的<code>filters</code>的结果比较</li></ul><p><img src="/assets/blog_images/ModelCompression/12_comparison.png" alt="comparison" title="12_comparison"></p><h2 id="5、结论"><a href="#5、结论" class="headerlink" title="5、结论"></a>5、结论</h2><ul><li>剪枝<code>filters</code>，减少计算量</li><li>注意有<code>Batch Normalization</code>层的对应剪枝后，<code>BN</code>层也要对应删除</li><li>[其实感觉方法挺简单的]</li></ul><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li><a href="https://arxiv.org/abs/1608.08710" target="_blank" rel="external">https://arxiv.org/abs/1608.08710</a></li><li><a href="http://lawlite.me/2017/09/07/%E8%AE%BA%E6%96%87%E8%AE%B0%E5%BD%95-DeepCompression-CompressingDeepNeuralNetworksWithPruning-TrainedQuantizationAndHuffmanCoding/" target="_blank" rel="external">http://lawlite.me/2017/09/07/%E8%AE%BA%E6%96%87%E8%AE%B0%E5%BD%95-DeepCompression-CompressingDeepNeuralNetworksWithPruning-TrainedQuantizationAndHuffmanCoding/</a></li><li><a href="http://blog.csdn.net/u013082989/article/details/53673602" target="_blank" rel="external">http://blog.csdn.net/u013082989/article/details/53673602</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;1、概述&quot;&gt;&lt;a href=&quot;#1、概述&quot; class=&quot;headerlink&quot; title=&quot;1、概述&quot;&gt;&lt;/a&gt;1、概述&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;一些剪枝的操作主要是&lt;strong&gt;减少了全连接层的参数&lt;/strong&gt;，全连接层的参数量占比最多（比如&lt;code&gt;VGG-16&lt;/code&gt;中全连接层操作占了&lt;code&gt;90%&lt;/code&gt;，计算量只占了不到&lt;code&gt;1%&lt;/code&gt;）, 但是主要的&lt;strong&gt;计算量集中在卷层操作&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;论文就是提出了&lt;strong&gt;对卷积层进行剪枝操作&lt;/strong&gt;，然后进行&lt;code&gt;retrain&lt;/code&gt;，不会造成&lt;strong&gt;稀疏连接&lt;/strong&gt;（像&lt;a href=&quot;http://lawlite.me/2017/09/07/%E8%AE%BA%E6%96%87%E8%AE%B0%E5%BD%95-DeepCompression-CompressingDeepNeuralNetworksWithPruning-TrainedQuantizationAndHuffmanCoding/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;上篇论文&lt;/a&gt;一样，稀疏矩阵操作需要特殊的库等来处理）&lt;/li&gt;
&lt;li&gt;全连接层可以使用&lt;strong&gt;平均池化层&lt;/strong&gt;来代替以减少参数量&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="Paper阅读记录" scheme="http://lawlite.cn/tags/Paper%E9%98%85%E8%AF%BB%E8%AE%B0%E5%BD%95/"/>
    
      <category term="ModelCompression" scheme="http://lawlite.cn/tags/ModelCompression/"/>
    
  </entry>
  
  <entry>
    <title>论文记录-Deep Compression:Compressing DeepNeural Networks With Pruning, Trained Quantization And Huffman Coding</title>
    <link href="http://lawlite.cn/2017/09/07/%E8%AE%BA%E6%96%87%E8%AE%B0%E5%BD%95-DeepCompression-CompressingDeepNeuralNetworksWithPruning-TrainedQuantizationAndHuffmanCoding/"/>
    <id>http://lawlite.cn/2017/09/07/论文记录-DeepCompression-CompressingDeepNeuralNetworksWithPruning-TrainedQuantizationAndHuffmanCoding/</id>
    <published>2017-09-07T02:43:52.000Z</published>
    <updated>2017-09-11T06:40:04.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1、概述"><a href="#1、概述" class="headerlink" title="1、概述"></a>1、概述</h2><ul><li>压缩主要分为<strong>三个阶段</strong>：剪枝(<code>pruning</code>)、训练分层(<code>trained quantization</code>)以及 哈夫曼编码(<code>Huffman coding</code>)</li><li>可以压缩<code>35</code>到<code>49</code>倍，并且不影响精度</li><li>[模型压缩的主要用于还是能够用于小型的设备上，例如手机端等，比如Google的<code>Mobile Net</code>, 但是准确度肯定要比正常的电脑端训练的大网络低一些，在所难免]</li><li>[一般的训练好的神经网络模型文件有<strong>几百兆</strong>的大小，比如<a href="https://github.com/tensorflow/models/tree/master/slim#pre-trained-models" target="_blank" rel="external">Google预训练好的模型</a>，计算量也很大，在手机端运行不太现实]</li></ul><a id="more"></a><h2 id="2、Pipline"><a href="#2、Pipline" class="headerlink" title="2、Pipline"></a>2、Pipline</h2><ul><li>剪枝可以压缩<strong>10倍</strong>左右，加上分层可以达到<strong>27到31倍</strong>，再加上哈夫曼编码可以达到<strong>35到49倍</strong><ul><li>剪枝：去除多余的连接，比如权重非常小的连接</li><li>分层：我感觉像是聚类，<strong>多个连接共享一个权重</strong></li></ul></li></ul><p><img src="/assets/blog_images/ModelCompression/01_pipline.png" alt="Pipline" title="01_pipline"></p><h2 id="3、剪枝"><a href="#3、剪枝" class="headerlink" title="3、剪枝"></a>3、剪枝</h2><ul><li>主要是删去权重值<code>weight</code>比较小的，(设置为0)，可以设置一个阈值(<code>threshold</code>)</li><li>所以<strong>权重矩阵</strong>变的比较稀疏，可以采用压缩行存储（<code>Compressed Row Storage(CRS)</code>）或列存储来存储稀疏矩阵<ul><li>主要包括<code>3</code>个数组，浮点值数组<code>val</code>，两个整形数组<code>col_index</code>, <code>row_ptr</code></li><li><code>val(k) = a(i,j), col_index(k) = j</code></li><li><code>row_ptr</code>是每行数据第一个<strong>非0</strong>元素在<code>val</code>中的索引，<strong>最后加上一位非0元素的个数</strong>，即<code>row_ptr(n+1) = a+1</code><ul><li>比如<br><img src="/assets/blog_images/ModelCompression/02_compressed-sparse-row.png" alt="矩阵" title="02_compressed-sparse-row"></li></ul></li></ul></li></ul><table><thead><tr><th>val</th><th>10</th><th>-2</th><th>3</th><th>9</th><th>3</th><th>7</th><th>8</th><th>7</th><th>3 … 9</th><th>13</th><th>4</th><th>2</th><th>-1</th></tr></thead><tbody><tr><td>col_index</td><td>1</td><td>5</td><td>1</td><td>2</td><td>6</td><td>2</td><td>3</td><td>4</td><td>1 … 5</td><td>6</td><td>2</td><td>5</td><td>6</td></tr></tbody></table><table><thead><tr><th>row_ptr</th><th>1</th><th>3</th><th>6</th><th>9</th><th>13</th><th>17</th><th>20</th></tr></thead><tbody><tr><td></td></tr></tbody></table><ul><li>所以总共需要的大小为：<code>2a+n+1</code><ul><li><code>a</code>为矩阵非零元素的个数</li><li><code>n</code>为行数</li></ul></li></ul><h2 id="4、训练分层量化"><a href="#4、训练分层量化" class="headerlink" title="4、训练分层量化"></a>4、训练分层量化</h2><ul><li>比如所有的权重聚成<code>4</code>类，<code>cluster index</code>表示每个权重对应的类别</li><li>梯度采用同一类别内进行<strong>累加</strong>，然后进行<strong>微调更新</strong></li></ul><p><img src="/assets/blog_images/ModelCompression/03_trained-quantization.png" alt="trained quantization" title="03_trained-quantization"></p><ul><li>假设有<code>n</code>个连接，每个连接的用<code>b</code> <code>bits</code>来表示，并假设有<code>k</code>个<code>cluster</code>, 只需要$log_2(k)$<code>bits</code>去表示索引，则压缩率可以为：$$r = {nb \over nlog_2(k)+kb}$$<ul><li><code>nb</code>即为没有聚类前总共需要的<code>bits</code></li><li>$nlog_2(k)+kb$就是聚类索引的<code>bits</code>加上聚类后连接需要的<code>bits</code></li><li>比如上面的例子为：${16<em>32 \over 16</em>2+4*32} = 3.2$</li></ul></li></ul><h3 id="4-1-权值共享"><a href="#4-1-权值共享" class="headerlink" title="4.1 权值共享"></a>4.1 权值共享</h3><ul><li>使用<code>k-means</code>算法进行聚类，确定<strong>每一层共享的权重</strong>，在一个<code>cluster</code>中的权重共享，注意这里<strong>没有跨层</strong></li><li>将$W={\{w_1, w_2, … ,w_n\}}$聚为$C={\{c_1,c_2, … ,c_k\}}$类, 其中<code>n&gt;&gt;k</code><ul><li>优化函数为：$$\mathop {\arg \min }\limits_c \sum\limits_{i=1}^k \sum\limits_{w \in c_i} |w-c_i|^2$$ <h3 id="4-2-共享权重的初始化方法（三种）"><a href="#4-2-共享权重的初始化方法（三种）" class="headerlink" title="4.2 共享权重的初始化方法（三种）"></a>4.2 <strong>共享权重的初始化方法</strong>（三种）</h3></li><li><code>Forgy</code>: 就是随机初始化方法初始化聚类的中心，如下图，因为权重分布有两个峰值，初始化的值都在峰值附近</li><li>基于密度的初始化方法：如下图，先是根据<strong>累积分布函数</strong>(<code>CDF</code>)<strong>线性等分y轴</strong>，然后根据<code>CDF</code>找到对应的<code>x</code>轴的坐标，即为聚类的中心。（也是在峰值附近，和<code>Forgy</code>方法相比更分散一些）</li><li>线性：就是根据权重的最小值和最大值等分，分散性最大</li></ul></li></ul><p><img src="/assets/blog_images/ModelCompression/04_centroids-initialization.png" alt="centroids initialization" title="04_centroids-initialization"></p><ul><li>神经网络中一般<strong>权重值越大，它的作用也就越大</strong>，所以对于前两种初始化方法都是在<strong>峰值附近</strong>，也就意味着值少的地方很小的概率会被初始化，所以不太好，实验中线性初始化的效果最好（但是<strong>大权重值的是很少的</strong>）</li></ul><h3 id="4-3-前向和反向传播"><a href="#4-3-前向和反向传播" class="headerlink" title="4.3 前向和反向传播"></a>4.3 前向和反向传播</h3><ul><li>计算时查表就可以了</li><li><strong>反向传播用于更新聚类中心的权重值</strong><br>$${\partial L \over \partial C_k} = \sum\limits_{ij}{\partial L \over \partial W_{ij}} {\partial W_{ij} \over \partial C_k} = \sum\limits_{ij} {\partial L \over\partial W_{ij}}\Gamma (I_{ij}=k)$$<ul><li>其中<code>L</code>是损失函数，$C_k$是第<code>k</code>个聚类的中心</li><li>$I_{ij}$为聚类中心的索引，如下图，就是<strong>同一类别梯度求和</strong><br><img src="/assets/blog_images/ModelCompression/03_trained-quantization.png" alt="权重" title="03_trained-quantization"></li></ul></li></ul><h2 id="5、哈夫曼编码"><a href="#5、哈夫曼编码" class="headerlink" title="5、哈夫曼编码"></a>5、哈夫曼编码</h2><ul><li>就是按照聚类中心的出现的概率从大到小排序进行<code>Huffman</code>编码</li><li>根据上面的结果，权重大都分布在两个峰值附近，所以利于<code>huffman</code>编码</li></ul><h2 id="6、结果及讨论"><a href="#6、结果及讨论" class="headerlink" title="6、结果及讨论"></a>6、结果及讨论</h2><ul><li>没有准确度损失<br><img src="/assets/blog_images/ModelCompression/05_compressed-result.png" alt="result" title="05_compressed-result"></li><li><code>pruning</code> 和 <code>quantization</code> 结合使用效果最好<br><img src="/assets/blog_images/ModelCompression/06_combination-of-pruning-and-uqantizaiton.png" alt="pruning and quantization" title="06_combination-of-pruning-and-uqantizaiton"></li><li>和之前别人的工作的比较<ul><li><code>SVD</code> 压缩了模型但是精度损失较大</li></ul></li></ul><p><img src="/assets/blog_images/ModelCompression/07_comparison.png" alt="和之前的工作比较" title="07_comparison"></p><ul><li>缺点就是在运行时现有的<code>GPU</code>不能进行间接的矩阵输入查找，以及相对索引 <code>CSC</code> 或 <code>CSR</code>, 还有剪枝的操作主要<strong>是在全连接层</strong>，但是计算量大的卷积层较少，虽然可以通过<code>BLAS libraries</code>或是<code>specialized hardware</code>进行加速，但是也是受限的（下篇<code>Paper</code>中也有提到）</li><li>[我觉得剪枝和权值共享其实是能够防止过拟合的，所以准确度没有损失]</li><li>[权值共享时是当前层的权值共享，不是整个网络的权值共享]</li></ul><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li><a href="https://arxiv.org/abs/1510.00149" target="_blank" rel="external">https://arxiv.org/abs/1510.00149</a></li><li><a href="http://blog.csdn.net/bigpiglet_zju/article/details/20791881" target="_blank" rel="external">http://blog.csdn.net/bigpiglet_zju/article/details/20791881</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;1、概述&quot;&gt;&lt;a href=&quot;#1、概述&quot; class=&quot;headerlink&quot; title=&quot;1、概述&quot;&gt;&lt;/a&gt;1、概述&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;压缩主要分为&lt;strong&gt;三个阶段&lt;/strong&gt;：剪枝(&lt;code&gt;pruning&lt;/code&gt;)、训练分层(&lt;code&gt;trained quantization&lt;/code&gt;)以及 哈夫曼编码(&lt;code&gt;Huffman coding&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;可以压缩&lt;code&gt;35&lt;/code&gt;到&lt;code&gt;49&lt;/code&gt;倍，并且不影响精度&lt;/li&gt;
&lt;li&gt;[模型压缩的主要用于还是能够用于小型的设备上，例如手机端等，比如Google的&lt;code&gt;Mobile Net&lt;/code&gt;, 但是准确度肯定要比正常的电脑端训练的大网络低一些，在所难免]&lt;/li&gt;
&lt;li&gt;[一般的训练好的神经网络模型文件有&lt;strong&gt;几百兆&lt;/strong&gt;的大小，比如&lt;a href=&quot;https://github.com/tensorflow/models/tree/master/slim#pre-trained-models&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Google预训练好的模型&lt;/a&gt;，计算量也很大，在手机端运行不太现实]&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="Paper阅读记录" scheme="http://lawlite.cn/tags/Paper%E9%98%85%E8%AF%BB%E8%AE%B0%E5%BD%95/"/>
    
      <category term="ModelCompression" scheme="http://lawlite.cn/tags/ModelCompression/"/>
    
  </entry>
  
  <entry>
    <title>R语言学习</title>
    <link href="http://lawlite.cn/2017/06/30/R%E8%AF%AD%E8%A8%80%E5%AD%A6%E4%B9%A0/"/>
    <id>http://lawlite.cn/2017/06/30/R语言学习/</id>
    <published>2017-06-30T08:48:15.000Z</published>
    <updated>2017-07-01T12:24:11.000Z</updated>
    
    <content type="html"><![CDATA[<ul><li>基础内容来自<code>W3C</code>，直接<a href="https://w3cschool.cn/r/" target="_blank" rel="external">查看这里</a>即可，这里只是个人学习的记录</li><li>只是最近感觉有必要学习一下<code>R</code>，哈哈</li></ul><h1 id="一、基础"><a href="#一、基础" class="headerlink" title="一、基础"></a>一、基础</h1><h2 id="1、概述"><a href="#1、概述" class="headerlink" title="1、概述"></a>1、概述</h2><ul><li>R语言是用于统计分析，图形表示和报告的编程语言和软件环境。</li><li>解释型语言<h2 id="2、Windows上安装"><a href="#2、Windows上安装" class="headerlink" title="2、Windows上安装"></a>2、<code>Windows</code>上安装</h2></li><li>下载地址：<a href="https://cran.r-project.org/bin/windows/base/" target="_blank" rel="external">R-3.4.0</a>，直接下载安装即可</li><li><code>IDE</code>使用<code>RStudio</code>: <a href="https://www.rstudio.com/products/rstudio/download/" target="_blank" rel="external">点击下载</a></li></ul><a id="more"></a><h2 id="3、数据类型"><a href="#3、数据类型" class="headerlink" title="3、数据类型"></a>3、数据类型</h2><ul><li>使用<code>&lt;-</code>进行赋值（<code>RStudio</code>中的快捷键是<code>Alt+-</code>）</li><li>变量分配有<strong>R对象</strong>，R对象的数据类型变为变量的数据类型。</li><li>使用<code>class()</code>来查看类型<h3 id="1-Vector向量"><a href="#1-Vector向量" class="headerlink" title="(1) Vector向量"></a>(1) <code>Vector</code>向量</h3></li><li>创建使用<code>c()</code>函数</li></ul><figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">&gt; apple = c(<span class="string">'apple'</span>, <span class="string">'banana'</span>)</div><div class="line">&gt; print(apple)</div><div class="line">[<span class="number">1</span>] <span class="string">"apple"</span>  <span class="string">"banana"</span></div></pre></td></tr></table></figure><h3 id="2-List列表"><a href="#2-List列表" class="headerlink" title="(2) List列表"></a>(2) <code>List</code>列表</h3><ul><li>列表是一个<strong>R对象</strong>，它可以在其中包含许多不同类型的元素</li></ul><figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">&gt; list &lt;- list(c(<span class="string">'apple'</span>,<span class="string">'banana'</span>), <span class="number">1</span>, <span class="number">2.0</span>)</div><div class="line">&gt; print(list)</div><div class="line">[[<span class="number">1</span>]]</div><div class="line">[<span class="number">4</span>] <span class="string">"apple"</span>  <span class="string">"banana"</span></div><div class="line"></div><div class="line">[[<span class="number">2</span>]]</div><div class="line">[<span class="number">5</span>] <span class="number">1</span></div><div class="line"></div><div class="line">[[<span class="number">3</span>]]</div><div class="line">[<span class="number">6</span>] <span class="number">2</span></div></pre></td></tr></table></figure><h3 id="3-Matrix矩阵"><a href="#3-Matrix矩阵" class="headerlink" title="(3) Matrix矩阵"></a>(3) <code>Matrix</code>矩阵</h3><ul><li>矩阵被限制为二维<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">&gt; M = matrix( c(<span class="string">'a'</span>,<span class="string">'a'</span>,<span class="string">'b'</span>,<span class="string">'c'</span>,<span class="string">'b'</span>,<span class="string">'a'</span>), nrow = <span class="number">2</span>, ncol = <span class="number">3</span>, byrow = <span class="literal">TRUE</span>)</div><div class="line">&gt; print(M)</div><div class="line">     [,<span class="number">1</span>] [,<span class="number">2</span>] [,<span class="number">3</span>]</div><div class="line">[<span class="number">1</span>,] <span class="string">"a"</span>  <span class="string">"a"</span>  <span class="string">"b"</span> </div><div class="line">[<span class="number">2</span>,] <span class="string">"c"</span>  <span class="string">"b"</span>  <span class="string">"a"</span></div></pre></td></tr></table></figure></li></ul><h3 id="4-Array数组"><a href="#4-Array数组" class="headerlink" title="(4) Array数组"></a>(4) <code>Array</code>数组</h3><ul><li><code>dim</code>指定维度，这里穿创建<code>3x3x2</code>的三维矩阵，输入是向量（一致循环创建）</li></ul><figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">&gt; a &lt;- array(c(<span class="string">'green'</span>,<span class="string">'yellow'</span>),dim = c(<span class="number">3</span>,<span class="number">3</span>,<span class="number">2</span>))</div><div class="line">&gt; print(a)</div><div class="line">, , <span class="number">1</span></div><div class="line"></div><div class="line">     [,<span class="number">1</span>]     [,<span class="number">2</span>]     [,<span class="number">3</span>]    </div><div class="line">[<span class="number">1</span>,] <span class="string">"green"</span>  <span class="string">"yellow"</span> <span class="string">"green"</span> </div><div class="line">[<span class="number">2</span>,] <span class="string">"yellow"</span> <span class="string">"green"</span>  <span class="string">"yellow"</span></div><div class="line">[<span class="number">3</span>,] <span class="string">"green"</span>  <span class="string">"yellow"</span> <span class="string">"green"</span> </div><div class="line"></div><div class="line">, , <span class="number">2</span></div><div class="line"></div><div class="line">     [,<span class="number">1</span>]     [,<span class="number">2</span>]     [,<span class="number">3</span>]    </div><div class="line">[<span class="number">1</span>,] <span class="string">"yellow"</span> <span class="string">"green"</span>  <span class="string">"yellow"</span></div><div class="line">[<span class="number">2</span>,] <span class="string">"green"</span>  <span class="string">"yellow"</span> <span class="string">"green"</span> </div><div class="line">[<span class="number">3</span>,] <span class="string">"yellow"</span> <span class="string">"green"</span>  <span class="string">"yellow"</span></div></pre></td></tr></table></figure><h3 id="5-Factor因子"><a href="#5-Factor因子" class="headerlink" title="(5) Factor因子"></a>(5) <code>Factor</code>因子</h3><ul><li>因子是使用<strong>向量创建的r对象</strong>。</li><li><code>nlevels</code>函数可以得到因子中不重复值的个数<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">&gt; factor_content &lt;- factor(c(<span class="string">'green'</span>, <span class="string">'red'</span>, <span class="string">'green'</span>, <span class="string">'yellow'</span>, <span class="string">'red'</span>, <span class="string">'green'</span>))</div><div class="line">&gt; print(factor_content)</div><div class="line">[<span class="number">1</span>] green  red    green  yellow red    green </div><div class="line">Levels: green red yellow</div><div class="line">&gt; print(nlevels(factor_content))</div><div class="line">[<span class="number">1</span>] <span class="number">3</span></div></pre></td></tr></table></figure></li></ul><h3 id="6-Data-Frame-数据帧"><a href="#6-Data-Frame-数据帧" class="headerlink" title="(6) Data Frame 数据帧"></a>(6) <code>Data Frame</code> 数据帧</h3><ul><li><p>数据表</p><figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">&gt; BMI &lt;- data.frame(</div><div class="line">+     gender = c(<span class="string">"Male"</span>, <span class="string">"Male"</span>,<span class="string">"Female"</span>), </div><div class="line">+     height = c(<span class="number">152</span>, <span class="number">171.5</span>, <span class="number">165</span>), </div><div class="line">+     weight = c(<span class="number">81</span>,<span class="number">93</span>, <span class="number">78</span>),</div><div class="line">+     Age = c(<span class="number">42</span>,<span class="number">38</span>,<span class="number">26</span>)</div><div class="line">+ )</div><div class="line">&gt; print(BMI)</div><div class="line">  gender height weight Age</div><div class="line"><span class="number">1</span>   Male  <span class="number">152.0</span>     <span class="number">81</span>  <span class="number">42</span></div><div class="line"><span class="number">2</span>   Male  <span class="number">171.5</span>     <span class="number">93</span>  <span class="number">38</span></div><div class="line"><span class="number">3</span> Female  <span class="number">165.0</span>     <span class="number">78</span>  <span class="number">26</span></div></pre></td></tr></table></figure></li><li><p><code>names(data.frame对象)</code> 查看列名</p></li><li><code>row.names</code> 查看行名</li></ul><h2 id="4、变量"><a href="#4、变量" class="headerlink" title="4、变量"></a>4、变量</h2><h3 id="1-命名"><a href="#1-命名" class="headerlink" title="(1) 命名"></a>(1) 命名</h3><ul><li>有效的变量名称由<strong>字母，数字和点</strong>或<strong>下划线</strong>字符组成</li><li>列如：<code>var_name2.</code>，<code>.var_name</code> <code>var.name</code><h3 id="2-赋值"><a href="#2-赋值" class="headerlink" title="(2) 赋值"></a>(2) 赋值</h3></li><li>可以使用<code>=, &lt;-, -&gt;</code><ul><li><code>=</code>和<code>&lt;-</code>的区别是一个是传值，一个是赋值，一般使用<code>&lt;-</code><figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">&gt; var.1 = c(<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>)</div><div class="line">&gt; var.2 &lt;- c(<span class="string">"learn"</span>,<span class="string">"R"</span>)</div><div class="line">&gt; c(<span class="literal">TRUE</span>,<span class="number">1</span>) -&gt; var.3  </div><div class="line">&gt; print(var.1)</div><div class="line">[<span class="number">1</span>] <span class="number">0</span> <span class="number">1</span> <span class="number">2</span> <span class="number">3</span></div><div class="line">&gt; print(var.2)</div><div class="line">[<span class="number">1</span>] <span class="string">"learn"</span> <span class="string">"R"</span>    </div><div class="line">&gt; print(var.3)</div><div class="line">[<span class="number">1</span>] <span class="number">1</span> <span class="number">1</span></div></pre></td></tr></table></figure></li></ul></li></ul><h3 id="3-查找变量"><a href="#3-查找变量" class="headerlink" title="(3) 查找变量"></a>(3) 查找变量</h3><ul><li>使用<code>ls()</code>函数</li></ul><figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">&gt; print(ls())</div><div class="line"> [<span class="number">1</span>] <span class="string">"a"</span>              <span class="string">"aisles"</span>         <span class="string">"apple"</span>          <span class="string">"BMI"</span>            <span class="string">"data"</span>          </div><div class="line"> [<span class="number">6</span>] <span class="string">"departments"</span>    <span class="string">"factor_content"</span> <span class="string">"list"</span>           <span class="string">"M"</span>              <span class="string">"orderp"</span>        </div><div class="line">[<span class="number">11</span>] <span class="string">"orders"</span>         <span class="string">"ordert"</span>         <span class="string">"path"</span>           <span class="string">"products"</span>       <span class="string">"s"</span>             </div><div class="line">[<span class="number">16</span>] <span class="string">"var.1"</span>          <span class="string">"var.2"</span>          <span class="string">"var.3"</span></div></pre></td></tr></table></figure><ul><li>通过<strong>模式匹配</strong>查找</li></ul><figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">&gt; print(ls(pattern = <span class="string">"var"</span>))</div><div class="line">[<span class="number">1</span>] <span class="string">"var.1"</span> <span class="string">"var.2"</span> <span class="string">"var.3"</span></div></pre></td></tr></table></figure><ul><li>以点<code>.</code>开头的变量名字会被隐藏，可以<code>print(ls(all.name = TRUE))</code>列出所有的</li></ul><h3 id="4-删除变量"><a href="#4-删除变量" class="headerlink" title="(4) 删除变量"></a>(4) 删除变量</h3><ul><li>单个变量通过<code>rm(变量名)</code>删除</li><li>删除所有变量：<code>rm(list = ls())</code></li></ul><h2 id="5、运算符"><a href="#5、运算符" class="headerlink" title="5、运算符"></a>5、运算符</h2><h3 id="1-算数运算符"><a href="#1-算数运算符" class="headerlink" title="(1) 算数运算符"></a>(1) 算数运算符</h3><ul><li><code>%%</code> : 求余</li><li><code>%/%</code> : 相除求商</li><li><code>^</code> : 指数<h3 id="2-关系运算符"><a href="#2-关系运算符" class="headerlink" title="(2) 关系运算符"></a>(2) 关系运算符</h3></li><li>和<code>c</code>等语言一样<h3 id="3-逻辑运算符"><a href="#3-逻辑运算符" class="headerlink" title="(3) 逻辑运算符"></a>(3) 逻辑运算符</h3></li><li><code>&amp;</code> : 与<code>and</code>，两个<code>&amp;&amp;</code>只比较第一个（比如向量只比较第一个元素）</li><li><code>|</code> : 或<code>or</code>， 两个<code>||</code>一样</li></ul><h3 id="4-其他"><a href="#4-其他" class="headerlink" title="(4) 其他"></a>(4) 其他</h3><ul><li><code>:</code> </li></ul><figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">&gt; <span class="number">2</span>:<span class="number">8</span></div><div class="line">[<span class="number">1</span>] <span class="number">2</span> <span class="number">3</span> <span class="number">4</span> <span class="number">5</span> <span class="number">6</span> <span class="number">7</span> <span class="number">8</span></div></pre></td></tr></table></figure><ul><li><code>%in%</code> : 元素是否在一个向量中</li></ul><h2 id="6、包"><a href="#6、包" class="headerlink" title="6、包"></a>6、包</h2><h3 id="1-介绍"><a href="#1-介绍" class="headerlink" title="(1) 介绍"></a>(1) 介绍</h3><ul><li><code>R</code>语言的包是<strong>R函数</strong>，编译代码和样本数据的集合。</li><li>存储在<code>R</code>语言环境中名为<code>“library”</code>的目录下</li><li>可用的<code>R</code>语言的包：<a href="https://cran.r-project.org/web/packages/available_packages_by_name.html" target="_blank" rel="external">点击查看</a><h3 id="2-常用命令"><a href="#2-常用命令" class="headerlink" title="(2) 常用命令"></a>(2) 常用命令</h3></li><li>查看库的位置：<code>.libPaths()</code></li><li>查看已安装所有软件包：<code>library()</code></li><li>当前环境加载的所有包：<code>search()</code></li><li>安装包：<code>install.packages(&quot;Package Name&quot;)</code></li><li>加载到当前<code>R</code>语言环境中：<code>library(&quot;package Name&quot;, lib.loc = &quot;path to library&quot;)</code><h3 id="3-手动安装包"><a href="#3-手动安装包" class="headerlink" title="(3) 手动安装包"></a>(3) 手动安装包</h3></li><li>将包作为<code>.zip</code>文件保存在本地系统中的适当位置。</li><li>安装：<code>install.packages(&quot;E:/XML_3.98-1.3.zip&quot;, repos = NULL, type = &quot;source&quot;)</code></li></ul><h2 id="7、数据重塑Dataframe"><a href="#7、数据重塑Dataframe" class="headerlink" title="7、数据重塑Dataframe"></a>7、数据重塑Dataframe</h2><h3 id="1-数据帧中加入行和列"><a href="#1-数据帧中加入行和列" class="headerlink" title="(1) 数据帧中加入行和列"></a>(1) 数据帧中加入行和列</h3><ul><li><code>cbind()</code>连接多个向量</li></ul><figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">&gt; city &lt;- c(<span class="string">"Tampa"</span>,<span class="string">"Seattle"</span>,<span class="string">"Hartford"</span>,<span class="string">"Denver"</span>)</div><div class="line">&gt; state &lt;- c(<span class="string">"FL"</span>,<span class="string">"WA"</span>,<span class="string">"CT"</span>,<span class="string">"CO"</span>)</div><div class="line">&gt; zipcode &lt;- c(<span class="number">33602</span>,<span class="number">98104</span>,<span class="number">06161</span>,<span class="number">80294</span>)</div><div class="line">&gt; addresses &lt;- cbind(city,state,zipcode)</div><div class="line">&gt; print(addresses)</div><div class="line">     city       state zipcode</div><div class="line">[<span class="number">1</span>,] <span class="string">"Tampa"</span>    <span class="string">"FL"</span>  <span class="string">"33602"</span></div><div class="line">[<span class="number">2</span>,] <span class="string">"Seattle"</span>  <span class="string">"WA"</span>  <span class="string">"98104"</span></div><div class="line">[<span class="number">3</span>,] <span class="string">"Hartford"</span> <span class="string">"CT"</span>  <span class="string">"6161"</span> </div><div class="line">[<span class="number">4</span>,] <span class="string">"Denver"</span>   <span class="string">"CO"</span>  <span class="string">"80294"</span></div></pre></td></tr></table></figure><ul><li><code>rbind()</code>按行拼接<h3 id="2-合并数据帧"><a href="#2-合并数据帧" class="headerlink" title="(2) 合并数据帧"></a>(2) 合并数据帧</h3></li><li>使用<code>merge()</code>函数合并两个数据帧。 数据帧必须具有相同的列名称，在其上进行合并。<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line">&gt; <span class="keyword">library</span>(MASS)</div><div class="line">&gt; merged.Pima &lt;- merge(x = Pima.te, y = Pima.tr,</div><div class="line">+                      by.x = c(<span class="string">"bp"</span>, <span class="string">"bmi"</span>),</div><div class="line">+                      by.y = c(<span class="string">"bp"</span>, <span class="string">"bmi"</span>)</div><div class="line">+ )</div><div class="line">&gt; print(merged.Pima)</div><div class="line">   bp  bmi npreg.x glu.x skin.x ped.x age.x type.x npreg.y glu.y skin.y ped.y age.y type.y</div><div class="line"><span class="number">1</span>  <span class="number">60</span> <span class="number">33.8</span>       <span class="number">1</span>   <span class="number">117</span>     <span class="number">23</span> <span class="number">0.466</span>    <span class="number">27</span>     No       <span class="number">2</span>   <span class="number">125</span>     <span class="number">20</span> <span class="number">0.088</span>    <span class="number">31</span>     No</div><div class="line"><span class="number">2</span>  <span class="number">64</span> <span class="number">29.7</span>       <span class="number">2</span>    <span class="number">75</span>     <span class="number">24</span> <span class="number">0.370</span>    <span class="number">33</span>     No       <span class="number">2</span>   <span class="number">100</span>     <span class="number">23</span> <span class="number">0.368</span>    <span class="number">21</span>     No</div><div class="line"><span class="number">3</span>  <span class="number">64</span> <span class="number">31.2</span>       <span class="number">5</span>   <span class="number">189</span>     <span class="number">33</span> <span class="number">0.583</span>    <span class="number">29</span>    Yes       <span class="number">3</span>   <span class="number">158</span>     <span class="number">13</span> <span class="number">0.295</span>    <span class="number">24</span>     No</div><div class="line"><span class="number">4</span>  <span class="number">64</span> <span class="number">33.2</span>       <span class="number">4</span>   <span class="number">117</span>     <span class="number">27</span> <span class="number">0.230</span>    <span class="number">24</span>     No       <span class="number">1</span>    <span class="number">96</span>     <span class="number">27</span> <span class="number">0.289</span>    <span class="number">21</span>     No</div><div class="line"><span class="number">5</span>  <span class="number">66</span> <span class="number">38.1</span>       <span class="number">3</span>   <span class="number">115</span>     <span class="number">39</span> <span class="number">0.150</span>    <span class="number">28</span>     No       <span class="number">1</span>   <span class="number">114</span>     <span class="number">36</span> <span class="number">0.289</span>    <span class="number">21</span>     No</div><div class="line"><span class="number">6</span>  <span class="number">68</span> <span class="number">38.5</span>       <span class="number">2</span>   <span class="number">100</span>     <span class="number">25</span> <span class="number">0.324</span>    <span class="number">26</span>     No       <span class="number">7</span>   <span class="number">129</span>     <span class="number">49</span> <span class="number">0.439</span>    <span class="number">43</span>    Yes</div><div class="line"><span class="number">7</span>  <span class="number">70</span> <span class="number">27.4</span>       <span class="number">1</span>   <span class="number">116</span>     <span class="number">28</span> <span class="number">0.204</span>    <span class="number">21</span>     No       <span class="number">0</span>   <span class="number">124</span>     <span class="number">20</span> <span class="number">0.254</span>    <span class="number">36</span>    Yes</div><div class="line"><span class="number">8</span>  <span class="number">70</span> <span class="number">33.1</span>       <span class="number">4</span>    <span class="number">91</span>     <span class="number">32</span> <span class="number">0.446</span>    <span class="number">22</span>     No       <span class="number">9</span>   <span class="number">123</span>     <span class="number">44</span> <span class="number">0.374</span>    <span class="number">40</span>     No</div><div class="line"><span class="number">9</span>  <span class="number">70</span> <span class="number">35.4</span>       <span class="number">9</span>   <span class="number">124</span>     <span class="number">33</span> <span class="number">0.282</span>    <span class="number">34</span>     No       <span class="number">6</span>   <span class="number">134</span>     <span class="number">23</span> <span class="number">0.542</span>    <span class="number">29</span>    Yes</div><div class="line"><span class="number">10</span> <span class="number">72</span> <span class="number">25.6</span>       <span class="number">1</span>   <span class="number">157</span>     <span class="number">21</span> <span class="number">0.123</span>    <span class="number">24</span>     No       <span class="number">4</span>    <span class="number">99</span>     <span class="number">17</span> <span class="number">0.294</span>    <span class="number">28</span>     No</div><div class="line"><span class="number">11</span> <span class="number">72</span> <span class="number">37.7</span>       <span class="number">5</span>    <span class="number">95</span>     <span class="number">33</span> <span class="number">0.370</span>    <span class="number">27</span>     No       <span class="number">6</span>   <span class="number">103</span>     <span class="number">32</span> <span class="number">0.324</span>    <span class="number">55</span>     No</div><div class="line"><span class="number">12</span> <span class="number">74</span> <span class="number">25.9</span>       <span class="number">9</span>   <span class="number">134</span>     <span class="number">33</span> <span class="number">0.460</span>    <span class="number">81</span>     No       <span class="number">8</span>   <span class="number">126</span>     <span class="number">38</span> <span class="number">0.162</span>    <span class="number">39</span>     No</div><div class="line"><span class="number">13</span> <span class="number">74</span> <span class="number">25.9</span>       <span class="number">1</span>    <span class="number">95</span>     <span class="number">21</span> <span class="number">0.673</span>    <span class="number">36</span>     No       <span class="number">8</span>   <span class="number">126</span>     <span class="number">38</span> <span class="number">0.162</span>    <span class="number">39</span>     No</div><div class="line"><span class="number">14</span> <span class="number">78</span> <span class="number">27.6</span>       <span class="number">5</span>    <span class="number">88</span>     <span class="number">30</span> <span class="number">0.258</span>    <span class="number">37</span>     No       <span class="number">6</span>   <span class="number">125</span>     <span class="number">31</span> <span class="number">0.565</span>    <span class="number">49</span>    Yes</div><div class="line"><span class="number">15</span> <span class="number">78</span> <span class="number">27.6</span>      <span class="number">10</span>   <span class="number">122</span>     <span class="number">31</span> <span class="number">0.512</span>    <span class="number">45</span>     No       <span class="number">6</span>   <span class="number">125</span>     <span class="number">31</span> <span class="number">0.565</span>    <span class="number">49</span>    Yes</div><div class="line"><span class="number">16</span> <span class="number">78</span> <span class="number">39.4</span>       <span class="number">2</span>   <span class="number">112</span>     <span class="number">50</span> <span class="number">0.175</span>    <span class="number">24</span>     No       <span class="number">4</span>   <span class="number">112</span>     <span class="number">40</span> <span class="number">0.236</span>    <span class="number">38</span>     No</div><div class="line"><span class="number">17</span> <span class="number">88</span> <span class="number">34.5</span>       <span class="number">1</span>   <span class="number">117</span>     <span class="number">24</span> <span class="number">0.403</span>    <span class="number">40</span>    Yes       <span class="number">4</span>   <span class="number">127</span>     <span class="number">11</span> <span class="number">0.598</span>    <span class="number">28</span>     No</div><div class="line">&gt; nrow(merged.Pima)</div><div class="line">[<span class="number">1</span>] <span class="number">17</span></div></pre></td></tr></table></figure></li></ul><h3 id="3-拆分和重组"><a href="#3-拆分和重组" class="headerlink" title="(3) 拆分和重组"></a>(3) 拆分和重组</h3><ul><li><p><code>melt()</code>拆分数据 (以船舶的数据集为例)</p><ul><li>需要安装<code>reshape</code>包</li><li><code>install.packages(&quot;reshape&quot;)</code><figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div></pre></td><td class="code"><pre><div class="line">&gt; <span class="keyword">library</span>(MASS)</div><div class="line">&gt; print(ships)</div><div class="line">   type year period service incidents</div><div class="line"><span class="number">1</span>     A   <span class="number">60</span>     <span class="number">60</span>     <span class="number">127</span>         <span class="number">0</span></div><div class="line"><span class="number">2</span>     A   <span class="number">60</span>     <span class="number">75</span>      <span class="number">63</span>         <span class="number">0</span></div><div class="line"><span class="number">3</span>     A   <span class="number">65</span>     <span class="number">60</span>    <span class="number">1095</span>         <span class="number">3</span></div><div class="line"><span class="number">4</span>     A   <span class="number">65</span>     <span class="number">75</span>    <span class="number">1095</span>         <span class="number">4</span></div><div class="line"><span class="number">5</span>     A   <span class="number">70</span>     <span class="number">60</span>    <span class="number">1512</span>         <span class="number">6</span></div><div class="line"><span class="number">6</span>     A   <span class="number">70</span>     <span class="number">75</span>    <span class="number">3353</span>        <span class="number">18</span></div><div class="line"><span class="number">7</span>     A   <span class="number">75</span>     <span class="number">60</span>       <span class="number">0</span>         <span class="number">0</span></div><div class="line"><span class="number">8</span>     A   <span class="number">75</span>     <span class="number">75</span>    <span class="number">2244</span>        <span class="number">11</span></div><div class="line"><span class="number">9</span>     B   <span class="number">60</span>     <span class="number">60</span>   <span class="number">44882</span>        <span class="number">39</span></div><div class="line"><span class="number">10</span>    B   <span class="number">60</span>     <span class="number">75</span>   <span class="number">17176</span>        <span class="number">29</span></div><div class="line"><span class="number">11</span>    B   <span class="number">65</span>     <span class="number">60</span>   <span class="number">28609</span>        <span class="number">58</span></div><div class="line"><span class="number">12</span>    B   <span class="number">65</span>     <span class="number">75</span>   <span class="number">20370</span>        <span class="number">53</span></div><div class="line"><span class="number">13</span>    B   <span class="number">70</span>     <span class="number">60</span>    <span class="number">7064</span>        <span class="number">12</span></div><div class="line"><span class="number">14</span>    B   <span class="number">70</span>     <span class="number">75</span>   <span class="number">13099</span>        <span class="number">44</span></div><div class="line"><span class="number">15</span>    B   <span class="number">75</span>     <span class="number">60</span>       <span class="number">0</span>         <span class="number">0</span></div><div class="line"><span class="number">16</span>    B   <span class="number">75</span>     <span class="number">75</span>    <span class="number">7117</span>        <span class="number">18</span></div><div class="line"><span class="number">17</span>    C   <span class="number">60</span>     <span class="number">60</span>    <span class="number">1179</span>         <span class="number">1</span></div><div class="line"><span class="number">18</span>    C   <span class="number">60</span>     <span class="number">75</span>     <span class="number">552</span>         <span class="number">1</span></div><div class="line"><span class="number">19</span>    C   <span class="number">65</span>     <span class="number">60</span>     <span class="number">781</span>         <span class="number">0</span></div><div class="line"><span class="number">20</span>    C   <span class="number">65</span>     <span class="number">75</span>     <span class="number">676</span>         <span class="number">1</span></div><div class="line"><span class="number">21</span>    C   <span class="number">70</span>     <span class="number">60</span>     <span class="number">783</span>         <span class="number">6</span></div><div class="line"><span class="number">22</span>    C   <span class="number">70</span>     <span class="number">75</span>    <span class="number">1948</span>         <span class="number">2</span></div><div class="line"><span class="number">23</span>    C   <span class="number">75</span>     <span class="number">60</span>       <span class="number">0</span>         <span class="number">0</span></div><div class="line"><span class="number">24</span>    C   <span class="number">75</span>     <span class="number">75</span>     <span class="number">274</span>         <span class="number">1</span></div><div class="line"><span class="number">25</span>    D   <span class="number">60</span>     <span class="number">60</span>     <span class="number">251</span>         <span class="number">0</span></div><div class="line"><span class="number">26</span>    D   <span class="number">60</span>     <span class="number">75</span>     <span class="number">105</span>         <span class="number">0</span></div><div class="line"><span class="number">27</span>    D   <span class="number">65</span>     <span class="number">60</span>     <span class="number">288</span>         <span class="number">0</span></div><div class="line"><span class="number">28</span>    D   <span class="number">65</span>     <span class="number">75</span>     <span class="number">192</span>         <span class="number">0</span></div><div class="line"><span class="number">29</span>    D   <span class="number">70</span>     <span class="number">60</span>     <span class="number">349</span>         <span class="number">2</span></div><div class="line"><span class="number">30</span>    D   <span class="number">70</span>     <span class="number">75</span>    <span class="number">1208</span>        <span class="number">11</span></div><div class="line"><span class="number">31</span>    D   <span class="number">75</span>     <span class="number">60</span>       <span class="number">0</span>         <span class="number">0</span></div><div class="line"><span class="number">32</span>    D   <span class="number">75</span>     <span class="number">75</span>    <span class="number">2051</span>         <span class="number">4</span></div><div class="line"><span class="number">33</span>    E   <span class="number">60</span>     <span class="number">60</span>      <span class="number">45</span>         <span class="number">0</span></div><div class="line"><span class="number">34</span>    E   <span class="number">60</span>     <span class="number">75</span>       <span class="number">0</span>         <span class="number">0</span></div><div class="line"><span class="number">35</span>    E   <span class="number">65</span>     <span class="number">60</span>     <span class="number">789</span>         <span class="number">7</span></div><div class="line"><span class="number">36</span>    E   <span class="number">65</span>     <span class="number">75</span>     <span class="number">437</span>         <span class="number">7</span></div><div class="line"><span class="number">37</span>    E   <span class="number">70</span>     <span class="number">60</span>    <span class="number">1157</span>         <span class="number">5</span></div><div class="line"><span class="number">38</span>    E   <span class="number">70</span>     <span class="number">75</span>    <span class="number">2161</span>        <span class="number">12</span></div><div class="line"><span class="number">39</span>    E   <span class="number">75</span>     <span class="number">60</span>       <span class="number">0</span>         <span class="number">0</span></div><div class="line"><span class="number">40</span>    E   <span class="number">75</span>     <span class="number">75</span>     <span class="number">542</span>         <span class="number">1</span></div></pre></td></tr></table></figure></li></ul></li><li><p>拆分</p><ul><li><code>melt()</code>将<strong>类型和年份以外</strong>的所有列转换为多行展示<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">library</span>(reshape)</div><div class="line">molten.ships &lt;- melt(ships, id = c(<span class="string">"type"</span>,<span class="string">"year"</span>))</div><div class="line">print(molten.ships)</div><div class="line"></div><div class="line">      type year  variable  value</div><div class="line"><span class="number">1</span>      A   <span class="number">60</span>    period      <span class="number">60</span></div><div class="line"><span class="number">2</span>      A   <span class="number">60</span>    period      <span class="number">75</span></div><div class="line"><span class="number">3</span>      A   <span class="number">65</span>    period      <span class="number">60</span></div><div class="line"><span class="number">4</span>      A   <span class="number">65</span>    period      <span class="number">75</span></div><div class="line">............</div><div class="line">............</div><div class="line"><span class="number">9</span>      B   <span class="number">60</span>    period      <span class="number">60</span></div><div class="line"><span class="number">10</span>     B   <span class="number">60</span>    period      <span class="number">75</span></div><div class="line"><span class="number">11</span>     B   <span class="number">65</span>    period      <span class="number">60</span></div><div class="line"><span class="number">12</span>     B   <span class="number">65</span>    period      <span class="number">75</span></div><div class="line"><span class="number">13</span>     B   <span class="number">70</span>    period      <span class="number">60</span></div><div class="line">...........</div><div class="line">...........</div><div class="line"><span class="number">41</span>     A   <span class="number">60</span>    service    <span class="number">127</span></div><div class="line"><span class="number">42</span>     A   <span class="number">60</span>    service     <span class="number">63</span></div><div class="line"><span class="number">43</span>     A   <span class="number">65</span>    service   <span class="number">1095</span></div><div class="line">...........</div><div class="line">...........</div><div class="line"><span class="number">70</span>     D   <span class="number">70</span>    service   <span class="number">1208</span></div><div class="line"><span class="number">71</span>     D   <span class="number">75</span>    service      <span class="number">0</span></div><div class="line"><span class="number">72</span>     D   <span class="number">75</span>    service   <span class="number">2051</span></div><div class="line"><span class="number">73</span>     E   <span class="number">60</span>    service     <span class="number">45</span></div><div class="line"><span class="number">74</span>     E   <span class="number">60</span>    service      <span class="number">0</span></div><div class="line"><span class="number">75</span>     E   <span class="number">65</span>    service    <span class="number">789</span></div><div class="line">...........</div><div class="line">...........</div><div class="line"><span class="number">101</span>    C   <span class="number">70</span>    incidents    <span class="number">6</span></div><div class="line"><span class="number">102</span>    C   <span class="number">70</span>    incidents    <span class="number">2</span></div><div class="line"><span class="number">103</span>    C   <span class="number">75</span>    incidents    <span class="number">0</span></div><div class="line"><span class="number">104</span>    C   <span class="number">75</span>    incidents    <span class="number">1</span></div><div class="line"><span class="number">105</span>    D   <span class="number">60</span>    incidents    <span class="number">0</span></div><div class="line"><span class="number">106</span>    D   <span class="number">60</span>    incidents    <span class="number">0</span></div><div class="line">...........</div><div class="line">...........</div></pre></td></tr></table></figure></li></ul></li><li><p><code>cast()</code>重构数据</p><ul><li>每年每种类型的船的总和<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line">&gt; recasted.ship &lt;- cast(molten.ships, type+year~variable,sum)</div><div class="line">&gt; print(recasted.ship)</div><div class="line">   type year period service incidents</div><div class="line"><span class="number">1</span>     A   <span class="number">60</span>    <span class="number">135</span>     <span class="number">190</span>         <span class="number">0</span></div><div class="line"><span class="number">2</span>     A   <span class="number">65</span>    <span class="number">135</span>    <span class="number">2190</span>         <span class="number">7</span></div><div class="line"><span class="number">3</span>     A   <span class="number">70</span>    <span class="number">135</span>    <span class="number">4865</span>        <span class="number">24</span></div><div class="line"><span class="number">4</span>     A   <span class="number">75</span>    <span class="number">135</span>    <span class="number">2244</span>        <span class="number">11</span></div><div class="line"><span class="number">5</span>     B   <span class="number">60</span>    <span class="number">135</span>   <span class="number">62058</span>        <span class="number">68</span></div><div class="line"><span class="number">6</span>     B   <span class="number">65</span>    <span class="number">135</span>   <span class="number">48979</span>       <span class="number">111</span></div><div class="line"><span class="number">7</span>     B   <span class="number">70</span>    <span class="number">135</span>   <span class="number">20163</span>        <span class="number">56</span></div><div class="line"><span class="number">8</span>     B   <span class="number">75</span>    <span class="number">135</span>    <span class="number">7117</span>        <span class="number">18</span></div><div class="line"><span class="number">9</span>     C   <span class="number">60</span>    <span class="number">135</span>    <span class="number">1731</span>         <span class="number">2</span></div><div class="line"><span class="number">10</span>    C   <span class="number">65</span>    <span class="number">135</span>    <span class="number">1457</span>         <span class="number">1</span></div><div class="line"><span class="number">11</span>    C   <span class="number">70</span>    <span class="number">135</span>    <span class="number">2731</span>         <span class="number">8</span></div><div class="line"><span class="number">12</span>    C   <span class="number">75</span>    <span class="number">135</span>     <span class="number">274</span>         <span class="number">1</span></div><div class="line"><span class="number">13</span>    D   <span class="number">60</span>    <span class="number">135</span>     <span class="number">356</span>         <span class="number">0</span></div><div class="line"><span class="number">14</span>    D   <span class="number">65</span>    <span class="number">135</span>     <span class="number">480</span>         <span class="number">0</span></div><div class="line"><span class="number">15</span>    D   <span class="number">70</span>    <span class="number">135</span>    <span class="number">1557</span>        <span class="number">13</span></div><div class="line"><span class="number">16</span>    D   <span class="number">75</span>    <span class="number">135</span>    <span class="number">2051</span>         <span class="number">4</span></div><div class="line"><span class="number">17</span>    E   <span class="number">60</span>    <span class="number">135</span>      <span class="number">45</span>         <span class="number">0</span></div><div class="line"><span class="number">18</span>    E   <span class="number">65</span>    <span class="number">135</span>    <span class="number">1226</span>        <span class="number">14</span></div><div class="line"><span class="number">19</span>    E   <span class="number">70</span>    <span class="number">135</span>    <span class="number">3318</span>        <span class="number">17</span></div><div class="line"><span class="number">20</span>    E   <span class="number">75</span>    <span class="number">135</span>     <span class="number">542</span>         <span class="number">1</span></div></pre></td></tr></table></figure></li></ul></li></ul><h2 id="8、函数"><a href="#8、函数" class="headerlink" title="8、函数"></a>8、函数</h2><h3 id="1-定义"><a href="#1-定义" class="headerlink" title="(1) 定义"></a>(1) 定义</h3><figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">function_name &lt;- <span class="keyword">function</span>(arg_1, arg_2, <span class="keyword">...</span>) &#123;</div><div class="line">   Function body </div><div class="line">&#125;</div></pre></td></tr></table></figure><h3 id="2-自定义函数和调用"><a href="#2-自定义函数和调用" class="headerlink" title="(2) 自定义函数和调用"></a>(2) 自定义函数和调用</h3><figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">&gt; new.function &lt;- <span class="keyword">function</span>(a) &#123;</div><div class="line">+     <span class="keyword">for</span>(i <span class="keyword">in</span> <span class="number">1</span>:a) &#123;</div><div class="line">+         b &lt;- i^<span class="number">2</span></div><div class="line">+         print(b)</div><div class="line">+     &#125;</div><div class="line">+ &#125;</div><div class="line">&gt; new.function(<span class="number">3</span>)</div><div class="line">[<span class="number">1</span>] <span class="number">1</span></div><div class="line">[<span class="number">1</span>] <span class="number">4</span></div><div class="line">[<span class="number">1</span>] <span class="number">9</span></div></pre></td></tr></table></figure><h2 id="9、字符串"><a href="#9、字符串" class="headerlink" title="9、字符串"></a>9、字符串</h2><h3 id="1-定义-1"><a href="#1-定义-1" class="headerlink" title="(1) 定义"></a>(1) 定义</h3><ul><li>可以使用<strong>单引号或双引号</strong></li></ul><figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">&gt; s &lt;- <span class="string">'hello world!'</span></div><div class="line">&gt; print(s)</div><div class="line">[<span class="number">1</span>] <span class="string">"hello world!"</span></div></pre></td></tr></table></figure><h3 id="2-字符串操作"><a href="#2-字符串操作" class="headerlink" title="(2) 字符串操作"></a>(2) 字符串操作</h3><ul><li>连接字符串：<code>paste(..., sep = &quot; &quot;, collapse = NULL)</code><ul><li><code>...</code>表示要组合的任意数量的自变量。</li><li><code>sep</code>表示参数之间的任何<strong>分隔符</strong>。 它是可选的。</li><li><code>collapse</code>用于消除<strong>两个字符串之间的空格</strong>。<strong>但</strong>不是一个字符串的两个字内的空间。</li></ul></li><li>格式化字符串：<code>format(x, digits, nsmall, scientific, width, justify = c(&quot;left&quot;, &quot;right&quot;, &quot;centre&quot;, &quot;none&quot;))</code><ul><li><code>x</code>是向量输入。</li><li><code>digits</code>是显示的总位数。</li><li><code>nsmall</code>是小数点右边的最小位数。</li><li>科学设置为<code>TRUE</code>以显示科学记数法。</li><li><code>width</code>指示通过在开始处填充空白来显示的最小宽度。</li><li><code>justify</code>是字符串向左，右或中心的显示。</li></ul></li><li>字符数：<code>nchar()</code></li><li>大小写：<code>toupper()</code> 和 <code>tolower()</code></li><li>截取：<code>substring(x,first,last)</code></li></ul><h2 id="10、向量"><a href="#10、向量" class="headerlink" title="10、向量"></a>10、向量</h2><ul><li>向量是<strong>最基本的R语言数据对象</strong>，有六种类型的原子向量。 它们是逻辑，整数，双精度，复杂，字符和原始。<h3 id="1-序列运算符seq"><a href="#1-序列运算符seq" class="headerlink" title="(1) 序列运算符seq"></a>(1) 序列运算符<code>seq</code></h3></li></ul><figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">&gt; print(seq(<span class="number">1</span>, <span class="number">10</span>, by=<span class="number">2</span>))</div><div class="line">[<span class="number">1</span>] <span class="number">1</span> <span class="number">3</span> <span class="number">5</span> <span class="number">7</span> <span class="number">9</span></div></pre></td></tr></table></figure><h3 id="2-访问向量元素"><a href="#2-访问向量元素" class="headerlink" title="(2) 访问向量元素"></a>(2) 访问向量元素</h3><ul><li>使用索引访问向量的元素。 <code>[]</code>括号用于建立索引。 索引从位置<code>1</code>开始。</li><li><p>位置索引</p><ul><li>注意位置是从<code>1</code>开始的<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">t &lt;- c(<span class="string">"Sun"</span>,<span class="string">"Mon"</span>,<span class="string">"Tue"</span>,<span class="string">"Wed"</span>,<span class="string">"Thurs"</span>,<span class="string">"Fri"</span>,<span class="string">"Sat"</span>)</div><div class="line">u &lt;- t[c(<span class="number">2</span>,<span class="number">3</span>,<span class="number">6</span>)]</div><div class="line">print(u)</div><div class="line">[<span class="number">1</span>] <span class="string">"Mon"</span> <span class="string">"Tue"</span> <span class="string">"Fri"</span></div></pre></td></tr></table></figure></li></ul></li><li><p>负数索引</p><ul><li>使用负数是<strong>丢弃</strong>掉对应正数的索引<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">x &lt;- t[c(-<span class="number">2</span>,-<span class="number">5</span>)]</div><div class="line">print(x)</div><div class="line">[<span class="number">1</span>] <span class="string">"Sun"</span> <span class="string">"Tue"</span> <span class="string">"Wed"</span> <span class="string">"Fri"</span> <span class="string">"Sat"</span></div></pre></td></tr></table></figure></li></ul></li><li><p>排序</p></li></ul><figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">&gt; v &lt;- c(<span class="number">3</span>,<span class="number">8</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">0</span>,<span class="number">11</span>, -<span class="number">9</span>, <span class="number">304</span>)</div><div class="line">&gt; v.result &lt;- sort(v)</div><div class="line">&gt; print(v.result)</div><div class="line">[<span class="number">1</span>]  -<span class="number">9</span>   <span class="number">0</span>   <span class="number">3</span>   <span class="number">4</span>   <span class="number">5</span>   <span class="number">8</span>  <span class="number">11</span> <span class="number">304</span></div></pre></td></tr></table></figure><h2 id="11、列表"><a href="#11、列表" class="headerlink" title="11、列表"></a>11、列表</h2><ul><li><p>可以通过索引和名字访问</p><figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line">&gt; list_data &lt;- list(c(<span class="string">"Jan"</span>,<span class="string">"Feb"</span>,<span class="string">"Mar"</span>), matrix(c(<span class="number">3</span>,<span class="number">9</span>,<span class="number">5</span>,<span class="number">1</span>,-<span class="number">2</span>,<span class="number">8</span>), nrow = <span class="number">2</span>),</div><div class="line">+                   list(<span class="string">"green"</span>,<span class="number">12.3</span>))</div><div class="line">&gt; print(list_data)</div><div class="line">[[<span class="number">1</span>]]</div><div class="line">[<span class="number">1</span>] <span class="string">"Jan"</span> <span class="string">"Feb"</span> <span class="string">"Mar"</span></div><div class="line"></div><div class="line">[[<span class="number">2</span>]]</div><div class="line">     [,<span class="number">1</span>] [,<span class="number">2</span>] [,<span class="number">3</span>]</div><div class="line">[<span class="number">1</span>,]    <span class="number">3</span>    <span class="number">5</span>   -<span class="number">2</span></div><div class="line">[<span class="number">2</span>,]    <span class="number">9</span>    <span class="number">1</span>    <span class="number">8</span></div><div class="line"></div><div class="line">[[<span class="number">3</span>]]</div><div class="line">[[<span class="number">3</span>]][[<span class="number">1</span>]]</div><div class="line">[<span class="number">1</span>] <span class="string">"green"</span></div><div class="line"></div><div class="line">[[<span class="number">3</span>]][[<span class="number">2</span>]]</div><div class="line">[<span class="number">1</span>] <span class="number">12.3</span></div><div class="line"></div><div class="line"></div><div class="line">&gt; names(list_data) &lt;- c(<span class="string">"1st Quarter"</span>, <span class="string">"A_Matrix"</span>, <span class="string">"A Inner list"</span>)</div><div class="line">&gt; print(list_data[<span class="number">1</span>])</div><div class="line">$`1st Quarter`</div><div class="line">[<span class="number">1</span>] <span class="string">"Jan"</span> <span class="string">"Feb"</span> <span class="string">"Mar"</span></div><div class="line"></div><div class="line">&gt; print(list_data$A_Matrix)</div><div class="line">     [,<span class="number">1</span>] [,<span class="number">2</span>] [,<span class="number">3</span>]</div><div class="line">[<span class="number">1</span>,]    <span class="number">3</span>    <span class="number">5</span>   -<span class="number">2</span></div><div class="line">[<span class="number">2</span>,]    <span class="number">9</span>    <span class="number">1</span>    <span class="number">8</span></div></pre></td></tr></table></figure></li><li><p>合并列表：<code>merged.list &lt;- c(list1,list2)</code></p></li><li>列表转向量：<code>v1 &lt;- unlist(list1)</code></li></ul><h2 id="12、矩阵"><a href="#12、矩阵" class="headerlink" title="12、矩阵"></a>12、矩阵</h2><ul><li>矩阵是其中元素以<strong>二维矩形</strong>布局布置的R对象。</li><li>包含相同原子类型的元素。</li><li><code>matrix(data, nrow, ncol, byrow, dimnames)</code></li><li>data数据是成为矩阵的数据元素的输入向量。</li><li>nrow是要创建的行数。</li><li>ncol是要创建的列数。</li><li>byrow是一个逻辑线索。如果为TRUE，则输入向量元素按行排列。</li><li>dimname是分配给行和列的名称。</li></ul><h2 id="13、数组"><a href="#13、数组" class="headerlink" title="13、数组"></a>13、数组</h2><h2 id="14、因子"><a href="#14、因子" class="headerlink" title="14、因子"></a>14、因子</h2><ul><li>因子是用于对数据进行分类并将其存储为级别的数据对象。 它们可以存储字符串和整数。 </li><li>它们在具有有限数量的唯一值的列中很有用。像“男性”，“女性”和True，False等。它们在统计建模的数据分析中很有用。</li></ul><figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">&gt; height &lt;- c(<span class="number">132</span>,<span class="number">151</span>,<span class="number">162</span>,<span class="number">139</span>,<span class="number">166</span>,<span class="number">147</span>,<span class="number">122</span>)</div><div class="line">&gt; weight &lt;- c(<span class="number">48</span>,<span class="number">49</span>,<span class="number">66</span>,<span class="number">53</span>,<span class="number">67</span>,<span class="number">52</span>,<span class="number">40</span>)</div><div class="line">&gt; gender &lt;- c(<span class="string">"male"</span>,<span class="string">"male"</span>,<span class="string">"female"</span>,<span class="string">"female"</span>,<span class="string">"male"</span>,<span class="string">"female"</span>,<span class="string">"male"</span>)</div><div class="line">&gt; </div><div class="line">&gt; input_data &lt;- data.frame(height,weight,gender)</div><div class="line">&gt; print(input_data)</div><div class="line">  height weight gender</div><div class="line"><span class="number">1</span>    <span class="number">132</span>     <span class="number">48</span>   male</div><div class="line"><span class="number">2</span>    <span class="number">151</span>     <span class="number">49</span>   male</div><div class="line"><span class="number">3</span>    <span class="number">162</span>     <span class="number">66</span> female</div><div class="line"><span class="number">4</span>    <span class="number">139</span>     <span class="number">53</span> female</div><div class="line"><span class="number">5</span>    <span class="number">166</span>     <span class="number">67</span>   male</div><div class="line"><span class="number">6</span>    <span class="number">147</span>     <span class="number">52</span> female</div><div class="line"><span class="number">7</span>    <span class="number">122</span>     <span class="number">40</span>   male</div><div class="line">&gt; print(input_data$height)</div><div class="line">[<span class="number">1</span>] <span class="number">132</span> <span class="number">151</span> <span class="number">162</span> <span class="number">139</span> <span class="number">166</span> <span class="number">147</span> <span class="number">122</span></div></pre></td></tr></table></figure><h2 id="15、数据帧"><a href="#15、数据帧" class="headerlink" title="15、数据帧"></a>15、数据帧</h2><ul><li><code>summary()</code></li></ul><figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line">&gt; emp.data &lt;- data.frame(</div><div class="line">+     emp_id = c (<span class="number">1</span>:<span class="number">5</span>), </div><div class="line">+     emp_name = c(<span class="string">"Rick"</span>,<span class="string">"Dan"</span>,<span class="string">"Michelle"</span>,<span class="string">"Ryan"</span>,<span class="string">"Gary"</span>),</div><div class="line">+     salary = c(<span class="number">623.3</span>,<span class="number">515.2</span>,<span class="number">611.0</span>,<span class="number">729.0</span>,<span class="number">843.25</span>), </div><div class="line">+     </div><div class="line">+     start_date = as.Date(c(<span class="string">"2012-01-01"</span>, <span class="string">"2013-09-23"</span>, <span class="string">"2014-11-15"</span>, <span class="string">"2014-05-11"</span>,</div><div class="line">+                            <span class="string">"2015-03-27"</span>)),</div><div class="line">+     stringsAsFactors = <span class="literal">FALSE</span></div><div class="line">+ )</div><div class="line">&gt; <span class="comment"># Print the summary.</span></div><div class="line">&gt; print(summary(emp.data)) </div><div class="line">     emp_id    emp_name             salary        start_date        </div><div class="line"> Min.   :<span class="number">1</span>   Length:<span class="number">5</span>           Min.   :<span class="number">515.2</span>   Min.   :<span class="number">2012</span>-<span class="number">01</span>-<span class="number">01</span>  </div><div class="line"> 1st Qu.:<span class="number">2</span>   Class :character   1st Qu.:<span class="number">611.0</span>   1st Qu.:<span class="number">2013</span>-<span class="number">09</span>-<span class="number">23</span>  </div><div class="line"> Median :<span class="number">3</span>   Mode  :character   Median :<span class="number">623.3</span>   Median :<span class="number">2014</span>-<span class="number">05</span>-<span class="number">11</span>  </div><div class="line"> Mean   :<span class="number">3</span>                      Mean   :<span class="number">664.4</span>   Mean   :<span class="number">2014</span>-<span class="number">01</span>-<span class="number">14</span>  </div><div class="line"> 3rd Qu.:<span class="number">4</span>                      3rd Qu.:<span class="number">729.0</span>   3rd Qu.:<span class="number">2014</span>-<span class="number">11</span>-<span class="number">15</span>  </div><div class="line"> Max.   :<span class="number">5</span>                      Max.   :<span class="number">843.2</span>   Max.   :<span class="number">2015</span>-<span class="number">03</span>-<span class="number">27</span>  </div><div class="line">&gt; print(emp.data)</div><div class="line">  emp_id emp_name salary start_date</div><div class="line"><span class="number">1</span>      <span class="number">1</span>     Rick <span class="number">623.30</span> <span class="number">2012</span>-<span class="number">01</span>-<span class="number">01</span></div><div class="line"><span class="number">2</span>      <span class="number">2</span>      Dan <span class="number">515.20</span> <span class="number">2013</span>-<span class="number">09</span>-<span class="number">23</span></div><div class="line"><span class="number">3</span>      <span class="number">3</span> Michelle <span class="number">611.00</span> <span class="number">2014</span>-<span class="number">11</span>-<span class="number">15</span></div><div class="line"><span class="number">4</span>      <span class="number">4</span>     Ryan <span class="number">729.00</span> <span class="number">2014</span>-<span class="number">05</span>-<span class="number">11</span></div><div class="line"><span class="number">5</span>      <span class="number">5</span>     Gary <span class="number">843.25</span> <span class="number">2015</span>-<span class="number">03</span>-<span class="number">27</span></div></pre></td></tr></table></figure><ul><li>取数据前两行<code>result &lt;- emp.data[1:2,]</code></li><li><code>result &lt;- emp.data[c(3,5),c(2,4)]</code></li><li>扩展数据<code>emp.data$dept &lt;- c(&quot;IT&quot;,&quot;Operations&quot;,&quot;IT&quot;,&quot;HR&quot;,&quot;Finance&quot;)</code></li><li>添加行：<code>emp.finaldata &lt;- rbind(emp.data,emp.newdata)</code></li></ul><h1 id="二、dplyr包"><a href="#二、dplyr包" class="headerlink" title="二、dplyr包"></a>二、<code>dplyr</code>包</h1><h2 id="1、基本操作"><a href="#1、基本操作" class="headerlink" title="1、基本操作"></a>1、基本操作</h2><ul><li>安装：<code>install.packages(&quot;dplyr&quot;)</code><h3 id="1-筛选"><a href="#1-筛选" class="headerlink" title="(1) 筛选"></a>(1) 筛选</h3></li><li><p>筛选行：<code>filter()</code></p><ul><li>前面是<code>dataframe</code>数据，后面是筛选的条件，这里是筛选列名为<code>eval_set</code>的数据</li><li>也可以使用<strong>管道</strong>的方式：datafame %&gt;% filter(eval_set == “prior”)<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">filter(dataframe, eval_set == <span class="string">"prior"</span>)</div></pre></td></tr></table></figure></li></ul></li><li><p>筛选列：<code>select()</code></p><ul><li>筛选列名为<code>user_id, product_id</code>的列，使用<strong>负号</strong>表示<strong>去除</strong>对应的列</li><li>可以使用<strong>管道</strong>的方式：<code>ordert %&gt;% select(user_id, product_id, reordered)</code><figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">select(ordert, user_id, product_id)</div></pre></td></tr></table></figure></li></ul></li></ul><h3 id="2-排序arrange"><a href="#2-排序arrange" class="headerlink" title="(2) 排序arrange()"></a>(2) 排序<code>arrange()</code></h3><ul><li><p>按给定的列名依次进行排序（默认升序）</p><ul><li><strong>管道</strong>的方式：<code>orders_products %&gt;% arrange(user_id, order_number, product_id)</code><figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">arrange(df, user_id, order_number, product_id)</div></pre></td></tr></table></figure></li></ul></li><li><p>降序排列</p></li></ul><figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">arrange(df, desc(ArrDelay))</div></pre></td></tr></table></figure><h3 id="3-变形mutate"><a href="#3-变形mutate" class="headerlink" title="(3) 变形mutate()"></a>(3) 变形<code>mutate()</code></h3><ul><li>对已有列进行数据运算并添加为新列</li></ul><figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">mutate(hflights_df, </div><div class="line">  gain = ArrDelay - DepDelay, </div><div class="line">  speed = Distance / AirTime * <span class="number">60</span>)</div></pre></td></tr></table></figure><h3 id="4-汇总"><a href="#4-汇总" class="headerlink" title="(4) 汇总"></a>(4) 汇总</h3><figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">prd &lt;- orders_products %&gt;%</div><div class="line">    arrange(user_id, order_number, product_id) %&gt;%</div><div class="line">    group_by(user_id, product_id) %&gt;%</div><div class="line">    mutate(product_time = row_number()) %&gt;%</div><div class="line">    ungroup() %&gt;%</div><div class="line">    group_by(product_id) %&gt;%</div><div class="line">    summarise(</div><div class="line">        prod_orders = n(),</div><div class="line">        prod_reorders = sum(reordered),</div><div class="line">        prod_first_orders = sum(product_time == <span class="number">1</span>),</div><div class="line">        prod_second_orders = sum(product_time == <span class="number">2</span>)</div><div class="line">    )</div></pre></td></tr></table></figure><h2 id="2、分组"><a href="#2、分组" class="headerlink" title="2、分组"></a>2、分组</h2><ul><li>当对数据集通过 <code>group_by()</code> 添加了分组信息后,<code>mutate(), arrange() 和 summarise()</code> 函数会自动对这些 <code>tbl</code> 类数据执行分组操作 (R语言泛型函数的优势).</li></ul><figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">prd &lt;- orders_products %&gt;%</div><div class="line">    arrange(user_id, order_number, product_id) %&gt;%</div><div class="line">    group_by(user_id, product_id) %&gt;%</div><div class="line">    mutate(product_time = row_number()) %&gt;%</div><div class="line">    ungroup() %&gt;%</div><div class="line">    group_by(product_id) %&gt;%</div><div class="line">    summarise(</div><div class="line">        prod_orders = n(),</div><div class="line">        prod_reorders = sum(reordered),</div><div class="line">        prod_first_orders = sum(product_time == <span class="number">1</span>),</div><div class="line">        prod_second_orders = sum(product_time == <span class="number">2</span>)</div><div class="line">    )</div></pre></td></tr></table></figure><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul><li><a href="https://w3cschool.cn/r/" target="_blank" rel="external">https://w3cschool.cn/r/</a></li><li><a href="https://cran.rstudio.com/web/packages/dplyr/dplyr.pdf" target="_blank" rel="external">https://cran.rstudio.com/web/packages/dplyr/dplyr.pdf</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;ul&gt;
&lt;li&gt;基础内容来自&lt;code&gt;W3C&lt;/code&gt;，直接&lt;a href=&quot;https://w3cschool.cn/r/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;查看这里&lt;/a&gt;即可，这里只是个人学习的记录&lt;/li&gt;
&lt;li&gt;只是最近感觉有必要学习一下&lt;code&gt;R&lt;/code&gt;，哈哈&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&quot;一、基础&quot;&gt;&lt;a href=&quot;#一、基础&quot; class=&quot;headerlink&quot; title=&quot;一、基础&quot;&gt;&lt;/a&gt;一、基础&lt;/h1&gt;&lt;h2 id=&quot;1、概述&quot;&gt;&lt;a href=&quot;#1、概述&quot; class=&quot;headerlink&quot; title=&quot;1、概述&quot;&gt;&lt;/a&gt;1、概述&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;R语言是用于统计分析，图形表示和报告的编程语言和软件环境。&lt;/li&gt;
&lt;li&gt;解释型语言&lt;h2 id=&quot;2、Windows上安装&quot;&gt;&lt;a href=&quot;#2、Windows上安装&quot; class=&quot;headerlink&quot; title=&quot;2、Windows上安装&quot;&gt;&lt;/a&gt;2、&lt;code&gt;Windows&lt;/code&gt;上安装&lt;/h2&gt;&lt;/li&gt;
&lt;li&gt;下载地址：&lt;a href=&quot;https://cran.r-project.org/bin/windows/base/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;R-3.4.0&lt;/a&gt;，直接下载安装即可&lt;/li&gt;
&lt;li&gt;&lt;code&gt;IDE&lt;/code&gt;使用&lt;code&gt;RStudio&lt;/code&gt;: &lt;a href=&quot;https://www.rstudio.com/products/rstudio/download/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;点击下载&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="R" scheme="http://lawlite.cn/tags/R/"/>
    
  </entry>
  
  <entry>
    <title>Tensorflow学习-工具相关</title>
    <link href="http://lawlite.cn/2017/06/24/Tensorflow%E5%AD%A6%E4%B9%A0-%E5%B7%A5%E5%85%B7%E7%9B%B8%E5%85%B3/"/>
    <id>http://lawlite.cn/2017/06/24/Tensorflow学习-工具相关/</id>
    <published>2017-06-24T11:30:05.000Z</published>
    <updated>2018-10-29T08:34:24.602Z</updated>
    
    <content type="html"><![CDATA[<ul><li><code>Tensorflow</code>版本(<code># 2017-06-24</code>)：<code>1.2.0</code>  </li><li><code>Python</code>版本：<code>3.5.3</code></li><li>包括：<ul><li><code>Tensorboard</code><strong>可视化</strong></li><li><code>tfdbg</code><strong>调试</strong></li><li>常用的<strong>高级函数</strong></li></ul></li></ul><a id="more"></a><h1 id="一、TensorBoard-可视化"><a href="#一、TensorBoard-可视化" class="headerlink" title="一、TensorBoard 可视化"></a>一、<code>TensorBoard</code> 可视化</h1><h2 id="1、可视化计算图"><a href="#1、可视化计算图" class="headerlink" title="1、可视化计算图"></a>1、可视化计算图</h2><ul><li>全部代码：<a href="https://github.com/lawlite19/Blog-Back-Up/blob/master/code/tensorflow-tools/tensorflow_graph.py" target="_blank" rel="external">点击查看</a></li><li>数据集使用<code>MNIST</code>手写数字</li><li>加载数据</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="string">'''加载数据'''</span></div><div class="line">data = input_data.read_data_sets(<span class="string">'MNIST_data'</span>, one_hot=<span class="keyword">True</span>)</div><div class="line">print(<span class="string">"Size of:"</span>)</div><div class="line">print(<span class="string">"\t\t training set:\t\t&#123;&#125;"</span>.format(len(data.train.labels)))</div><div class="line">print(<span class="string">"\t\t test set: \t\t\t&#123;&#125;"</span>.format(len(data.test.labels)))</div><div class="line">print(<span class="string">"\t\t validation set:\t&#123;&#125;"</span>.format(len(data.validation.labels)))</div></pre></td></tr></table></figure><h3 id="1-全连接网络"><a href="#1-全连接网络" class="headerlink" title="(1) 全连接网络"></a>(1) 全连接网络</h3><ul><li>超参数</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="string">'''超参数'''</span></div><div class="line">img_size = <span class="number">28</span></div><div class="line">img_flatten_size = img_size ** <span class="number">2</span></div><div class="line">img_shape = (img_size, img_size)</div><div class="line">num_classes = <span class="number">10</span></div><div class="line">learning_rate = <span class="number">1e-4</span></div></pre></td></tr></table></figure><ul><li><p>定义添加一层的函数</p><ul><li><code>num_layer</code>指定是第几层</li><li><code>activation</code>指定激励函数，若不指定跳过<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="string">'''定义添加一层'''</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">add_fully_layer</span><span class="params">(inputs, input_size, output_size, num_layer, activation=None)</span>:</span></div><div class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">'layer_'</span>+num_layer):</div><div class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">'Weights'</span>):</div><div class="line">            W = tf.Variable(initial_value=tf.random_normal(shape=[input_size, output_size]), name=<span class="string">'W'</span>)</div><div class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">'biases'</span>):</div><div class="line">            b = tf.Variable(initial_value=tf.zeros(shape=[<span class="number">1</span>, output_size]) + <span class="number">0.1</span>, name=<span class="string">'b'</span>)</div><div class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">'Wx_plus_b'</span>):</div><div class="line">            Wx_plus_b = tf.matmul(inputs, W) + b</div><div class="line">        <span class="keyword">if</span> activation <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</div><div class="line">            outputs = activation(Wx_plus_b)</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            outputs = Wx_plus_b</div><div class="line">        <span class="keyword">return</span> outputs</div></pre></td></tr></table></figure></li></ul></li><li><p>定义输入，计算图结构，loss和优化器</p></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="string">'''placehoder'''</span></div><div class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'inputs'</span>):</div><div class="line">    x = tf.placeholder(tf.float32, shape=[<span class="keyword">None</span>, img_flatten_size], name=<span class="string">'x'</span>)</div><div class="line">    y = tf.placeholder(tf.float32, shape=[<span class="keyword">None</span>, num_classes], name=<span class="string">'y'</span>)</div><div class="line"></div><div class="line"><span class="string">'''结构'''</span></div><div class="line">hidden_layer1 = add_fully_layer(x, img_flatten_size, <span class="number">20</span>, <span class="string">'1'</span>, activation=tf.nn.relu)</div><div class="line">logits = add_fully_layer(hidden_layer1, <span class="number">20</span>, num_classes, <span class="string">'2'</span>)</div><div class="line">predictions = tf.nn.softmax(logits)</div><div class="line"></div><div class="line"><span class="string">'''loss'''</span></div><div class="line"><span class="comment">#cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=y,logits=logits)</span></div><div class="line">cross_entropy = -tf.reduce_sum(y*tf.log(predictions), reduction_indices=[<span class="number">1</span>])</div><div class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'losses'</span>):</div><div class="line">    losses = tf.reduce_mean(cross_entropy)</div><div class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'train'</span>):</div><div class="line">    train_step = tf.train.AdamOptimizer(learning_rate).minimize(losses)</div></pre></td></tr></table></figure><ul><li>定义<code>Session</code>和<code>tf.summary.FileWriter</code></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="string">'''session'''</span></div><div class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</div><div class="line">    sess.run(tf.global_variables_initializer())</div><div class="line">    writer = tf.summary.FileWriter(<span class="string">'logs'</span>, sess.graph)  <span class="comment"># 将计算图写入文件</span></div></pre></td></tr></table></figure><ul><li><p>最后在<code>logs</code>的上级目录打开命令行输入：<code>tensorboard --logdir=logs/</code>，浏览器中输入网址：<code>http://localhost:6006</code>即可查看</p></li><li><p>结果</p><ul><li>自定义的<code>cross_entropy = -tf.reduce_sum(y*tf.log(predictions), reduction_indices=[1])</code><br><img src="/assets/blog_images/Tensorflow-tool/01_fully_connected_graph.png" alt="全连接网络结构，cross_entropy自定义" title="01_fully_connected_graph"></li><li>使用<code>tensorflow</code>中自带的<code>cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=y,logits=predictions)</code><br><img src="/assets/blog_images/Tensorflow-tool/02_fully_connected_graph.png" alt="全连接网络结构，自带的cross_entropy" title="02_fully_connected_graph"></li></ul></li><li>可以看出<code>tf.name_scope</code>定义的名字就是其中的方框，点击里面的可以查看里面对应的内容</li></ul><h3 id="2-CNN卷积神经网络"><a href="#2-CNN卷积神经网络" class="headerlink" title="(2) CNN卷积神经网络"></a>(2) <code>CNN</code>卷积神经网络</h3><ul><li><p>添加一层卷积层和<code>pooling</code>层</p><ul><li>这里默认<code>pooling</code>使用<code>maxpooling</code>, 大小为<code>2</code><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="string">'''CNN 定义添加一层卷积层，包括pooling(使用maxpooling, size=2)'''</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">add_conv_layer</span><span class="params">(inputs, filter_size, input_channels, output_channels, num_layer, activation=tf.nn.relu)</span>:</span></div><div class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">'conv_layer_'</span>+num_layer):</div><div class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">'Weights'</span>):</div><div class="line">            Weights = tf.Variable(tf.truncated_normal(stddev=<span class="number">0.1</span>, shape=[filter_size, filter_size, input_channels, output_channels]), name=<span class="string">'W'</span>)</div><div class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">'biases'</span>):</div><div class="line">            b = tf.Variable(tf.constant(<span class="number">0.1</span>, shape=[output_channels]))</div><div class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">'conv2d'</span>):</div><div class="line">            conv2d_plus_b = tf.nn.conv2d(inputs, Weights, strides=[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>], padding=<span class="string">'SAME'</span>, name=<span class="string">'conv'</span>) + b</div><div class="line">            activation_conv_outputs = activation(conv2d_plus_b)</div><div class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">'max_pool'</span>):</div><div class="line">            max_pool_outputs = tf.nn.max_pool(activation_conv_outputs, ksize=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>], strides=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>], padding=<span class="string">'SAME'</span>)</div><div class="line">        <span class="keyword">return</span> max_pool_outputs</div></pre></td></tr></table></figure></li></ul></li><li><p>将卷积层展开</p><ul><li>返回展开层和数量（因为全连接会用到）<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="string">'''将卷积层展开'''</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">flatten_layer</span><span class="params">(layer)</span>:</span></div><div class="line">    <span class="string">'''</span></div><div class="line">    @param layer: the conv layer</div><div class="line">    '''</div><div class="line">    layer_shape = layer.get_shape() <span class="comment"># 获取形状(layer_shape == [num_images, img_height, img_width, num_channels])</span></div><div class="line">    num_features = layer_shape[<span class="number">1</span>:<span class="number">4</span>].num_elements()  <span class="comment"># [1:4] 是最后3个维度，就是展开的长度</span></div><div class="line">    layer_flat = tf.reshape(layer, [<span class="number">-1</span>, num_features])   <span class="comment"># 展开</span></div><div class="line">    <span class="keyword">return</span> layer_flat, num_features</div></pre></td></tr></table></figure></li></ul></li><li><p>定义输入</p><ul><li>需要将<code>x</code>转成图片矩阵的形式<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="string">'''placehoder'''</span></div><div class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'inputs'</span>):</div><div class="line">    x = tf.placeholder(tf.float32, shape=[<span class="keyword">None</span>, img_flatten_size], name=<span class="string">'x'</span>)</div><div class="line">    y = tf.placeholder(tf.float32, shape=[<span class="keyword">None</span>, num_classes], name=<span class="string">'y'</span>)</div><div class="line">    x_image = tf.reshape(x, shape = [<span class="number">-1</span>, img_size, img_size, n_channels], name=<span class="string">'x_images'</span>)</div></pre></td></tr></table></figure></li></ul></li><li><p>定义计算图结构</p></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="string">'''CNN卷积网络结构'''</span></div><div class="line">conv_layer1 = add_conv_layer(x_image, filter_size=<span class="number">5</span>, input_channels=<span class="number">1</span>, </div><div class="line">                            output_channels=<span class="number">32</span>, </div><div class="line">                            num_layer=<span class="string">'1'</span>)</div><div class="line">conv_layer2 = add_conv_layer(conv_layer1, filter_size=<span class="number">5</span>, input_channels=<span class="number">32</span>, </div><div class="line">                            output_channels=<span class="number">64</span>, </div><div class="line">                            num_layer=<span class="string">'2'</span>)</div><div class="line"></div><div class="line"><span class="string">'''全连接层'''</span></div><div class="line">conv_layer2_flat, num_features = flatten_layer(conv_layer2)   <span class="comment"># 将最后操作的数据展开</span></div><div class="line">hidden_layer1 = add_fully_layer(conv_layer2_flat, num_features, <span class="number">1000</span>, num_layer=<span class="string">'1'</span>, activation=tf.nn.relu)</div><div class="line">logits = add_fully_layer(hidden_layer1, <span class="number">1000</span>, num_classes, num_layer=<span class="string">'2'</span>)</div><div class="line">predictions = tf.nn.softmax(logits)</div></pre></td></tr></table></figure><ul><li>结果<ul><li><code>CNN</code>总结构<br><img src="/assets/blog_images/Tensorflow-tool/03_conv_graph.png" alt="CNN结构" title="03_conv_graph"></li><li>第一层卷积和<code>pooling</code>内部结构<br><img src="/assets/blog_images/Tensorflow-tool/04_conv_graph.png" alt="卷积的内容结构" title="04_conv_graph"></li></ul></li></ul><h3 id="3-RNN-LSTM循环神经网络"><a href="#3-RNN-LSTM循环神经网络" class="headerlink" title="(3) RNN_LSTM循环神经网络"></a>(3) <code>RNN_LSTM</code>循环神经网络</h3><ul><li><p>声明<code>placeholder</code></p><ul><li>图片中每一行当做当前的输入，共有<code>n_steps=28</code>步遍历完一张图片，所以输入<code>x</code>的<code>shape=(batch_size, n_steps, n_inputs)</code></li><li><code>n_inputs</code>就是<strong>一行</strong>的像素值<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="string">'''placehoder'''</span></div><div class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'inputs'</span>):</div><div class="line">    <span class="string">'''RNN'''</span></div><div class="line">    x = tf.placeholder(tf.float32, shape=[batch_size, n_steps, n_inputs], name=<span class="string">'x'</span>)</div><div class="line">    y = tf.placeholder(tf.float32, shape=[batch_size, num_classes], name=<span class="string">'y'</span>)</div></pre></td></tr></table></figure></li></ul></li><li><p>添加一层<code>cell</code></p><ul><li>我们最后只需要遍历<code>n_steps</code>之后的输出即可（<strong>遍历完一张图然后分类</strong>），所以对应的是<code>final_state[1]</code>（有两个<code>state</code>, 一个是<code>c state</code>,一个是<code>h state</code>， 输出是<code>h state</code>）</li><li><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="string">'''RNN 添加一层cell'''</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">add_RNN_Cell</span><span class="params">(inputs)</span>:</span></div><div class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">'RNN_LSTM_Cell'</span>):</div><div class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">'weights'</span>):</div><div class="line">            weights = tf.Variable(tf.random_normal(shape=[state_size, num_classes]), name=<span class="string">'W'</span>)</div><div class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">'biases'</span>):</div><div class="line">            biases = tf.Variable(tf.constant(<span class="number">0.1</span>, shape=[num_classes,]), name=<span class="string">'b'</span>)</div><div class="line">        cell = tf.nn.rnn_cell.BasicLSTMCell(num_units=state_size)</div><div class="line">        init_state = cell.zero_state(batch_size, dtype=tf.float32)</div><div class="line">        rnn_outputs, final_state = tf.nn.dynamic_rnn(cell=cell, inputs=x, </div><div class="line">                                                     initial_state=init_state)</div><div class="line">        logits = tf.matmul(final_state[<span class="number">1</span>], weights) + biases</div><div class="line">        <span class="keyword">return</span> logits</div></pre></td></tr></table></figure></li></ul></li><li><p>网络结果和<code>loss</code></p></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="string">'''RNN网络结构'''</span></div><div class="line">logits = add_RNN_Cell(inputs=x)</div><div class="line"></div><div class="line">predictions = tf.nn.softmax(logits)    </div><div class="line"><span class="string">'''loss'''</span></div><div class="line"><span class="comment">#cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=y,logits=predictions)</span></div><div class="line">cross_entropy = -tf.reduce_sum(y*tf.log(predictions), reduction_indices=[<span class="number">1</span>])</div><div class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'losses'</span>):</div><div class="line">    losses = tf.reduce_mean(cross_entropy)</div></pre></td></tr></table></figure><ul><li>结果<ul><li>总体结构<br><img src="/assets/blog_images/Tensorflow-tool/05_rnn_graph.png" alt="RNN LSTM结构" title="05_rnn_graph"></li><li><code>RNN</code>内部结构<br><img src="/assets/blog_images/Tensorflow-tool/06_rnn_graph.png" alt="内部结构" title="06_rnn_graph"></li></ul></li></ul><h2 id="2、可视化训练过程"><a href="#2、可视化训练过程" class="headerlink" title="2、可视化训练过程"></a>2、可视化训练过程</h2><ul><li>全部代码：<a href="https://github.com/lawlite19/Blog-Back-Up/blob/master/code/tensorflow-tools/tensorflow_train_process.py" target="_blank" rel="external">点击查看</a><h3 id="1-权重weights，偏置biases，损失值loss"><a href="#1-权重weights，偏置biases，损失值loss" class="headerlink" title="(1) 权重weights，偏置biases，损失值loss"></a>(1) 权重<code>weights</code>，偏置<code>biases</code>，损失值<code>loss</code></h3></li><li><p>加入一层全连接层的函数变成这样</p><ul><li>加入<code>tf.summary.histogram(name=layer_name+&#39;/Weights&#39;, values=W)</code>即可<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line"><span class="string">'''定义添加一层全连接层'''</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">add_fully_layer</span><span class="params">(inputs, input_size, output_size, num_layer, activation=None)</span>:</span></div><div class="line">    layer_name = <span class="string">'layer_'</span> + num_layer</div><div class="line">    <span class="keyword">with</span> tf.name_scope(layer_name):</div><div class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">'Weights'</span>):</div><div class="line">            low = <span class="number">-4</span>*np.sqrt(<span class="number">6.0</span>/(input_size + output_size)) <span class="comment"># use 4 for sigmoid, 1 for tanh activation </span></div><div class="line">            high = <span class="number">4</span>*np.sqrt(<span class="number">6.0</span>/(input_size + output_size))</div><div class="line">            <span class="comment">#'''xavier方法初始化'''</span></div><div class="line">            <span class="comment">##sigmoid</span></div><div class="line">            <span class="comment">#Weights = tf.Variable(tf.random_uniform(shape=[input_size, output_size], minval=low, maxval=high, dtype=tf.float32), name='W')</span></div><div class="line">            <span class="comment">##relu</span></div><div class="line">            W = tf.Variable(initial_value=tf.random_uniform(shape=[input_size, output_size], minval=low, maxval=high, dtype=tf.float32)/<span class="number">2</span>, name=<span class="string">'W'</span>)</div><div class="line">            tf.summary.histogram(name=layer_name+<span class="string">'/Weights'</span>, values=W)  <span class="comment"># summary.histogram</span></div><div class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">'biases'</span>):</div><div class="line">            b = tf.Variable(initial_value=tf.zeros(shape=[<span class="number">1</span>, output_size]) + <span class="number">0.1</span>, name=<span class="string">'b'</span>)</div><div class="line">            tf.summary.histogram(name=layer_name+<span class="string">'/biases'</span>, values=b)    <span class="comment"># summary.histogram</span></div><div class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">'Wx_plus_b'</span>):</div><div class="line">            Wx_plus_b = tf.matmul(inputs, W) + b</div><div class="line">        <span class="keyword">if</span> activation <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</div><div class="line">            outputs = activation(Wx_plus_b)</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            outputs = Wx_plus_b</div><div class="line">        tf.summary.histogram(name=layer_name+<span class="string">'/outputs'</span>, values=outputs)   <span class="comment"># summary.histogram</span></div><div class="line">    <span class="keyword">return</span> outputs</div></pre></td></tr></table></figure></li></ul></li><li><p>损失</p><ul><li>损失因为是个具体的数，所以使用<code>scalar</code> （上面的权重和偏置都是矩阵，向量）<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">tf.summary.scalar(name=<span class="string">'loss_value'</span>, tensor=losses)</div></pre></td></tr></table></figure></li></ul></li><li><p>训练时<code>merge</code></p><ul><li><code>merged = tf.summary.merge_all()</code>合并所有的<code>summary</code></li><li>对于<code>loss</code>, 训练时执行<code>merge</code>, 然后随步数不断加入<ul><li><code>merged_result = sess.run(merged, feed_dict=feed_dict_train)   # 执行merged</code></li><li><code>writer.add_summary(summary=merged_result, global_step=i)</code><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="string">'''训练'''</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">optimize</span><span class="params">(n_epochs)</span>:</span></div><div class="line">    <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</div><div class="line">        merged = tf.summary.merge_all()</div><div class="line">        writer = tf.summary.FileWriter(<span class="string">'logs'</span>, sess.graph)  <span class="comment"># 将计算图写入文件</span></div><div class="line">        sess.run(tf.global_variables_initializer())</div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(n_epochs):</div><div class="line">            batch_x, batch_y = data.train.next_batch(batch_size)</div><div class="line">            feed_dict_train = &#123;x: batch_x, y: batch_y&#125;</div><div class="line">            sess.run(train_step, feed_dict=feed_dict_train)</div><div class="line">            <span class="keyword">if</span> i % <span class="number">20</span> == <span class="number">0</span>:</div><div class="line">                print(<span class="string">"epoch:&#123;0&#125;, accuracy:&#123;1&#125;"</span>.format(i, sess.run(accuracy, feed_dict=feed_dict_train)))</div><div class="line">                merged_result = sess.run(merged, feed_dict=feed_dict_train)   <span class="comment"># 执行merged</span></div><div class="line">                writer.add_summary(summary=merged_result, global_step=i)      <span class="comment"># 加入到writer</span></div><div class="line">optimize(<span class="number">1001</span>)</div></pre></td></tr></table></figure></li></ul></li></ul></li><li><p>结果</p><ul><li><code>loss</code><br><img src="/assets/blog_images/Tensorflow-tool/07_loss.png" alt="loss value" title="07_loss"></li><li>权重和偏置的数据分布<br><img src="/assets/blog_images/Tensorflow-tool/08_weights_biases.png" alt="权重和偏置" title="08_weights_biases"></li></ul></li></ul><h2 id="3、可视化embedding"><a href="#3、可视化embedding" class="headerlink" title="3、可视化embedding"></a>3、可视化embedding</h2><ul><li>全部代码： <a href="https://github.com/lawlite19/Blog-Back-Up/blob/master/code/tensorflow-tools/tensorflow_embeddings.py" target="_blank" rel="external">点击查看</a></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#-*- coding: utf-8 -*-</span></div><div class="line"><span class="comment"># Author: Lawlite</span></div><div class="line"><span class="comment"># Date: 2017/07/26</span></div><div class="line"><span class="comment"># Associate Blog: http://lawlite.me/2017/06/24/Tensorflow学习-工具相关/#1、可视化embedding</span></div><div class="line"><span class="comment"># License: MIT</span></div><div class="line"></div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line"><span class="keyword">from</span> tensorflow.contrib.tensorboard.plugins <span class="keyword">import</span> projector</div><div class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</div><div class="line"><span class="keyword">import</span> os</div><div class="line"></div><div class="line">MNIST_DATA_PATH = <span class="string">'MNIST_data'</span></div><div class="line">LOG_DIR = <span class="string">'log'</span></div><div class="line">SPRITE_IMAGE_FILE = <span class="string">'mnist_10k_sprite.png'</span></div><div class="line">META_DATA_FILE = <span class="string">'metadata.tsv'</span></div><div class="line">IMAGE_NUM = <span class="number">100</span></div><div class="line"></div><div class="line">mnist = input_data.read_data_sets(MNIST_DATA_PATH, one_hot=<span class="keyword">False</span>)</div><div class="line">plot_array = mnist.test.images[:IMAGE_NUM]   <span class="comment"># 取前100个图片</span></div><div class="line">np.savetxt(os.path.join(LOG_DIR, META_DATA_FILE), mnist.test.labels[:IMAGE_NUM], fmt=<span class="string">'%d'</span>)  <span class="comment"># label 保存为metadata.tsv文件</span></div><div class="line"></div><div class="line"></div><div class="line"><span class="string">'''可视化embedding, 3个步骤'''</span></div><div class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</div><div class="line">    <span class="string">'''1、 将2D矩阵放入Variable中'''</span></div><div class="line">    embeddings_var = tf.Variable(plot_array, name=<span class="string">'embedding'</span>)</div><div class="line">    tf.global_variables_initializer().run()</div><div class="line">    </div><div class="line">    <span class="string">'''2、 保存到文件中'''</span></div><div class="line">    saver = tf.train.Saver()</div><div class="line">    sess.run(embeddings_var.initializer)</div><div class="line">    saver.save(sess, os.path.join(LOG_DIR, <span class="string">"model.ckpt"</span>))</div><div class="line">    </div><div class="line">    <span class="string">'''3、 关联metadata和sprite图片'''</span></div><div class="line">    summary_writer = tf.summary.FileWriter(LOG_DIR)</div><div class="line">    config = projector.ProjectorConfig()</div><div class="line">    embedding = config.embeddings.add()</div><div class="line">    embedding.tensor_name = embeddings_var.name</div><div class="line">    embedding.metadata_path = META_DATA_FILE</div><div class="line">    embedding.sprite.image_path = SPRITE_IMAGE_FILE</div><div class="line">    embedding.sprite.single_image_dim.extend([<span class="number">28</span>, <span class="number">28</span>])</div><div class="line">    projector.visualize_embeddings(summary_writer, config)</div></pre></td></tr></table></figure><ul><li>结果</li></ul><p><img src="/assets/blog_images/Tensorflow-tool/13_visualize_embeddings.png" alt="可视化embedding" title="13_visualize_embeddings"></p><h1 id="二、Tensorflow-调试tfdbg"><a href="#二、Tensorflow-调试tfdbg" class="headerlink" title="二、Tensorflow 调试tfdbg"></a>二、<code>Tensorflow</code> 调试<code>tfdbg</code></h1><ul><li>官网教程：<a href="https://www.tensorflow.org/programmers_guide/debugger" target="_blank" rel="external">点击查看</a></li><li><code>Youtube</code>视频简短教程：<a href="https://www.youtube.com/watch?v=CA7fjRfduOI&amp;t=29s" target="_blank" rel="external">点击查看</a></li><li><code>Tensorflow Debugger (tfdbg)</code>是专业的调试工具，可以查看计算图中内容部的数据等<h2 id="1、本地调试"><a href="#1、本地调试" class="headerlink" title="1、本地调试"></a>1、本地调试</h2></li></ul><h3 id="1-加入代码"><a href="#1-加入代码" class="headerlink" title="(1) 加入代码"></a>(1) 加入代码</h3><ul><li>导入调试的包：<code>from tensorflow.python import debug as tf_debug</code></li><li><code>Wrapper Session</code>和添加<code>filter</code>: <ul><li><code>filter</code>也可以自己定义<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</div><div class="line">    sess = tf_debug.LocalCLIDebugWrapperSession(sess)</div><div class="line">    sess.add_tensor_filter(filter_name=<span class="string">'inf or nan'</span>, tensor_filter=tf_debug.has_inf_or_nan)</div></pre></td></tr></table></figure></li></ul></li></ul><h3 id="2-运行"><a href="#2-运行" class="headerlink" title="(2) 运行"></a>(2) 运行</h3><ul><li>命令行中执行：<code>python xxx.py --debug</code>即可进入调试<ul><li>支持鼠标点击的可以直接点击查看变量的信息</li></ul></li><li><code>run</code>或者<code>r</code>可以查看所有的<code>tensor</code>的名字等信息<ul><li>第一次<code>run</code>还没有初始化变量，<code>pt tenser_name</code>打印<code>tensor</code>的信息<br><img src="/assets/blog_images/Tensorflow-tool/09_debug.png" alt="第一次run" title="09_debug"></li></ul></li><li>再执行一次就是初始化变量<br><img src="/assets/blog_images/Tensorflow-tool/10_debug.png" alt="再次执行run" title="10_debug"><ul><li>可以进行<code>slice</code><br><img src="/assets/blog_images/Tensorflow-tool/12_debug.png" alt="slice" title="12_debug"></li></ul></li><li><a href="https://www.tensorflow.org/programmers_guide/debugger#tfdbg_cli_frequently-used_commands" target="_blank" rel="external">更多命令行</a><br><img src="/assets/blog_images/Tensorflow-tool/11_debug.png" alt="命令行" title="11_debug"></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;ul&gt;
&lt;li&gt;&lt;code&gt;Tensorflow&lt;/code&gt;版本(&lt;code&gt;# 2017-06-24&lt;/code&gt;)：&lt;code&gt;1.2.0&lt;/code&gt;  &lt;/li&gt;
&lt;li&gt;&lt;code&gt;Python&lt;/code&gt;版本：&lt;code&gt;3.5.3&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;包括：&lt;ul&gt;
&lt;li&gt;&lt;code&gt;Tensorboard&lt;/code&gt;&lt;strong&gt;可视化&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;tfdbg&lt;/code&gt;&lt;strong&gt;调试&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;常用的&lt;strong&gt;高级函数&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="Tensorflow" scheme="http://lawlite.cn/tags/Tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>（Unfinished）RNN-循环神经网络之LSTM和GRU-04介绍及推导</title>
    <link href="http://lawlite.cn/2017/06/21/RNN-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B9%8BLSTM%E5%92%8CGRU-04%E4%BB%8B%E7%BB%8D%E5%8F%8A%E6%8E%A8%E5%AF%BC/"/>
    <id>http://lawlite.cn/2017/06/21/RNN-循环神经网络之LSTM和GRU-04介绍及推导/</id>
    <published>2017-06-21T14:45:44.000Z</published>
    <updated>2017-06-25T08:49:42.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="（Unfinished）尚未完成"><a href="#（Unfinished）尚未完成" class="headerlink" title="（Unfinished）尚未完成"></a>（Unfinished）尚未完成</h1><h1 id="一、说明"><a href="#一、说明" class="headerlink" title="一、说明"></a>一、说明</h1><ul><li>关于<code>LSTM</code>的<code>cell</code>结构和一些计算在之前已经介绍了，可以<a href="http://lawlite.me/2017/06/14/RNN-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8CLSTM-01%E5%9F%BA%E7%A1%80/#%E5%9B%9B%E3%80%81Long-Short-Term-Memory-LSTM%EF%BC%8C%E9%95%BF%E7%9F%AD%E6%97%B6%E8%AE%B0%E5%BF%86%E7%BD%91%E7%BB%9C" target="_blank" rel="external">点击这里查看</a></li><li>本篇博客主要涉及一下内容：<ul><li><code>LSTM</code>前向计算说明(之前的博客中<a href="http://lawlite.me/2017/06/14/RNN-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8CLSTM-01%E5%9F%BA%E7%A1%80/#%E5%9B%9B%E3%80%81Long-Short-Term-Memory-LSTM%EF%BC%8C%E9%95%BF%E7%9F%AD%E6%97%B6%E8%AE%B0%E5%BF%86%E7%BD%91%E7%BB%9C" target="_blank" rel="external">LSTM部分</a>实际已经提到过，这里结合图更详细说明)</li></ul></li></ul><a id="more"></a><h1 id="二、LSTM前向计算step-by-step"><a href="#二、LSTM前向计算step-by-step" class="headerlink" title="二、LSTM前向计算step by step"></a>二、<code>LSTM</code>前向计算<code>step by step</code></h1><h2 id="1、结构review"><a href="#1、结构review" class="headerlink" title="1、结构review"></a>1、结构<code>review</code></h2><ul><li>我们知道<code>RNN</code>的结构如下图<ul><li>注意<code>cell</code>中的<strong>神经元可以有多个</strong><br><img src="/assets/blog_images/RNN/RNN_LSTM_19.png" alt="RNN 结构 -w150" title="RNN_LSTM_19"></li></ul></li><li><code>LSTM</code>就是对<code>cell</code>结构的改进<br><img src="/assets/blog_images/RNN/RNN_LSTM_20.png" alt="LSTM结构" title="RNN_LSTM_20"></li><li>符号说明<br><img src="/assets/blog_images/RNN/RNN_LSTM_21.png" alt="符号说明" title="RNN_LSTM_21"></li><li><code>LSTM</code>的关键就是<code>state</code>,就是对应上面的<strong>主线数据</strong>的传递<br><img src="/assets/blog_images/RNN/RNN_LSTM_22.png" alt="LSTM state传递" title="RNN_LSTM_22"></li></ul><h2 id="2、前向计算step-by-step"><a href="#2、前向计算step-by-step" class="headerlink" title="2、前向计算step by step"></a>2、前向计算<code>step by step</code></h2><h3 id="1-决定抛弃的信息"><a href="#1-决定抛弃的信息" class="headerlink" title="(1) 决定抛弃的信息"></a>(1) 决定抛弃的信息</h3><ul><li>遗忘门 (<code>forget gate layer</code>)</li><li>$\sigma$是<code>Sigmoid</code>激励函数，因为它的值域是<code>(0,1)</code>，<code>0</code>代表遗忘所有信息，<code>1</code>代表保留所有信息</li></ul><p><img src="/assets/blog_images/RNN/RNN_LSTM_23.png" alt="遗忘门 forget gate layer" title="RNN_LSTM_23"></p><h3 id="2-决定存储的新信息"><a href="#2-决定存储的新信息" class="headerlink" title="(2) 决定存储的新信息"></a>(2) 决定存储的新信息</h3><ul><li>包括两个部分<ul><li>第一个是输入门 (<code>input gate layer</code>)，对应的是<code>Sigmoid</code>函数</li><li>第二个是经过<code>tanh</code>激励函数</li></ul></li></ul><p><img src="/assets/blog_images/RNN/RNN_LSTM_24.png" alt="决定存储的新信息" title="RNN_LSTM_24"></p><h3 id="3-更新state-C-t-1-成-C-t"><a href="#3-更新state-C-t-1-成-C-t" class="headerlink" title="(3) 更新state$C_{t-1}$成$C_t$"></a>(3) 更新<code>state</code>$C_{t-1}$成$C_t$</h3><ul><li>$f_t$是经过<code>Sigmoid</code>函数的，所以值域在<code>(0,1)</code>之间，$C_{t-1}$点乘<code>0-1</code>之间的数实际就是对$C_{t-1}$的一种<strong>缩放</strong>，（可以认为是记住之前信息的程度）</li><li>然后加入进来的新的信息<br><img src="/assets/blog_images/RNN/RNN_LSTM_25.png" alt="更新新的state" title="RNN_LSTM_25"></li></ul><h3 id="4-最后计算输出"><a href="#4-最后计算输出" class="headerlink" title="(4) 最后计算输出"></a>(4) 最后计算输出</h3><ul><li>输出门(<code>output gate layer</code>)</li></ul><p><img src="/assets/blog_images/RNN/RNN_LSTM_26.png" alt="计算输出" title="RNN_LSTM_26"></p><ul><li>最后再放一下之前的图, 数据流向可能更清晰</li></ul><p><img src="/assets/blog_images/RNN/RNN_LSTM_08.png" alt="LSTM cell" title="RNN_LSTM_08"></p><h1 id="三、GRU-Gated-Recurrent-Unit"><a href="#三、GRU-Gated-Recurrent-Unit" class="headerlink" title="三、GRU (Gated Recurrent Unit)"></a>三、GRU (Gated Recurrent Unit)</h1><h2 id="1、结构和前向计算"><a href="#1、结构和前向计算" class="headerlink" title="1、结构和前向计算"></a>1、结构和前向计算</h2><ul><li>如下图所示<ul><li>相比<code>LSTM</code>，<code>GRU</code>结合了遗忘门和输入门</li><li>同样也合并了<code>cell state</code>和<code>hidden state</code> （就是<code>LSTM</code>中的<code>c</code>和<code>h</code>）</li><li><code>GRU</code>比<code>LSTM</code>更加简单</li></ul></li></ul><p><img src="./images/RNN_GRU_27.png" alt="GRU cell结构" title="RNN_GRU_27"></p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul><li><a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank" rel="external">https://colah.github.io/posts/2015-08-Understanding-LSTMs/</a></li><li><a href="https://r2rt.com/written-memories-understanding-deriving-and-extending-the-lstm.html#dealing-with-vanishing-and-exploding-gradients" target="_blank" rel="external">https://r2rt.com/written-memories-understanding-deriving-and-extending-the-lstm.html#dealing-with-vanishing-and-exploding-gradients</a></li><li><a href="http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf" target="_blank" rel="external">http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf</a></li><li><a href="http://lawlite.me/2016/12/20/%E8%AE%BA%E6%96%87%E8%AE%B0%E5%BD%95-UnderstandingTheDifficultyOfTrainingDeepFeedforwardNeuralNetworks/" target="_blank" rel="external">http://lawlite.me/2016/12/20/%E8%AE%BA%E6%96%87%E8%AE%B0%E5%BD%95-UnderstandingTheDifficultyOfTrainingDeepFeedforwardNeuralNetworks/</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;（Unfinished）尚未完成&quot;&gt;&lt;a href=&quot;#（Unfinished）尚未完成&quot; class=&quot;headerlink&quot; title=&quot;（Unfinished）尚未完成&quot;&gt;&lt;/a&gt;（Unfinished）尚未完成&lt;/h1&gt;&lt;h1 id=&quot;一、说明&quot;&gt;&lt;a href=&quot;#一、说明&quot; class=&quot;headerlink&quot; title=&quot;一、说明&quot;&gt;&lt;/a&gt;一、说明&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;关于&lt;code&gt;LSTM&lt;/code&gt;的&lt;code&gt;cell&lt;/code&gt;结构和一些计算在之前已经介绍了，可以&lt;a href=&quot;http://lawlite.me/2017/06/14/RNN-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8CLSTM-01%E5%9F%BA%E7%A1%80/#%E5%9B%9B%E3%80%81Long-Short-Term-Memory-LSTM%EF%BC%8C%E9%95%BF%E7%9F%AD%E6%97%B6%E8%AE%B0%E5%BF%86%E7%BD%91%E7%BB%9C&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;点击这里查看&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;本篇博客主要涉及一下内容：&lt;ul&gt;
&lt;li&gt;&lt;code&gt;LSTM&lt;/code&gt;前向计算说明(之前的博客中&lt;a href=&quot;http://lawlite.me/2017/06/14/RNN-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8CLSTM-01%E5%9F%BA%E7%A1%80/#%E5%9B%9B%E3%80%81Long-Short-Term-Memory-LSTM%EF%BC%8C%E9%95%BF%E7%9F%AD%E6%97%B6%E8%AE%B0%E5%BF%86%E7%BD%91%E7%BB%9C&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;LSTM部分&lt;/a&gt;实际已经提到过，这里结合图更详细说明)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="Python" scheme="http://lawlite.cn/tags/Python/"/>
    
      <category term="DeepLearning" scheme="http://lawlite.cn/tags/DeepLearning/"/>
    
      <category term="RNN" scheme="http://lawlite.cn/tags/RNN/"/>
    
      <category term="LSTM" scheme="http://lawlite.cn/tags/LSTM/"/>
    
      <category term="GRU" scheme="http://lawlite.cn/tags/GRU/"/>
    
  </entry>
  
  <entry>
    <title>RNN-LSTM循环神经网络-03Tensorflow进阶实现</title>
    <link href="http://lawlite.cn/2017/06/21/RNN-LSTM%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-03Tensorflow%E8%BF%9B%E9%98%B6%E5%AE%9E%E7%8E%B0/"/>
    <id>http://lawlite.cn/2017/06/21/RNN-LSTM循环神经网络-03Tensorflow进阶实现/</id>
    <published>2017-06-21T08:54:28.000Z</published>
    <updated>2017-06-25T08:49:18.000Z</updated>
    
    <content type="html"><![CDATA[<ul><li>全部代码：<a href="https://github.com/lawlite19/Blog-Back-Up/tree/master/code/rnn/rnn_tensorflow" target="_blank" rel="external">点击这里查看</a></li><li>关于<code>Tensorflow</code>实现一个简单的<strong>二元序列的例子</strong>可以<a href="http://lawlite.me/2017/06/16/RNN-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-02Tensorflow%E4%B8%AD%E7%9A%84%E5%AE%9E%E7%8E%B0/" target="_blank" rel="external">点击这里查看</a></li><li>关于<code>RNN</code>和<code>LSTM</code>的基础可以<a href="http://lawlite.me/2017/06/14/RNN-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8CLSTM-01%E5%9F%BA%E7%A1%80/" target="_blank" rel="external">查看这里</a></li><li>这篇博客主要包含以下内容<ul><li>训练一个<code>RNN</code>模型<strong>逐字符生成文本数据</strong>(最后的部分)</li><li>使用<code>Tensorflow</code>的<code>scan</code>函数实现<code>dynamic_rnn</code><strong>动态创建</strong>的效果</li><li>使用<code>multiple RNN</code>创建<strong>多层</strong>的<code>RNN</code></li><li>实现<code>Dropout</code>和<code>Layer Normalization</code>的功能</li></ul></li></ul><a id="more"></a><h1 id="一、模型说明和数据处理"><a href="#一、模型说明和数据处理" class="headerlink" title="一、模型说明和数据处理"></a>一、模型说明和数据处理</h1><h2 id="1、模型说明"><a href="#1、模型说明" class="headerlink" title="1、模型说明"></a>1、模型说明</h2><ul><li>我们要使用<code>RNN</code>学习一个语言模型(<code>language model</code>)去生成字符序列</li><li><code>githbub</code>上有别人实现好的<ul><li><code>Torch</code>中的实现：<a href="https://github.com/karpathy/char-rnn" target="_blank" rel="external">https://github.com/karpathy/char-rnn</a></li><li><code>Tensorflow</code>中的实现：<a href="https://github.com/sherjilozair/char-rnn-tensorflow" target="_blank" rel="external">https://github.com/sherjilozair/char-rnn-tensorflow</a></li></ul></li><li>接下来我们来看如何实现<h2 id="2、数据处理"><a href="#2、数据处理" class="headerlink" title="2、数据处理"></a>2、数据处理</h2></li><li>数据集使用<em>莎士比亚</em>的一段文集，<a href="https://raw.githubusercontent.com/jcjohnson/torch-rnn/master/data/tiny-shakespeare.txt" target="_blank" rel="external">点击这里查看</a>, 实际也可以使用别的</li><li><strong>大小写字符</strong>视为<strong>不同</strong>的字符</li><li>下载并读取数据</li></ul><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="string">''</span><span class="string">'下载数据并读取数据'</span><span class="string">''</span></div><div class="line">file_url = <span class="string">'https://raw.githubusercontent.com/jcjohnson/torch-rnn/master/data/tiny-shakespeare.txt'</span></div><div class="line">file_name = <span class="string">'tinyshakespeare.txt'</span></div><div class="line"><span class="keyword">if</span> not os<span class="selector-class">.path</span><span class="selector-class">.exists</span>(file_name):</div><div class="line">    urllib<span class="selector-class">.request</span><span class="selector-class">.urlretrieve</span>(file_url, filename=file_name)</div><div class="line">with open(file_name, <span class="string">'r'</span>) as f:</div><div class="line">    raw_data = f.read()</div><div class="line">    print(<span class="string">"数据长度"</span>, len(raw_data))</div></pre></td></tr></table></figure><ul><li><p>处理字符数据，转换为数字</p><ul><li>使用<code>set</code>去重，得到所有的唯一字符</li><li>然后一个字符对应一个数字（使用字典）</li><li>然后遍历原始数据，得到所有字符对应的数字<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">'''处理字符数据，转换为数字'''</div><div class="line">vocab = set(raw_data)                    # 使用set去重，这里就是去除重复的字母(大小写是区分的)</div><div class="line">vocab_size = len(vocab)      </div><div class="line">idx_to_vocab = dict(enumerate(vocab))    # 这里将set转为了字典，每个字符对应了一个数字0,1,2,3..........(vocab_size-1)</div><div class="line">vocab_to_idx = dict(zip(idx_to_vocab.values(), idx_to_vocab.keys())) # 这里将字典的(key, value)转换成(value, key)</div><div class="line"></div><div class="line">data = [vocab_to_idx[c] for c in raw_data]   # 处理raw_data, 根据字符，得到对应的value,就是数字</div><div class="line">del raw_data</div></pre></td></tr></table></figure></li></ul></li><li><p>生成<code>batch</code>数据</p><ul><li><code>Tensorflow models</code>给出的<strong>PTB模型</strong>：<a href="https://github.com/tensorflow/models/tree/master/tutorials/rnn/ptb" target="_blank" rel="external">https://github.com/tensorflow/models/tree/master/tutorials/rnn/ptb</a></li></ul></li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">'''超参数'''</div><div class="line">num_steps=200             # 学习的步数</div><div class="line">batch_size=32</div><div class="line">state_size=100            # cell的size</div><div class="line">num_classes = vocab_size</div><div class="line">learning_rate = 1e-4</div><div class="line"></div><div class="line">def gen_epochs(num_epochs, num_steps, batch_size):</div><div class="line">    for i in range(num_epochs):</div><div class="line">        yield reader.ptb_iterator_oldversion(data, batch_size, num_steps)</div></pre></td></tr></table></figure><ul><li><ul><li>ptb_iterator函数实现：<ul><li>返回数据<code>X,Y</code>的<code>shape=[batch_size, num_steps]</code><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div></pre></td><td class="code"><pre><div class="line">def ptb_iterator_oldversion(raw_data, batch_size, num_steps):</div><div class="line">  """Iterate on the raw PTB data.</div><div class="line">  This generates batch_size pointers into the raw PTB data, and allows</div><div class="line">  minibatch iteration along these pointers.</div><div class="line">  Args:</div><div class="line">  raw_data: one of the raw data outputs from ptb_raw_data.</div><div class="line">  batch_size: int, the batch size.</div><div class="line">  num_steps: int, the number of unrolls.</div><div class="line">  Yields:</div><div class="line">  Pairs of the batched data, each a matrix of shape [batch_size, num_steps].</div><div class="line">  The second element of the tuple is the same data time-shifted to the</div><div class="line">  right by one.</div><div class="line">  Raises:</div><div class="line">  ValueError: if batch_size or num_steps are too high.</div><div class="line">  """</div><div class="line">  raw_data = np.array(raw_data, dtype=np.int32)</div><div class="line">  </div><div class="line">  data_len = len(raw_data)</div><div class="line">  batch_len = data_len // batch_size</div><div class="line">  data = np.zeros([batch_size, batch_len], dtype=np.int32)</div><div class="line">  for i in range(batch_size):</div><div class="line">    data[i] = raw_data[batch_len * i:batch_len * (i + 1)]</div><div class="line">  </div><div class="line">  epoch_size = (batch_len - 1) // num_steps</div><div class="line">  </div><div class="line">  if epoch_size == 0:</div><div class="line">    raise ValueError("epoch_size == 0, decrease batch_size or num_steps")</div><div class="line">  </div><div class="line">  for i in range(epoch_size):</div><div class="line">    x = data[:, i*num_steps:(i+1)*num_steps]</div><div class="line">    y = data[:, i*num_steps+1:(i+1)*num_steps+1]</div><div class="line">    yield (x, y)</div></pre></td></tr></table></figure></li></ul></li></ul></li></ul><h1 id="二、使用tf-scan函数和dynamic-rnn"><a href="#二、使用tf-scan函数和dynamic-rnn" class="headerlink" title="二、使用tf.scan函数和dynamic_rnn"></a>二、使用<code>tf.scan</code>函数和<code>dynamic_rnn</code></h1><h2 id="1、为什么使用tf-scan和dynamic-rnn"><a href="#1、为什么使用tf-scan和dynamic-rnn" class="headerlink" title="1、为什么使用tf.scan和dynamic_rnn"></a>1、为什么使用<code>tf.scan</code>和<code>dynamic_rnn</code></h2><ul><li>之前我们<a href="http://lawlite.me/2017/06/16/RNN-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-02Tensorflow%E4%B8%AD%E7%9A%84%E5%AE%9E%E7%8E%B0/#1-实现过程" target="_blank" rel="external">实现的第一个例子</a>中没有用<code>dynamic_rnn</code>的部分是将输入的三维数据<code>[batch_size,num_steps, state_size]</code>按<code>num_steps</code>维度进行拆分，然后每计算一步都存到<code>list</code>列表中，如下图</li></ul><p><img src="/assets/blog_images/RNN/RNN_list_tensor19.png" alt="计算之后存到list中" title="RNN_list_tensor19"></p><ul><li>这种构建方式<strong>很耗时</strong>，在我们例子中没有体现出来，但是如果我们要<strong>学习的步数</strong>很大(<code>num_steps</code>，也可以说要学习的依赖关系很长），如果再使用<strong>深层的RNN</strong>，这种就不合适了</li><li><p>为了方便比较和<code>dynamic_rnn</code>的运行耗时，下面还是给出使用<code>list</code></p><h2 id="2、使用list的方式-static-rnn"><a href="#2、使用list的方式-static-rnn" class="headerlink" title="2、使用list的方式(static_rnn)"></a>2、使用<code>list</code>的方式(<code>static_rnn</code>)</h2></li><li><p>构建计算图</p><ul><li>我这里<code>tensorflow</code>的版本是<code>1.2.0</code>，与<code>1.0</code> 些许不一样</li><li>和之前的例子差不多，这里不再累述<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div></pre></td><td class="code"><pre><div class="line">'''使用list的方式'''</div><div class="line">def build_basic_rnn_graph_with_list(</div><div class="line">    state_size = state_size,</div><div class="line">    num_classes = num_classes,</div><div class="line">    batch_size = batch_size,</div><div class="line">    num_steps = num_steps,</div><div class="line">    num_layers = 3,</div><div class="line">    learning_rate = learning_rate):</div><div class="line"></div><div class="line">    reset_graph()</div><div class="line"></div><div class="line">    x = tf.placeholder(tf.int32, [batch_size, num_steps], name='x')</div><div class="line">    y = tf.placeholder(tf.int32, [batch_size, num_steps], name='y')</div><div class="line"></div><div class="line">    x_one_hot = tf.one_hot(x, num_classes)   # (batch_size, num_steps, num_classes)</div><div class="line">    '''这里按第二维拆开num_steps*(batch_size, num_classes)'''</div><div class="line">    rnn_inputs = [tf.squeeze(i,squeeze_dims=[1]) for i in tf.split(x_one_hot, num_steps, 1)]</div><div class="line"></div><div class="line">    cell = tf.nn.rnn_cell.BasicRNNCell(state_size)</div><div class="line">    init_state = cell.zero_state(batch_size, tf.float32)</div><div class="line">    '''使用static_rnn方式'''</div><div class="line">    rnn_outputs, final_state = tf.contrib.rnn.static_rnn(cell=cell, inputs=rnn_inputs, </div><div class="line">                                                        initial_state=init_state)</div><div class="line">    #rnn_outputs, final_state = tf.nn.rnn(cell, rnn_inputs, initial_state=init_state) # tensorflow 1.0的方式</div><div class="line">    with tf.variable_scope('softmax'):</div><div class="line">        W = tf.get_variable('W', [state_size, num_classes])</div><div class="line">        b = tf.get_variable('b', [num_classes], initializer=tf.constant_initializer(0.0))</div><div class="line">    logits = [tf.matmul(rnn_output, W) + b for rnn_output in rnn_outputs]</div><div class="line"></div><div class="line">    y_as_list = [tf.squeeze(i, squeeze_dims=[1]) for i in tf.split(y, num_steps, 1)]</div><div class="line"></div><div class="line">    #loss_weights = [tf.ones([batch_size]) for i in range(num_steps)]</div><div class="line">    losses = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y_as_list, </div><div class="line">                                                  logits=logits)</div><div class="line">    #losses = tf.nn.seq2seq.sequence_loss_by_example(logits, y_as_list, loss_weights)  # tensorflow 1.0的方式</div><div class="line">    total_loss = tf.reduce_mean(losses)</div><div class="line">    train_step = tf.train.AdamOptimizer(learning_rate).minimize(total_loss)</div><div class="line"></div><div class="line">    return dict(</div><div class="line">        x = x,</div><div class="line">        y = y,</div><div class="line">        init_state = init_state,</div><div class="line">        final_state = final_state,</div><div class="line">        total_loss = total_loss,</div><div class="line">        train_step = train_step</div><div class="line">    )</div></pre></td></tr></table></figure></li></ul></li><li><p>训练神经网络函数</p><ul><li>和之前例子类似<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line">'''训练rnn网络的函数'''</div><div class="line">def train_rnn(g, num_epochs, num_steps=num_steps, batch_size=batch_size, verbose=True, save=False):</div><div class="line">    tf.set_random_seed(2345)</div><div class="line">    with tf.Session() as sess:</div><div class="line">        sess.run(tf.initialize_all_variables())</div><div class="line">        training_losses = []</div><div class="line">        for idx, epoch in enumerate(gen_epochs(num_epochs, num_steps, batch_size)):</div><div class="line">            training_loss = 0</div><div class="line">            steps = 0</div><div class="line">            training_state = None</div><div class="line">            for X, Y in epoch:</div><div class="line">                steps += 1</div><div class="line">                feed_dict = &#123;g['x']: X, g['y']: Y&#125;</div><div class="line">                if training_state is not None:</div><div class="line">                    feed_dict[g['init_state']] = training_state</div><div class="line">                training_loss_, training_state, _ = sess.run([g['total_loss'],</div><div class="line">                                                           g['final_state'],</div><div class="line">                                                           g['train_step']],</div><div class="line">                                                          feed_dict=feed_dict)</div><div class="line">                training_loss += training_loss_ </div><div class="line">            if verbose:</div><div class="line">                print('epoch: &#123;0&#125;的平均损失值：&#123;1&#125;'.format(idx, training_loss/steps))</div><div class="line">            training_losses.append(training_loss/steps)</div><div class="line">        </div><div class="line">        if isinstance(save, str):</div><div class="line">            g['saver'].save(sess, save)</div><div class="line">    return training_losses</div></pre></td></tr></table></figure></li></ul></li><li><p>调用执行：</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">start_time = <span class="selector-tag">time</span>.time()</div><div class="line">g = build_basic_rnn_graph_with_list()</div><div class="line"><span class="function"><span class="title">print</span><span class="params">(<span class="string">"构建图耗时"</span>, time.time()</span></span>-start_time)</div><div class="line">start_time = <span class="selector-tag">time</span>.time()</div><div class="line"><span class="function"><span class="title">train_rnn</span><span class="params">(g, <span class="number">3</span>)</span></span></div><div class="line"><span class="function"><span class="title">print</span><span class="params">(<span class="string">"训练耗时："</span>, time.time()</span></span>-start_time)</div></pre></td></tr></table></figure></li><li><p>运行结果</p><ul><li>构建计算图耗时: <code>113.43532419204712</code></li><li><code>3</code>个<code>epoch</code>运行耗时：<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">epoch: 0的平均损失值：3.6314958388777985</div><div class="line">epoch: 1的平均损失值：3.287133811534136</div><div class="line">epoch: 2的平均损失值：3.250853428895446</div><div class="line">训练耗时： 84.2816972732544</div></pre></td></tr></table></figure></li></ul></li><li><p>可以看出在<strong>构建图的时候非常耗时</strong>，这里仅仅<strong>一层的cell</strong></p></li></ul><h2 id="3、dynamic-rnn的使用"><a href="#3、dynamic-rnn的使用" class="headerlink" title="3、dynamic_rnn的使用"></a>3、<code>dynamic_rnn</code>的使用</h2><ul><li>之前在我们<a href="http://lawlite.me/2017/06/16/RNN-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-02Tensorflow%E4%B8%AD%E7%9A%84%E5%AE%9E%E7%8E%B0/#2-%E4%BD%BF%E7%94%A8dynamic-rnn%E6%96%B9%E5%BC%8F" target="_blank" rel="external">第一个例子</a>中实际已经使用过了，这里使用<code>MultiRNNCell</code>实现<strong>多层cell</strong>，具体下面再讲</li><li>构建模型：<ul><li><code>tf.nn.embedding_lookup(params, ids)</code>函数是在<code>params</code>中查找<code>ids</code>的<strong>表示</strong>， 和在<code>matrix</code>中用<code>array</code>索引类似, 这里是在<strong>二维embeddings</strong>中找<strong>二维的ids</strong>, <code>ids</code>每一行中的一个数对应<code>embeddings</code>中的一行，所以最后是<code>[batch_size, num_steps, state_size]</code>，关于具体的输出可以<a href="https://gist.github.com/lawlite19/02a598c6d9b91a53d0a38fd081f9cbc4" target="_blank" rel="external">查看这里</a></li><li>这里我认为就是某个<strong>字母的表示</strong>,之前上面我们的<code>statci_rnn</code>就是<code>one-hot</code>来表示的</li></ul></li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div></pre></td><td class="code"><pre><div class="line">'''使用dynamic_rnn方式</div><div class="line">   - 之前我们自己实现的cell和static_rnn的例子都是将得到的tensor使用list存起来，这种方式构建计算图时很慢</div><div class="line">   - dynamic可以在运行时构建计算图</div><div class="line">'''</div><div class="line">def build_multilayer_lstm_graph_with_dynamic_rnn(</div><div class="line">    state_size = state_size,</div><div class="line">    num_classes = num_classes,</div><div class="line">    batch_size = batch_size,</div><div class="line">    num_steps = num_steps,</div><div class="line">    num_layers = 3,</div><div class="line">    learning_rate = learning_rate</div><div class="line">    ):</div><div class="line">    reset_graph()</div><div class="line">    x = tf.placeholder(tf.int32, [batch_size, num_steps], name='x')</div><div class="line">    y = tf.placeholder(tf.int32, [batch_size, num_steps], name='y')</div><div class="line">    embeddings = tf.get_variable(name='embedding_matrix', shape=[num_classes, state_size])</div><div class="line">    '''这里的输入是三维的[batch_size, num_steps, state_size]</div><div class="line">        - embedding_lookup(params, ids)函数是在params中查找ids的表示， 和在matrix中用array索引类似,</div><div class="line">          这里是在二维embeddings中找二维的ids, ids每一行中的一个数对应embeddings中的一行，所以最后是[batch_size, num_steps, state_size]</div><div class="line">    '''</div><div class="line">    rnn_inputs = tf.nn.embedding_lookup(params=embeddings, ids=x)</div><div class="line">    cell = tf.nn.rnn_cell.LSTMCell(num_units=state_size, state_is_tuple=True)</div><div class="line">    cell = tf.nn.rnn_cell.MultiRNNCell(cells=[cell]*num_layers, state_is_tuple=True)</div><div class="line">    init_state = cell.zero_state(batch_size, dtype=tf.float32)</div><div class="line">    '''使用dynamic_rnn方式'''</div><div class="line">    rnn_outputs, final_state = tf.nn.dynamic_rnn(cell=cell, inputs=rnn_inputs, </div><div class="line">                                                 initial_state=init_state)    </div><div class="line">    with tf.variable_scope('softmax'):</div><div class="line">        W = tf.get_variable('W', [state_size, num_classes])</div><div class="line">        b = tf.get_variable('b', [num_classes], initializer=tf.constant_initializer(0.0))</div><div class="line">    </div><div class="line">    rnn_outputs = tf.reshape(rnn_outputs, [-1, state_size])   # 转成二维的矩阵</div><div class="line">    y_reshape = tf.reshape(y, [-1])</div><div class="line">    logits = tf.matmul(rnn_outputs, W) + b                    # 进行矩阵运算</div><div class="line">    total_loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=y_reshape))</div><div class="line">    train_step = tf.train.AdamOptimizer(learning_rate).minimize(total_loss)</div><div class="line">    </div><div class="line">    return dict(x = x,</div><div class="line">                y = y,</div><div class="line">                init_state = init_state,</div><div class="line">                final_state = final_state,</div><div class="line">                total_loss = total_loss,</div><div class="line">                train_step = train_step)</div></pre></td></tr></table></figure><ul><li>调用执行即可</li></ul><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">start_time = <span class="selector-tag">time</span>.time()</div><div class="line">g = build_multilayer_lstm_graph_with_dynamic_rnn()</div><div class="line"><span class="function"><span class="title">print</span><span class="params">(<span class="string">"构建图耗时"</span>, time.time()</span></span>-start_time)</div><div class="line">start_time = <span class="selector-tag">time</span>.time()</div><div class="line"><span class="function"><span class="title">train_rnn</span><span class="params">(g, <span class="number">3</span>)</span></span></div><div class="line"><span class="function"><span class="title">print</span><span class="params">(<span class="string">"训练耗时："</span>, time.time()</span></span>-start_time)</div></pre></td></tr></table></figure><ul><li>运行结果（注意这是<strong>3层的LSTM</strong>）：<ul><li>构建计算图耗时 <code>7.616888523101807</code>，相比第一种<code>static_rnn</code>很快</li><li>训练耗时(这是<strong>3层的LSTM</strong>，所以还是挺慢的)：<figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">epoch: <span class="number">0</span>的平均损失值：<span class="number">3.604653576324726</span></div><div class="line">epoch: <span class="number">1</span>的平均损失值：<span class="number">3.3202743626188957</span></div><div class="line">epoch: <span class="number">2</span>的平均损失值：<span class="number">3.3155322650383257</span></div><div class="line">训练耗时： <span class="number">303.5468375682831</span></div></pre></td></tr></table></figure></li></ul></li></ul><h2 id="4、tf-scan实现的方式"><a href="#4、tf-scan实现的方式" class="headerlink" title="4、tf.scan实现的方式"></a>4、<code>tf.scan</code>实现的方式</h2><ul><li>如果你不了解<code>tf.scan</code>，建议看下<a href="https://www.tensorflow.org/api_docs/python/tf/scan" target="_blank" rel="external">官方API</a>, 还是有点复杂的。<ul><li>或者Youtube上有个介绍，<a href="https://www.youtube.com/watch?v=A6qJMB3stE4&amp;t=621s" target="_blank" rel="external">点击这里查看</a></li></ul></li><li><code>scan</code>是个高阶函数，<strong>一般的计算方式是</strong>：给定一个<strong>序列</strong>$[x_0, x_1,…..,x_n]$和<strong>初试状态</strong>$y_{-1}$,根据$y_t = f(x_t, y_{t-1})$ 计算得到<strong>最终序列</strong>$[y_0, y_1,……,y_n]$</li><li>构建计算图<ul><li><code>tf.transpose(rnn_inputs, [1,0,2])</code>  是将<code>rnn_inputs</code>的<strong>第一个和第二个维度调换</strong>，即变成<code>[num_steps,batch_size, state_size]</code>, 在<code>dynamic_rnn</code>函数有个<strong>time_major参数</strong>，就是指定<code>num_steps</code>是否在第一个维度上，默认是<code>false</code>的,即不在第一维 </li><li><code>tf.scan</code>会将<code>elems</code>按照<strong>第一维拆开</strong>，所以一次就是一个<code>step</code>的数据（和我们<code>static_rnn</code>的例子类似） </li><li><strong>参数a</strong>的结构和<strong>initializer的结构一致</strong>，所以<code>a[1]</code>就是对应的<code>state</code>，<code>cell</code>需要传入<code>x</code>和<code>state</code>计算 </li><li>每次迭代<code>cell</code>返回的是一个<code>rnn_output, shape=(batch_size,state_size)</code>和对应的<code>state</code>,<code>num_steps</code>之后的<code>rnn_outputs</code>的<code>shape</code>就是<code>(num_steps, batch_size, state_size)</code> ，<code>state</code>同理</li><li>每次输入的<code>x</code>都会得到的<code>state--&gt;(final_states)</code>，我们只要的最后的<code>final_state</code> </li></ul></li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div></pre></td><td class="code"><pre><div class="line">'''使用scan实现dynamic_rnn的效果'''</div><div class="line">def build_multilayer_lstm_graph_with_scan(</div><div class="line">    state_size = state_size,</div><div class="line">    num_classes = num_classes,</div><div class="line">    batch_size = batch_size,</div><div class="line">    num_steps = num_steps,</div><div class="line">    num_layers = 3,</div><div class="line">    learning_rate = learning_rate</div><div class="line">    ):</div><div class="line">    reset_graph()</div><div class="line">    x = tf.placeholder(tf.int32, [batch_size, num_steps], name='x')</div><div class="line">    y = tf.placeholder(tf.int32, [batch_size, num_steps], name='y')</div><div class="line">    embeddings = tf.get_variable(name='embedding_matrix', shape=[num_classes, state_size])</div><div class="line">    '''这里的输入是三维的[batch_size, num_steps, state_size]'''</div><div class="line">    rnn_inputs = tf.nn.embedding_lookup(params=embeddings, ids=x)</div><div class="line">    '''构建多层的cell, 先构建一个cell, 然后使用MultiRNNCell函数构建即可'''</div><div class="line">    cell = tf.nn.rnn_cell.LSTMCell(num_units=state_size, state_is_tuple=True)</div><div class="line">    cell = tf.nn.rnn_cell.MultiRNNCell(cells=[cell]*num_layers, state_is_tuple=True)  </div><div class="line">    init_state = cell.zero_state(batch_size, dtype=tf.float32)</div><div class="line">    '''使用tf.scan方式</div><div class="line">       - tf.transpose(rnn_inputs, [1,0,2])  是将rnn_inputs的第一个和第二个维度调换，即[num_steps,batch_size, state_size],</div><div class="line">           在dynamic_rnn函数有个time_major参数，就是指定num_steps是否在第一个维度上，默认是false的,即不在第一维</div><div class="line">       - tf.scan会将elems按照第一维拆开，所以一次就是一个step的数据（和我们static_rnn的例子类似）</div><div class="line">       - a的结构和initializer的结构一致，所以a[1]就是对应的state，cell需要传入x和state计算</div><div class="line">       - 每次迭代cell返回的是一个rnn_output(batch_size,state_size)和对应的state,num_steps之后的rnn_outputs的shape就是(num_steps, batch_size, state_size)</div><div class="line">       - 每次输入的x都会得到的state(final_states)，我们只要的最后的final_state</div><div class="line">    '''</div><div class="line">    def testfn(a, x):</div><div class="line">        return cell(x, a[1])</div><div class="line">    rnn_outputs, final_states = tf.scan(fn=testfn, elems=tf.transpose(rnn_inputs, [1,0,2]),</div><div class="line">                                        initializer=(tf.zeros([batch_size,state_size]),init_state)</div><div class="line">                                        )</div><div class="line">    '''或者使用lambda的方式'''</div><div class="line">    #rnn_outputs, final_states = tf.scan(lambda a,x: cell(x, a[1]), tf.transpose(rnn_inputs, [1,0,2]),</div><div class="line">                                        #initializer=(tf.zeros([batch_size, state_size]),init_state))</div><div class="line">    final_state = tuple([tf.nn.rnn_cell.LSTMStateTuple(</div><div class="line">        tf.squeeze(tf.slice(c, [num_steps-1,0,0], [1,batch_size,state_size])),</div><div class="line">        tf.squeeze(tf.slice(h, [num_steps-1,0,0], [1,batch_size,state_size]))) for c, h in final_states])</div><div class="line"></div><div class="line">    with tf.variable_scope('softmax'):</div><div class="line">        W = tf.get_variable('W', [state_size, num_classes])</div><div class="line">        b = tf.get_variable('b', [num_classes], initializer=tf.constant_initializer(0.0))</div><div class="line">    </div><div class="line">    rnn_outputs = tf.reshape(rnn_outputs, [-1, state_size])</div><div class="line">    y_reshape = tf.reshape(y, [-1])</div><div class="line">    logits = tf.matmul(rnn_outputs, W) + b</div><div class="line">    total_loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=y_reshape))</div><div class="line">    train_step = tf.train.AdamOptimizer(learning_rate).minimize(total_loss)</div><div class="line">    </div><div class="line">    return dict(x = x,</div><div class="line">                y = y,</div><div class="line">                init_state = init_state,</div><div class="line">                final_state = final_state,</div><div class="line">                total_loss = total_loss,</div><div class="line">                train_step = train_step)</div></pre></td></tr></table></figure><ul><li>运行结果<ul><li>构建计算图耗时: <code>8.685610055923462</code> （比<code>dynamic_rnn</code>稍微慢一点）</li><li>训练耗时（和<code>dynamic_rnn</code>耗时差不多）</li></ul></li><li>使用<code>scan</code>的方式只比<code>dynamic_rnn</code>慢一点点，但是对我们来说更加灵活和清楚执行的过程。也方便我们修改代码（比如从<code>state</code>的<code>t-2</code>时刻跳过一个时刻直接到<code>t</code>）</li></ul><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">epoch: <span class="number">0</span>的平均损失值：<span class="number">3.6226147892831384</span></div><div class="line">epoch: <span class="number">1</span>的平均损失值：<span class="number">3.3211338095281318</span></div><div class="line">epoch: <span class="number">2</span>的平均损失值：<span class="number">3.3158331972429123</span></div><div class="line">训练耗时： <span class="number">303.2535448074341</span></div></pre></td></tr></table></figure><h1 id="三、关于多层RNN"><a href="#三、关于多层RNN" class="headerlink" title="三、关于多层RNN"></a>三、关于多层<code>RNN</code></h1><h2 id="1、结构"><a href="#1、结构" class="headerlink" title="1、结构"></a>1、结构</h2><ul><li><code>LSTM</code>中包含两个<code>state</code>,一个是<code>c</code>记忆单元（<code>memory cell</code>），另外一个是<code>h</code>隐藏状态(<code>hidden state</code>), 在<code>Tensorflow</code>中是以<code>tuple</code>元组的形式，所以才有上面构建<code>dynamic_rnn</code>时的参数<code>state_is_tuple</code>的参数，这种方式执行更快</li><li>多层的结构如下图<br><img src="/assets/blog_images/RNN/RNN_LSTM_27.png" alt="多层RNN" title="RNN_LSTM_27"></li><li>我们可以将其<strong>包装起来</strong>, 看起来像一个<code>cell</code>一样<br><img src="/assets/blog_images/RNN/RNN_LSTM_28.png" alt="包装 cell" title="RNN_LSTM_28"></li></ul><h2 id="2、代码"><a href="#2、代码" class="headerlink" title="2、代码"></a>2、代码</h2><ul><li><code>Tensorflow</code>中的实现就是使用<code>tf.nn.rnn_cell.MultiRNNCell</code><ul><li>声明一个<code>cell</code></li><li><code>MultiRNNCell</code>中传入<code>[cell]*num_layers</code>就可以了</li><li>注意如果是<code>LSTM</code>，定义参数<code>state_is_tuple=True</code><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">cell = tf<span class="selector-class">.nn</span><span class="selector-class">.rnn_cell</span><span class="selector-class">.LSTMCell</span>(num_units=state_size, state_is_tuple=True)</div><div class="line">cell = tf<span class="selector-class">.nn</span><span class="selector-class">.rnn_cell</span><span class="selector-class">.MultiRNNCell</span>(cells=[cell]*num_layers, state_is_tuple=True)</div><div class="line">init_state = cell.zero_state(batch_size, dtype=tf.float32)</div></pre></td></tr></table></figure></li></ul></li></ul><h1 id="四、Dropout操作"><a href="#四、Dropout操作" class="headerlink" title="四、Dropout操作"></a>四、Dropout操作</h1><ul><li>应用在一层<code>cell</code>的<strong>输入和输出</strong>，<strong>不应用在循环的部分</strong></li></ul><h2 id="1、一层的cell"><a href="#1、一层的cell" class="headerlink" title="1、一层的cell"></a>1、一层的<code>cell</code></h2><ul><li><code>static_rnn</code>中实现<ul><li>声明<code>placeholder</code>：<code>keep_prob = tf.placeholder(tf.float32, name=&#39;keep_prob&#39;)</code></li><li>输入：<code>rnn_inputs = [tf.nn.dropout(rnn_input, keep_prob) for rnn_input in rnn_inputs]</code></li><li>输出：<code>rnn_outputs = [tf.nn.dropout(rnn_output, keep_prob) for rnn_output in rnn_outputs]</code></li><li><code>feed_dict</code>中加入即可：<code>feed_dict = {g[&#39;x&#39;]: X, g[&#39;y&#39;]: Y, g[&#39;keep_prob&#39;]: keep_prob}</code></li></ul></li><li><code>dynamic_rnn</code>或者<code>scan</code>中实现<ul><li>直接添加即可，其余类似：<code>rnn_inputs = tf.nn.dropout(rnn_inputed, keep_prob)</code></li></ul></li></ul><h2 id="2、多层cell"><a href="#2、多层cell" class="headerlink" title="2、多层cell"></a>2、多层<code>cell</code></h2><ul><li>我们之前说使用<code>MultiRNNCell</code>将多层<code>cell</code>看作一个<code>cell</code>, 那么怎么实现对每层<code>cell</code>使用<code>dropout</code>呢</li><li>可以使用<code>tf.nn.rnn_cell.DropoutWrapper</code>来实现</li><li><strong>方式一</strong>：<code>cell = tf.nn.rnn_cell.DropoutWrapper(cell, input_keep_prob=input_keep_prob, output_keep_prob=output_drop_prob)</code><ul><li>如果同时使用了<code>input_keep_prob</code>和<code>output_keep_prob</code>都是<code>0.9</code>, 那么层之间的<code>drop_out=0.9*0.9=0.81</code></li></ul></li><li><strong>方式二</strong>: 对于<code>basic cell</code>只使用一个<code>input_keep_prob</code>或者<code>output_keep_prob</code>，对<code>MultiRNNCell</code>也使用一个<code>input_keep_prob</code>或者<code>output_keep_prob</code></li></ul><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">cell = tf<span class="selector-class">.nn</span><span class="selector-class">.rnn_cell</span><span class="selector-class">.LSTMCell</span>(num_units=state_size, state_is_tuple=True)</div><div class="line">cell = tf<span class="selector-class">.nn</span><span class="selector-class">.rnn_cell</span><span class="selector-class">.DropoutWrapper</span>(cell, input_keep_prob=keep_prob)</div><div class="line">cell = tf<span class="selector-class">.nn</span><span class="selector-class">.rnn_cell</span><span class="selector-class">.MultiRNNCell</span>(cells=[cell]*num_layers, state_is_tuple=True)</div><div class="line">cell = tf<span class="selector-class">.nn</span><span class="selector-class">.rnn_cell</span><span class="selector-class">.DropoutWrapper</span>(cell,output_keep_prob=keep_prob)</div></pre></td></tr></table></figure><h1 id="五、层标准化-Layer-Normalization"><a href="#五、层标准化-Layer-Normalization" class="headerlink" title="五、层标准化 (Layer Normalization)"></a>五、层标准化 (<code>Layer Normalization</code>)</h1><h2 id="1、说明"><a href="#1、说明" class="headerlink" title="1、说明"></a>1、说明</h2><ul><li><code>Layer Normalization</code>是受<code>Batch Normalization</code>的启发而来，针对于<strong>RNN</strong>，可以查看<a href="https://arxiv.org/abs/1607.06450" target="_blank" rel="external">相关论文</a></li><li><code>Batch Normalization</code>主要针对于<strong>传统的深度神经网络</strong>和<strong>CNN</strong>，关于<code>Batch Normalization</code>的操作和推导可以看<a href="http://lawlite.me/2017/06/23/%E8%AE%BA%E6%96%87%E8%AE%B0%E5%BD%95-Batch-Normalization/" target="_blank" rel="external">我之前的博客</a></li><li>可以加快训练的速度，得到更好的结果等</li></ul><h2 id="2、代码-1"><a href="#2、代码-1" class="headerlink" title="2、代码"></a>2、代码</h2><ul><li>找到<code>LSTMCell</code>的源码拷贝一份修改即可</li><li><p><code>layer normalization</code>函数</p><ul><li>传入的<code>tensor</code>是二维的，对其进行<code>batch normalization</code>操作</li><li><code>tf.nn.moment</code>是计算<code>tensor</code>的<code>mean value</code>和<code>variance value</code></li><li>然后对其进行缩放(<code>scale</code>)和平移(<code>shift</code>)<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">'''layer normalization'''</div><div class="line">def ln(tensor, scope=None, epsilon=1e-5):</div><div class="line">  assert(len(tensor.get_shape()) == 2)</div><div class="line">  m, v = tf.nn.moments(tensor, [1], keep_dims=True)</div><div class="line">  if not isinstance(scope, str):</div><div class="line">    scope = ''</div><div class="line">  with tf.variable_scope(scope+'layer_norm'):</div><div class="line">    scale = tf.get_variable(name='scale', </div><div class="line">                            shape=[tensor.get_shape()[1]], </div><div class="line">                            initializer=tf.constant_initializer(1))</div><div class="line">    shift = tf.get_variable('shift',</div><div class="line">                            [tensor.get_shape()[1]],</div><div class="line">                            initializer=tf.constant_initializer(0))</div><div class="line">  LN_initial = (tensor - m) / tf.sqrt(v + epsilon)</div><div class="line">  return LN_initial*scale + shift</div></pre></td></tr></table></figure></li></ul></li><li><p><code>LSTMCell</code>中的<code>call</code>方法<code>i,j,f,o</code>调用<code>layer normalization</code>操作</p><ul><li><code>_linear</code>函数中的<code>bias</code>设为<code>False</code>， 因为<code>BN</code>会加上<code>shift</code><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">'''这里bias设置为false, 因为bn会加上shift'''</div><div class="line">lstm_matrix = _linear([inputs, m_prev], 4 * self._num_units, bias=False)</div><div class="line">i, j, f, o = array_ops.split(</div><div class="line">    value=lstm_matrix, num_or_size_splits=4, axis=1)</div><div class="line">'''执行ln'''</div><div class="line">i = ln(i, scope = 'i/')</div><div class="line">j = ln(j, scope = 'j/')</div><div class="line">f = ln(f, scope = 'f/')</div><div class="line">o = ln(o, scope = 'o/')</div></pre></td></tr></table></figure></li></ul></li><li><p>构建计算图</p><ul><li>可以选择<code>RNN GRU LSTM</code></li><li><code>Dropout</code></li><li><code>Layer Normalization</code><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div></pre></td><td class="code"><pre><div class="line">'''最终的整合模型，</div><div class="line">   - 普通RNN，GRU，LSTM</div><div class="line">   - dropout</div><div class="line">   - BN</div><div class="line">'''</div><div class="line">from LayerNormalizedLSTMCell import LayerNormalizedLSTMCell # 导入layer normalization的LSTMCell 文件</div><div class="line"></div><div class="line">def build_final_graph(</div><div class="line">    cell_type = None,</div><div class="line">    state_size = state_size,</div><div class="line">    num_classes = num_classes,</div><div class="line">    batch_size = batch_size,</div><div class="line">    num_steps = num_steps,</div><div class="line">    num_layers = 3,</div><div class="line">    build_with_dropout = False,</div><div class="line">    learning_rate = learning_rate):</div><div class="line">    </div><div class="line">    reset_graph()</div><div class="line">    x = tf.placeholder(tf.int32, [batch_size, num_steps], name='x')</div><div class="line">    y = tf.placeholder(tf.int32, [batch_size, num_steps], name='y')</div><div class="line">    keep_prob = tf.placeholder(tf.float32, name='keep_prob')</div><div class="line">    embeddings = tf.get_variable('embedding_matrix', [num_classes, state_size])</div><div class="line">    rnn_inputs = tf.nn.embedding_lookup(embeddings, x)</div><div class="line">    if cell_type == 'GRU':</div><div class="line">        cell = tf.nn.rnn_cell.GRUCell(state_size)</div><div class="line">    elif cell_type == 'LSTM':</div><div class="line">        cell = tf.nn.rnn_cell.LSTMCell(state_size, state_is_tuple=True)</div><div class="line">    elif cell_type == 'LN_LSTM':</div><div class="line">        cell = LayerNormalizedLSTMCell(state_size)  # 自己修改的代码，导入对应的文件</div><div class="line">    else:</div><div class="line">        cell = tf.nn.rnn_cell.BasicRNNCell(state_size)</div><div class="line">    if build_with_dropout:</div><div class="line">        cell = tf.nn.rnn_cell.DropoutWrapper(cell, input_keep_prob=keep_prob)</div><div class="line">        </div><div class="line">    init_state = cell.zero_state(batch_size, tf.float32)</div><div class="line">    '''dynamic_rnn'''</div><div class="line">    rnn_outputs, final_state = tf.nn.dynamic_rnn(cell, rnn_inputs, initial_state=init_state)</div><div class="line">    with tf.variable_scope('softmax'):</div><div class="line">        W = tf.get_variable('W', [state_size, num_classes])</div><div class="line">        b = tf.get_variable('b', [num_classes], initializer=tf.constant_initializer(0.0))</div><div class="line">    rnn_outputs = tf.reshape(rnn_outputs, [-1, state_size])</div><div class="line">    y_reshaped = tf.reshape(y, [-1])</div><div class="line">    logits = tf.matmul(rnn_outputs, W) + b</div><div class="line"></div><div class="line">    predictions = tf.nn.softmax(logits)</div><div class="line"></div><div class="line">    total_loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=y_reshaped))</div><div class="line">    train_step = tf.train.AdamOptimizer(learning_rate).minimize(total_loss)</div><div class="line"></div><div class="line">    return dict(</div><div class="line">        x = x,</div><div class="line">        y = y,</div><div class="line">        keep_prob = keep_prob,</div><div class="line">        init_state = init_state,</div><div class="line">        final_state = final_state,</div><div class="line">        total_loss = total_loss,</div><div class="line">        train_step = train_step,</div><div class="line">        preds = predictions,</div><div class="line">        saver = tf.train.Saver()</div><div class="line">    )</div></pre></td></tr></table></figure></li></ul></li></ul><h1 id="六、生成文本"><a href="#六、生成文本" class="headerlink" title="六、生成文本"></a>六、生成文本</h1><h2 id="1、说明-1"><a href="#1、说明-1" class="headerlink" title="1、说明"></a>1、说明</h2><ul><li>训练完成之后将计算图保存到本地磁盘，下次直接读取就可以了</li><li>我们给出<strong>第一个字符</strong>，<code>RNN</code>接着一个个生成字符，每次都是<strong>根据前一个字符</strong><ul><li>所以<code>num_steps=1</code>, <code>batch_size=1</code>（可以想象生成<code>prediction</code>的<code>shape</code>是<code>(1, num_classes)</code>中选择一个概率,–&gt; <code>num_steps=1</code> ）</li></ul></li></ul><h2 id="2、代码-2"><a href="#2、代码-2" class="headerlink" title="2、代码"></a>2、代码</h2><ul><li>构建图（直接传入参数即可）：<code>g = build_final_graph(cell_type=&#39;LN_LSTM&#39;, num_steps=1, batch_size=1)</code></li><li><p>生成文本</p><ul><li>读取训练好的文件</li><li>得到给出的第一个字符对应的数字</li><li>循环遍历要生成多少个字符， 每次循环生成一个字符<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line">'''生成文本'''</div><div class="line">def generate_characters(g, checkpoint, num_chars, prompt='A', pick_top_chars=None):</div><div class="line">    with tf.Session() as sess:</div><div class="line">        sess.run(tf.global_variables_initializer())</div><div class="line">        g['saver'].restore(sess, checkpoint)   # 读取文件</div><div class="line">        state = None</div><div class="line">        current_char = vocab_to_idx[prompt]    # 得到给出的字母对应的数字</div><div class="line">        chars = [current_char]                          </div><div class="line">        for i in range(num_chars):             # 总共生成多少数字</div><div class="line">            if state is not None:              # 第一次state为None,因为计算图中定义了刚开始为0</div><div class="line">                feed_dict=&#123;g['x']: [[current_char]], g['init_state']: state&#125; # 传入当前字符</div><div class="line">            else:</div><div class="line">                feed_dict=&#123;g['x']: [[current_char]]&#125;</div><div class="line">            preds, state = sess.run([g['preds'],g['final_state']], feed_dict)   # 得到预测结果（概率）preds的shape就是（1，num_classes）</div><div class="line">            if pick_top_chars is not None:              # 如果设置了概率较大的前多少个</div><div class="line">                p = np.squeeze(preds)</div><div class="line">                p[np.argsort(p)[:-pick_top_chars]] = 0  # 其余的置为0</div><div class="line">                p = p / np.sum(p)                       # 因为下面np.random.choice函数p的概率和要求是1，处理一下</div><div class="line">                current_char = np.random.choice(vocab_size, 1, p=p)[0]    # 根据概率选择一个</div><div class="line">            else:</div><div class="line">                current_char = np.random.choice(vocab_size, 1, p=np.squeeze(preds))[0]</div><div class="line"></div><div class="line">            chars.append(current_char)</div><div class="line">    chars = map(lambda x: idx_to_vocab[x], chars)</div><div class="line">    result = "".join(chars)</div><div class="line">    print(result)</div><div class="line">    return result</div></pre></td></tr></table></figure></li></ul></li><li><p>结果</p><ul><li>由于训练耗时很长，这里使用<code>LSTM</code>训练了<code>30</code>个<code>epoch</code>，结果如下</li><li>可以自己调整参数，可能会得到更好的结果<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div></pre></td><td class="code"><pre><div class="line">ANK</div><div class="line">O: HFOFMFRone s the statlighte thithe thit.</div><div class="line"></div><div class="line">BODEN --</div><div class="line"></div><div class="line">I I's a tomir.</div><div class="line">I't</div><div class="line">shis and on ar tald the theand this he sile be cares hat s ond tho fo hour he singe sime shind and somante tat ond treang tatsing of the an the to to fook.. Ir ard the with ane she stale..</div><div class="line">ANTE --</div><div class="line"></div><div class="line">KINE</div><div class="line">Show the ard and a beat the weringe be thing or.</div><div class="line"></div><div class="line">Bo hith tho he melan to the mute steres.</div><div class="line"></div><div class="line">The singer stis ard stis.</div><div class="line"></div><div class="line">BACE CANKONS CORE</div><div class="line">Sard the sids ing tho the the sackes tom the</div><div class="line"></div><div class="line">IN</div><div class="line">We stoe shit a dome thor</div><div class="line"></div><div class="line">ate seomser hith.</div><div class="line"></div><div class="line">That</div><div class="line">thow ound</div><div class="line"></div><div class="line"></div><div class="line">TANTONT. SEAT THONTITE SERTI                         1  23</div><div class="line">SHe the mathe a tomoner</div><div class="line">ind is ingit ofres treacentit. Sher stard on this the tor an the candin he whor he sath heres and</div><div class="line">stha dortour tit thas stand. I'd and or a</div></pre></td></tr></table></figure></li></ul></li><li><p><strong>#2017/06/25</strong> 运行结果更新</p><ul><li>更换了一个大点的数据集，<a href="https://gist.githubusercontent.com/spitis/59bfafe6966bfe60cc206ffbb760269f/raw/030a08754aada17cef14eed6fac7797cda830fe8/variousscripts.txt" target="_blank" rel="external">点击查看</a>，使用了<code>layer normalized</code>的<code>LSTM</code>模型</li><li>参数设置：<ul><li><code>num_steps=80</code>            </li><li><code>batch_size=50</code></li><li><code>state_size=512</code>            </li><li><code>num_classes = vocab_size</code></li><li><code>learning_rate = 5e-4</code></li><li><code>30</code>个<code>epochs</code></li></ul></li><li>在实验室电脑跑了一晚上，结果是不是好一点了<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div></pre></td><td class="code"><pre><div class="line">AKTIN:  Yousa hand it have to turn you, sir.</div><div class="line">I have. I've got to here hard on my</div><div class="line">play as a space state, and why he</div><div class="line">happened. What we alwaws who</div><div class="line">this?</div><div class="line"></div><div class="line">JOCASTAND :</div><div class="line"></div><div class="line">PADM </div><div class="line">You, sir!</div><div class="line"></div><div class="line">A battle. An arm of the ship is still.</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line">THE WINDEN'S CORUS</div><div class="line"></div><div class="line">Han's laser guns at the forest fire.  The crowd spots his black</div><div class="line">folkwark and sees the bedroom and twists and sees Leia</div><div class="line">who is shaking.  A huge creature has a long time,</div><div class="line">hold her hand and his timmed, that we see the saulyand.  The</div><div class="line">crowd ruised by the staircase.</div><div class="line"></div><div class="line">EXT. MAZ' CASTLE RUINS - DAY</div><div class="line"></div><div class="line">Rey and Wicket and CAMERA is heard.   Here as so they helf</div><div class="line">this tonight, he spins and sit in a startled bright.</div><div class="line"></div><div class="line">LUKE</div><div class="line">(into propecy)</div><div class="line">The defenstity! Thank you.</div><div class="line"></div><div class="line">LUKE</div><div class="line">I'm afraid to have a lossing live,</div><div class="line">or help. We're</div></pre></td></tr></table></figure></li></ul></li></ul><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul><li><a href="https://r2rt.com/recurrent-neural-networks-in-tensorflow-ii.html" target="_blank" rel="external">https://r2rt.com/recurrent-neural-networks-in-tensorflow-ii.html</a></li><li><a href="https://karpathy.github.io/2015/05/21/rnn-effectiveness/" target="_blank" rel="external">https://karpathy.github.io/2015/05/21/rnn-effectiveness/</a></li><li><a href="http://jmlr.org/proceedings/papers/v37/ioffe15.pdf" target="_blank" rel="external">http://jmlr.org/proceedings/papers/v37/ioffe15.pdf</a></li><li><code>tensorflow scan</code>：<ul><li><a href="https://www.tensorflow.org/api_docs/python/tf/scan" target="_blank" rel="external">https://www.tensorflow.org/api_docs/python/tf/scan</a></li><li><a href="https://www.youtube.com/watch?v=A6qJMB3stE4&amp;t=621s" target="_blank" rel="external">https://www.youtube.com/watch?v=A6qJMB3stE4&amp;t=621s</a></li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;ul&gt;
&lt;li&gt;全部代码：&lt;a href=&quot;https://github.com/lawlite19/Blog-Back-Up/tree/master/code/rnn/rnn_tensorflow&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;点击这里查看&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;关于&lt;code&gt;Tensorflow&lt;/code&gt;实现一个简单的&lt;strong&gt;二元序列的例子&lt;/strong&gt;可以&lt;a href=&quot;http://lawlite.me/2017/06/16/RNN-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-02Tensorflow%E4%B8%AD%E7%9A%84%E5%AE%9E%E7%8E%B0/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;点击这里查看&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;关于&lt;code&gt;RNN&lt;/code&gt;和&lt;code&gt;LSTM&lt;/code&gt;的基础可以&lt;a href=&quot;http://lawlite.me/2017/06/14/RNN-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8CLSTM-01%E5%9F%BA%E7%A1%80/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;查看这里&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;这篇博客主要包含以下内容&lt;ul&gt;
&lt;li&gt;训练一个&lt;code&gt;RNN&lt;/code&gt;模型&lt;strong&gt;逐字符生成文本数据&lt;/strong&gt;(最后的部分)&lt;/li&gt;
&lt;li&gt;使用&lt;code&gt;Tensorflow&lt;/code&gt;的&lt;code&gt;scan&lt;/code&gt;函数实现&lt;code&gt;dynamic_rnn&lt;/code&gt;&lt;strong&gt;动态创建&lt;/strong&gt;的效果&lt;/li&gt;
&lt;li&gt;使用&lt;code&gt;multiple RNN&lt;/code&gt;创建&lt;strong&gt;多层&lt;/strong&gt;的&lt;code&gt;RNN&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;实现&lt;code&gt;Dropout&lt;/code&gt;和&lt;code&gt;Layer Normalization&lt;/code&gt;的功能&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="Python" scheme="http://lawlite.cn/tags/Python/"/>
    
      <category term="DeepLearning" scheme="http://lawlite.cn/tags/DeepLearning/"/>
    
      <category term="RNN" scheme="http://lawlite.cn/tags/RNN/"/>
    
      <category term="LSTM" scheme="http://lawlite.cn/tags/LSTM/"/>
    
  </entry>
  
  <entry>
    <title>RNN-循环神经网络-02Tensorflow中的实现</title>
    <link href="http://lawlite.cn/2017/06/16/RNN-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-02Tensorflow%E4%B8%AD%E7%9A%84%E5%AE%9E%E7%8E%B0/"/>
    <id>http://lawlite.cn/2017/06/16/RNN-循环神经网络-02Tensorflow中的实现/</id>
    <published>2017-06-16T12:59:28.000Z</published>
    <updated>2017-06-25T08:49:26.000Z</updated>
    
    <content type="html"><![CDATA[<ul><li>关于基本的<code>RNN</code>和<code>LSTM</code>的概念和<code>BPTT</code>算法可以<a href="http://lawlite.me/2017/06/14/RNN-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8CLSTM-01%E5%9F%BA%E7%A1%80/" target="_blank" rel="external">查看这里</a></li><li>参考文章：<ul><li><a href="https://r2rt.com/recurrent-neural-networks-in-tensorflow-i.html" target="_blank" rel="external">https://r2rt.com/recurrent-neural-networks-in-tensorflow-i.html</a></li><li><a href="https://r2rt.com/styles-of-truncated-backpropagation.html" target="_blank" rel="external">https://r2rt.com/styles-of-truncated-backpropagation.html</a></li></ul></li></ul><h1 id="一、源代码实现一个binary例子"><a href="#一、源代码实现一个binary例子" class="headerlink" title="一、源代码实现一个binary例子"></a>一、源代码实现一个<code>binary</code>例子</h1><h2 id="1、例子描述"><a href="#1、例子描述" class="headerlink" title="1、例子描述"></a>1、例子描述</h2><h3 id="1-数据描述"><a href="#1-数据描述" class="headerlink" title="(1) 数据描述"></a>(1) 数据描述</h3><ul><li>输入数据<code>X</code>是<strong>二进制的一串序列</strong>, 在<code>t</code>时刻，有<code>50%</code>的概率是<code>1</code>，<code>50%</code>的概率是<code>0</code>，比如：<code>X=[1,1,0,0,1,0.....]</code> </li></ul><a id="more"></a><ul><li>输出数据<code>Y</code>：<ul><li>在时刻<code>t</code>，<code>50%</code>的概率是<code>1</code>，<code>50%</code>的概率是<code>0</code>；</li><li>如果$X_{t-3}$是<code>1</code>，则$Y_{t}$ <code>100%</code>是<code>1</code>（增加<code>50%</code>）；</li><li>如果$X_{t-8}$是<code>1</code>，则$Y_{t}$ <code>25%</code>是<code>1</code>（减少<code>25%</code>）；<ul><li>所以如果$X_{t-3}$和$X_{t-8}$都是<code>1</code>，则$Y_{t}$ <code>50%+50%-25%=75%</code>的概率是<code>1</code></li></ul></li></ul></li><li>所以，输出数据是有两个<strong>依赖关系</strong>的</li></ul><h3 id="2-损失函数"><a href="#2-损失函数" class="headerlink" title="(2) 损失函数"></a>(2) 损失函数</h3><ul><li>使用<code>cross-entropy</code>损失函数进行训练</li><li>这里例子很简单，<strong>根据数据生成的规则</strong>，我们可以简单的计算一下不同情况下的<code>cross-entropy</code>值</li><li><strong>[1]</strong> 如果<code>rnn</code><strong>没有学到两个依赖关系</strong>, 则最终预测正确的概率是<code>62.5%</code>，<code>cross entropy</code>值为<strong>0.66</strong>计算如下<ul><li>${X_{t-3}}=\{ {\matrix{<br>{1} \rightarrow X_{t-8}=\{  {\matrix{<br>{ 1 \rightarrow 0.5+0.5-0.25=0.75} \cr<br>{ 0 \rightarrow 0.5+0.5=1 \quad \quad \quad \quad} \cr<br>}} \cr<br>{0} \rightarrow X_{t-8} = \{ {\matrix{<br>{1 \rightarrow 0.5-0.25=0.25 } \quad \quad\cr<br>{0 \rightarrow 0.5 \quad \quad \quad \quad \quad \quad \quad \quad} \cr<br>}}\cr<br>}}$</li><li>所以正确预测<code>1</code>的概率为：<code>(0.75+1+0.25+0.5)/4=0.625</code></li><li>所以<code>cross entropy</code>值为：<code>-[plog(p)+(1-p)log(1-p)]=0.66</code></li></ul></li><li><strong>[2]</strong> 如果<code>rnn</code>学到<strong>第一个</strong>依赖关系，<code>50%</code>的情况下预测准确度为<code>87.5%</code>，<code>50%</code>的情况下预测准确度为<code>62.5%</code>，<code>cross entropy</code>值为<strong>0.52</strong><ul><li>因为<code>X</code>是随机生成，<code>0/1</code>各占<code>50%</code>,想象生成了很多的数，根据<strong>大数定律</strong>，<code>50%</code>的情况是<code>1</code>，对应到 <strong>[1]</strong> 中的<strong>上面</strong>的情况就是:<code>(0.75+1)/2=0.875</code>的概率预测正确，其余的<code>50%</code>就和<strong>[1]</strong>中一样了（去除学到的一个依赖，其余就是没有学到依赖）<code>62.5%</code></li><li>损失值：<code>-0.5 * (0.875 * .log(0.875) + 0.125 * log(0.125))-0.5 * (0.625 * np.log(0.625) + 0.375 * log(0.375)))=0.52</code></li></ul></li><li><strong>[3]</strong> 如果<code>rnn</code>两个依赖都学到了，则<code>25%</code>的情况下<code>100%</code>预测正确，<code>25%</code>的情况下<code>50%</code>预测正确，<code>50%</code>的情况向<code>75%</code>预测正确，<code>cross entropy</code>值为<strong>0.45</strong><ul><li><code>1/4</code>的情况就是$X_{t-3}=1 和 X_{t-8}=0$ <code>100%</code>预测正确 </li><li><code>1/4</code>的情况就是$X_{t-3}=0 和 X_{t-8}=0$ <code>50%</code>预测正确</li><li><code>1/2</code>的情况<code>75%</code>预测正确（0.5+0.5-0.25）</li><li>损失值：<code>-0.50 * (0.75 * np.log(0.75) + 0.25 * np.log(0.25)) - 0.25 * (2 * 0.50 * np.log (0.50)) - 0.25 * (0) = 0.45</code></li></ul></li></ul><h2 id="2、网络结构"><a href="#2、网络结构" class="headerlink" title="2、网络结构"></a>2、网络结构</h2><ul><li>根据时刻<code>t</code>的输入向量$X_t$和时刻<code>t-1</code>的状态向量<code>state</code> $S_{t-1}$计算得出当前的状态向量$S_t$和输出的结果概率向量$P_t$</li><li>Label数据是<code>Y</code></li><li><p>所以有：$${S_t} = {tanh(W(X_t \bigoplus S_{t-1})) + b_s}$$ $${P_t = softmax(US_t + b_p)}$$</p><ul><li>这里$\bigoplus$表示<strong>向量的拼接</strong></li><li><p>$W \in R^{d \times (2+d)}, {b_s} \in R^d , U \in R^{2 \times d}, b_p \in R^2$ </p><ul><li><code>d</code>是 <code>state</code> 向量的长度</li><li>W是二维的矩阵，因为是将$X_t 和 S_{t-1}$拼接起来和<strong>W</strong>运算的，<code>2</code>对应输入的<strong>X</strong> <code>one-hot</code>之后，所以是<code>2</code></li><li><code>U</code>是最后输出预测的权值</li></ul></li><li><p>初始化<code>state</code> $S_{-1}$ 为<strong>0向量</strong></p></li></ul></li></ul><p><img src="/assets/blog_images/RNN/RNN_11.png" alt="RNN结构" title="RNN_11"></p><ul><li>需要注意的是 <code>cell</code> 并<strong>不一定是只有一个neuron unit，而是有n个hidden units</strong><ul><li>下图的<code>state size=4</code></li></ul></li></ul><p><img src="/assets/blog_images/RNN/RNN_13.png" alt="RNN详细的结构" title="RNN_13"></p><h2 id="3、Tensorflow中RNN-BPTT实现方式"><a href="#3、Tensorflow中RNN-BPTT实现方式" class="headerlink" title="3、Tensorflow中RNN BPTT实现方式"></a>3、Tensorflow中RNN BPTT实现方式</h2><h4 id="1-截断反向传播（truncated-backpropagation）"><a href="#1-截断反向传播（truncated-backpropagation）" class="headerlink" title="1) 截断反向传播（truncated backpropagation）"></a>1) 截断反向传播（truncated backpropagation）</h4><ul><li>假设我们训练含有<code>1000000</code>个数据的序列，如果<strong>全部训练</strong>的话，整个的序列都feed进<code>RNN</code>中，容易造成梯度消失或爆炸的问题</li><li>所以解决的方法就是<code>truncated backpropagation</code>，我们将序列<strong>截断</strong>来进行训练(<code>num_steps</code>)</li></ul><h3 id="2-tensorflow中的BPTT算法实现"><a href="#2-tensorflow中的BPTT算法实现" class="headerlink" title="2) tensorflow中的BPTT算法实现"></a>2) tensorflow中的BPTT算法实现</h3><ul><li>一般截断的反向传播是：在当前时间<code>t</code>,往前反向传播<code>num_steps</code>步即可<ul><li>如下图，长度为<code>6</code>的序列，截断步数是<code>3</code></li></ul></li></ul><p><img src="/assets/blog_images/RNN/RNN_truncated_bptt_16.png" alt="一般的截断反向传播" title="RNN_truncated_bptt_16"></p><ul><li>但是<code>Tensorflow</code>中的实现并不是这样(如下图)<ul><li>它是将长度为<code>6</code>的序列分为了<strong>两部分</strong>，每一部分长度为<code>3</code></li><li>前一部分计算得到的<code>final state</code>用于下一部分计算的<code>initial state</code></li></ul></li></ul><p><img src="/assets/blog_images/RNN/RNN_truncated_bptt_tensorflow_style17.png" alt="tensorflow 风格的bptt" title="RNN_truncated_bptt_tensorflow_style17"></p><ul><li>所以<code>tensorflow</code>风格的反向传播并没有有效的反向传播<code>num_steps</code>步(对比一般的方式，依赖关系变的弱一些)<ul><li>所以比如想要学习有<code>8</code>依赖关系的序列（我们的例子中就是），<strong>一般要设置的大于8</strong></li></ul></li><li>另外，有人做实验比较了两种方式<a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.56.7941&amp;rep=rep1&amp;type=pdf" target="_blank" rel="external">here</a>，发现一般的实现方式中的<code>n</code>步和<code>Tensorflow</code>中截断设置为<code>2n</code>的<strong>结果相似</strong></li></ul><h3 id="3-关于这个例子，tensorflow风格的实现"><a href="#3-关于这个例子，tensorflow风格的实现" class="headerlink" title="3) 关于这个例子，tensorflow风格的实现"></a>3) 关于这个例子，tensorflow风格的实现</h3><ul><li>如下图，<code>num_steps=5, state_size=4</code>，就是<strong>截断反向传播的步数</strong>truncated backprop steps是<code>5</code>步，<code>state_size</code>就是<code>cell</code>中的神经元的个数</li><li>如果需要截断的步数增多，可以适当增加<code>state_size</code>来记录更多的信息<ul><li>好比传统的神经网络，就是增加隐藏层的神经元个数</li></ul></li><li>途中的注释是下面的列子代码中定义变量的<code>shape</code>, 可以对照参考</li></ul><p><img src="/assets/blog_images/RNN/RNN_14.jpg" alt="例子，num_steps=5,state_size=4" title="RNN_14"></p><h2 id="4、自己实现例子中的RNN"><a href="#4、自己实现例子中的RNN" class="headerlink" title="4、自己实现例子中的RNN"></a>4、自己实现例子中的<code>RNN</code></h2><ul><li>全部代码：<a href="https://github.com/lawlite19/Blog-Back-Up/blob/master/code/rnn/rnn_implement.py" target="_blank" rel="external">https://github.com/lawlite19/Blog-Back-Up/blob/master/code/rnn/rnn_implement.py</a></li></ul><h3 id="1-实现过程"><a href="#1-实现过程" class="headerlink" title="1) 实现过程"></a>1) 实现过程</h3><ul><li>导入包：</li></ul><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">import numpy as np</div><div class="line">import tensorflow as tf</div><div class="line">from tensorflow<span class="selector-class">.python</span> import debug as tf_debug</div><div class="line">import matplotlib<span class="selector-class">.pyplot</span> as plt</div></pre></td></tr></table></figure><ul><li><p>超参数</p><ul><li>这里<code>num_steps=5</code>就是只能记忆<strong>5步</strong>, 所以只能学习到一个依赖(因为至少8步才能学到第二个依赖)，我们看结果最后的<code>cross entropy</code>是否在<code>0.52</code>左右<figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="string">''</span><span class="string">'超参数'</span><span class="string">''</span></div><div class="line">num_steps = <span class="number">5</span></div><div class="line">batch_size = <span class="number">200</span></div><div class="line">num_classes = <span class="number">2</span></div><div class="line">state_size = <span class="number">4</span></div><div class="line">learning_rate = <span class="number">0.1</span></div></pre></td></tr></table></figure></li></ul></li><li><p>生成数据</p><ul><li>就是按照我们描述的规则</li></ul></li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line">'''生成数据</div><div class="line">就是按照文章中提到的规则，这里生成1000000个</div><div class="line">'''</div><div class="line">def gen_data(size=1000000):</div><div class="line">    X = np.array(np.random.choice(2, size=(size,)))</div><div class="line">    Y = []</div><div class="line">    '''根据规则生成Y'''</div><div class="line">    for i in range(size):   </div><div class="line">        threshold = 0.5</div><div class="line">        if X[i-3] == 1:</div><div class="line">            threshold += 0.5</div><div class="line">        if X[i-8] == 1:</div><div class="line">            threshold -=0.25</div><div class="line">        if np.random.rand() &gt; threshold:</div><div class="line">            Y.append(0)</div><div class="line">        else:</div><div class="line">            Y.append(1)</div><div class="line">    return X, np.array(Y)</div></pre></td></tr></table></figure><ul><li>生成<code>batch</code>数据，因为我们使用<code>sgd</code>训练</li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line">'''生成batch数据'''</div><div class="line">def gen_batch(raw_data, batch_size, num_step):</div><div class="line">    raw_x, raw_y = raw_data</div><div class="line">    data_length = len(raw_x)</div><div class="line">    batch_patition_length = data_length // batch_size                         # -&gt;5000</div><div class="line">    data_x = np.zeros([batch_size, batch_patition_length], dtype=np.int32)    # -&gt;(200, 5000)</div><div class="line">    data_y = np.zeros([batch_size, batch_patition_length], dtype=np.int32)    # -&gt;(200, 5000)</div><div class="line">    '''填到矩阵的对应位置'''</div><div class="line">    for i in range(batch_size):</div><div class="line">        data_x[i] = raw_x[batch_patition_length*i:batch_patition_length*(i+1)]# 每一行取batch_patition_length个数，即5000</div><div class="line">        data_y[i] = raw_y[batch_patition_length*i:batch_patition_length*(i+1)]</div><div class="line">    epoch_size = batch_patition_length // num_steps                           # -&gt;5000/5=1000 就是每一轮的大小</div><div class="line">    for i in range(epoch_size):   # 抽取 epoch_size 个数据</div><div class="line">        x = data_x[:, i * num_steps:(i + 1) * num_steps]                      # -&gt;(200, 5)</div><div class="line">        y = data_y[:, i * num_steps:(i + 1) * num_steps]</div><div class="line">        yield (x, y)    # yield 是生成器，生成器函数在生成值后会自动挂起并暂停他们的执行和状态（最后就是for循环结束后的结果，共有1000个(x, y)）</div><div class="line">def gen_epochs(n, num_steps):</div><div class="line">    for i in range(n):</div><div class="line">        yield gen_batch(gen_data(), batch_size, num_steps)</div></pre></td></tr></table></figure><ul><li><p>定义RNN的输入</p><ul><li>这里每个数需要<code>one-hot</code>处理</li><li><code>unstack</code>方法就是将<code>n</code>维的数据拆成若开个<code>n-1</code>的数据，<code>axis</code>指定根据哪个维度拆的，比如<code>(200,5,2)</code>三维数据，按<code>axis=1</code>会有<code>5</code>个<code>(200,2)</code>的二维数据<figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="string">''</span><span class="string">'定义placeholder'</span><span class="string">''</span></div><div class="line">x = tf.placeholder(tf<span class="selector-class">.int32</span>, [batch_size, num_steps], name=<span class="string">"x"</span>)</div><div class="line">y = tf.placeholder(tf<span class="selector-class">.int32</span>, [batch_size, num_steps], name=<span class="string">'y'</span>)</div><div class="line">init_state = tf.zeros([batch_size, state_size])</div><div class="line"><span class="string">''</span><span class="string">'RNN输入'</span><span class="string">''</span></div><div class="line">x_one_hot = tf.one_hot(x, num_classes)</div><div class="line">rnn_inputs = tf.unstack(x_one_hot, axis=<span class="number">1</span>)</div></pre></td></tr></table></figure></li></ul></li><li><p>定义<code>RNN</code>的<code>cell</code>（<strong>关键步骤</strong>）</p><ul><li>这里关于<code>name_scope</code>和<code>variable_scope</code>的用法可以<a href="https://morvanzhou.github.io/tutorials/machine-learning/tensorflow/5-12-scope/" target="_blank" rel="external">查看这里</a><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">'''定义RNN cell'''</div><div class="line">with tf.variable_scope('rnn_cell'):</div><div class="line">    W = tf.get_variable('W', [num_classes + state_size, state_size])</div><div class="line">    b = tf.get_variable('b', [state_size], initializer=tf.constant_initializer(0.0))</div><div class="line">    </div><div class="line">def rnn_cell(rnn_input, state):</div><div class="line">    with tf.variable_scope('rnn_cell', reuse=True):</div><div class="line">        W = tf.get_variable('W', [num_classes+state_size, state_size])</div><div class="line">        b = tf.get_variable('b', [state_size], initializer=tf.constant_initializer(0.0))</div><div class="line">    return tf.tanh(tf.matmul(tf.concat([rnn_input, state],1),W) + b)</div></pre></td></tr></table></figure></li></ul></li><li><p>将<code>cell</code>添加到计算图中</p></li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">'''将rnn cell添加到计算图中'''</div><div class="line">state = init_state</div><div class="line">rnn_outputs = []</div><div class="line">for rnn_input in rnn_inputs:</div><div class="line">    state = rnn_cell(rnn_input, state)  # state会重复使用，循环</div><div class="line">    rnn_outputs.append(state)</div><div class="line">final_state = rnn_outputs[-1]        # 得到最后的state</div></pre></td></tr></table></figure><ul><li><p>定义预测，损失函数，和优化方法</p><ul><li><code>sparse_softmax_cross_entropy_with_logits</code>会自动<code>one-hot</code><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="string">''</span><span class="string">'预测，损失，优化'</span><span class="string">''</span></div><div class="line">with tf.variable_scope(<span class="string">'softmax'</span>):</div><div class="line">    W = tf.get_variable(<span class="string">'W'</span>, [state_size, num_classes])</div><div class="line">    <span class="selector-tag">b</span> = tf.get_variable(<span class="string">'b'</span>, [num_classes], initializer=tf.constant_initializer(<span class="number">0.0</span>))</div><div class="line">logits = [tf.matmul(rnn_output, W) + <span class="selector-tag">b</span> <span class="keyword">for</span> rnn_output <span class="keyword">in</span> rnn_outputs]</div><div class="line">predictions = [tf<span class="selector-class">.nn</span><span class="selector-class">.softmax</span>(logit) <span class="keyword">for</span> logit <span class="keyword">in</span> logits]</div><div class="line"></div><div class="line">y_as_list = tf.unstack(y, num=num_steps, axis=<span class="number">1</span>)</div><div class="line">losses = [tf<span class="selector-class">.nn</span><span class="selector-class">.sparse_softmax_cross_entropy_with_logits</span>(labels=<span class="selector-tag">label</span>,logits=logit) <span class="keyword">for</span> logit, <span class="selector-tag">label</span> <span class="keyword">in</span> zip(logits, y_as_list)]</div><div class="line">total_loss = tf.reduce_mean(losses)</div><div class="line">train_step = tf<span class="selector-class">.train</span><span class="selector-class">.AdagradOptimizer</span>(learning_rate).minimize(total_loss)</div></pre></td></tr></table></figure></li></ul></li><li><p>训练网络</p></li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line">'''训练网络'''</div><div class="line">def train_rnn(num_epochs, num_steps, state_size=4, verbose=True):</div><div class="line">    with tf.Session() as sess:</div><div class="line">        sess.run(tf.global_variables_initializer())</div><div class="line">        #sess = tf_debug.LocalCLIDebugWrapperSession(sess)</div><div class="line">        training_losses = []</div><div class="line">        for idx, epoch in enumerate(gen_epochs(num_epochs, num_steps)):</div><div class="line">            training_loss = 0</div><div class="line">            training_state = np.zeros((batch_size, state_size))   # -&gt;(200, 4)</div><div class="line">            if verbose:</div><div class="line">                print('\nepoch', idx)</div><div class="line">            for step, (X, Y) in enumerate(epoch):</div><div class="line">                tr_losses, training_loss_, training_state, _ = \</div><div class="line">                    sess.run([losses, total_loss, final_state, train_step], feed_dict=&#123;x:X, y:Y, init_state:training_state&#125;)</div><div class="line">                training_loss += training_loss_</div><div class="line">                if step % 100 == 0 and step &gt; 0:</div><div class="line">                    if verbose:</div><div class="line">                        print('第 &#123;0&#125; 步的平均损失 &#123;1&#125;'.format(step, training_loss/100))</div><div class="line">                    training_losses.append(training_loss/100)</div><div class="line">                    training_loss = 0</div><div class="line">    return training_losses</div></pre></td></tr></table></figure><ul><li>显示结果</li></ul><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">training_losses = train_rnn(num_epochs=<span class="number">1</span>, num_steps=num_steps, state_size=state_size)</div><div class="line"><span class="function"><span class="title">print</span><span class="params">(training_losses[<span class="number">0</span>])</span></span></div><div class="line">plt.plot(training_losses)</div><div class="line">plt.show()</div></pre></td></tr></table></figure><h3 id="2-实验结果"><a href="#2-实验结果" class="headerlink" title="2) 实验结果"></a>2) 实验结果</h3><ul><li><code>num_steps=5, state=4</code><ul><li>可以看到初试的损失值大约<code>0.66</code>, 最后学到一个依赖关系，最终损失值<code>0.52</code>左右</li></ul></li></ul><p><img src="/assets/blog_images/RNN/RNN_14.png" alt="num_step=5结果，只学到一个依赖" title="RNN_14"></p><ul><li><code>num_step=10, state=16</code><ul><li>学到了两个依赖，最终损失值接近<code>0.45</code></li></ul></li></ul><p><img src="/assets/blog_images/RNN/RNN_15.png" alt="num_step=10,学到两个依赖" title="RNN_15"></p><h2 id="5、使用Tensorflow的cell实现"><a href="#5、使用Tensorflow的cell实现" class="headerlink" title="5、使用Tensorflow的cell实现"></a>5、使用Tensorflow的cell实现</h2><h3 id="1-使用static-rnn方式"><a href="#1-使用static-rnn方式" class="headerlink" title="1) 使用static rnn方式"></a>1) 使用<code>static rnn</code>方式</h3><ul><li>将我们之前自己实现的<code>cell</code>和添加到计算图中步骤改为如下即可</li></ul><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">cell = tf<span class="selector-class">.contrib</span><span class="selector-class">.rnn</span><span class="selector-class">.BasicRNNCell</span>(num_units=state_size)</div><div class="line">rnn_outputs, final_state = tf<span class="selector-class">.contrib</span><span class="selector-class">.rnn</span><span class="selector-class">.static_rnn</span>(cell=cell, inputs=rnn_inputs, </div><div class="line">                                                    initial_state=init_state)</div></pre></td></tr></table></figure><h3 id="2-使用dynamic-rnn方式"><a href="#2-使用dynamic-rnn方式" class="headerlink" title="2) 使用dynamic_rnn方式"></a>2) 使用<code>dynamic_rnn</code>方式</h3><ul><li><p>这里仅仅替换<code>cell</code>就不行了，<code>RNN</code>输入</p><ul><li>直接就是<strong>三维</strong>的形式<figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="string">''</span><span class="string">'RNN输入'</span><span class="string">''</span></div><div class="line">rnn_inputs = tf.one_hot(x, num_classes)</div></pre></td></tr></table></figure></li></ul></li><li><p>使用<code>dynamic_rnn</code></p></li></ul><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">cell = tf<span class="selector-class">.contrib</span><span class="selector-class">.rnn</span><span class="selector-class">.BasicRNNCell</span>(num_units=state_size)</div><div class="line">rnn_outputs, final_state = tf<span class="selector-class">.nn</span><span class="selector-class">.dynamic_rnn</span>(cell, rnn_inputs, initial_state=init_state)</div></pre></td></tr></table></figure><ul><li>预测，损失<ul><li>由于<code>rnn_inputs</code>是三维的，所以先<strong>转成二维的</strong>，计算结束后再转换回三维<code>[batch_size, num_steps, num_classes]</code><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">    '''因为rnn_outputs是三维的，这里需要将其转成2维的，</div><div class="line">       矩阵运算后再转换回来[batch_size, num_steps, num_classes]'''</div><div class="line">logits = tf.reshape(tf.matmul(tf.reshape(rnn_outputs, [-1, state_size]), W) +b, \</div><div class="line">                    shape=[batch_size, num_steps, num_classes])</div><div class="line">predictions = tf.nn.softmax(logits)</div><div class="line"></div><div class="line">y_as_list = tf.unstack(y, num=num_steps, axis=1)</div><div class="line">losses = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,logits=logits)</div><div class="line">total_loss = tf.reduce_mean(losses)</div><div class="line">train_step = tf.train.AdagradOptimizer(learning_rate).minimize(total_loss)</div></pre></td></tr></table></figure></li></ul></li></ul><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul><li><a href="https://r2rt.com/recurrent-neural-networks-in-tensorflow-i.html" target="_blank" rel="external">https://r2rt.com/recurrent-neural-networks-in-tensorflow-i.html</a></li><li><a href="https://r2rt.com/styles-of-truncated-backpropagation.html" target="_blank" rel="external">https://r2rt.com/styles-of-truncated-backpropagation.html</a></li><li><a href="https://web.stanford.edu/class/psych209a/ReadingsByDate/02_25/Williams%20Zipser95RecNets.pdf" target="_blank" rel="external">https://web.stanford.edu/class/psych209a/ReadingsByDate/02_25/Williams%20Zipser95RecNets.pdf</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;ul&gt;
&lt;li&gt;关于基本的&lt;code&gt;RNN&lt;/code&gt;和&lt;code&gt;LSTM&lt;/code&gt;的概念和&lt;code&gt;BPTT&lt;/code&gt;算法可以&lt;a href=&quot;http://lawlite.me/2017/06/14/RNN-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8CLSTM-01%E5%9F%BA%E7%A1%80/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;查看这里&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;参考文章：&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://r2rt.com/recurrent-neural-networks-in-tensorflow-i.html&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://r2rt.com/recurrent-neural-networks-in-tensorflow-i.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://r2rt.com/styles-of-truncated-backpropagation.html&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://r2rt.com/styles-of-truncated-backpropagation.html&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&quot;一、源代码实现一个binary例子&quot;&gt;&lt;a href=&quot;#一、源代码实现一个binary例子&quot; class=&quot;headerlink&quot; title=&quot;一、源代码实现一个binary例子&quot;&gt;&lt;/a&gt;一、源代码实现一个&lt;code&gt;binary&lt;/code&gt;例子&lt;/h1&gt;&lt;h2 id=&quot;1、例子描述&quot;&gt;&lt;a href=&quot;#1、例子描述&quot; class=&quot;headerlink&quot; title=&quot;1、例子描述&quot;&gt;&lt;/a&gt;1、例子描述&lt;/h2&gt;&lt;h3 id=&quot;1-数据描述&quot;&gt;&lt;a href=&quot;#1-数据描述&quot; class=&quot;headerlink&quot; title=&quot;(1) 数据描述&quot;&gt;&lt;/a&gt;(1) 数据描述&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;输入数据&lt;code&gt;X&lt;/code&gt;是&lt;strong&gt;二进制的一串序列&lt;/strong&gt;, 在&lt;code&gt;t&lt;/code&gt;时刻，有&lt;code&gt;50%&lt;/code&gt;的概率是&lt;code&gt;1&lt;/code&gt;，&lt;code&gt;50%&lt;/code&gt;的概率是&lt;code&gt;0&lt;/code&gt;，比如：&lt;code&gt;X=[1,1,0,0,1,0.....]&lt;/code&gt; &lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="Python" scheme="http://lawlite.cn/tags/Python/"/>
    
      <category term="DeepLearning" scheme="http://lawlite.cn/tags/DeepLearning/"/>
    
      <category term="RNN" scheme="http://lawlite.cn/tags/RNN/"/>
    
  </entry>
  
  <entry>
    <title>RNN-循环神经网络和LSTM_01基础</title>
    <link href="http://lawlite.cn/2017/06/14/RNN-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8CLSTM-01%E5%9F%BA%E7%A1%80/"/>
    <id>http://lawlite.cn/2017/06/14/RNN-循环神经网络和LSTM-01基础/</id>
    <published>2017-06-14T15:42:32.000Z</published>
    <updated>2017-06-25T08:49:35.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一、介绍"><a href="#一、介绍" class="headerlink" title="一、介绍"></a>一、介绍</h1><h2 id="1、什么是RNN"><a href="#1、什么是RNN" class="headerlink" title="1、什么是RNN"></a>1、什么是RNN</h2><ul><li>传统的神经网络是<strong>层与层</strong>之间是<strong>全连接</strong>的，但是每层之间的神经元是没有连接的（其实是假设各个数据之间是<strong>独立的</strong>）<ul><li>这种结构不善于处理<strong>序列化的问题</strong>。比如要预测句子中的下一个单词是什么，这往往与前面的单词有很大的关联，因为句子里面的单词并不是独立的。</li></ul></li><li><code>RNN</code> 的结构说明当前的的输出与前面的输出也有关，即隐层之间的节点不再是无连接的，而是有连接的<ul><li>基本的结构如图，可以看到有个<strong>循环的结构</strong>，将其展开就是右边的结构</li></ul></li></ul><a id="more"></a><p>  <img src="/assets/blog_images/RNN/rnn_01.jpg" alt="RNN基本结构" title="rnn_01"></p><h2 id="2、运算说明"><a href="#2、运算说明" class="headerlink" title="2、运算说明"></a>2、运算说明</h2><ul><li>如上图，<strong>输入单元</strong>(<code>inputs units</code>): $\{ {x_0},{x_1}, \cdots  \cdots ,{x_t},{x_{t + 1}}, \cdots  \cdots \}$, <ul><li><strong>输出单元</strong>(output units)为：$\{ {o_0},{o_1}, \cdots  \cdots ,{o_t},{o_{t + 1}}, \cdots  \cdots \}$, </li><li><strong>隐藏单元</strong>(hidden units)输出集: $\{ {s_0},{s_1}, \cdots  \cdots ,{ost},{s_{t + 1}}, \cdots  \cdots \}$</li></ul></li><li>时间 <code>t</code> 隐层单元的输出为：${s_t} = f(U{x_t} + W{s_{t - 1}})$<ul><li><code>f</code>就是激励函数，一般是<code>sigmoid,tanh, relu</code>等</li><li>计算${s_{0}}$时，即第一个的隐藏层状态，需要用到${s_{-1}}$，但是其并不存在，在实现中一般置为<strong>0向量</strong></li><li>（如果将上面的竖着立起来，其实很像传统的神经网络，哈哈）</li></ul></li><li>时间 <code>t</code> 的输出为：${o_t}=Softmax(V{s_t})$<ul><li>可以认为隐藏层状态${s_t}$是<strong>网络的记忆单元</strong>. ${s_t}$包含了前面所有步的隐藏层状态。而输出层的输出${o_t}$只与当前步的${s_t}$有关。</li><li>（在实践中，为了降低网络的复杂度，往往${s_t}$只包含<strong>前面若干步</strong>而不是所有步的隐藏层状态）</li></ul></li><li>在<code>RNNs</code>中，每输入一步，每一层都<strong>共享参数</strong><code>U,V,W</code>，（因为是将循环的部分展开，天然应该相等）</li><li><code>RNNs</code>的关键之处在于<strong>隐藏层</strong>，隐藏层能够捕捉序列的信息。</li></ul><h2 id="3、应用方面"><a href="#3、应用方面" class="headerlink" title="3、应用方面"></a>3、应用方面</h2><ul><li>循环神经网络(<code>Recurrent Neural Networks，RNNs</code>)已经在众多自然语言处理(<code>Natural Language Processing, NLP</code>)中取得了巨大成功以及广泛应用。目前使用最广泛最成功的模型便是<code>LSTMs</code>(Long Short-Term Memory，长短时记忆模型)模型<h3 id="1-语言模型和文本生成"><a href="#1-语言模型和文本生成" class="headerlink" title="(1) 语言模型和文本生成"></a>(1) 语言模型和文本生成</h3></li><li>给定一个单词序列，根据前面的单词预测下面单词的可能性</li><li>也可以根据概率生成新的词</li><li>这里给出了3篇论文<ul><li><a href="http://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf" target="_blank" rel="external">Recurrent neural network based language model</a></li><li><a href="http://www.fit.vutbr.cz/research/groups/speech/publi/2011/mikolov_icassp2011_5528.pdf" target="_blank" rel="external">Extensions of Recurrent neural network based language model</a></li><li><a href="http://machinelearning.wustl.edu/mlpapers/paper_files/ICML2011Sutskever_524.pdf" target="_blank" rel="external">Generating Text with Recurrent Neural Networks</a><h3 id="2-机器翻译"><a href="#2-机器翻译" class="headerlink" title="(2) 机器翻译"></a>(2) 机器翻译</h3></li></ul></li><li>和上面的语言模型很像，只不过是根据一段过生成另外的一段话</li><li>注意的是开始的输出是在全部输入结束后生成的</li><li>一些论文<ul><li><a href="http://www.aclweb.org/anthology/P14-1140.pdf" target="_blank" rel="external">A Recursive Recurrent Neural Network for Statistical Machine Translation</a></li><li><a href="http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf" target="_blank" rel="external">Sequence to Sequence Learning with Neural Networks</a></li></ul></li></ul><p><img src="/assets/blog_images/RNN/RNN_07.png" alt="机器翻译" title="RNN_07"></p><h3 id="3-语音识别"><a href="#3-语音识别" class="headerlink" title="(3) 语音识别"></a>(3) 语音识别</h3><ul><li>论文<ul><li><a href="http://proceedings.mlr.press/v32/graves14.pdf" target="_blank" rel="external">Towards End-to-End Speech Recognition with Recurrent Neural Networks</a><h3 id="4-图像描述生成"><a href="#4-图像描述生成" class="headerlink" title="(4) 图像描述生成"></a>(4) 图像描述生成</h3></li></ul></li><li>根据图像，生成一段描述图像的话</li><li>需要和<code>CNN</code>结合使用</li></ul><h1 id="二、结构"><a href="#二、结构" class="headerlink" title="二、结构"></a>二、结构</h1><h2 id="1、One-to-One"><a href="#1、One-to-One" class="headerlink" title="1、One to One"></a>1、One to One</h2><ul><li>即一个输入对应一个输出，就是上面的图<h2 id="2、Many-to-One"><a href="#2、Many-to-One" class="headerlink" title="2、Many to One"></a>2、Many to One</h2></li><li>即多个输入对应一个输出，比如<strong>情感分析</strong>，一段话中很多次，判断这段话的情感  </li><li>其中$x_{1},x_{2},\ldots,x_{t}$表示句子中的<code>t</code>个词，<code>o</code>代表最终输出的情感标签</li><li>前向计算就是：$$f(x)=Vs_{t}=V(Ux_{t}+Ws_{t-1})=V(Ux_{t}+W(Ux_{t-1}+Ws_{t-2}))\cdots$$<br><img src="/assets/blog_images/RNN/RNN_02.png" alt="Many to one" title="RNN_02"><h2 id="3、One-to-Many"><a href="#3、One-to-Many" class="headerlink" title="3、One to Many"></a>3、One to Many</h2></li><li>前向计算类似，不再给出<br><img src="/assets/blog_images/RNN/RNN_03.png" alt="One to Many" title="RNN_03"><h2 id="4、Many-to-Many"><a href="#4、Many-to-Many" class="headerlink" title="4、Many to Many"></a>4、Many to Many</h2></li><li>前向计算类似，不再给出<br><img src="/assets/blog_images/RNN/RNN_04.png" alt="Many to Many" title="RNN_04"><h2 id="5、双向RNN（Bidirectional-RNN）"><a href="#5、双向RNN（Bidirectional-RNN）" class="headerlink" title="5、双向RNN（Bidirectional RNN）"></a>5、双向RNN（Bidirectional RNN）</h2></li><li>比如翻译问题往往需要联系<strong>上下文内容</strong>才能正确的翻译，我们上面的结构线性传递允许“联系上文”，但是联系下文并没有，所以就有<strong>双向RNN</strong></li><li>前向运算稍微复杂一点，以<code>t</code>时刻为例<br>$<br>o_{t} = W_t^{(os)}s_t + W_t^{(oh)}h_t \\<br>\quad = W_t^{(os)} (W_{t-1}^{(ss)} s_{t-1} + W_{t}^{(sx)} x_{t-1}) + W_t^{(oh)} (W_t^{(hh)} h_{t+1} + W_t^{(hx)}x_t)<br>$<br><img src="/assets/blog_images/RNN/RNN_05.png" alt="RNN" title="RNN_05"></li></ul><h2 id="6、深层的RNN"><a href="#6、深层的RNN" class="headerlink" title="6、深层的RNN"></a>6、深层的RNN</h2><ul><li>上面的结构都是只含有一层的<code>state</code>层，根据传统NN和CNN，深层次的结构有更加号的效果，结构如图<br><img src="/assets/blog_images/RNN/RNN_06.png" alt="深层的RNN" title="RNN_06"></li></ul><h1 id="三、Back-Propagation-Through-Time-BPTT-训练"><a href="#三、Back-Propagation-Through-Time-BPTT-训练" class="headerlink" title="三、Back Propagation Through Time(BPTT)训练"></a>三、Back Propagation Through Time(BPTT)训练</h1><ul><li>关于传统神经网络BP算法可以<a href="https://github.com/lawlite19/MachineLearning_Python" target="_blank" rel="external">查看这里</a>神经网络部分的推导<h2 id="1、符号等说明"><a href="#1、符号等说明" class="headerlink" title="1、符号等说明"></a>1、符号等说明</h2></li><li>以下图为例</li></ul><p><img src="/assets/blog_images/RNN/RNN_09.png" alt="RNN基本结构" title="RNN_09"></p><ul><li>符号说明<ul><li>$\phi$………………………………………………隐藏层的激励函数</li><li>$\varphi$………………………………………………输出层的变换函数</li><li>$L_{t} = L_{t}\left( o_{t},y_{t} \right)$……………………………模型的损失函数<ul><li>标签数据$y_{t}$是一个 <code>one-hot</code> 向量</li></ul></li></ul></li></ul><h2 id="2、反向传播过程"><a href="#2、反向传播过程" class="headerlink" title="2、反向传播过程"></a>2、反向传播过程</h2><ul><li>接受完序列中所有样本后再<strong>统一计算损失</strong>，此时模型的总损失可以表示为（假设输入序列长度为<code>n</code>）：$$L = \sum_{t = 1}^{n}L_{t}$$<br><img src="/assets/blog_images/RNN/RNN_10.png" alt="RNN" title="RNN_10"></li><li>$o_{t} = \varphi(Vs_t) = \varphi(V(Ux_t + Ws_{t-1}))$<ul><li>其中$s_{0} = \mathbf{0 =}( 0,0,\ldots,0 )^{T}$</li></ul></li><li>令：${o_{t}^* = Vs_{t}}, \quad {s_{t}^{*} = Ux_{t} + Ws_{t - 1}}…………(1)$   (就是没有经过激励函数和变换函数前)<ul><li>则：$o_{t} = \varphi( o_{t}^*)$</li><li>$s_{t} = \phi(s_{t}^{*})$</li></ul></li></ul><h3 id="1-矩阵V的更新"><a href="#1-矩阵V的更新" class="headerlink" title="(1) 矩阵V的更新"></a>(1) 矩阵V的更新</h3><ul><li>对<strong>矩阵 V</strong> 的更新过程,根据(1)式可得， (和传统的神经网络一致，根据求导的<strong>链式法则</strong>):<ul><li>$${{{\partial {L_t} \over \partial o_t^{\ast}}} = {{\partial L_t \over \partial o_t } \ast {\partial o_t \over \partial o_{t}^{\ast} }} = {{\partial L_t \over \partial o_t} \ast \varphi ^{'} (o_t^{\ast})}}$$</li><li>$${{{\partial L_t} \over {\partial V }}} = {{\partial L_t \over \partial Vs_t} } \ast {{\partial Vs_t \over \partial V}} = {{\partial L_t \over \partial o_t^\ast}} \times s_t^T = ({{\partial L_t \over \partial o_t} \ast \varphi ^{'} (o_{t}^\ast)}) \times s_t^T$$- 因为${L = \sum_{t = 1}^{n}L_{t}}$，所以对矩阵V的更新对应的导数:<br>$${{\partial L \over \partial V} = {\sum\limits_{t=1}^n ({\partial L_t \over \partial o_t} \ast \varphi ^{'} (o_t^\ast)) \times s_t^T}}$$</li></ul></li></ul><h3 id="2-矩阵U和W的更新"><a href="#2-矩阵U和W的更新" class="headerlink" title="(2) 矩阵U和W的更新"></a>(2) 矩阵U和W的更新</h3><ul><li><code>RNN</code> 的 <code>BP</code> 算法的主要难点在于它 <code>State</code> 之间的通信</li><li>可以采用循环的方法来计算各个梯度，<code>t</code>应从<code>n</code>开始降序循环至 <code>1</code></li><li>计算时间通道上的局部梯度（同样根据链式法则）<br>$$ {{\partial L_t \over \partial s_t^{\ast}}} = {{\partial L_t \over \partial Vs_t}} \times {{\partial s_t^{T} V_t^{T} \over \partial s_t}} \ast {{\partial s_t \over \partial s_t^{\ast}}}  = V^T \times ({{\partial L_t \over \partial o_t}} * {\varphi ^{‘} (o_t^{\ast}))} $$</li></ul><p>$$ {{\partial L_t \over \partial s_{k-1}^\ast}} ={{\partial s_k^\ast \over \partial s_{k-1}^\ast}} \times {{\partial L_t \over \partial s_{k}^\ast}} = W_T \times ({{\partial L_t \over \partial s_k^\ast} * {\phi ^{'} (s_{k-1}^\ast)}}) , (k=1,……,t) ………(2)$$</p><ul><li>利用局部梯度计算<code>U</code>和<code>W</code>的梯度<ul><li>这里累加是因为权值是共享的，所以往前推算一直用的是一样的权值<br>$${\partial L_t \over \partial U} + = {\sum\limits_{k=1}^t {\partial L_t \over \partial s_k^\ast} \times {\partial s_k^\ast \over \partial U}} = {\sum\limits_{k=1}^t {\partial L_t\over \partial s_k^\ast}} \times x_t^T $$<br>$${\partial L_t \over \partial W} + = {\sum\limits_{k=1}^t {\partial L_t \over \partial s_k^\ast} \times {\partial s_k^\ast \over \partial W}} = {\sum\limits_{k=1}^t {\partial L_t\over \partial s_k^\ast}} \times s_{t-1}^T ………………..(3)$$</li></ul></li></ul><h2 id="3、训练问题"><a href="#3、训练问题" class="headerlink" title="3、训练问题"></a>3、训练问题</h2><ul><li>从<strong> 公式(2)和(3) </strong>中可以看出，时间维度上的权重<code>W</code>更新需要计算$\phi^{‘} (s_k^{\ast})$，即经过激励函数的导数</li><li>如果时间维度上很长，则这个梯度是<strong>累积</strong>的，所以造成<strong>梯度消失或爆炸</strong><ul><li>可以想象将结构图竖起来，就是一个深层的神经网络，所以容易出现梯度问题</li><li>关于梯度消失的问题可以查看我<a href="http://lawlite.me/2016/12/20/%E8%AE%BA%E6%96%87%E8%AE%B0%E5%BD%95-UnderstandingTheDifficultyOfTrainingDeepFeedforwardNeuralNetworks/" target="_blank" rel="external">这里一遍博客</a></li></ul></li><li><code>RNN</code> 主要的作用就是能够记住之前的信息，但是<strong>梯度消失</strong>的问题又告诉我们不能记住太久之前的信息，改进的思路有两点<ul><li>一是使用一些<code>trick</code>,比如合适的激励函数，初始化，BN等等</li><li>二是改进<code>state</code>的传递方式，比如就是下面提及的<code>LSTM</code><ul><li>关于为何 <code>LSTMs</code> 能够解决梯度消失，直观上来说就是<strong>上方时间通道</strong>是简单的<strong>线性组合</strong></li></ul></li></ul></li></ul><h1 id="四、Long-Short-Term-Memory-LSTM，长短时记忆网络"><a href="#四、Long-Short-Term-Memory-LSTM，长短时记忆网络" class="headerlink" title="四、Long Short-Term Memory(LSTM，长短时记忆网络)"></a>四、Long Short-Term Memory(LSTM，长短时记忆网络)</h1><h2 id="1、介绍"><a href="#1、介绍" class="headerlink" title="1、介绍"></a>1、介绍</h2><ul><li><code>LSTM</code> 是一般 <code>RNN</code> 的升级，因为一些序列问题，我们可能需要忘记一些东西， <code>LSTM</code> 和普通 <code>RNN</code> 相比, 多出了三个<strong>控制器</strong>. (输入控制, 输出控制, 忘记控制)</li><li>在<code>LSTM</code>里，这个叫做<code>cell</code>（其实就是前面的<code>state</code>,只是这里更加复杂了）, 可以看作一个黑盒，这个<code>cell</code>结合前面<code>cell</code>的输出$h_{t-1}$和当前的输入$x_{t}$来决定是否记忆下来，该网络结构在对<strong>长序列依赖问题</strong>中非常有效</li></ul><h2 id="2、结构"><a href="#2、结构" class="headerlink" title="2、结构"></a>2、结构</h2><ul><li>一个经典的<code>cell</code>结构如下图<ul><li>$\phi_{1} $是<code>sigmoid</code>函数，$\phi_{2}$ 是<code>tanh</code>函数</li><li><code>*</code>表示 <code>element wise</code> 乘法(就是点乘)，使用<code>X</code>表示矩阵乘法</li></ul></li><li><code>LSTMs</code> 的 <code>cell</code> 的时间通道有<strong>两条</strong>。<ul><li>上方的时间通道（$h^{\left( {old} \right)} \rightarrow h^{\left( {new} \right)}$）仅包含了两个<strong>代数运算</strong>,这意味着它信息传递的方式会更为<strong>直接</strong> $$h^{(new)} = h^{(old)}*r_1 + r_2$$</li><li>位于下方的时间通道（$s^{\left( {old} \right)} \rightarrow s^{\left( {new} \right)}$）则运用了大量的<strong>层结构</strong>,在 <code>LSTMs</code> 中，我们通常称这些层结构为门（<code>Gates</code>）</li></ul></li></ul><p><img src="/assets/blog_images/RNN/RNN_LSTM_08.png" alt="LSTM cell结构" title="RNN_LSTM_08"></p><h2 id="3、运算说明"><a href="#3、运算说明" class="headerlink" title="3、运算说明"></a>3、运算说明</h2><ul><li><code>Sigmoid</code> 函数取值区间为 <code>0-1</code>，那么当 <code>Sigmoid</code> 对应的层结构输出 <code>0</code> 时，就对应着<strong>遗忘</strong>这个过程；当输出 <code>1</code> 时，自然就对应着<strong>接受</strong>这个过程。<ul><li>事实上这也是 <code>Sigmoid</code> 层叫门的原因——它能<strong>决定“放哪些数据进来”和决定“不让哪些数据通过”</strong></li></ul></li><li>最左边的<code>Sigmoid gate</code> 叫做<strong>遗忘门</strong>, 控制着时间通道信息的遗忘程度<ul><li>前向计算: $r_1 = \phi_1(W_1 \times x^*)$<ul><li>其中 $x^*  \buildrel \Delta \over =[x,s^{(old)}] $，表示当前输入样本和下方时间通道$s^{(old)}$连接(<code>concat</code>)起来</li></ul></li></ul></li><li>第二个 <code>Sigmoid Gate</code> 通常被称为<strong>输入门</strong>（Input Gate）, 控制着当前输入和下方通道信息对<strong>上方通道信息的影响</strong><ul><li>前向运算为：$g_{1} = \phi_{1} ( W_{2} \times x^{*} )$,</li></ul></li><li>第三个 <code>Tanh Gate</code> 则允许网络结构<strong>驳回</strong>历史信息, 因为<code>tanh</code>的值域是(-1,1)<ul><li>前向运算为：$g_{2} = \phi_{2} ( W_{3} \times x^{*} )$</li><li>$r_{2} = g_{1}*g_{2}$</li></ul></li><li>第四个 <code>Sigmoid Gate</code> 通常被称为<strong>输出门</strong>（Output Gate），它为输出和传向下一个 <code>cell</code> 的下方通道信息作出了贡献。<ul><li>对应的前向传导算法为：$g_{3} = \phi_{1}\left( W_{4} \times x^{*} \right)$</li></ul></li><li>最终<code>cell</code>的输出为：$o = s^{\left( \text{new} \right)} = \phi_{2}\left( h^{\left( \text{new} \right)} \right)*g_{3}$</li><li>每个 <code>Gate</code> 对应的权值矩阵是不同的（$W_{1}\sim W_{4}$），切勿以为它们会共享权值</li></ul><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul><li><a href="http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/" target="_blank" rel="external">http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/</a></li><li><a href="http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-2-implementing-a-language-model-rnn-with-python-numpy-and-theano/" target="_blank" rel="external">http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-2-implementing-a-language-model-rnn-with-python-numpy-and-theano/</a></li><li><a href="https://zhuanlan.zhihu.com/p/26891871" target="_blank" rel="external">https://zhuanlan.zhihu.com/p/26891871</a></li><li><a href="https://zhuanlan.zhihu.com/p/26892413" target="_blank" rel="external">https://zhuanlan.zhihu.com/p/26892413</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;一、介绍&quot;&gt;&lt;a href=&quot;#一、介绍&quot; class=&quot;headerlink&quot; title=&quot;一、介绍&quot;&gt;&lt;/a&gt;一、介绍&lt;/h1&gt;&lt;h2 id=&quot;1、什么是RNN&quot;&gt;&lt;a href=&quot;#1、什么是RNN&quot; class=&quot;headerlink&quot; title=&quot;1、什么是RNN&quot;&gt;&lt;/a&gt;1、什么是RNN&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;传统的神经网络是&lt;strong&gt;层与层&lt;/strong&gt;之间是&lt;strong&gt;全连接&lt;/strong&gt;的，但是每层之间的神经元是没有连接的（其实是假设各个数据之间是&lt;strong&gt;独立的&lt;/strong&gt;）&lt;ul&gt;
&lt;li&gt;这种结构不善于处理&lt;strong&gt;序列化的问题&lt;/strong&gt;。比如要预测句子中的下一个单词是什么，这往往与前面的单词有很大的关联，因为句子里面的单词并不是独立的。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;code&gt;RNN&lt;/code&gt; 的结构说明当前的的输出与前面的输出也有关，即隐层之间的节点不再是无连接的，而是有连接的&lt;ul&gt;
&lt;li&gt;基本的结构如图，可以看到有个&lt;strong&gt;循环的结构&lt;/strong&gt;，将其展开就是右边的结构&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="DeepLearning" scheme="http://lawlite.cn/tags/DeepLearning/"/>
    
      <category term="RNN" scheme="http://lawlite.cn/tags/RNN/"/>
    
      <category term="LSTM" scheme="http://lawlite.cn/tags/LSTM/"/>
    
  </entry>
  
  <entry>
    <title>Seaborn绘图</title>
    <link href="http://lawlite.cn/2017/06/14/Seaborn%E7%BB%98%E5%9B%BE/"/>
    <id>http://lawlite.cn/2017/06/14/Seaborn绘图/</id>
    <published>2017-06-14T13:25:43.000Z</published>
    <updated>2017-06-25T08:49:50.000Z</updated>
    
    <content type="html"><![CDATA[<ul><li>全部代码：<a href="https://github.com/lawlite19/Blog-Back-Up/blob/master/code/seaborn_study.py" target="_blank" rel="external">https://github.com/lawlite19/Blog-Back-Up/blob/master/code/seaborn_study.py</a></li></ul><h1 id="一、介绍与安装"><a href="#一、介绍与安装" class="headerlink" title="一、介绍与安装"></a>一、介绍与安装</h1><h2 id="1、介绍"><a href="#1、介绍" class="headerlink" title="1、介绍"></a>1、介绍</h2><ul><li>官网：<a href="http://seaborn.pydata.org/index.html" target="_blank" rel="external">http://seaborn.pydata.org/index.html</a></li><li>Github: <a href="https://github.com/mwaskom/seaborn" target="_blank" rel="external">https://github.com/mwaskom/seaborn</a></li><li><code>Seaborn</code> 其实是在matplotlib的基础上进行了更高级的 <code>API</code> 封装，从而使得作图更加容易</li><li>在大多数情况下使用<code>seaborn</code>就能做出很具有吸引力的图，而使用<code>matplotlib</code>就能制作具有更多特色的图。应该把<code>Seaborn</code>视为<code>matplotlib</code>的补充<h2 id="2、安装"><a href="#2、安装" class="headerlink" title="2、安装"></a>2、安装</h2></li><li>直接 <code>pip3 install seaborn</code>即可</li></ul><a id="more"></a><h1 id="二、分布图"><a href="#二、分布图" class="headerlink" title="二、分布图"></a>二、分布图</h1><h2 id="1、distplot"><a href="#1、distplot" class="headerlink" title="1、distplot"></a>1、distplot</h2><ul><li>导入包</li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">#-*- coding: utf-8 -*-</div><div class="line">import numpy as np</div><div class="line">import pandas as pd</div><div class="line">import matplotlib as mpl</div><div class="line">import matplotlib.pyplot as plt</div><div class="line">import seaborn as sns</div><div class="line">#%matplotlib inline  # 为了在jupyter notebook里作图，需要用到这个命令</div></pre></td></tr></table></figure><ul><li>加载 <code>seaborn</code>中的数据集：<code>tips = sns.load_dataset(&#39;tips&#39;)</code></li><li><p>分布图</p><ul><li><code>kde</code>是高斯分布密度图，绘图在0-1之间</li><li><code>hist</code>是否画直方图</li><li><code>rug</code>在X轴上画一些分布线</li><li><code>fit</code>可以制定某个分布进行<strong>拟合</strong></li><li><code>label</code> legend时的值</li><li><code>axlabel</code>制定横轴的说明<figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">sns.distplot(tips[<span class="string">'total_bill'</span>], bins=None, hist=True, kde=False, rug=True, fit=None, </div><div class="line">            hist_kws=None, kde_kws=None, rug_kws=None, </div><div class="line">            fit_kws=None, <span class="attribute">color</span>=None, vertical=False, </div><div class="line">            norm_hist=False, axlabel=None, label=None, ax=None)</div><div class="line">sns<span class="selector-class">.plt</span><span class="selector-class">.show</span>()</div></pre></td></tr></table></figure></li></ul></li><li><p>拟合分布</p><ul><li>这里使用了<strong>gamma分布</strong>拟合<figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">from scipy import stats</div><div class="line">sns.distplot(tips<span class="selector-class">.total_bill</span>, fit=stats<span class="selector-class">.gamma</span>, kde=False)</div><div class="line">sns<span class="selector-class">.plt</span><span class="selector-class">.show</span>()</div></pre></td></tr></table></figure></li></ul></li></ul><p><img src="/assets/blog_images/seaborn_images/seaborn_dis_01.png" alt="seaborn gamma分布拟合" title="seaborn_dis_01"></p><h2 id="2、kdeplot"><a href="#2、kdeplot" class="headerlink" title="2、kdeplot"></a>2、kdeplot</h2><ul><li>高斯概率密度图<ul><li><code>data2</code>可以是二维的分布</li><li><code>shade</code>是否填充 </li><li><code>kernel</code>核函数，还有很多核函数，比如<strong>cos, biw</strong>等</li><li><code>cumulative</code>累积的作图，最后的值应该是接近<strong>1</strong></li><li><code>gridsize</code>多少个点估计</li></ul></li></ul><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">ax = sns.kdeplot(tips[<span class="string">'total_bill'</span>], data2=tips<span class="selector-class">.tip</span>, shade=False, vertical=False, </div><div class="line">                kernel=<span class="string">"gau"</span>, bw=<span class="string">"scott"</span>, </div><div class="line">                gridsize=<span class="number">100</span>, cut=<span class="number">3</span>, <span class="attribute">clip</span>=None, </div><div class="line">                legend=True, cumulative=False, </div><div class="line">                shade_lowest=True, ax=None)</div><div class="line">sns<span class="selector-class">.plt</span><span class="selector-class">.show</span>()</div></pre></td></tr></table></figure><p><img src="/assets/blog_images/seaborn_images/seaborn_kde_01.png" alt="二维的分布" title="seaborn_kde_01"></p><h1 id="二、pairplot"><a href="#二、pairplot" class="headerlink" title="二、pairplot"></a>二、pairplot</h1><h2 id="1、两两作图"><a href="#1、两两作图" class="headerlink" title="1、两两作图"></a>1、两两作图</h2><ul><li><code>iris</code> 为例<ul><li><code>data</code>: DataFrame格式的数据</li><li><code>hue</code>: label类别对应的column name</li><li><code>vars</code>: 指定feature的列名</li><li><code>kind</code>: 作图的方式，可以是<strong>reg或scatter</strong></li><li><code>diag_kind</code>: 对角线作图的方式，可以是<strong>hist或kde</strong><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">iris = sns.load_dataset(<span class="string">'iris'</span>)</div><div class="line">g = sns.pairplot(iris, hue=<span class="string">'species'</span>, hue_order=None, palette=None, </div><div class="line">                 vars=list(iris<span class="selector-class">.columns</span>[<span class="number">0</span>:-<span class="number">1</span>]), </div><div class="line">                 x_vars=None, y_vars=None, </div><div class="line">                 kind=<span class="string">"reg"</span>, diag_kind=<span class="string">"hist"</span>, </div><div class="line">                 markers=[<span class="string">'o'</span>,<span class="string">'s'</span>,<span class="string">'D'</span>], size=<span class="number">1.5</span>, aspect=<span class="number">1</span>, </div><div class="line">                 dropna=True, plot_kws=None, </div><div class="line">                 diag_kws=None, grid_kws=None)</div><div class="line">sns<span class="selector-class">.plt</span><span class="selector-class">.show</span>()</div></pre></td></tr></table></figure></li></ul></li></ul><p><img src="/assets/blog_images/seaborn_images/seaborn_pair_01.png" alt="seaborn pairplot作图" title="seaborn_pair_01"></p><h1 id="三、stripplot和swarmplot"><a href="#三、stripplot和swarmplot" class="headerlink" title="三、stripplot和swarmplot"></a>三、stripplot和swarmplot</h1><h2 id="1、stripplot"><a href="#1、stripplot" class="headerlink" title="1、stripplot"></a>1、stripplot</h2><ul><li>tips为例，查看每天的数据信息<ul><li><code>x</code>: X轴数据</li><li><code>y</code>: Y轴数据</li><li><code>hue</code>: 区分不同种类数据的column name</li><li><code>data</code>: DataFrame类型数据</li><li><code>jitter</code>: 将数据分开点，防止重叠<figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">tips = sns.load_dataset(<span class="string">'tips'</span>)</div><div class="line">ax = sns.stripplot(x=<span class="string">'day'</span>, y=<span class="string">'total_bill'</span>, hue=None, data=tips, <span class="attribute">order</span>=None, </div><div class="line">                  hue_order=None, jitter=True, </div><div class="line">                  split=False, orient=None, </div><div class="line">                  <span class="attribute">color</span>=None, palette=None, size=<span class="number">5</span>, </div><div class="line">                  edgecolor=<span class="string">"gray"</span>, linewidth=<span class="number">0</span>, </div><div class="line">                  ax=None)</div></pre></td></tr></table></figure></li></ul></li></ul><p><img src="/assets/blog_images/seaborn_images/seaborn_stripplot_01.png" alt="stripplot 查看每天的数据信息" title="seaborn_stripplot_01"></p><ul><li>查看关于性别消费的信息</li></ul><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">ax = sns.stripplot(x=<span class="string">'sex'</span>, y=<span class="string">'total_bill'</span>, hue=<span class="string">'day'</span>, data=tips, <span class="attribute">order</span>=None, </div><div class="line">                  hue_order=None, jitter=True, </div><div class="line">                  split=False, orient=None, </div><div class="line">                  <span class="attribute">color</span>=None, palette=None, size=<span class="number">5</span>, </div><div class="line">                  edgecolor=<span class="string">"gray"</span>, linewidth=<span class="number">0</span>, </div><div class="line">                  ax=None)</div></pre></td></tr></table></figure><p><img src="/assets/blog_images/seaborn_images/seaborn_stripplot_02.png" alt="sripplot 查看敢于性别消费的信息" title="seaborn_stripplot_02"></p><h2 id="2、swarmplot"><a href="#2、swarmplot" class="headerlink" title="2、swarmplot"></a>2、swarmplot</h2><ul><li>与stripplot类似，只是数据点不会重叠 (适合小数据量)</li></ul><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">tips = sns.load_dataset(<span class="string">'tips'</span>)</div><div class="line">ax = sns.swarmplot(x=<span class="string">'sex'</span>, y=<span class="string">'total_bill'</span>, hue=<span class="string">'day'</span>, data=tips)</div><div class="line">sns<span class="selector-class">.plt</span><span class="selector-class">.show</span>()</div></pre></td></tr></table></figure><p><img src="/assets/blog_images/seaborn_images/seaborn_swarmplot_01.png" alt="seaborn swarmplot" title="seaborn_swarmplot_01"></p><h1 id="四、boxplot"><a href="#四、boxplot" class="headerlink" title="四、boxplot"></a>四、boxplot</h1><h2 id="1、boxplot示意图"><a href="#1、boxplot示意图" class="headerlink" title="1、boxplot示意图"></a>1、boxplot示意图</h2><p><img src="/assets/blog_images/seaborn_images/seaborn_boxplot_01.jpg" alt="box图" title="seaborn_boxplot_01"></p><ul><li>函数<ul><li><code>x, y</code>：指定X轴，Y轴的columns name值</li><li><code>hue</code>: 指定要区分的类别<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">tips = sns.load_dataset('tips')</div><div class="line">ax = sns.boxplot(x='day', y='total_bill', hue=None, data=tips, order=None, </div><div class="line">                hue_order=None, orient=None, </div><div class="line">                color=None, palette=None, </div><div class="line">                saturation=.75, width=.8, </div><div class="line">                fliersize=5, linewidth=None, </div><div class="line">                whis=1.5, notch=False, ax=None)</div><div class="line">sns.plt.show()</div></pre></td></tr></table></figure></li></ul></li></ul><p><img src="/assets/blog_images/seaborn_images/seaborn_boxplot_02.png" alt="box图" title="seaborn_boxplot_02"></p><ul><li>可以和上面的stripplot一起用</li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">tips = sns.load_dataset('tips')</div><div class="line">ax = sns.boxplot(x='day', y='total_bill', hue=None, data=tips, order=None, </div><div class="line">                hue_order=None, orient=None, </div><div class="line">                color=None, palette=None, </div><div class="line">                saturation=.75, width=.8, </div><div class="line">                fliersize=5, linewidth=None, </div><div class="line">                whis=1.5, notch=False, ax=None)</div><div class="line">sns.stripplot(x='day', y='total_bill', hue=None, data=tips, order=None, </div><div class="line">             hue_order=None, jitter=True, split=False, </div><div class="line">             orient=None, color=None, palette=None, </div><div class="line">             size=5, edgecolor="gray", linewidth=0, </div><div class="line">             ax=None)</div><div class="line">sns.plt.show()</div></pre></td></tr></table></figure><p><img src="/assets/blog_images/seaborn_images/seaborn_boxplot_03.png" alt="seaborn boxplot和stripplot" title="seaborn_boxplot_03"></p><h1 id="五、jointplot"><a href="#五、jointplot" class="headerlink" title="五、jointplot"></a>五、jointplot</h1><h2 id="1、jointplot"><a href="#1、jointplot" class="headerlink" title="1、jointplot"></a>1、jointplot</h2><ul><li>联合作图<ul><li><code>kind</code>: 有<strong>scatter” | “reg” | “resid” | “kde” | “hex</strong><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">tips = sns.load_dataset(<span class="string">'tips'</span>)</div><div class="line">from scipy import stats</div><div class="line">g = sns.jointplot(x=<span class="string">'total_bill'</span>, y=<span class="string">'tip'</span>,</div><div class="line">                  data=tips, kind=<span class="string">"reg"</span>, </div><div class="line">                  stat_func=stats<span class="selector-class">.pearsonr</span>, </div><div class="line">                  <span class="attribute">color</span>=None, size=<span class="number">6</span>, ratio=<span class="number">5</span>, </div><div class="line">                  space=.<span class="number">2</span>, dropna=True, xlim=None, </div><div class="line">                  ylim=None, joint_kws=None, </div><div class="line">                  marginal_kws=None, annot_kws=None)</div><div class="line">sns<span class="selector-class">.plt</span><span class="selector-class">.show</span>()</div></pre></td></tr></table></figure></li></ul></li></ul><p><img src="/assets/blog_images/seaborn_images/seaborn_jointplot_01.png" alt="jointplot" title="seaborn_jointplot_01"></p><ul><li>可以在基础上再作图<ul><li>plot_joint就是在联合分布上作图</li><li>plot_marginals就是在边缘分布上再作图<figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">g = (sns.jointplot(x=<span class="string">'total_bill'</span>, y=<span class="string">'tip'</span>,data=tips).plot_joint(sns.kdeplot))</div></pre></td></tr></table></figure></li></ul></li></ul><p><img src="/assets/blog_images/seaborn_images/seaborn_jointplot_02.png" alt="joint plot" title="seaborn_jointplot_02"></p><h1 id="六、violinplot"><a href="#六、violinplot" class="headerlink" title="六、violinplot"></a>六、violinplot</h1><h2 id="1、小提琴图，和boxplot很像"><a href="#1、小提琴图，和boxplot很像" class="headerlink" title="1、小提琴图，和boxplot很像"></a>1、小提琴图，和<strong>boxplot</strong>很像</h2><ul><li>对称的<strong>kde图</strong></li><li>中间的白点是<strong>中位数</strong>，黑色粗线对应<strong>分位数</strong><br><img src="/assets/blog_images/seaborn_images/seaborn_violinplot_02.png" alt="小提琴图1" title="seaborn_violinplot_02"></li><li><ul><li><code>inner</code>: 指定图里面用什么划分，有<code>&quot;box&quot;, &quot;quartile&quot;, &quot;point&quot;, &quot;stick&quot;, None</code><ul><li><code>quartile</code>为四分位数划分</li><li><code>stick</code>很像<strong>rug</strong>，就是可以看出密度情况</li></ul></li><li><code>scale</code>: 缩放每个图对应的area, 取值有 <code>&quot;area&quot;, &quot;count&quot;, &quot;width&quot;</code><ul><li><code>area</code>指定每个有相同的area</li><li><code>count</code>会按数量缩放（数量少的就比较<strong>窄扁</strong>）<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">tips = sns.load_dataset('tips')</div><div class="line">ax = sns.violinplot(x='day', y='total_bill', </div><div class="line">                    hue='smoker', data=tips, order=None, </div><div class="line">                    hue_order=None, bw="scott", </div><div class="line">                    cut=2, scale="area", </div><div class="line">                    scale_hue=True, gridsize=100, </div><div class="line">                    width=.8, inner="quartile", </div><div class="line">                    split=False, orient=None, </div><div class="line">                    linewidth=None, color=None, </div><div class="line">                    palette='muted', saturation=.75, </div><div class="line">                    ax=None) </div><div class="line">sns.plt.show()</div></pre></td></tr></table></figure></li></ul></li></ul></li></ul><p><img src="/assets/blog_images/seaborn_images/seaborn_violinplot_01.png" alt="小提琴图" title="seaborn_violinplot_01"></p><h1 id="七、pointplot-bar"><a href="#七、pointplot-bar" class="headerlink" title="七、pointplot, bar"></a>七、pointplot, bar</h1><h2 id="1、pointplot"><a href="#1、pointplot" class="headerlink" title="1、pointplot"></a>1、pointplot</h2><ul><li>点图<ul><li><code>estimator</code>：点的取值是，默认是<code>np.mean</code><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">tips = sns.load_dataset(<span class="string">'tips'</span>)</div><div class="line">sns.pointplot(x=<span class="string">'time'</span>, y=<span class="string">'total_bill'</span>, hue=<span class="string">'smoker'</span>, data=tips, <span class="attribute">order</span>=None, </div><div class="line">             hue_order=None, estimator=np<span class="selector-class">.mean</span>, ci=<span class="number">95</span>, </div><div class="line">             n_boot=<span class="number">1000</span>, units=None, markers=<span class="string">"o"</span>, </div><div class="line">             linestyles=<span class="string">"-"</span>, dodge=False, join=True, </div><div class="line">             scale=<span class="number">1</span>, orient=None, <span class="attribute">color</span>=None, </div><div class="line">             palette=None, ax=None, errwidth=None, </div><div class="line">             capsize=None)</div><div class="line">sns<span class="selector-class">.plt</span><span class="selector-class">.show</span>()</div></pre></td></tr></table></figure></li></ul></li></ul><p><img src="/assets/blog_images/seaborn_images/seaborn_pointplot_01.png" alt="pointplot" title="seaborn_pointplot_01"></p><h2 id="2、barplot"><a href="#2、barplot" class="headerlink" title="2、barplot"></a>2、barplot</h2><ul><li>条形图<ul><li>y轴是<code>mean value</code>，和点图其实差不多<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">tips = sns.load_dataset('tips')</div><div class="line">sns.barplot(x='day', y='total_bill', hue='sex', data=tips, order=None, </div><div class="line">           hue_order=None, estimator=np.mean, ci=95, </div><div class="line">           n_boot=1000, units=None, orient=None, </div><div class="line">           color=None, palette=None, saturation=.75, </div><div class="line">           errcolor=".26", errwidth=None, capsize=None, </div><div class="line">           ax=None)</div><div class="line">sns.plt.show()</div></pre></td></tr></table></figure></li></ul></li></ul><p><img src="/assets/blog_images/seaborn_images/seaborn_barplot_01.png" alt="bar" title="seaborn_barplot_01"></p><h2 id="3、countplot"><a href="#3、countplot" class="headerlink" title="3、countplot"></a>3、countplot</h2><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">tips = sns.load_dataset(<span class="string">'tips'</span>)</div><div class="line">sns.countplot(x=<span class="string">'day'</span>, hue=<span class="string">'sex'</span>, data=tips)</div><div class="line">sns<span class="selector-class">.plt</span><span class="selector-class">.show</span>()</div></pre></td></tr></table></figure><p><img src="/assets/blog_images/seaborn_images/seaborn_countplot_01.png" alt="count" title="seaborn_countplot_01"></p><h1 id="八、factorplot"><a href="#八、factorplot" class="headerlink" title="八、factorplot"></a>八、factorplot</h1><h2 id="1、可以通过这个函数绘制以上几种图"><a href="#1、可以通过这个函数绘制以上几种图" class="headerlink" title="1、可以通过这个函数绘制以上几种图"></a>1、可以通过这个函数绘制以上几种图</h2><ul><li>指定<code>kind</code>即可，有<code>point</code>, <code>bar</code>, <code>count</code>, <code>box</code>, <code>violin</code>, <code>strip</code></li><li><code>row</code>和<code>col</code>指定绘制的行数和列数，给出一个<strong>种类类型的列名</strong>即可 <figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">titanic = sns.load_dataset(<span class="string">'titanic'</span>)</div><div class="line">sns.factorplot(x=<span class="string">'age'</span>, y=<span class="string">'embark_town'</span>, </div><div class="line">               hue=<span class="string">'sex'</span>, data=titanic,</div><div class="line">               row=<span class="string">'class'</span>, col=<span class="string">'sex'</span>, </div><div class="line">               col_wrap=None, estimator=np<span class="selector-class">.mean</span>, ci=<span class="number">95</span>, </div><div class="line">               n_boot=<span class="number">1000</span>, units=None, <span class="attribute">order</span>=None, </div><div class="line">               hue_order=None, row_order=None, </div><div class="line">               col_order=None, kind=<span class="string">"box"</span>, size=<span class="number">4</span>, </div><div class="line">               aspect=<span class="number">1</span>, orient=None, <span class="attribute">color</span>=None, </div><div class="line">               palette=None, legend=True, </div><div class="line">               legend_out=True, sharex=True, </div><div class="line">               sharey=True, margin_titles=False, </div><div class="line">               facet_kws=None)</div><div class="line">sns<span class="selector-class">.plt</span><span class="selector-class">.show</span>()</div></pre></td></tr></table></figure></li></ul><p><img src="/assets/blog_images/seaborn_images/seaborn_factorplot_01.png" alt="factorplot box" title="seaborn_factorplot_01"></p><h1 id="九、heatmap"><a href="#九、heatmap" class="headerlink" title="九、heatmap"></a>九、heatmap</h1><h2 id="1、heatmap"><a href="#1、heatmap" class="headerlink" title="1、heatmap"></a>1、heatmap</h2><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">flight = sns.load_dataset(<span class="string">'flights'</span>)</div><div class="line">flights = flight.pivot(<span class="string">'month'</span>,<span class="string">'year'</span>,<span class="string">'passengers'</span>)</div><div class="line">sns.heatmap(flights, annot=True, fmt=<span class="string">'d'</span>)</div><div class="line">sns<span class="selector-class">.plt</span><span class="selector-class">.show</span>()</div></pre></td></tr></table></figure><p><img src="/assets/blog_images/seaborn_images/seaborn_heatmap_01.png" alt="heatmap" title="seaborn_heatmap_01"></p><h1 id="十、时序绘图"><a href="#十、时序绘图" class="headerlink" title="十、时序绘图"></a>十、时序绘图</h1><h2 id="1、tsplot"><a href="#1、tsplot" class="headerlink" title="1、tsplot"></a>1、tsplot</h2><ul><li>condition: 和<code>hue</code>差不多，指定类别</li><li>estimator: 默认为<code>np.mean</code><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">gammas = sns.load_dataset('gammas')</div><div class="line">sns.tsplot(data=gammas, time='timepoint', unit='subject', </div><div class="line">           condition='ROI', value='BOLD signal', </div><div class="line">           err_style="ci_band", ci=68, interpolate=True, </div><div class="line">           color=None, estimator=np.mean, n_boot=5000, </div><div class="line">           err_palette=None, err_kws=None, legend=True, </div><div class="line">           ax=None)</div><div class="line">sns.plt.show()</div></pre></td></tr></table></figure></li></ul><p><img src="/assets/blog_images/seaborn_images/seaborn_tsplot_01.png" alt="tsplot" title="seaborn_tsplot_01"></p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul><li>Youtube: <a href="https://www.youtube.com/playlist?list=PLgJhDSE2ZLxYlhQx0UfVlnF1F7OWF-9rp" target="_blank" rel="external">https://www.youtube.com/playlist?list=PLgJhDSE2ZLxYlhQx0UfVlnF1F7OWF-9rp</a></li><li>Github: <a href="https://github.com/knathanieltucker/seaborn-weird-parts" target="_blank" rel="external">https://github.com/knathanieltucker/seaborn-weird-parts</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;ul&gt;
&lt;li&gt;全部代码：&lt;a href=&quot;https://github.com/lawlite19/Blog-Back-Up/blob/master/code/seaborn_study.py&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://github.com/lawlite19/Blog-Back-Up/blob/master/code/seaborn_study.py&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&quot;一、介绍与安装&quot;&gt;&lt;a href=&quot;#一、介绍与安装&quot; class=&quot;headerlink&quot; title=&quot;一、介绍与安装&quot;&gt;&lt;/a&gt;一、介绍与安装&lt;/h1&gt;&lt;h2 id=&quot;1、介绍&quot;&gt;&lt;a href=&quot;#1、介绍&quot; class=&quot;headerlink&quot; title=&quot;1、介绍&quot;&gt;&lt;/a&gt;1、介绍&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;官网：&lt;a href=&quot;http://seaborn.pydata.org/index.html&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://seaborn.pydata.org/index.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Github: &lt;a href=&quot;https://github.com/mwaskom/seaborn&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://github.com/mwaskom/seaborn&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Seaborn&lt;/code&gt; 其实是在matplotlib的基础上进行了更高级的 &lt;code&gt;API&lt;/code&gt; 封装，从而使得作图更加容易&lt;/li&gt;
&lt;li&gt;在大多数情况下使用&lt;code&gt;seaborn&lt;/code&gt;就能做出很具有吸引力的图，而使用&lt;code&gt;matplotlib&lt;/code&gt;就能制作具有更多特色的图。应该把&lt;code&gt;Seaborn&lt;/code&gt;视为&lt;code&gt;matplotlib&lt;/code&gt;的补充&lt;h2 id=&quot;2、安装&quot;&gt;&lt;a href=&quot;#2、安装&quot; class=&quot;headerlink&quot; title=&quot;2、安装&quot;&gt;&lt;/a&gt;2、安装&lt;/h2&gt;&lt;/li&gt;
&lt;li&gt;直接 &lt;code&gt;pip3 install seaborn&lt;/code&gt;即可&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="Python" scheme="http://lawlite.cn/tags/Python/"/>
    
      <category term="可视化" scheme="http://lawlite.cn/tags/%E5%8F%AF%E8%A7%86%E5%8C%96/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch学习</title>
    <link href="http://lawlite.cn/2017/05/10/PyTorch/"/>
    <id>http://lawlite.cn/2017/05/10/PyTorch/</id>
    <published>2017-05-10T11:30:05.000Z</published>
    <updated>2017-07-06T08:50:11.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一、PyTorch介绍"><a href="#一、PyTorch介绍" class="headerlink" title="一、PyTorch介绍"></a>一、PyTorch介绍</h1><h2 id="1、说明"><a href="#1、说明" class="headerlink" title="1、说明"></a>1、说明</h2><ul><li><code>PyTorch</code> 是 <code>Torch</code> 在 <code>Python</code> 上的衍生（<code>Torch</code> 是一个使用 <code>Lua</code> 语言的神经网络库）</li><li>和<code>tensorflow</code>比较<ul><li><code>PyTorch</code>建立的神经网络是<strong>动态的</strong></li><li><code>Tensorflow</code>是建立<strong>静态图</strong></li><li><code>Tensorflow</code> 的高度工业化, 它的底层代码是很难看懂的. </li><li><code>PyTorch</code> 好那么一点点, 如果你深入 <code>API</code>, 你至少能比看 <code>Tensorflow</code> 多看懂一点点 <code>PyTorch</code> 的底层在干嘛.</li></ul></li></ul><a id="more"></a><h2 id="2、安装PyTorch"><a href="#2、安装PyTorch" class="headerlink" title="2、安装PyTorch"></a>2、安装<code>PyTorch</code></h2><ul><li>官网：<a href="http://pytorch.org/" target="_blank" rel="external">http://pytorch.org/</a></li><li>进入官网之后可以选择对应的安装选项<ul><li>目前只支持<code>Linux</code>和<code>MacOS</code>版本（<code>2017-05-06</code>）</li><li>执行下面对应的安装命令即可<br><img src="/assets/blog_images/PyTorch/PyTorch_01.png" alt="安装" title="PyTorch_01"></li></ul></li><li>安装 <code>PyTorch</code> 会安装两个模块<ul><li>一个是 <code>torch</code>, 一个 <code>torchvision</code>, <code>torch</code> 是<strong>主模块</strong>, 用来搭建神经网络的, </li><li><code>torchvision</code> 是<strong>辅模块</strong>,有数据库,还有一些已经训练好的神经网络等着你直接用, 比如 (<code>VGG, AlexNet, ResNet</code>).</li></ul></li></ul><hr><ul><li>上面在<code>ubuntu14</code>下自带的<code>python2.7</code>安装没有问题，在<code>CentOS6.5</code>下的<code>python3.5</code>中安装可能报错<ul><li>安装<code>python3.5</code>时的配置：</li></ul></li></ul><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">./configure --enable-shared \</div><div class="line">            --prefix=/usr/local/python3.<span class="number">5</span> \</div><div class="line">            LDFLAGS=<span class="string">"-Wl,--rpath=/usr/local/lib"</span></div></pre></td></tr></table></figure><ul><li>然后运行<code>python</code>可能报<code>loading shared libraries: libpython3.5m.so.1.0: cannot open shared object file: No such file or directory</code>的错误，拷贝一份<code>libpython3.5m.so.1.0</code>到<code>/usr/lib64</code>目录下即可</li></ul><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">cp /home/Python/Python-<span class="number">3.5</span>.<span class="number">3</span>/libpython3.<span class="number">5</span>m<span class="selector-class">.so</span>.<span class="number">1.0</span> /usr/lib64</div></pre></td></tr></table></figure><h1 id="二、基础知识"><a href="#二、基础知识" class="headerlink" title="二、基础知识"></a>二、基础知识</h1><h2 id="1、和Numpy相似之处"><a href="#1、和Numpy相似之处" class="headerlink" title="1、和Numpy相似之处"></a>1、和<code>Numpy</code>相似之处</h2><h3 id="1-数据转换"><a href="#1-数据转换" class="headerlink" title="(1) 数据转换"></a>(1) 数据转换</h3><ul><li>导入包：<code>import torch</code></li><li>将<code>numpy</code>数据转为<code>torch</code>数据</li></ul><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">np_data = np.arange(<span class="number">6</span>).reshape((<span class="number">2</span>, <span class="number">3</span>))</div><div class="line">torch_data = torch.from_numpy(np_data)</div></pre></td></tr></table></figure><ul><li>将<code>torch</code>数据转为<code>numpy</code>数据</li></ul><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">tensor2array = torch_data.numpy()</div></pre></td></tr></table></figure><h3 id="2-Torch中的运算"><a href="#2-Torch中的运算" class="headerlink" title="(2) Torch中的运算"></a>(2) <code>Torch</code>中的运算</h3><ul><li>API：<a href="http://pytorch.org/docs/torch.html#math-operations" target="_blank" rel="external">http://pytorch.org/docs/torch.html#math-operations</a></li><li><code>torch</code> 中 <code>tensor</code> 的运算和 <code>numpy array</code>运算很相似，比如<ul><li><code>np.abs() --&gt; torch.abs()</code></li><li><code>np.sin() --&gt; torch.sin()</code>等</li></ul></li><li>矩阵相乘：<ul><li><code>data = [[1,2], [3,4]]</code></li><li><code>tensor = torch.FloatTensor(data)  # 转换成32位浮点 tensor</code></li><li><code>torch.mm(tensor, tensor)</code><h2 id="2、变量Variable"><a href="#2、变量Variable" class="headerlink" title="2、变量Variable"></a>2、变量<code>Variable</code></h2><h3 id="1-说明"><a href="#1-说明" class="headerlink" title="(1) 说明"></a>(1) 说明</h3></li></ul></li><li><code>Variable</code> 就是一个存放会<strong>变化的值</strong>的位置</li><li>这里<strong>变化的值</strong>就是<strong>tensor</strong>数据<h3 id="2-使用"><a href="#2-使用" class="headerlink" title="(2) 使用"></a>(2) 使用</h3></li><li><p>导入包</p><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">import torch</div><div class="line">from torch.autograd import Variable # torch 中 Variable 模块</div></pre></td></tr></table></figure></li><li><p>定义<code>tensor</code>: <code>tensor = torch.FloatTensor([[1,2],[3,4]])</code></p></li><li>将<code>tensor</code>放入<code>Variable</code>: <code>variable = Variable(tensor, requires_grad=True)</code><ul><li><code>requires_grad</code> 是参不参与误差反向传播, 要不要计算<strong>梯度</strong></li><li><code>print(variable)</code> 会输出，(多出<code>Variable containing:</code>，表明是<code>Variable</code>)</li></ul></li></ul><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">Variable containing:</div><div class="line"> <span class="number">1</span>  <span class="number">2</span></div><div class="line"> <span class="number">3</span>  <span class="number">4</span></div><div class="line">[torch<span class="selector-class">.FloatTensor</span> of size <span class="number">2</span>x2]</div></pre></td></tr></table></figure><h3 id="3-计算梯度"><a href="#3-计算梯度" class="headerlink" title="(3) 计算梯度"></a>(3) 计算梯度</h3><ul><li><code>v_out = torch.mean(variable*variable)   # x^2</code></li><li><code>v_out.backward()      # 模拟 v_out 的误差反向传递</code></li><li><code>print(variable.grad)  # 显示 Variable 的梯度</code><ul><li>输出结果如下</li><li>因为<code>torch.mean(variable*variable)</code>是<code>1/4*x^2</code>，导数就是<code>1/2x</code><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="number">0.5000</span>  <span class="number">1.0000</span></div><div class="line"><span class="number">1.5000</span>  <span class="number">2.0000</span></div></pre></td></tr></table></figure></li></ul></li></ul><h3 id="4-Variable里面的数据"><a href="#4-Variable里面的数据" class="headerlink" title="(4) Variable里面的数据"></a>(4) Variable里面的数据</h3><ul><li>直接<code>print(variable)</code>只会输出 <code>Variable</code> 形式的数据, 在很多时候是用不了的(比如想要用 <code>plt</code> 画图), 所以我们要转换一下, 将它变成 <code>tensor</code> 形式.</li><li>获取 <code>tensor</code> 数据：<code>print(variable.data)    # tensor 形式</code><ul><li>然后也可以转而<code>numpy</code>数据：<code>print(variable.data.numpy())    # numpy 形式</code><h2 id="3、Torch-中的激励函数"><a href="#3、Torch-中的激励函数" class="headerlink" title="3、Torch 中的激励函数"></a>3、<code>Torch</code> 中的激励函数</h2></li></ul></li><li>导入包：<code>import torch.nn.functional as F     # 激励函数都在这</code></li><li>平时要用到的就这几个. <code>relu, sigmoid, tanh, softplus</code></li><li>激励函数<ul><li><code>x</code>是<code>Variable</code>数据，<code>F.relu(x)</code>也是返回<code>Variable</code>数据，然后<code>.data</code>获取 <code>tensor</code> 数据<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"># 做一些假数据来观看图像</div><div class="line">x = torch.linspace(-5, 5, 200)  # x data (tensor), shape=(100, 1)</div><div class="line">x = Variable(x)</div><div class="line">x_np = x.data.numpy()   # 换成 numpy array, 出图时用</div></pre></td></tr></table></figure></li></ul></li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">y_relu = F.relu(x).data.numpy()</div><div class="line">y_sigmoid = F.sigmoid(x).data.numpy()</div><div class="line">y_tanh = F.tanh(x).data.numpy()</div><div class="line">y_softplus = F.softplus(x).data.numpy()</div><div class="line"># y_softmax = F.softmax(x)  softmax 比较特殊, 不能直接显示, 不过他是关于概率的, 用于分类</div></pre></td></tr></table></figure><ul><li><code>softplus</code>的公式为：<code>f(x)=ln(1+ex)</code></li></ul><h1 id="三、建立基础的神经网络"><a href="#三、建立基础的神经网络" class="headerlink" title="三、建立基础的神经网络"></a>三、建立基础的神经网络</h1><h2 id="1、回归问题"><a href="#1、回归问题" class="headerlink" title="1、回归问题"></a>1、回归问题</h2><h3 id="1-准备工作"><a href="#1-准备工作" class="headerlink" title="(1) 准备工作"></a>(1) 准备工作</h3><ul><li>导入包</li></ul><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">import torch</div><div class="line">from torch<span class="selector-class">.autograd</span> import Variable</div><div class="line">import torch<span class="selector-class">.nn</span><span class="selector-class">.functional</span> as F </div><div class="line">import matplotlib<span class="selector-class">.pyplot</span> as plt</div></pre></td></tr></table></figure><ul><li><p>制造假数据</p><ul><li><code>torch.unsqueeze</code>是转成2维的数据<code>[[]]</code>,加上一个<strong>假的维度</strong><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">x = torch.unsqueeze(torch.linspace(-1, 1, 100), dim=1)  # x data (tensor), shape=(100, 1)</div><div class="line">y = x.pow(2) + 0.2*torch.rand(x.size())                 # noisy y data (tensor), shape=(100, 1)</div></pre></td></tr></table></figure></li></ul></li><li><p>定义<code>Variable</code>: <code>x, y = torch.autograd.Variable(x), Variable(y)</code></p><h3 id="2-建立神经网络"><a href="#2-建立神经网络" class="headerlink" title="(2) 建立神经网络"></a>(2) 建立神经网络</h3></li><li><p>使用类的方式class</p><ul><li>继承<code>torch.nn.Module</code></li><li>这里只包含一个隐层，<code>__init__</code>只是定义了几个层</li><li><code>forward</code>进行传播，也就是整个网络的搭建，<strong>因为是预测，最后一层不需要激励函数</strong><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">class Net(torch.nn.Module):  # 继承 torch 的 Module</div><div class="line">    def __init__(self, n_feature, n_hidden, n_output):</div><div class="line">        super(Net, self).__init__()     # 继承 __init__ 功能</div><div class="line">        # 定义每层用什么样的形式</div><div class="line">        self.hidden = torch.nn.Linear(n_feature, n_hidden)   # 隐藏层线性输出</div><div class="line">        self.predict = torch.nn.Linear(n_hidden, n_output)   # 输出层线性输出</div><div class="line"></div><div class="line">    def forward(self, x):   # 这同时也是 Module 中的 forward 功能</div><div class="line">        # 正向传播输入值, 神经网络分析出输出值</div><div class="line">        x = F.relu(self.hidden(x))      # 激励函数(隐藏层的线性值)</div><div class="line">        x = self.predict(x)             # 输出值</div><div class="line">        return x</div></pre></td></tr></table></figure></li></ul></li><li><p>使用：<code>net = Net(n_feature=1, n_hidden=10, n_output=1)</code></p></li><li>输出：<code>print(net)</code>，结果为</li></ul><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">Net (</div><div class="line">  (hidden): Linear (<span class="number">1</span> -&gt; <span class="number">10</span>)</div><div class="line">  (predict): Linear (<span class="number">10</span> -&gt; <span class="number">1</span>)</div><div class="line">)</div></pre></td></tr></table></figure><h3 id="3-训练网络"><a href="#3-训练网络" class="headerlink" title="(3) 训练网络"></a>(3) 训练网络</h3><ul><li>定义优化器：<code>optimizer = torch.optim.SGD(net.parameters(), lr=0.5)  # 传入 net 的所有参数, 学习率lr</code></li><li>定义损失函数：<code>loss_func = torch.nn.MSELoss()      # 预测值和真实值的误差计算公式 (均方差)</code></li><li>训练</li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">for t in range(100):</div><div class="line">    prediction = net(x)     # 喂给 net 训练数据 x, 输出预测值</div><div class="line"></div><div class="line">    loss = loss_func(prediction, y)     # 计算两者的误差</div><div class="line"></div><div class="line">    optimizer.zero_grad()   # 清空上一步的残余更新参数值</div><div class="line">    loss.backward()         # 误差反向传播, 计算参数更新值</div><div class="line">    optimizer.step()        # 将参数更新值施加到 net 的 parameters 上</div></pre></td></tr></table></figure><h2 id="2、分类问题"><a href="#2、分类问题" class="headerlink" title="2、分类问题"></a>2、分类问题</h2><h3 id="1-准备工作-1"><a href="#1-准备工作-1" class="headerlink" title="(1) 准备工作"></a>(1) 准备工作</h3><ul><li>导入包</li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">import torch</div><div class="line">from torch.autograd import Variable</div><div class="line">import torch.nn.functional as F     # 激励函数都在这</div><div class="line">import matplotlib.pyplot as plt</div></pre></td></tr></table></figure><ul><li><p>制造假数据</p><ul><li><code>x0</code>是一个类别的<code>x1,x2</code></li><li><code>y0</code>就是对应这个类别的 <code>label</code>，这里是<code>0</code></li><li>然后将<code>x0,x1</code>，<code>y0,y1</code>合并在一起<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"># 假数据</div><div class="line">n_data = torch.ones(100, 2)         # 数据的基本形态</div><div class="line">x0 = torch.normal(2*n_data, 1)      # 类型0 x data (tensor), shape=(100, 2)</div><div class="line">y0 = torch.zeros(100)               # 类型0 y data (tensor), shape=(100, 1)</div><div class="line">x1 = torch.normal(-2*n_data, 1)     # 类型1 x data (tensor), shape=(100, 1)</div><div class="line">y1 = torch.ones(100)                # 类型1 y data (tensor), shape=(100, 1)</div><div class="line"># 注意 x, y 数据的数据形式是一定要像下面一样 (torch.cat 是在合并数据)</div><div class="line">x = torch.cat((x0, x1), 0).type(torch.FloatTensor)  # FloatTensor = 32-bit floating</div><div class="line">y = torch.cat((y0, y1), ).type(torch.LongTensor)    # LongTensor = 64-bit integer</div></pre></td></tr></table></figure></li></ul></li><li><p>定义Variable: <code>x, y = Variable(x), Variable(y)</code></p></li></ul><h3 id="2-建立网络"><a href="#2-建立网络" class="headerlink" title="(2) 建立网络"></a>(2) 建立网络</h3><ul><li><p>与上面回归的例子类似</p><ul><li>使用<code>relu</code>激励函数</li><li>这里最后一层并没有使用<strong>激励函数</strong>或是<code>softmax</code>，因为下面使用了<code>CrossEntropyLoss</code>，这个里面默认会调用<code>log_softmax</code>函数(<code>nll_loss(log_softmax(input), target, weight, size_average)</code>)</li><li>当然这里也可以最后返回<code>F.softmax(x)</code>, 那么下面的损失函数就是<code>loss = F.nll_loss(out, y)</code><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">class Net(torch.nn.Module):     # 继承 torch 的 Module</div><div class="line">    def __init__(self, n_feature, n_hidden, n_output):</div><div class="line">        super(Net, self).__init__()     # 继承 __init__ 功能</div><div class="line">        self.hidden = torch.nn.Linear(n_feature, n_hidden)   # 隐藏层线性输出</div><div class="line">        self.out = torch.nn.Linear(n_hidden, n_output)       # 输出层线性输出</div><div class="line"></div><div class="line">    def forward(self, x):</div><div class="line">        # 正向传播输入值, 神经网络分析出输出值</div><div class="line">        x = F.relu(self.hidden(x))      # 激励函数(隐藏层的线性值)</div><div class="line">        x = self.out(x)                 # 输出值, 但是这个不是预测值, 预测值还需要再另外计算</div><div class="line">        return x</div></pre></td></tr></table></figure></li></ul></li><li><p>建立网络：<code>net = Net(n_feature=2, n_hidden=10, n_output=2) # 几个类别就几个 output</code></p><h3 id="3-训练网络-1"><a href="#3-训练网络-1" class="headerlink" title="(3) 训练网络"></a>(3) 训练网络</h3></li><li>优化器：<code>optimizer = torch.optim.SGD(net.parameters(), lr=0.02)  # 传入 net 的所有参数, 学习率</code></li><li>损失函数：<code>loss_func = torch.nn.CrossEntropyLoss()</code></li><li>训练：</li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">for t in range(100):</div><div class="line">    out = net(x)     # 喂给 net 训练数据 x, 输出分析值</div><div class="line"></div><div class="line">    loss = loss_func(out, y)     # 计算两者的误差</div><div class="line"></div><div class="line">    optimizer.zero_grad()   # 清空上一步的残余更新参数值</div><div class="line">    loss.backward()         # 误差反向传播, 计算参数更新值</div><div class="line">    optimizer.step()        # 将参数更新值施加到 net 的 parameters 上</div></pre></td></tr></table></figure><ul><li>预测：<ul><li>输出至最大的那个（概率最大的）坐标<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"># 过了一道 softmax 的激励函数后的最大概率才是预测值</div><div class="line">        prediction = torch.max(F.softmax(out), 1)[1]</div></pre></td></tr></table></figure></li></ul></li></ul><h2 id="3、快速搭建神经网络"><a href="#3、快速搭建神经网络" class="headerlink" title="3、快速搭建神经网络"></a>3、快速搭建神经网络</h2><ul><li>使用<code>torch.nn.Sequential</code></li></ul><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">net2 = torch<span class="selector-class">.nn</span><span class="selector-class">.Sequential</span>(</div><div class="line">    torch<span class="selector-class">.nn</span><span class="selector-class">.Linear</span>(<span class="number">1</span>, <span class="number">10</span>),</div><div class="line">    torch<span class="selector-class">.nn</span><span class="selector-class">.ReLU</span>(),</div><div class="line">    torch<span class="selector-class">.nn</span><span class="selector-class">.Linear</span>(<span class="number">10</span>, <span class="number">1</span>)</div><div class="line">)</div></pre></td></tr></table></figure><ul><li>输出：<code>print(net2)</code><ul><li>相比我们之前自己定义的类，<strong>激励函数</strong>也显示出来了<figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">Sequential (</div><div class="line">  (<span class="number">0</span>): Linear (<span class="number">1</span> -&gt; <span class="number">10</span>)</div><div class="line">  (<span class="number">1</span>): ReLU ()</div><div class="line">  (<span class="number">2</span>): Linear (<span class="number">10</span> -&gt; <span class="number">1</span>)</div><div class="line">)</div></pre></td></tr></table></figure></li></ul></li></ul><h2 id="4、保存和提取"><a href="#4、保存和提取" class="headerlink" title="4、保存和提取"></a>4、保存和提取</h2><h3 id="1-保存（两种方法）"><a href="#1-保存（两种方法）" class="headerlink" title="(1) 保存（两种方法）"></a>(1) 保存（两种方法）</h3><ul><li>保存整个网络<ul><li><code>torch.save(net1, &#39;net.pkl&#39;)</code>  # 保存整个网络，<code>net1</code> 就是定义的网络</li></ul></li><li>只保存网络中的参数<ul><li><code>torch.save(net1.state_dict(), &#39;net_params.pkl&#39;)</code>   # 只保存网络中的参数 (速度快, 占内存少)<h3 id="2-提取（也是两种方法）"><a href="#2-提取（也是两种方法）" class="headerlink" title="(2) 提取（也是两种方法）"></a>(2) 提取（也是两种方法）</h3></li></ul></li><li>提取整个网络：<ul><li><code>net2 = torch.load(&#39;net.pkl&#39;)</code></li><li><code>prediction = net2(x)</code></li></ul></li><li>只提取网络参数<ul><li>首先需要定义一样的神经网络</li><li><code>net3.load_state_dict(torch.load(&#39;net_params.pkl&#39;))</code></li><li><code>prediction = net3(x)</code><h2 id="5、批训练SGD"><a href="#5、批训练SGD" class="headerlink" title="5、批训练SGD"></a>5、批训练SGD</h2></li></ul></li><li>上面我们虽然是<code>torch.optim.SGD</code>进行优化，但是还是将所有数据放进去训练<h3 id="1-DataLoader"><a href="#1-DataLoader" class="headerlink" title="(1) DataLoader"></a>(1) DataLoader</h3></li><li>是 <code>torch</code> 用来包装你的数据(<code>tensor</code>)的工具</li><li>导入包： <code>import torch.utils.data as Data</code></li><li><p>将tensor数据转为<code>torch</code>能识别的<code>Dataset</code></p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">torch_dataset = Data.TensorDataset(data_tensor=x, target_tensor=y)</div></pre></td></tr></table></figure></li><li><p>把 dataset 放入 DataLoader</p><ul><li><code>BATCH_SIZE</code>是我们定义的<code>batch</code>大小<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">loader = Data.DataLoader(</div><div class="line">    dataset=torch_dataset,      # torch TensorDataset format</div><div class="line">    batch_size=BATCH_SIZE,      # mini batch size</div><div class="line">    shuffle=True,               # 要不要打乱数据 (打乱比较好)</div><div class="line">    num_workers=2,              # 多线程来读数据</div><div class="line">)</div></pre></td></tr></table></figure></li></ul></li><li><p>就可以得到<code>batch</code>数据了</p><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">for epoch in range(3):   # 训练所有!整套!数据 3 次</div><div class="line">    for step, (batch_x, batch_y) in enumerate(loader):  # 每一步 loader 释放一小批数据用来学习</div><div class="line">        # 假设这里就是你训练的地方...</div><div class="line"></div><div class="line">        # 打出来一些数据</div><div class="line">        print('Epoch: ', epoch, '| Step: ', step, '| batch x: ',</div><div class="line">              batch_x.numpy(), '| batch y: ', batch_y.numpy())</div></pre></td></tr></table></figure></li><li><p>这里还是<code>tensor</code>数据，真正训练时还要放到<code>Variable</code>中</p></li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">b_x = Variable(batch_x)  # 务必要用 Variable 包一下</div><div class="line">b_y = Variable(batch_y)</div></pre></td></tr></table></figure><h2 id="6、优化器-optimizer"><a href="#6、优化器-optimizer" class="headerlink" title="6、优化器 optimizer"></a>6、优化器 optimizer</h2><ul><li><code>SGD</code><ul><li>就是随机梯度下降</li></ul></li><li><code>momentum</code><ul><li>动量加速</li><li>在<code>SGD</code>函数里指定<code>momentum</code>的值即可</li></ul></li><li><code>RMSprop</code><ul><li>指定参数<code>alpha</code></li></ul></li><li><code>Adam</code><ul><li>参数<code>betas=(0.9, 0.99)</code><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">opt_SGD         = torch<span class="selector-class">.optim</span><span class="selector-class">.SGD</span>(net_SGD.parameters(), lr=LR)</div><div class="line">opt_Momentum    = torch<span class="selector-class">.optim</span><span class="selector-class">.SGD</span>(net_Momentum.parameters(), lr=LR, momentum=<span class="number">0.8</span>)</div><div class="line">opt_RMSprop     = torch<span class="selector-class">.optim</span><span class="selector-class">.RMSprop</span>(net_RMSprop.parameters(), lr=LR, alpha=<span class="number">0.9</span>)</div><div class="line">opt_Adam        = torch<span class="selector-class">.optim</span><span class="selector-class">.Adam</span>(net_Adam.parameters(), lr=LR, betas=(<span class="number">0.9</span>, <span class="number">0.99</span>))</div></pre></td></tr></table></figure></li></ul></li></ul><h1 id="四、高级神经网络"><a href="#四、高级神经网络" class="headerlink" title="四、高级神经网络"></a>四、高级神经网络</h1><h2 id="1、卷积神经网络-CNN"><a href="#1、卷积神经网络-CNN" class="headerlink" title="1、卷积神经网络 CNN"></a>1、卷积神经网络 <code>CNN</code></h2><ul><li>使用mnist数据集</li><li>导入包</li></ul><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">import torch</div><div class="line">from torch<span class="selector-class">.autograd</span> import Variable</div><div class="line">import torch<span class="selector-class">.utils</span><span class="selector-class">.data</span> as Data</div><div class="line">import torchvision</div><div class="line">from matplotlib import pyplot as plt</div></pre></td></tr></table></figure><ul><li>下载数据集</li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">EPOCH = 10</div><div class="line">BATCH_SIZE = 50</div><div class="line">LR = 0.001</div><div class="line">train_data = torchvision.datasets.MNIST(root='./mnist', </div><div class="line">                                        transform=torchvision.transforms.ToTensor(),</div><div class="line">                                        download=False)  # first set True, then set False</div><div class="line"></div><div class="line">print(train_data.train_data.size())</div><div class="line">test_data = torchvision.datasets.MNIST(root='./mnist', train=False)</div></pre></td></tr></table></figure><ul><li><p>处理数据</p><ul><li>使用 <code>DataLoader</code> 进行<code>batch</code>训练</li><li>将测试数据放到<code>Variable</code>里，并加上一个维度（在第二维位置dim=1），因为下面训练时是<code>(batch_size, 1, 28, 28)</code><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">train_loader = Data.DataLoader(dataset=train_data, batch_size=128, shuffle=True)</div><div class="line"># shape from (total_size, 28, 28) to (total_size, 1, 28, 28)</div><div class="line">test_x = Variable(torch.unsqueeze(test_data.test_data, dim=1), volatile=True).type(torch.FloatTensor)/255.0</div><div class="line"></div><div class="line">test_y = test_data.test_labels</div></pre></td></tr></table></figure></li></ul></li><li><p>建立计算图模型</p></li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line">class CNN(torch.nn.Module):</div><div class="line">    def __init__(self):</div><div class="line">        super(CNN, self).__init__()</div><div class="line">        self.conv1 = torch.nn.Sequential( # input shape (1, 28, 28)</div><div class="line">            torch.nn.Conv2d(in_channels=1,</div><div class="line">                            out_channels=16,</div><div class="line">                            kernel_size=5,</div><div class="line">                            stride=1, </div><div class="line">                            padding=2),</div><div class="line">            torch.nn.ReLU(),</div><div class="line">            torch.nn.MaxPool2d(kernel_size=2) </div><div class="line">        )   # output shape (16, 14, 14)</div><div class="line">        self.conv2 = torch.nn.Sequential(</div><div class="line">            torch.nn.Conv2d(16, 32, 5, 1, 2),</div><div class="line">            torch.nn.ReLU(),</div><div class="line">            torch.nn.MaxPool2d(2)</div><div class="line">        )  # output shape (32, 7, 7)</div><div class="line">        self.out = torch.nn.Linear(in_features=32*7*7, out_features=10)</div><div class="line">    def forward(self, x):</div><div class="line">        x = self.conv1(x)</div><div class="line">        x = self.conv2(x)</div><div class="line">        x = x.view(x.size(0), -1) # 展平多维的卷积图成 (batch_size, 32 * 7 * 7)</div><div class="line">        output = self.out(x)</div><div class="line">        return output</div><div class="line">cnn = CNN()</div></pre></td></tr></table></figure><ul><li>定义优化器<code>optimizer</code>和损失</li></ul><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">optimizer = torch<span class="selector-class">.optim</span><span class="selector-class">.Adam</span>(cnn.parameters(), lr=LR)</div><div class="line">loss_func = torch<span class="selector-class">.nn</span><span class="selector-class">.CrossEntropyLoss</span>()</div></pre></td></tr></table></figure><ul><li>进行batch训练</li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">for epoch in range(EPOCH):</div><div class="line">    for i, (x, y) in enumerate(train_loader):</div><div class="line">        batch_x = Variable(x)</div><div class="line">        batch_y = Variable(y)</div><div class="line">        output = cnn(batch_x)</div><div class="line">        loss = loss_func(output, batch_y)</div><div class="line">        optimizer.zero_grad()</div><div class="line">        loss.backward()</div><div class="line">        optimizer.step()</div><div class="line">        if i % 50 == 0:</div><div class="line">            # 用 test 数据来验证准确率</div><div class="line">            test_output = cnn(test_x)</div><div class="line">            pred_y = torch.max(test_output, 1)[1].data.squeeze()</div><div class="line">            accuracy = sum(pred_y == test_y) / float(test_y.size(0))</div><div class="line">            print('Epoch: ', epoch, '| train loss: %.4f' % loss.data[0], '| test accuracy: %.2f' % accuracy)</div></pre></td></tr></table></figure><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul><li><a href="https://morvanzhou.github.io/tutorials/machine-learning/torch/" target="_blank" rel="external">https://morvanzhou.github.io/tutorials/machine-learning/torch/</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;一、PyTorch介绍&quot;&gt;&lt;a href=&quot;#一、PyTorch介绍&quot; class=&quot;headerlink&quot; title=&quot;一、PyTorch介绍&quot;&gt;&lt;/a&gt;一、PyTorch介绍&lt;/h1&gt;&lt;h2 id=&quot;1、说明&quot;&gt;&lt;a href=&quot;#1、说明&quot; class=&quot;headerlink&quot; title=&quot;1、说明&quot;&gt;&lt;/a&gt;1、说明&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;code&gt;PyTorch&lt;/code&gt; 是 &lt;code&gt;Torch&lt;/code&gt; 在 &lt;code&gt;Python&lt;/code&gt; 上的衍生（&lt;code&gt;Torch&lt;/code&gt; 是一个使用 &lt;code&gt;Lua&lt;/code&gt; 语言的神经网络库）&lt;/li&gt;
&lt;li&gt;和&lt;code&gt;tensorflow&lt;/code&gt;比较&lt;ul&gt;
&lt;li&gt;&lt;code&gt;PyTorch&lt;/code&gt;建立的神经网络是&lt;strong&gt;动态的&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Tensorflow&lt;/code&gt;是建立&lt;strong&gt;静态图&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Tensorflow&lt;/code&gt; 的高度工业化, 它的底层代码是很难看懂的. &lt;/li&gt;
&lt;li&gt;&lt;code&gt;PyTorch&lt;/code&gt; 好那么一点点, 如果你深入 &lt;code&gt;API&lt;/code&gt;, 你至少能比看 &lt;code&gt;Tensorflow&lt;/code&gt; 多看懂一点点 &lt;code&gt;PyTorch&lt;/code&gt; 的底层在干嘛.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="Python" scheme="http://lawlite.cn/tags/Python/"/>
    
      <category term="PyTorch" scheme="http://lawlite.cn/tags/PyTorch/"/>
    
  </entry>
  
  <entry>
    <title>Hexo+yilia主题实现文章目录和添加视频</title>
    <link href="http://lawlite.cn/2017/04/17/Hexo-yilia%E4%B8%BB%E9%A2%98%E5%AE%9E%E7%8E%B0%E6%96%87%E7%AB%A0%E7%9B%AE%E5%BD%95%E5%92%8C%E6%B7%BB%E5%8A%A0%E8%A7%86%E9%A2%91/"/>
    <id>http://lawlite.cn/2017/04/17/Hexo-yilia主题实现文章目录和添加视频/</id>
    <published>2017-04-17T07:00:21.000Z</published>
    <updated>2017-09-08T10:40:44.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一、说明"><a href="#一、说明" class="headerlink" title="一、说明"></a>一、说明</h1><ul><li>文章目录功能可以<a href="http://lawlite.me/2017/04/13/Hexo-Github%E5%AE%9E%E7%8E%B0%E7%9B%B8%E5%86%8C%E5%8A%9F%E8%83%BD/" target="_blank" rel="external">点击这里查看</a></li><li>视频页面可以<a href="http://lawlite.me/photos/videos.html" target="_blank" rel="external">点击这里查看</a></li><li>粗略实现，有问题可以在下方评论区留言，或者<a href="http://lawlite.me/%E7%95%99%E8%A8%80%E6%9D%BF/" target="_blank" rel="external">留言板</a>留言</li></ul><h1 id="二、文章目录功能"><a href="#二、文章目录功能" class="headerlink" title="二、文章目录功能"></a>二、文章目录功能</h1><h2 id="1、添加CSS样式"><a href="#1、添加CSS样式" class="headerlink" title="1、添加CSS样式"></a>1、添加CSS样式</h2><ul><li>打开<code>themes\yilia\source</code>下的<code>main.234bc0.css</code>文件，添加如下代码：<ul><li><code>css</code>样式我也放到了<code>github</code>上：<a href="https://raw.githubusercontent.com/lawlite19/Blog-Back-Up/master/css/main.234bc0.css" target="_blank" rel="external">https://raw.githubusercontent.com/lawlite19/Blog-Back-Up/master/css/main.234bc0.css</a></li><li>使用的是别人的<code>css</code>，可能有冗余的部分</li></ul></li></ul><a id="more"></a><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">/* 新添加的 */</div><div class="line">#container .show-toc-btn,#container .toc-article&#123;display:block&#125;</div><div class="line">.toc-article&#123;z-index:100;background:#fff;border:1px solid #ccc;max-width:250px;min-width:150px;max-height:500px;overflow-y:auto;-webkit-box-shadow:5px 5px 2px #ccc;box-shadow:5px 5px 2px #ccc;font-size:12px;padding:10px;position:fixed;right:35px;top:129px&#125;.toc-article .toc-close&#123;font-weight:700;font-size:20px;cursor:pointer;float:right;color:#ccc&#125;.toc-article .toc-close:hover&#123;color:#000&#125;.toc-article .toc&#123;font-size:12px;padding:0;line-height:20px&#125;.toc-article .toc .toc-number&#123;color:#333&#125;.toc-article .toc .toc-text:hover&#123;text-decoration:underline;color:#2a6496&#125;.toc-article li&#123;list-style-type:none&#125;.toc-article .toc-level-1&#123;margin:4px 0&#125;.toc-article .toc-child&#123;&#125;@-moz-keyframes cd-bounce-1&#123;0%&#123;opacity:0;-o-transform:scale(1);-webkit-transform:scale(1);-moz-transform:scale(1);-ms-transform:scale(1);transform:scale(1)&#125;60%&#123;opacity:1;-o-transform:scale(1.01);-webkit-transform:scale(1.01);-moz-transform:scale(1.01);-ms-transform:scale(1.01);transform:scale(1.01)&#125;100%&#123;-o-transform:scale(1);-webkit-transform:scale(1);-moz-transform:scale(1);-ms-transform:scale(1);transform:scale(1)&#125;&#125;@-webkit-keyframes cd-bounce-1&#123;0%&#123;opacity:0;-o-transform:scale(1);-webkit-transform:scale(1);-moz-transform:scale(1);-ms-transform:scale(1);transform:scale(1)&#125;60%&#123;opacity:1;-o-transform:scale(1.01);-webkit-transform:scale(1.01);-moz-transform:scale(1.01);-ms-transform:scale(1.01);transform:scale(1.01)&#125;100%&#123;-o-transform:scale(1);-webkit-transform:scale(1);-moz-transform:scale(1);-ms-transform:scale(1);transform:scale(1)&#125;&#125;@-o-keyframes cd-bounce-1&#123;0%&#123;opacity:0;-o-transform:scale(1);-webkit-transform:scale(1);-moz-transform:scale(1);-ms-transform:scale(1);transform:scale(1)&#125;60%&#123;opacity:1;-o-transform:scale(1.01);-webkit-transform:scale(1.01);-moz-transform:scale(1.01);-ms-transform:scale(1.01);transform:scale(1.01)&#125;100%&#123;-o-transform:scale(1);-webkit-transform:scale(1);-moz-transform:scale(1);-ms-transform:scale(1);transform:scale(1)&#125;&#125;@keyframes cd-bounce-1&#123;0%&#123;opacity:0;-o-transform:scale(1);-webkit-transform:scale(1);-moz-transform:scale(1);-ms-transform:scale(1);transform:scale(1)&#125;60%&#123;opacity:1;-o-transform:scale(1.01);-webkit-transform:scale(1.01);-moz-transform:scale(1.01);-ms-transform:scale(1.01);transform:scale(1.01)&#125;100%&#123;-o-transform:scale(1);-webkit-transform:scale(1);-moz-transform:scale(1);-ms-transform:scale(1);transform:scale(1)&#125;&#125;.show-toc-btn&#123;display:none;z-index:10;width:30px;min-height:14px;overflow:hidden;padding:4px 6px 8px 5px;border:1px solid #ddd;border-right:none;position:fixed;right:40px;text-align:center;background-color:#f9f9f9&#125;.show-toc-btn .btn-bg&#123;margin-top:2px;display:block;width:16px;height:14px;background:url(http://7xtawy.com1.z0.glb.clouddn.com/show.png) no-repeat;-webkit-background-size:100%;-moz-background-size:100%;background-size:100%&#125;.show-toc-btn .btn-text&#123;color:#999;font-size:12px&#125;.show-toc-btn:hover&#123;cursor:pointer&#125;.show-toc-btn:hover .btn-bg&#123;background-position:0 -16px&#125;.show-toc-btn:hover .btn-text&#123;font-size:12px;color:#ea8010&#125;</div><div class="line"></div><div class="line">.toc-article li ol, .toc-article li ul &#123;</div><div class="line">    margin-left: 30px;</div><div class="line">&#125;</div><div class="line">.toc-article ol, .toc-article ul &#123;</div><div class="line">    margin: 10px 0;</div><div class="line">&#125;</div></pre></td></tr></table></figure><h2 id="2、修改article-ejs文件"><a href="#2、修改article-ejs文件" class="headerlink" title="2、修改article.ejs文件"></a>2、修改article.ejs文件</h2><ul><li>使用的是<code>Hexo</code>的<code>toc</code>函数</li><li>打开<code>themes\yilia\layout\_partial</code>文件夹下的<code>article.ejs</code>文件</li><li>在<code>&lt;/header&gt; &lt;% } %&gt;</code>下面加入如下内容（注意位置）</li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line">&lt;!-- 目录内容 --&gt;</div><div class="line">&lt;% if (!index &amp;&amp; post.toc)&#123; %&gt;</div><div class="line">    &lt;p class="show-toc-btn" id="show-toc-btn" onclick="showToc();" style="display:none"&gt;</div><div class="line">          &lt;span class="btn-bg"&gt;&lt;/span&gt;</div><div class="line">          &lt;span class="btn-text"&gt;文章导航&lt;/span&gt;</div><div class="line">          &lt;/p&gt;</div><div class="line">&lt;div id="toc-article" class="toc-article"&gt;</div><div class="line">    &lt;span id="toc-close" class="toc-close" title="隐藏导航" onclick="showBtn();"&gt;×&lt;/span&gt;</div><div class="line">&lt;strong class="toc-title"&gt;文章目录&lt;/strong&gt;</div><div class="line">           &lt;%- toc(post.content) %&gt;</div><div class="line">         &lt;/div&gt;</div><div class="line">   &lt;script type="text/javascript"&gt;</div><div class="line">function showToc()&#123;</div><div class="line">var toc_article = document.getElementById("toc-article");</div><div class="line">var show_toc_btn = document.getElementById("show-toc-btn");</div><div class="line">toc_article.setAttribute("style","display:block");</div><div class="line">show_toc_btn.setAttribute("style","display:none");</div><div class="line">&#125;;</div><div class="line">function showBtn()&#123;</div><div class="line">var toc_article = document.getElementById("toc-article");</div><div class="line">var show_toc_btn = document.getElementById("show-toc-btn");</div><div class="line">toc_article.setAttribute("style","display:none");</div><div class="line">show_toc_btn.setAttribute("style","display:block");</div><div class="line">&#125;;</div><div class="line">   &lt;/script&gt;</div><div class="line">      &lt;% &#125; %&gt;</div><div class="line">&lt;!-- 目录内容结束 --&gt;</div></pre></td></tr></table></figure><ul><li>然后若想要文章显示目录，在每篇文章开头加入：<code>toc: true</code></li></ul><h2 id="3、最终效果"><a href="#3、最终效果" class="headerlink" title="3、最终效果"></a>3、最终效果</h2><h3 id="1-电脑端"><a href="#1-电脑端" class="headerlink" title="(1) 电脑端"></a>(1) 电脑端</h3><ul><li><img src="/assets/blog_images/hexo+github/15.png" alt="电脑端显示目录" title="15"></li><li><img src="/assets/blog_images/hexo+github/16.png" alt="电脑端关闭目录" title="16"></li></ul><h3 id="2-手机端"><a href="#2-手机端" class="headerlink" title="(2) 手机端"></a>(2) 手机端</h3><ul><li><img src="/assets/blog_images/hexo+github/17.png" alt="手机端显示目录" title="17"></li><li><img src="/assets/blog_images/hexo+github/18.png" alt="手机端关闭目录" title="18"></li></ul><h1 id="三、添加视频"><a href="#三、添加视频" class="headerlink" title="三、添加视频"></a>三、添加视频</h1><ul><li>是在之前<strong>相册功能</strong>的基础之上，相册功能的实现<a href="http://lawlite.me/2017/04/13/Hexo-Github%E5%AE%9E%E7%8E%B0%E7%9B%B8%E5%86%8C%E5%8A%9F%E8%83%BD/" target="_blank" rel="external">点击这里查看</a></li></ul><h2 id="1、添加视频样式css"><a href="#1、添加视频样式css" class="headerlink" title="1、添加视频样式css"></a>1、添加视频样式css</h2><ul><li>打开<strong>当前博客</strong><code>source\photos</code>文件夹下的<code>ins.css</code>文件</li><li>加入如下内容</li></ul><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/* ====== video ===== */</span></div><div class="line"><span class="selector-class">.video-container</span> &#123;</div><div class="line"><span class="attribute">z-index</span>: <span class="number">1</span>;</div><div class="line"><span class="attribute">position</span>: relative;</div><div class="line"><span class="attribute">padding-bottom</span>: <span class="number">56.25%</span>;</div><div class="line"><span class="attribute">margin</span>: <span class="number">0</span> auto;</div><div class="line"></div><div class="line">&#125;</div><div class="line"><span class="selector-class">.video-container</span> <span class="selector-tag">iframe</span>, <span class="selector-class">.video-container</span> <span class="selector-tag">object</span>, <span class="selector-class">.video-container</span> embed &#123;<span class="attribute">z-index</span>: <span class="number">1</span>;<span class="attribute">position</span>: absolute;<span class="attribute">top</span>: <span class="number">0</span>;<span class="attribute">left</span>: <span class="number">7%</span>;<span class="attribute">width</span>: <span class="number">85%</span>;<span class="attribute">height</span>: <span class="number">85%</span>;<span class="attribute">box-shadow</span>: <span class="number">0px</span> <span class="number">0px</span> <span class="number">20px</span> <span class="number">2px</span> <span class="number">#888888</span>;&#125;</div></pre></td></tr></table></figure><h2 id="2、添加视频"><a href="#2、添加视频" class="headerlink" title="2、添加视频"></a>2、添加视频</h2><ul><li>我这里添加的是<strong>优酷</strong>上面的视频</li><li>在<strong>当前博客</strong><code>source\photos</code>文件夹下建立<code>videos.ejs</code>文件</li><li>内容如下：</li></ul><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div></pre></td><td class="code"><pre><div class="line">---</div><div class="line">layout: post</div><div class="line">slug: <span class="string">"photos"</span></div><div class="line">title: <span class="string">"相册"</span></div><div class="line">noDate: <span class="string">"true"</span></div><div class="line">comments: <span class="string">"true"</span></div><div class="line">reward: <span class="string">"true"</span></div><div class="line">open_in_new: false</div><div class="line">---</div><div class="line">&lt;link rel=<span class="string">"stylesheet"</span> href=<span class="string">"./ins.css"</span>&gt;</div><div class="line">&lt;<span class="selector-tag">div</span> class=<span class="string">"photos-btn-wrap"</span>&gt;</div><div class="line">&lt;<span class="selector-tag">a</span> class=<span class="string">"photos-btn"</span> href=<span class="string">"/photos"</span>&gt;Photos&lt;/a&gt;</div><div class="line">&lt;<span class="selector-tag">a</span> class=<span class="string">"photos-btn active"</span> href=<span class="string">"/photos/videos.html"</span>&gt;Videos&lt;/a&gt;</div><div class="line">&lt;/div&gt;</div><div class="line"></div><div class="line"></div><div class="line">&lt;center&gt;&lt;h1&gt;指弹_女儿情&lt;/h1&gt;&lt;/center&gt;</div><div class="line">&lt;hr/&gt;</div><div class="line"></div><div class="line">&lt;center&gt;</div><div class="line">&lt;<span class="selector-tag">div</span> class=<span class="string">"video-container"</span>&gt;</div><div class="line">&lt;<span class="selector-tag">iframe</span> <span class="attribute">height</span>=<span class="string">"80%"</span> width=<span class="string">"80%"</span> src=<span class="string">"http://player.youku.com/embed/XMjUzMzY4OTM3Ng=="</span> </div><div class="line">frameborder=<span class="number">0</span> allowfullscreen&gt;</div><div class="line">&lt;/iframe&gt;</div><div class="line">&lt;/div&gt;</div><div class="line">&lt;/center&gt;</div><div class="line"></div><div class="line">&lt;hr/&gt;</div><div class="line"></div><div class="line">&lt;center&gt;&lt;h1&gt;指弹_友谊地久天长&lt;/h1&gt;&lt;/center&gt;</div><div class="line">&lt;hr/&gt;</div><div class="line">&lt;center&gt;</div><div class="line">&lt;<span class="selector-tag">div</span> class=<span class="string">"video-container"</span>&gt;</div><div class="line">&lt;<span class="selector-tag">iframe</span> <span class="attribute">height</span>=<span class="string">"80%"</span> width=<span class="string">"80%"</span> src=<span class="string">"http://player.youku.com/embed/XMjQ5MDExOTY2MA=="</span> </div><div class="line">frameborder=<span class="number">0</span> allowfullscreen&gt;</div><div class="line">&lt;/iframe&gt;</div><div class="line">&lt;/div&gt;</div><div class="line">&lt;/center&gt;</div><div class="line">&lt;hr/&gt;</div><div class="line"></div><div class="line">&lt;center&gt;&lt;h1&gt;指弹_Always with me&lt;/h1&gt;&lt;/center&gt;</div><div class="line">&lt;hr/&gt;</div><div class="line">&lt;center&gt;</div><div class="line">&lt;<span class="selector-tag">div</span> class=<span class="string">"video-container"</span>&gt;</div><div class="line">&lt;<span class="selector-tag">iframe</span> <span class="attribute">height</span>=<span class="string">"80%"</span> width=<span class="string">"80%"</span> src=<span class="string">"http://player.youku.com/embed/XMjQ4MDQyNTQ0MA=="</span> </div><div class="line">frameborder=<span class="number">0</span> allowfullscreen&gt;</div><div class="line">&lt;/iframe&gt;</div><div class="line">&lt;/div&gt;</div><div class="line">&lt;/center&gt;</div></pre></td></tr></table></figure><h2 id="3、最终效果-1"><a href="#3、最终效果-1" class="headerlink" title="3、最终效果"></a>3、最终效果</h2><h3 id="1-电脑端-1"><a href="#1-电脑端-1" class="headerlink" title="(1) 电脑端"></a>(1) 电脑端</h3><ul><li><img src="/assets/blog_images/hexo+github/19.png" alt="电脑端" title="19"></li></ul><h3 id="2-手机端-1"><a href="#2-手机端-1" class="headerlink" title="(2) 手机端"></a>(2) 手机端</h3><ul><li><img src="/assets/blog_images/hexo+github/20.png" alt="手机端" title="20"></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;一、说明&quot;&gt;&lt;a href=&quot;#一、说明&quot; class=&quot;headerlink&quot; title=&quot;一、说明&quot;&gt;&lt;/a&gt;一、说明&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;文章目录功能可以&lt;a href=&quot;http://lawlite.me/2017/04/13/Hexo-Github%E5%AE%9E%E7%8E%B0%E7%9B%B8%E5%86%8C%E5%8A%9F%E8%83%BD/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;点击这里查看&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;视频页面可以&lt;a href=&quot;http://lawlite.me/photos/videos.html&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;点击这里查看&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;粗略实现，有问题可以在下方评论区留言，或者&lt;a href=&quot;http://lawlite.me/%E7%95%99%E8%A8%80%E6%9D%BF/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;留言板&lt;/a&gt;留言&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&quot;二、文章目录功能&quot;&gt;&lt;a href=&quot;#二、文章目录功能&quot; class=&quot;headerlink&quot; title=&quot;二、文章目录功能&quot;&gt;&lt;/a&gt;二、文章目录功能&lt;/h1&gt;&lt;h2 id=&quot;1、添加CSS样式&quot;&gt;&lt;a href=&quot;#1、添加CSS样式&quot; class=&quot;headerlink&quot; title=&quot;1、添加CSS样式&quot;&gt;&lt;/a&gt;1、添加CSS样式&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;打开&lt;code&gt;themes\yilia\source&lt;/code&gt;下的&lt;code&gt;main.234bc0.css&lt;/code&gt;文件，添加如下代码：&lt;ul&gt;
&lt;li&gt;&lt;code&gt;css&lt;/code&gt;样式我也放到了&lt;code&gt;github&lt;/code&gt;上：&lt;a href=&quot;https://raw.githubusercontent.com/lawlite19/Blog-Back-Up/master/css/main.234bc0.css&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://raw.githubusercontent.com/lawlite19/Blog-Back-Up/master/css/main.234bc0.css&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;使用的是别人的&lt;code&gt;css&lt;/code&gt;，可能有冗余的部分&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="Github" scheme="http://lawlite.cn/tags/Github/"/>
    
      <category term="Hexo" scheme="http://lawlite.cn/tags/Hexo/"/>
    
  </entry>
  
  <entry>
    <title>Hexo+Github实现相册功能</title>
    <link href="http://lawlite.cn/2017/04/13/Hexo-Github%E5%AE%9E%E7%8E%B0%E7%9B%B8%E5%86%8C%E5%8A%9F%E8%83%BD/"/>
    <id>http://lawlite.cn/2017/04/13/Hexo-Github实现相册功能/</id>
    <published>2017-04-13T12:27:03.000Z</published>
    <updated>2017-06-25T08:48:03.000Z</updated>
    
    <content type="html"><![CDATA[<ul><li>最终效果请看这里：<a href="http://lawlite.me/photos/" target="_blank" rel="external">http://lawlite.me/photos/</a></li><li>粗略实现，有问题可以在下方评论区评论，或者<a href="http://lawlite.me/%E7%95%99%E8%A8%80%E6%9D%BF/" target="_blank" rel="external">留言区</a>留言</li></ul><h2 id="一、说明"><a href="#一、说明" class="headerlink" title="一、说明"></a>一、说明</h2><h3 id="1、关于"><a href="#1、关于" class="headerlink" title="1、关于"></a>1、关于</h3><ul><li>我使用的主题是<a href="https://github.com/litten/hexo-theme-yilia" target="_blank" rel="external">hexo-theme-yilia</a>，其中实现相册功能的方案是同步<code>instagram</code>上面的图片，但是现在<code>instagram</code>被禁，不能使用了</li><li>下面是通过自己的方式实现了相册功能，其中的<strong>样式</strong>还是使用该主题提供的<h3 id="2、方案"><a href="#2、方案" class="headerlink" title="2、方案"></a>2、方案</h3></li><li>在<code>github</code>上<strong>新建一个仓库</strong>，主要用于存储图片，可以通过<code>url</code>访问到，也方便管理</li><li>将要放到相册的图片处理成<code>json</code>格式的数据，然后进行访问，这里<code>json</code>的格式需要配合要使用的样式，所以需要处理成<strong>特定格式</strong>的<code>json</code>数据，下面会给出</li><li><strong>图片裁剪</strong>，因为相册显示的样式最好是<strong>正方形</strong>的的图片，这里使用脚本处理一下</li><li><strong>图片压缩</strong>，相册显示的图片是压缩后的图片，提高加载的速度，打开后的图片是原图。</li></ul><a id="more"></a><h2 id="二、实现"><a href="#二、实现" class="headerlink" title="二、实现"></a>二、实现</h2><h3 id="1、github操作"><a href="#1、github操作" class="headerlink" title="1、github操作"></a>1、github操作</h3><ul><li>建立一个用于存储相册的仓库，我这里建立了名为<code>Blog-Back-Up</code>的仓库<br><img src="/assets/blog_images/hexo+github/10.png" alt="enter description here" title="10"></li><li>关于<code>git</code>的命令行操作和配置不再给出</li></ul><h3 id="2、博客操作"><a href="#2、博客操作" class="headerlink" title="2、博客操作"></a>2、博客操作</h3><ul><li>在<strong>博客</strong>的<code>source</code>文件夹下建立一个<code>photos</code>文件夹</li><li>将样式文件放到<code>photos</code>文件夹下，样式文件我都放到了<code>github</code>上：<a href="https://github.com/lawlite19/Blog-Back-Up/tree/master/blog_photos_copy" target="_blank" rel="external">https://github.com/lawlite19/Blog-Back-Up/tree/master/blog_photos_copy</a></li><li>修改<code>ins.js</code>文件，主要是里面的<code>render</code>函数<ul><li>其中的<code>url</code>对应到你的<code>github</code>放图片的地址<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line">var render = function render(res) &#123;</div><div class="line">  var ulTmpl = "";</div><div class="line">  for (var j = 0, len2 = res.list.length; j &lt; len2; j++) &#123;</div><div class="line">    var data = res.list[j].arr;</div><div class="line">    var liTmpl = "";</div><div class="line">    for (var i = 0, len = data.link.length; i &lt; len; i++) &#123;</div><div class="line">      var minSrc = 'https://raw.githubusercontent.com/lawlite19/blog-back-up/master/min_photos/' + data.link[i];</div><div class="line">      var src = 'https://raw.githubusercontent.com/lawlite19/blog-back-up/master/photos/' + data.link[i];</div><div class="line">      var type = data.type[i];</div><div class="line">      var target = src + (type === 'video' ? '.mp4' : '.jpg');</div><div class="line">      src += '';</div><div class="line"></div><div class="line">      liTmpl += '&lt;figure class="thumb" itemprop="associatedMedia" itemscope="" itemtype="http://schema.org/ImageObject"&gt;\</div><div class="line">            &lt;a href="' + src + '" itemprop="contentUrl" data-size="1080x1080" data-type="' + type + '" data-target="' + src + '"&gt;\</div><div class="line">              &lt;img class="reward-img" data-type="' + type + '" data-src="' + minSrc + '" src="/assets/img/empty.png" itemprop="thumbnail" onload="lzld(this)"&gt;\</div><div class="line">            &lt;/a&gt;\</div><div class="line">            &lt;figcaption style="display:none" itemprop="caption description"&gt;' + data.text[i] + '&lt;/figcaption&gt;\</div><div class="line">        &lt;/figure&gt;';</div><div class="line">    &#125;</div><div class="line">    ulTmpl = ulTmpl + '&lt;section class="archives album"&gt;&lt;h1 class="year"&gt;' + data.year + '年&lt;em&gt;' + data.month + '月&lt;/em&gt;&lt;/h1&gt;\</div><div class="line">    &lt;ul class="img-box-ul"&gt;' + liTmpl + '&lt;/ul&gt;\</div><div class="line">    &lt;/section&gt;';</div><div class="line">  &#125;</div></pre></td></tr></table></figure></li></ul></li></ul><h3 id="3、图片处理"><a href="#3、图片处理" class="headerlink" title="3、图片处理"></a>3、图片处理</h3><ul><li><strong>python脚本文件</strong>都放在了这里：<a href="https://github.com/lawlite19/Blog-Back-Up" target="_blank" rel="external">https://github.com/lawlite19/Blog-Back-Up</a><h4 id="1-裁剪图片"><a href="#1-裁剪图片" class="headerlink" title="(1) 裁剪图片"></a>(1) 裁剪图片</h4></li><li>去图片的中间部分，裁剪为<strong>正方形</strong></li><li>对应的裁剪函数<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line">def cut_by_ratio(self):  </div><div class="line">    """按照图片长宽进行分割</div><div class="line">    </div><div class="line">    ------------</div><div class="line">    取中间的部分，裁剪成正方形</div><div class="line">    """  </div><div class="line">    im = Image.open(self.infile)  </div><div class="line">    (x, y) = im.size  </div><div class="line">    if x &gt; y:  </div><div class="line">        region = (int(x/2-y/2), 0, int(x/2+y/2), y)  </div><div class="line">        #裁切图片  </div><div class="line">        crop_img = im.crop(region)  </div><div class="line">        #保存裁切后的图片  </div><div class="line">        crop_img.save(self.outfile)             </div><div class="line">    elif x &lt; y:  </div><div class="line">        region = (0, int(y/2-x/2), x, int(y/2+x/2))</div><div class="line">        #裁切图片  </div><div class="line">        crop_img = im.crop(region)  </div><div class="line">        #保存裁切后的图片  </div><div class="line">        crop_img.save(self.outfile)</div></pre></td></tr></table></figure></li></ul><h4 id="2-压缩图片"><a href="#2-压缩图片" class="headerlink" title="(2) 压缩图片"></a>(2) 压缩图片</h4><ul><li>把图片进行压缩，方便相册的加载<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line">def compress(choose, des_dir, src_dir, file_list):</div><div class="line">    """压缩算法，img.thumbnail对图片进行压缩，</div><div class="line">    </div><div class="line">    参数</div><div class="line">    -----------</div><div class="line">    choose: str</div><div class="line">            选择压缩的比例，有4个选项，越大压缩后的图片越小</div><div class="line">    """</div><div class="line">    if choose == '1':</div><div class="line">        scale = SIZE_normal</div><div class="line">    if choose == '2':</div><div class="line">        scale = SIZE_small</div><div class="line">    if choose == '3':</div><div class="line">        scale = SIZE_more_small</div><div class="line">    if choose == '4':</div><div class="line">        scale = SIZE_more_small_small</div><div class="line">    for infile in file_list:</div><div class="line">        img = Image.open(src_dir+infile)</div><div class="line">        # size_of_file = os.path.getsize(infile)</div><div class="line">        w, h = img.size</div><div class="line">        img.thumbnail((int(w/scale), int(h/scale)))</div><div class="line">        img.save(des_dir + infile)</div></pre></td></tr></table></figure></li></ul><h3 id="4、github提交"><a href="#4、github提交" class="headerlink" title="4、github提交"></a>4、github提交</h3><ul><li>处理完成之后需要将处理后的图片提交到<code>github</code>上</li><li>这里同样使用脚本的方式，需要将<code>git</code>命令行配置到环境变量中<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">def git_operation():</div><div class="line">    '''</div><div class="line">    git 命令行函数，将仓库提交</div><div class="line">    </div><div class="line">    ----------</div><div class="line">    需要安装git命令行工具，并且添加到环境变量中</div><div class="line">    '''</div><div class="line">    os.system('git add --all')</div><div class="line">    os.system('git commit -m "add photos"')</div><div class="line">    os.system('git push origin master')</div></pre></td></tr></table></figure></li></ul><h3 id="5、json数据处理"><a href="#5、json数据处理" class="headerlink" title="5、json数据处理"></a>5、json数据处理</h3><ul><li>下面就需要将图片信息处理成<code>json</code>数据格式了，这里为重点</li><li>最终需要的json格式的数据如下图：<br><img src="/assets/blog_images/hexo+github/11.png" alt="json数据格式" title="11"></li><li>这里我采用的方式是读取<strong>图片的名字</strong>作为其中的<strong>text</strong>的内容，图片的命名如下图<ul><li>最前面是日期，然后用<code>_</code>进行分隔</li><li>后面是图片的描述信息，注意不要包含<code>_</code>和<code>.</code>符号<br><img src="/assets/blog_images/hexo+github/12.png" alt="enter description here" title="12"></li></ul></li><li>实现代码：<ul><li>注意代码中<code>../lawlite19.github.io/source/photos/data.json</code>是对应到我的博客的路径，这里根据需要改成自己博客的路径</li></ul></li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div></pre></td><td class="code"><pre><div class="line">def handle_photo():</div><div class="line">    '''根据图片的文件名处理成需要的json格式的数据</div><div class="line">    </div><div class="line">    -----------</div><div class="line">    最后将data.json文件存到博客的source/photos文件夹下</div><div class="line">    '''</div><div class="line">    src_dir, des_dir = "photos/", "min_photos/"</div><div class="line">    file_list = list_img_file(src_dir)</div><div class="line">    list_info = []</div><div class="line">    for i in range(len(file_list)):</div><div class="line">        filename = file_list[i]</div><div class="line">        date_str, info = filename.split("_")</div><div class="line">        info, _ = info.split(".")</div><div class="line">        date = datetime.strptime(date_str, "%Y-%m-%d")</div><div class="line">        year_month = date_str[0:7]            </div><div class="line">        if i == 0:  # 处理第一个文件</div><div class="line">            new_dict = &#123;"date": year_month, "arr":&#123;'year': date.year,</div><div class="line">                                                                   'month': date.month,</div><div class="line">                                                                   'link': [filename],</div><div class="line">                                                                   'text': [info],</div><div class="line">                                                                   'type': ['image']</div><div class="line">                                                                   &#125;</div><div class="line">                                        &#125; </div><div class="line">            list_info.append(new_dict)</div><div class="line">        elif year_month != list_info[-1]['date']:  # 不是最后的一个日期，就新建一个dict</div><div class="line">            new_dict = &#123;"date": year_month, "arr":&#123;'year': date.year,</div><div class="line">                                                   'month': date.month,</div><div class="line">                                                   'link': [filename],</div><div class="line">                                                   'text': [info],</div><div class="line">                                                   'type': ['image']</div><div class="line">                                                   &#125;</div><div class="line">                        &#125;</div><div class="line">            list_info.append(new_dict)</div><div class="line">        else:  # 同一个日期</div><div class="line">            list_info[-1]['arr']['link'].append(filename)</div><div class="line">            list_info[-1]['arr']['text'].append(info)</div><div class="line">            list_info[-1]['arr']['type'].append('image')</div><div class="line">    list_info.reverse()  # 翻转</div><div class="line">    final_dict = &#123;"list": list_info&#125;</div><div class="line">    with open("../lawlite19.github.io/source/photos/data.json","w") as fp:</div><div class="line">        json.dump(final_dict, fp)</div></pre></td></tr></table></figure><ul><li>每次图片有改动都需要执行此脚本文件<h2 id="三、其他"><a href="#三、其他" class="headerlink" title="三、其他"></a>三、其他</h2></li><li>你可以根据需要进行修改<code>python</code>脚本代码，这里一些细节可能处理的不好</li><li>留言板：<a href="http://lawlite.me/%E7%95%99%E8%A8%80%E6%9D%BF/" target="_blank" rel="external">http://lawlite.me/%E7%95%99%E8%A8%80%E6%9D%BF/</a></li><li>效果展示<ul><li>相册<br><img src="/assets/blog_images/hexo+github/13.png" alt="enter description here" title="13"></li><li>留言板<br><img src="/assets/blog_images/hexo+github/14.png" alt="enter description here" title="14"></li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;ul&gt;
&lt;li&gt;最终效果请看这里：&lt;a href=&quot;http://lawlite.me/photos/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://lawlite.me/photos/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;粗略实现，有问题可以在下方评论区评论，或者&lt;a href=&quot;http://lawlite.me/%E7%95%99%E8%A8%80%E6%9D%BF/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;留言区&lt;/a&gt;留言&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;一、说明&quot;&gt;&lt;a href=&quot;#一、说明&quot; class=&quot;headerlink&quot; title=&quot;一、说明&quot;&gt;&lt;/a&gt;一、说明&lt;/h2&gt;&lt;h3 id=&quot;1、关于&quot;&gt;&lt;a href=&quot;#1、关于&quot; class=&quot;headerlink&quot; title=&quot;1、关于&quot;&gt;&lt;/a&gt;1、关于&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;我使用的主题是&lt;a href=&quot;https://github.com/litten/hexo-theme-yilia&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;hexo-theme-yilia&lt;/a&gt;，其中实现相册功能的方案是同步&lt;code&gt;instagram&lt;/code&gt;上面的图片，但是现在&lt;code&gt;instagram&lt;/code&gt;被禁，不能使用了&lt;/li&gt;
&lt;li&gt;下面是通过自己的方式实现了相册功能，其中的&lt;strong&gt;样式&lt;/strong&gt;还是使用该主题提供的&lt;h3 id=&quot;2、方案&quot;&gt;&lt;a href=&quot;#2、方案&quot; class=&quot;headerlink&quot; title=&quot;2、方案&quot;&gt;&lt;/a&gt;2、方案&lt;/h3&gt;&lt;/li&gt;
&lt;li&gt;在&lt;code&gt;github&lt;/code&gt;上&lt;strong&gt;新建一个仓库&lt;/strong&gt;，主要用于存储图片，可以通过&lt;code&gt;url&lt;/code&gt;访问到，也方便管理&lt;/li&gt;
&lt;li&gt;将要放到相册的图片处理成&lt;code&gt;json&lt;/code&gt;格式的数据，然后进行访问，这里&lt;code&gt;json&lt;/code&gt;的格式需要配合要使用的样式，所以需要处理成&lt;strong&gt;特定格式&lt;/strong&gt;的&lt;code&gt;json&lt;/code&gt;数据，下面会给出&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;图片裁剪&lt;/strong&gt;，因为相册显示的样式最好是&lt;strong&gt;正方形&lt;/strong&gt;的的图片，这里使用脚本处理一下&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;图片压缩&lt;/strong&gt;，相册显示的图片是压缩后的图片，提高加载的速度，打开后的图片是原图。&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="Github" scheme="http://lawlite.cn/tags/Github/"/>
    
      <category term="Hexo" scheme="http://lawlite.cn/tags/Hexo/"/>
    
  </entry>
  
  <entry>
    <title>Hexo+Github搭建自己的博客</title>
    <link href="http://lawlite.cn/2017/04/10/Hexo-Github%E6%90%AD%E5%BB%BA%E8%87%AA%E5%B7%B1%E7%9A%84%E5%8D%9A%E5%AE%A2/"/>
    <id>http://lawlite.cn/2017/04/10/Hexo-Github搭建自己的博客/</id>
    <published>2017-04-10T14:49:15.000Z</published>
    <updated>2017-09-08T10:40:29.000Z</updated>
    
    <content type="html"><![CDATA[<ul><li>最终效果可以查看：<a href="http://lawlite.me/" target="_blank" rel="external">http://lawlite.me/</a></li><li>后序继续完善，有问题可以联系我或是下面评论</li></ul><h2 id="一、说明"><a href="#一、说明" class="headerlink" title="一、说明"></a>一、说明</h2><ul><li>关于一些基本软件的安装和配置这里不再给出<ul><li>安装<code>NodeJS</code>：<a href="http://nodejs.cn/download/" target="_blank" rel="external">http://nodejs.cn/download/</a><ul><li>需要配置环境变量</li></ul></li><li>安装<code>git</code>工具：<a href="https://git-for-windows.github.io/" target="_blank" rel="external">https://git-for-windows.github.io/</a><ul><li>注册<code>github</code>账号</li><li>配置<code>SSH-key</code></li><li>创建名为<code>userName.github.io</code>的仓库,<code>userName</code>是你申请的用户名</li></ul></li></ul></li></ul><a id="more"></a><h2 id="二、安装Hexo和基本使用"><a href="#二、安装Hexo和基本使用" class="headerlink" title="二、安装Hexo和基本使用"></a>二、安装Hexo和基本使用</h2><ul><li>安装<strong>Hexo</strong>: <code>npm install -g hexo</code></li><li>初始化<strong>Hexo</strong>: <code>hexo init</code></li><li>生成静态页面：<code>hexo generate</code> 或者 <code>hexo g</code></li><li>启动服务器：<code>hexo server</code> 或者 <code>hexo s</code><ul><li>注意：服务器默认是<code>4000</code>端口，若是安装了<strong>福昕阅读器</strong>可能端口冲突</li><li>可以制定端口：<code>hexo s -p5000</code></li></ul></li><li>浏览器中访问：<code>http://localhost:4000</code></li></ul><h2 id="三、更换主题Theme及基本配置"><a href="#三、更换主题Theme及基本配置" class="headerlink" title="三、更换主题Theme及基本配置"></a>三、更换主题Theme及基本配置</h2><h3 id="1、更换主题"><a href="#1、更换主题" class="headerlink" title="1、更换主题"></a>1、更换主题</h3><ul><li>默认主题是<code>landscape</code>，在<code>themes</code>文件夹下，可以使用别人开发好的主题，<a href="https://github.com/hexojs/hexo/wiki/Themes" target="_blank" rel="external">这里</a>有很多，我使用的是这一个: <a href="https://github.com/litten/hexo-theme-yilia" target="_blank" rel="external">https://github.com/litten/hexo-theme-yilia</a><ul><li>下载之后放到<code>themes</code>文件夹下即可：<code>git clone git@github.com:litten/hexo-theme-yilia.git</code></li></ul></li></ul><h3 id="2、主题基本配置"><a href="#2、主题基本配置" class="headerlink" title="2、主题基本配置"></a>2、主题基本配置</h3><ul><li>配置在<code>_config.yml</code>文件中，基本的配置尝试一下就知道了，不在给出<h4 id="1-图片的位置："><a href="#1-图片的位置：" class="headerlink" title="(1) 图片的位置："></a>(1) 图片的位置：</h4></li><li>比如<strong>打赏</strong>的支付宝二维码图片，是在<strong>当前博客</strong>的<code>source/assets/img/</code>下 （不是当前主题）<br><img src="/assets/blog_images/hexo+github/01.png" alt="enter description here" title="01"></li><li>配置：<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"># 打赏基础设定：0-关闭打赏； 1-文章对应的md文件里有reward:true属性，才有打赏； 2-所有文章均有打赏</div><div class="line">reward_type: 1</div><div class="line"># 打赏wording</div><div class="line">reward_wording: '谢谢你请我吃糖果'</div><div class="line"># 支付宝二维码图片地址，跟你设置头像的方式一样。比如：/assets/img/alipay.jpg</div><div class="line">alipay: /assets/img/alipay.jpg</div><div class="line"># 微信二维码图片地址</div><div class="line">weixin: /assets/img/weixin.png</div></pre></td></tr></table></figure></li></ul><h4 id="2-百度、谷歌统计配置"><a href="#2-百度、谷歌统计配置" class="headerlink" title="(2) 百度、谷歌统计配置"></a>(2) 百度、谷歌统计配置</h4><ul><li>申请账号：<a href="https://tongji.baidu.com/web/welcome/login" target="_blank" rel="external">https://tongji.baidu.com/web/welcome/login</a></li><li>在<strong>代码获取</strong>的地方只要填入<strong>key</strong>即可<br><img src="/assets/blog_images/hexo+github/02.png" alt="enter description here" title="02"></li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"># Miscellaneous</div><div class="line">baidu_analytics: '454d1a5ba8ed29xxxxxxxx'</div><div class="line">google_analytics: 'UA-9700xxxxxxxx'</div></pre></td></tr></table></figure><ul><li>就可以统计网站访问情况了，如下图，<br><img src="/assets/blog_images/hexo+github/03.png" alt="enter description here" title="03"></li><li>谷歌统计同理</li></ul><h4 id="3-文章评论设置"><a href="#3-文章评论设置" class="headerlink" title="(3) 文章评论设置"></a>(3) 文章评论设置</h4><ul><li>由于主题之实现了<strong>多说</strong>和<strong>disqus</strong>的第三方评论功能，这里不配置</li><li>因为<strong>多说</strong>6月份要关闭了，<strong>disqus</strong>需要翻墙访问才行，还有<strong>友言</strong>不支持<code>https</code>协议，因为<code>github</code>使用的是<code>https</code>协议</li><li>下面会给出使用<strong>网易云跟帖</strong>和<strong>来必力</strong>的第三方评论功能</li></ul><h2 id="四、博客的基本配置"><a href="#四、博客的基本配置" class="headerlink" title="四、博客的基本配置"></a>四、博客的基本配置</h2><h3 id="1、部署配置"><a href="#1、部署配置" class="headerlink" title="1、部署配置"></a>1、部署配置</h3><ul><li>配置到<code>github</code>对应的仓库中<ul><li>使用<code>hexo deploy</code>或<code>hexo d</code>命令即可发布到github仓库中</li><li>浏览器输入网址<code>https://userName.github.io</code>即可访问（<code>userName</code>对应你的用户名）</li></ul></li></ul><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">deploy:</div><div class="line">  type: git</div><div class="line">  repo: git@github<span class="selector-class">.com</span>:lawlite19/lawlite19<span class="selector-class">.github</span><span class="selector-class">.io</span><span class="selector-class">.git</span></div><div class="line">  branch: master</div></pre></td></tr></table></figure><h3 id="2、主题配置"><a href="#2、主题配置" class="headerlink" title="2、主题配置"></a>2、主题配置</h3><ul><li>设置为你下载的主题：<code>theme: yilia</code><h3 id="3、其他"><a href="#3、其他" class="headerlink" title="3、其他"></a>3、其他</h3></li><li>加入如下配置，<figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line">jsonContent:</div><div class="line">  meta: false</div><div class="line">  pages: false</div><div class="line">  posts:</div><div class="line">    title: true</div><div class="line">    date: true</div><div class="line">    path: true</div><div class="line">    text: true</div><div class="line">    raw: false</div><div class="line">    <span class="attribute">content</span>: false</div><div class="line">    slug: false</div><div class="line">    updated: false</div><div class="line">    comments: false</div><div class="line">    link: false</div><div class="line">    permalink: false</div><div class="line">    excerpt: false</div><div class="line">    categories: false</div><div class="line">    tags: true</div></pre></td></tr></table></figure></li></ul><h2 id="五、进阶功能配置"><a href="#五、进阶功能配置" class="headerlink" title="五、进阶功能配置"></a>五、进阶功能配置</h2><h3 id="1、网站访问量显示"><a href="#1、网站访问量显示" class="headerlink" title="1、网站访问量显示"></a>1、网站访问量显示</h3><h4 id="1-效果"><a href="#1-效果" class="headerlink" title="(1) 效果"></a>(1) 效果</h4><p><img src="/assets/blog_images/hexo+github/04.png" alt="enter description here" title="04"></p><h4 id="2-实现"><a href="#2-实现" class="headerlink" title="(2) 实现"></a>(2) 实现</h4><ul><li>我使用了<strong>不蒜子</strong>第三方的统计插件，网址：<a href="http://ibruce.info/2015/04/04/busuanzi/" target="_blank" rel="external">http://ibruce.info/2015/04/04/busuanzi/</a></li><li>在<code>themes\yilia\layout\_partial</code>下的<code>footer.ejs</code>中加入如下代码即可</li></ul><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">&lt;script async src=<span class="string">"//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"</span>&gt;</div><div class="line">&lt;/script&gt;</div><div class="line">&lt;<span class="selector-tag">span</span> id=<span class="string">"busuanzi_container_site_pv"</span>&gt;</div><div class="line">  本站总访问量&lt;<span class="selector-tag">span</span> id=<span class="string">"busuanzi_value_site_pv"</span>&gt;&lt;/span&gt;次</div><div class="line">&lt;/span&gt;</div><div class="line">&lt;<span class="selector-tag">span</span> id=<span class="string">"busuanzi_container_site_uv"</span>&gt;</div><div class="line">总访客数&lt;<span class="selector-tag">span</span> id=<span class="string">"busuanzi_value_site_uv"</span>&gt;&lt;/span&gt;人次</div><div class="line">&lt;/span&gt;</div></pre></td></tr></table></figure><h3 id="2、实现单篇文章浏览统计和评论统计"><a href="#2、实现单篇文章浏览统计和评论统计" class="headerlink" title="2、实现单篇文章浏览统计和评论统计"></a>2、实现单篇文章浏览统计和评论统计</h3><ul><li>评论数的统计是<strong>网易云跟帖</strong>中获取的，下面给出<h4 id="1-效果-1"><a href="#1-效果-1" class="headerlink" title="(1) 效果"></a>(1) 效果</h4><img src="/assets/blog_images/hexo+github/05.png" alt="enter description here" title="05"><h4 id="2-实现-1"><a href="#2-实现-1" class="headerlink" title="(2) 实现"></a>(2) 实现</h4></li><li>修改<code>themes\yilia\layout\_partial</code>文件夹下的<code>article.ejs</code>文件</li><li>在<code>&lt;%- partial(&#39;post/title&#39;, {class_name: &#39;article-title&#39;}) %&gt;</code>节点下加入：<ul><li>注意这里<strong>网易云跟帖</strong>还没设置，而<strong>评论数</strong>中使用到了，这里运行会有问题，下面给出<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">&lt;!-- 显示阅读和评论数 --&gt;</div><div class="line">&lt;% if (!index &amp;&amp; post.comments &amp;&amp; config.wangYi)&#123; %&gt;</div><div class="line">&lt;br/&gt;</div><div class="line">&lt;a class="cloud-tie-join-count" href="javascript:void(0);" style="color:gray;font-size:14px;"&gt;</div><div class="line">&lt;span class="icon-sort"&gt;&lt;/span&gt;</div><div class="line">&lt;span id="busuanzi_container_page_pv" style="color:#ef7522;font-size:14px;"&gt;</div><div class="line">          阅读数: &lt;span id="busuanzi_value_page_pv"&gt;&lt;/span&gt;次 &amp;nbsp;&amp;nbsp;</div><div class="line">&lt;/span&gt;</div><div class="line">&lt;/a&gt;</div><div class="line">&lt;a class="cloud-tie-join-count" href="javascript:void(0);" style="color:#ef7522;font-size:14px;"&gt;</div><div class="line">&lt;span class="icon-comment"&gt;&lt;/span&gt;</div><div class="line">&lt;span class="join-text" style="color:#ef7522;font-size:14px;"&gt;评论数:&lt;/span&gt;</div><div class="line">&lt;span class="join-count"&gt;0&lt;/span&gt;次</div><div class="line">&lt;/a&gt;</div><div class="line">&lt;% &#125; %&gt;</div></pre></td></tr></table></figure></li></ul></li></ul><h3 id="3、实现网易云跟帖评论"><a href="#3、实现网易云跟帖评论" class="headerlink" title="3、实现网易云跟帖评论"></a>3、实现网易云跟帖评论</h3><h4 id="1-效果-2"><a href="#1-效果-2" class="headerlink" title="(1) 效果"></a>(1) 效果</h4><p><img src="/assets/blog_images/hexo+github/06.png" alt="enter description here" title="06"></p><h4 id="2-实现-2"><a href="#2-实现-2" class="headerlink" title="(2) 实现"></a>(2) 实现</h4><ul><li>注册账号：<a href="https://gentie.163.com/info.html" target="_blank" rel="external">https://gentie.163.com/info.html</a></li><li>填写完成之后获取<code>WEB</code>代码</li><li>修改<code>themes\yilia\layout\_partial</code>文件夹下的<code>article.ejs</code>文件</li><li>在最后加入<ul><li>这里需要注意下，一个站点不同端标识文章的方式必须统一（同一站点可以采用以下方式标识文章：①<code>URL</code>；②<code>Sourceid+productKey</code> ；③<code>URL+Sourceid+productKey</code>，建议用②或者③），否则跟贴数据可能错乱。比如同一站点PC端采用URL，APP采用Sourceid+productKey，这种情况跟贴数据会错乱，必选采用统一方式标识。</li><li>这里使用方式②</li></ul></li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">&lt;!-- 网易云跟帖 --&gt;</div><div class="line">&lt;% if (!index &amp;&amp; post.comments &amp;&amp; config.wangYi)&#123; %&gt;</div><div class="line">&lt;section class="duoshuo" id="comments"&gt;</div><div class="line">&lt;div id="cloud-tie-wrapper" class="cloud-tie-wrapper"&gt;&lt;/div&gt;</div><div class="line">&lt;script&gt;</div><div class="line">  var cloudTieConfig = &#123;</div><div class="line">    url: "", </div><div class="line">    sourceId: "&lt;%= post.path%&gt;",</div><div class="line">    productKey: "&lt;%= config.wangYi%&gt;",</div><div class="line">    target: "cloud-tie-wrapper"</div><div class="line">  &#125;;</div><div class="line">&lt;/script&gt;</div><div class="line">&lt;script src="https://img1.cache.netease.com/f2e/tie/yun/sdk/loader.js"&gt;&lt;/script&gt;</div><div class="line">&lt;/section&gt;</div><div class="line">&lt;% &#125; %&gt;</div></pre></td></tr></table></figure><ul><li>在<strong>博客</strong>的配置文件<code>_config.yml</code>最后加入获取代码中的<code>productKey</code><br><img src="/assets/blog_images/hexo+github/07.png" alt="enter description here" title="07"></li></ul><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">wangYi: <span class="number">06</span>ab5cdc0b4c45efb39xxxxxxxxxxxx</div></pre></td></tr></table></figure><ul><li>发布到<code>github</code>上可以查看效果</li></ul><h2 id="六、绑定到申请的域名"><a href="#六、绑定到申请的域名" class="headerlink" title="六、绑定到申请的域名"></a>六、绑定到申请的域名</h2><ul><li>可以绑定到自己申请的域名上，不用使用<code>userName.github.io</code>访问了，直接使用自己的域名访问<h3 id="1、申请域名"><a href="#1、申请域名" class="headerlink" title="1、申请域名"></a>1、申请域名</h3></li><li>我在万网购买的域名，地址：<a href="https://wanwang.aliyun.com/domain/com?spm=5176.8142029.388261.137.LoKzy7" target="_blank" rel="external">https://wanwang.aliyun.com/domain/com?spm=5176.8142029.388261.137.LoKzy7</a></li><li>我这里是<code>.me</code>结尾的域名，一年<code>13</code>大洋<h3 id="2、解析域名"><a href="#2、解析域名" class="headerlink" title="2、解析域名"></a>2、解析域名</h3></li><li>添加如下的解析<br><img src="/assets/blog_images/hexo+github/09.png" alt="enter description here" title="09"><h3 id="3、配置一下"><a href="#3、配置一下" class="headerlink" title="3、配置一下"></a>3、配置一下</h3></li><li>在<strong>博客</strong>的<code>source</code>文件夹下建立一个<code>CNAME</code>的文件</li><li>内容写入你的域名信息，比如我这里是<code>lawlite.me</code></li><li>发布到<code>github</code>即可<h3 id="4、细节说明"><a href="#4、细节说明" class="headerlink" title="4、细节说明"></a>4、细节说明</h3></li><li>之前<strong>网易云跟帖</strong>，<strong>百度统计</strong>设置的域名这里对应该过来一下<h2 id="七、写作的一些说明"><a href="#七、写作的一些说明" class="headerlink" title="七、写作的一些说明"></a>七、写作的一些说明</h2></li><li>执行命令：<code>hexo new  &quot;xxxx&quot;</code>创建<code>Markdown</code>文件，在<strong>博客</strong>的<code>source\_posts</code>文件夹下</li><li>比如如下例子，<ul><li><code>comments</code>设置为<code>true</code>允许评论，若设置为<code>false</code>则不能评论</li><li><code>reward</code>设置为<code>true</code>允许打赏，若设置为<code>false</code>则不能打赏，（注意对应主题的配置文件<code>reward_type</code>: 设置的为<code>1</code>）<br><img src="/assets/blog_images/hexo+github/08.png" alt="enter description here" title="08"></li></ul></li><li>在文章中加入<code>&lt;!-- more --&gt;</code>将文章<strong>截断</strong>显示在<strong>主页</strong></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;ul&gt;
&lt;li&gt;最终效果可以查看：&lt;a href=&quot;http://lawlite.me/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://lawlite.me/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;后序继续完善，有问题可以联系我或是下面评论&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;一、说明&quot;&gt;&lt;a href=&quot;#一、说明&quot; class=&quot;headerlink&quot; title=&quot;一、说明&quot;&gt;&lt;/a&gt;一、说明&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;关于一些基本软件的安装和配置这里不再给出&lt;ul&gt;
&lt;li&gt;安装&lt;code&gt;NodeJS&lt;/code&gt;：&lt;a href=&quot;http://nodejs.cn/download/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://nodejs.cn/download/&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;需要配置环境变量&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;安装&lt;code&gt;git&lt;/code&gt;工具：&lt;a href=&quot;https://git-for-windows.github.io/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://git-for-windows.github.io/&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;注册&lt;code&gt;github&lt;/code&gt;账号&lt;/li&gt;
&lt;li&gt;配置&lt;code&gt;SSH-key&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;创建名为&lt;code&gt;userName.github.io&lt;/code&gt;的仓库,&lt;code&gt;userName&lt;/code&gt;是你申请的用户名&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="Github" scheme="http://lawlite.cn/tags/Github/"/>
    
      <category term="Hexo" scheme="http://lawlite.cn/tags/Hexo/"/>
    
  </entry>
  
  <entry>
    <title>致火影</title>
    <link href="http://lawlite.cn/2017/03/24/%E8%87%B4%E7%81%AB%E5%BD%B1/"/>
    <id>http://lawlite.cn/2017/03/24/致火影/</id>
    <published>2017-03-24T13:57:12.000Z</published>
    <updated>2017-06-25T09:17:26.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/assets/随笔_images/鸣人-雏田.png" alt="enter description here" title="鸣人-雏田"><br> <center> <h1>致火影</h1></center></p><p><center>——只要有树叶飞舞的地方，火就会燃烧。</center><br>　　昨天就知道火影动漫也完结了，但是没有马上去看，想抽个正式点的时间。<br><a id="more"></a></p><p>　　漫画是700集完结，当时动漫到700的时候就有个打算想写点东西记录一下，但是没有动手。今天准备看时还在思考，看完一些回忆涌上，果断提笔。<br>　　初三的暑假，当时是在补课，一位小伙伴有火影的光盘，当时就借来看看。记得每天最多能看几十集，当时光盘里面应该是有300集左右。<br>　　暑假结束，步入高一，当时并不知道有漫画（毕竟高一才有的QQ），还在军训，班级里面有同学买的关于火影的海报，那时漫画里讲到鼬双重间谍的身份，以及多么爱他的弟弟佐助。后来有了个诺亚舟学习机（当然现在还在），有时周末就去网吧下载火影动漫看。现在来说有的一集看了不止10遍，当然我周围的小伙伴也有一块看的。<br>　　高一结束分班，我后面一排的一位小伙伴也看火影，每次周日下午回校，他都和我讨论，当时讨论的还有死神（死神、柯南都有看，但火影是我唯一看的完整的动漫（不算死亡笔记这种比较短的动漫））。<br>　　大三的时候火影漫画700完结（700之后的5话是番外），当时写了一段话，但没有发出来。当动漫700之后几集的片尾曲唱到：さようなら（再见）的时候，些许感慨，之后看的时候的片头曲和片尾曲很少跳过。<br>　　还记得岸本齐史（AB大叔）有说过，刚开始画火影的时候他还没有结婚，就像鸣人一样希望得到别人的注意，后来结婚，漫画里的鸣人也渐渐的有了朋友。<br>　　最后定格在鸣人雏田结婚。<br>　　16岁到24岁，谢谢鸣人，谢谢火影!</p><p align="right">——思念你的人所在的地方就是你的归宿！</p><p align="right">2017年3月24日</p><p><img src="/assets/随笔_images/记录.jpg" alt="enter description here" title="IMG_20170324_214803"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;/assets/随笔_images/鸣人-雏田.png&quot; alt=&quot;enter description here&quot; title=&quot;鸣人-雏田&quot;&gt;&lt;br&gt; &lt;center&gt; &lt;h1&gt;致火影&lt;/h1&gt;&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;&lt;center&gt;——只要有树叶飞舞的地方，火就会燃烧。&lt;/center&gt;&lt;br&gt;　　昨天就知道火影动漫也完结了，但是没有马上去看，想抽个正式点的时间。&lt;br&gt;
    
    </summary>
    
    
      <category term="随笔" scheme="http://lawlite.cn/tags/%E9%9A%8F%E7%AC%94/"/>
    
  </entry>
  
  <entry>
    <title>Keras学习</title>
    <link href="http://lawlite.cn/2017/02/14/Keras%E5%AD%A6%E4%B9%A0/"/>
    <id>http://lawlite.cn/2017/02/14/Keras学习/</id>
    <published>2017-02-14T12:25:43.000Z</published>
    <updated>2017-06-25T08:48:43.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一、Keras概述"><a href="#一、Keras概述" class="headerlink" title="一、Keras概述"></a>一、Keras概述</h2><h3 id="1、介绍"><a href="#1、介绍" class="headerlink" title="1、介绍"></a>1、介绍</h3><ul><li><code>Keras</code> 是一个兼容 <code>Theano</code> 和 <code>Tensorflow</code> 的神经网络高级包</li><li>用他来组件一个神经网络更加快速, 几条语句就搞定</li><li><code>Keras</code> 可以再在 <code>Windows</code> 和 <code>MacOS</code> 或者 <code>Linux</code> 上运行</li><li>网站：<a href="https://keras.io/" target="_blank" rel="external">https://keras.io/</a></li></ul><a id="more"></a><h3 id="2、安装Keras"><a href="#2、安装Keras" class="headerlink" title="2、安装Keras"></a>2、安装Keras</h3><ul><li>需要事先安装好<code>numpy</code>和<code>scipy</code></li><li>直接pip安装：<code>pip install keras</code></li><li><code>Keras</code>有两个<strong>backend</strong>,就是是基于什么进行运算的，一个是<strong>Tensorflow</strong>，一个是<strong>Theano</strong></li><li><p>通过修改配置文件永久修改</p><ul><li>默认配置是<strong>Tensorflow</strong>，这里改为<strong>Theano</strong></li><li>Windows在用户的文件夹下有个配置文件：<code>C:\Users\bob\.keras</code>文件夹下的<code>keras.json</code>文件</li><li>修改即可<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">&#123;</div><div class="line">  &quot;image_dim_ordering&quot;: &quot;tf&quot;, </div><div class="line">  &quot;epsilon&quot;: 1e-07, </div><div class="line">  &quot;floatx&quot;: &quot;float32&quot;, </div><div class="line">  &quot;backend&quot;: &quot;theano&quot;</div><div class="line">&#125;</div></pre></td></tr></table></figure></li></ul></li><li><p>修改当前脚本的环境变量</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">import os </div><div class="line">os.environ[&apos;KERAS_BACKEND&apos;]=&apos;tensorflow&apos;  # 或者theano</div><div class="line">import keras</div></pre></td></tr></table></figure></li></ul><h2 id="二、搭建神经网络"><a href="#二、搭建神经网络" class="headerlink" title="二、搭建神经网络"></a>二、搭建神经网络</h2><h3 id="1、一个神经网络例子"><a href="#1、一个神经网络例子" class="headerlink" title="1、一个神经网络例子"></a>1、一个神经网络例子</h3><ul><li><p>导入包</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">import keras</div><div class="line">import numpy as np</div><div class="line">from keras.models import Sequential   # Sequential顺序建立</div><div class="line">from keras.layers import Dense        # 全连接层</div><div class="line">import matplotlib.pyplot as plt</div></pre></td></tr></table></figure></li><li><p>制造数据</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">&apos;&apos;&apos;制造数据，并且显示&apos;&apos;&apos;</div><div class="line">X = np.linspace(-1,1,200)</div><div class="line">np.random.shuffle(X)</div><div class="line">Y = 0.5 * X + 2 + np.random.normal(0,0.05,(200,))</div><div class="line">plt.scatter(X,Y)</div><div class="line">plt.show()</div><div class="line">X_train,Y_train = X[:160],Y[:160]</div><div class="line">X_test,Y_test = X[160:],Y[160:]</div></pre></td></tr></table></figure></li><li><p>建立模型</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">&apos;&apos;&apos;建立模型&apos;&apos;&apos;</div><div class="line">    model = Sequential()   # 通过Sequential建立model</div><div class="line">    model.add(Dense(output_dim=1, input_dim=1))   # model.add添加神经层，指定输入和输出维度</div></pre></td></tr></table></figure></li><li><p>激活模型</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">&apos;&apos;&apos;激活模型&apos;&apos;&apos;</div><div class="line">    model.compile(optimizer=&apos;sgd&apos;, loss=&apos;mse&apos;)</div></pre></td></tr></table></figure></li><li><p>训练模型</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">for i in range(500):</div><div class="line">    cost = model.train_on_batch(X_train,Y_train)  # 使用批训练</div><div class="line">    if i % 50 == 0:</div><div class="line">        print(cost)</div></pre></td></tr></table></figure></li><li><p>测试集的cost误差</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">cost = model.evaluate(X_test, Y_test, batch_size=40)</div><div class="line">print(cost)</div></pre></td></tr></table></figure></li><li><p>学到的权重和偏置</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">&apos;&apos;&apos;输出学到的权重和偏置&apos;&apos;&apos;</div><div class="line">    W,b = model.layers[0].get_weights()</div><div class="line">    print(W,b)</div></pre></td></tr></table></figure></li><li><p>预测</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">Y_pred = model.predict(X_test)</div></pre></td></tr></table></figure></li></ul><h3 id="2、手写数字识别例子–mnist"><a href="#2、手写数字识别例子–mnist" class="headerlink" title="2、手写数字识别例子–mnist"></a>2、手写数字识别例子–mnist</h3><ul><li><p>导入包：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">import keras</div><div class="line">from keras.datasets import mnist</div><div class="line">from keras.utils import np_utils</div><div class="line">import numpy as np</div><div class="line">from keras.models import Sequential   # Sequential顺序建立</div><div class="line">from keras.layers import Dense,Activation  # 全连接层</div><div class="line">from keras.optimizers import RMSprop</div></pre></td></tr></table></figure></li><li><p>加载并预处理数据</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">&apos;&apos;&apos;加载和预处理数据&apos;&apos;&apos;</div><div class="line">(X_train,y_train),(X_test,y_test) = mnist.load_data()   # 下载数据集，windows在当前用户的对应目录下：C:\Users\bob\.keras\datasets</div><div class="line">X_train = X_train.reshape(X_train.shape[0],-1)/255      # X_train是(60000, 28, 28)，reshape一下变成(60000,784),然后在标准化</div><div class="line">X_test  = X_test.reshape(X_test.shape[0],-1)/255</div><div class="line">y_train = np_utils.to_categorical(y_train,nb_classes=10) # y_train对应的数字1，2，3....转换为0/1映射</div><div class="line">y_test  = np_utils.to_categorical(y_test,nb_classes=10)</div></pre></td></tr></table></figure></li><li><p>建立模型</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">&apos;&apos;&apos;建立模型&apos;&apos;&apos;</div><div class="line">model = Sequential(layers=[</div><div class="line">    Dense(output_dim=32,input_dim=784),   # 第一层，输入为784维，输出为32维</div><div class="line">    Activation(&apos;relu&apos;),                   # 激励函数为relu</div><div class="line">    Dense(10),                            # 第二层，这里不需要指定输入层维度，全连接会使用上一层的输出，这里即32</div><div class="line">    Activation(&apos;softmax&apos;),                # 激励函数，也是最后的预测函数使用softmax</div><div class="line">    ])</div></pre></td></tr></table></figure></li><li><p>激活模型</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">&apos;&apos;&apos;定义optimizer&apos;&apos;&apos;</div><div class="line">rmsprop = RMSprop()</div><div class="line">&apos;&apos;&apos;激活模型&apos;&apos;&apos;</div><div class="line">model.compile(optimizer=rmsprop, </div><div class="line">              loss=&apos;categorical_crossentropy&apos;,   # 分类中使用交叉熵损失函数</div><div class="line">              metrics=[&apos;accuracy&apos;])              # 计算准确度</div></pre></td></tr></table></figure></li><li><p>训练模型</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">model.fit(X_train,y_train,nb_epoch=2,batch_size=100)  # nb_epoch整个训练集训练次数</div></pre></td></tr></table></figure></li><li><p>测试集上预测信息</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">&apos;&apos;&apos;测试集测试训练出的模型&apos;&apos;&apos;</div><div class="line">loss,accuracy = model.evaluate(X_test,y_test)</div><div class="line">print(&apos;loss:&apos;,loss)</div><div class="line">print(&apos;accuracy&apos;,accuracy)</div></pre></td></tr></table></figure></li></ul><h3 id="3、卷积神经网络CNN–mnist"><a href="#3、卷积神经网络CNN–mnist" class="headerlink" title="3、卷积神经网络CNN–mnist"></a>3、卷积神经网络CNN–mnist</h3><ul><li><p>导入包</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">import keras</div><div class="line">from keras.datasets import mnist</div><div class="line">from keras.utils import np_utils</div><div class="line">import numpy as np</div><div class="line">from keras.models import Sequential   # Sequential顺序建立</div><div class="line">from keras.layers import Dense,Activation,Convolution2D,MaxPooling2D,Flatten</div><div class="line">from keras.optimizers import RMSprop,Adam</div></pre></td></tr></table></figure></li><li><p>建立模型</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div></pre></td><td class="code"><pre><div class="line">model = Sequential()</div><div class="line">## 第一层卷积</div><div class="line">model.add(Convolution2D(nb_filter=32,    # 32个filter，即从32个特征提取</div><div class="line">                        nb_row=5,        # patch大小</div><div class="line">                        nb_col=5, </div><div class="line">                        border_mode=&apos;same&apos;, </div><div class="line">                        dim_ordering=&apos;th&apos;,  # theano使用th,TensorFlow使用tf</div><div class="line">                        input_shape=(1,28,28,)  # 输入的大小，1表示输入的channel通道，由于是黑白图所以是1,若是rgb是3个通道</div><div class="line">                        ))</div><div class="line">## 第一层激活层</div><div class="line">model.add(Activation(&apos;relu&apos;))</div><div class="line">## 第一层池化层</div><div class="line">model.add(MaxPooling2D(</div><div class="line">    pool_size=(2,2),    # 2x2的大小</div><div class="line">    strides=(2,2),      # 步长为2，纵向和横向</div><div class="line">    border_mode=&apos;same&apos;</div><div class="line">))</div><div class="line">### 第二层卷积层</div><div class="line">model.add(Convolution2D(nb_filter=64,      # 不需要指定输入的大小了</div><div class="line">                        nb_row=5,</div><div class="line">                        nb_col=5, </div><div class="line">                       border_mode=&apos;same&apos;</div><div class="line">                       ))</div><div class="line">### 第二层激活层</div><div class="line">model.add(Activation(&apos;relu&apos;))</div><div class="line">### 第二层池化层</div><div class="line">model.add(MaxPooling2D(border_mode=&apos;same&apos;))</div><div class="line">#### 全连接层</div><div class="line">model.add(Flatten())   # 展开</div><div class="line">model.add(Dense(output_dim=1024))  # 输出维度为1024</div><div class="line">model.add(Activation(&apos;relu&apos;))</div><div class="line">model.add(Dense(output_dim=10))    # 最终输出为10类</div><div class="line">model.add(Activation(&apos;softmax&apos;))</div></pre></td></tr></table></figure></li><li><p>激活模型</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">adam = Adam()</div><div class="line">model.compile(optimizer=adam,     # 使用adam的optimizer</div><div class="line">              loss=&apos;categorical_crossentropy&apos;,</div><div class="line">              metrics=[&apos;accuracy&apos;])</div></pre></td></tr></table></figure></li><li><p>训练模型</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">model.fit(X_train, y_train)</div></pre></td></tr></table></figure></li><li><p>测试集计算结果</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">&apos;&apos;&apos;测试集模型&apos;&apos;&apos;</div><div class="line">loss,accuracy = model.evaluate(X_test,y_test)</div><div class="line">print(&quot;loss&quot;,loss)</div><div class="line">print(&apos;accuracy&apos;,accuracy)</div></pre></td></tr></table></figure></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;一、Keras概述&quot;&gt;&lt;a href=&quot;#一、Keras概述&quot; class=&quot;headerlink&quot; title=&quot;一、Keras概述&quot;&gt;&lt;/a&gt;一、Keras概述&lt;/h2&gt;&lt;h3 id=&quot;1、介绍&quot;&gt;&lt;a href=&quot;#1、介绍&quot; class=&quot;headerlink&quot; title=&quot;1、介绍&quot;&gt;&lt;/a&gt;1、介绍&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;code&gt;Keras&lt;/code&gt; 是一个兼容 &lt;code&gt;Theano&lt;/code&gt; 和 &lt;code&gt;Tensorflow&lt;/code&gt; 的神经网络高级包&lt;/li&gt;
&lt;li&gt;用他来组件一个神经网络更加快速, 几条语句就搞定&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Keras&lt;/code&gt; 可以再在 &lt;code&gt;Windows&lt;/code&gt; 和 &lt;code&gt;MacOS&lt;/code&gt; 或者 &lt;code&gt;Linux&lt;/code&gt; 上运行&lt;/li&gt;
&lt;li&gt;网站：&lt;a href=&quot;https://keras.io/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://keras.io/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="Python" scheme="http://lawlite.cn/tags/Python/"/>
    
      <category term="DeepLearning" scheme="http://lawlite.cn/tags/DeepLearning/"/>
    
  </entry>
  
  <entry>
    <title>Theano学习</title>
    <link href="http://lawlite.cn/2017/02/10/Theano%E5%AD%A6%E4%B9%A0/"/>
    <id>http://lawlite.cn/2017/02/10/Theano学习/</id>
    <published>2017-02-10T13:25:43.000Z</published>
    <updated>2017-06-25T08:50:20.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一、Theano概述"><a href="#一、Theano概述" class="headerlink" title="一、Theano概述"></a>一、Theano概述</h2><h3 id="1、介绍"><a href="#1、介绍" class="headerlink" title="1、介绍"></a>1、介绍</h3><ul><li><code>Theano</code> 是神经网络python机器学习的模块，和 <code>Tensowflow</code> 类似</li><li>可以在MacOS、Linux、Windows上运行</li><li>theano 可以使用 GPU 进行运算</li><li>网址：<a href="http://deeplearning.net/software/theano/" target="_blank" rel="external">http://deeplearning.net/software/theano/</a> </li></ul><h3 id="2、安装"><a href="#2、安装" class="headerlink" title="2、安装"></a>2、安装</h3><ul><li>Windows上直接：<code>pip install theano</code><ul><li>可能提示个警告：<code>WARNING (theano.configdefaults): g++ not detected ! Theano will be unable to execute optimized C-implementations (for both CPU and GPU) and will default to Python implementations. Performance will be severely degraded. To remove this warning, set Theano flags cxx to an empty string.</code></li></ul></li></ul><a id="more"></a><h2 id="二、Theano基础"><a href="#二、Theano基础" class="headerlink" title="二、Theano基础"></a>二、Theano基础</h2><h3 id="1、基本用法"><a href="#1、基本用法" class="headerlink" title="1、基本用法"></a>1、基本用法</h3><ul><li><p>导入包</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">import numpy as np</div><div class="line">import theano.tensor as T</div><div class="line">from theano import function</div></pre></td></tr></table></figure></li><li><p>常量和方程定义</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">x = T.dscalar(&apos;x&apos;)  # 建立 x 的容器</div><div class="line">y = T.dscalar(&apos;y&apos;)  # 建立 y 的容器</div><div class="line">z = x + y           # 建立方程</div><div class="line">f = function([x, y],z) # 使用function定义方程，将输入值 x, y 放在 [] 里,  输出值 z 放在后面</div></pre></td></tr></table></figure></li><li><p><strong>pretty print</strong>打印原始方程</p><ul><li>导入包：<code>from theano import pp</code></li><li>打印即可：<code>print(pp(z))</code></li></ul></li><li>矩阵相乘<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">x = T.dmatrix(&apos;x&apos;)  # float64的矩阵,fmatrix对应float32</div><div class="line">y = T.dmatrix(&apos;y&apos;)</div><div class="line">z = T.dot(x,y)      # 相乘</div><div class="line">f = function([x,y],z)  # 定义function</div><div class="line">print(f(np.arange(12).reshape(3,4),</div><div class="line">      np.ones((4,3))))</div><div class="line">print(pp(z))</div></pre></td></tr></table></figure></li></ul><p>输出：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">[[  6.   6.   6.]</div><div class="line"> [ 22.  22.  22.]</div><div class="line"> [ 38.  38.  38.]]</div><div class="line">(x \dot y)</div></pre></td></tr></table></figure></p><h3 id="2、function用法"><a href="#2、function用法" class="headerlink" title="2、function用法"></a>2、function用法</h3><ul><li>theano 当中的 function 就和 python 中的 function类似，但是在theano中由于涉及到GPU加速以及CPU的并行的运算，所以他的function会有不同。</li><li><p>多输入和多输出</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">a,b = T.dmatrices(&apos;a&apos;,&apos;b&apos;)   # 定义两个容器</div><div class="line">diff = a - b</div><div class="line">abs_diff = abs(diff)         # 绝对值</div><div class="line">diff_square = diff ** 2      </div><div class="line">f = function([a,b],[diff,abs_diff,diff_square])  # function同样前面指定输入，后面是输出</div><div class="line">x1,x2,x3 = f(np.ones((2,2)),</div><div class="line">             np.arange(4).reshape(2,2))</div><div class="line">print(x1,x2,x3)</div></pre></td></tr></table></figure></li><li><p>指定function默认值和名字name</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">x,y,z = T.dscalars(&apos;x&apos;,&apos;y&apos;,&apos;z&apos;)  # 定义三个scalar容器</div><div class="line">w = (x + y) * z</div><div class="line">f = function([x,</div><div class="line">              theano.In(y,value=1),  # 输入y,默认值为1</div><div class="line">              theano.In(z,value=2,name=&apos;weights&apos;)], # 输入z,默认值为2，同时指定名字为weights，后面可以通过名字复制</div><div class="line">             w)</div><div class="line">print(f(2))         # 使用默认值</div><div class="line">print(f(2,2,3))     # 指定值</div><div class="line">print(f(2,2,weights=3)) # 通过name赋值</div></pre></td></tr></table></figure></li></ul><h3 id="3、Shared变量"><a href="#3、Shared变量" class="headerlink" title="3、Shared变量"></a>3、Shared变量</h3><ul><li><strong>Shared 变量</strong>，意思是这些变量可以在运算过程中，不停地进行交换和更新值。 在定义 <code>weights</code> 和 <code>bias</code> 的情况下，会需要用到这样的变量</li><li>定义shared变量<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">state = theano.shared(np.array(0,dtype=np.float64), name=&apos;state&apos;)  # state初值为0，为float64</div><div class="line">increase = T.scalar(&apos;increase&apos;,dtype=state.dtype)   # 定义一个容器，这里注意dtype为state.dtype,若是np.float64会报错</div><div class="line">accmulator = theano.function([increase],   # 输入</div><div class="line">                             state,        # 输出</div><div class="line">                             updates=[(state,state+increase)]) # 指定每次更新为累加</div><div class="line">print(state.get_value())# get_value()获取值</div><div class="line">accmulator(1)</div><div class="line">print(state.get_value())</div><div class="line">state.set_value(-1)     # set_value()设置值</div><div class="line">accmulator(1)</div><div class="line">print(state.get_value())</div></pre></td></tr></table></figure></li></ul><p>输出：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">0.0</div><div class="line">1.0</div><div class="line">0.0</div></pre></td></tr></table></figure></p><ul><li>临时使用shared变量<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">state = theano.shared(np.array(0,dtype=np.float64), name=&apos;state&apos;)  # state初值为0，为float64</div><div class="line">increase = T.scalar(&apos;increase&apos;,dtype=state.dtype)   # 定义一个容器，这里注意dtype为state.dtype,若是np.float64会报错</div><div class="line">tmp_func = state *2 +increase</div><div class="line">a = T.scalar(dtype=state.dtype)   # 定义标量a，后面用来带起state</div><div class="line">skip_shared = theano.function([increase,a],</div><div class="line">                              tmp_func,</div><div class="line">                              givens=[(state,a)])   # 指定givens参数用a代替state</div><div class="line">print(skip_shared(2,3))</div><div class="line">print(state.get_value())  # 输出state还是0</div></pre></td></tr></table></figure></li></ul><h3 id="4、Theano中的激励函数"><a href="#4、Theano中的激励函数" class="headerlink" title="4、Theano中的激励函数"></a>4、Theano中的激励函数</h3><ul><li><strong>sigmoid</strong>: <code>theano.tensor.nnet.nnet.sigmoid(x)</code></li><li>还有<code>relu</code>,<code>tanh</code>,<code>softmax</code>,<code>softplus</code>等</li><li>在隐含层中常用<code>relu,tanh,softplus</code>等非线性激励函数</li><li>在输出层常用<code>sigmoid，softmax</code>求概率</li></ul><h2 id="三、搭建神经网络"><a href="#三、搭建神经网络" class="headerlink" title="三、搭建神经网络"></a>三、搭建神经网络</h2><h3 id="1、定义Layer类或者函数"><a href="#1、定义Layer类或者函数" class="headerlink" title="1、定义Layer类或者函数"></a>1、定义<strong>Layer类</strong>或者<strong>函数</strong></h3><ul><li><p>以后想这样直接使用：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">l1 = Layer(inputs,1,10,T.nnet.relu)</div><div class="line">l2 = Layer(l1.outputs,10,1,None)</div></pre></td></tr></table></figure></li><li><p>实现代码：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">import theano</div><div class="line">import theano.tensor as T</div><div class="line">import numpy as np</div><div class="line">class Layer(object):</div><div class="line">    def __init__(self,inputs,in_size,out_size,activation_function=None):</div><div class="line">        self.W = theano.shared(np.random.normal(0,1,(in_size,out_size)))  # 使用高斯函数初始化，大小为in_size*out_size</div><div class="line">        self.b = theano.shared(np.zeros(out_size) + 0.1)                  # 指定偏置，大小为out_size，注意+0.1在shared内，否则使用会报错</div><div class="line">        self.Wx_plus_b = T.dot(inputs,self.W) + self.b</div><div class="line">        self.activation_function = activation_function</div><div class="line">        if activation_function is None:</div><div class="line">            self.outpus = self.Wx_plus_b</div><div class="line">        else:</div><div class="line">            self.outputs = self.activation_function(self.Wx_plus_b)</div></pre></td></tr></table></figure></li><li><p>指定构造函数的参数：<code>inputs,in_size,out_size,activation_function</code></p></li></ul><h3 id="2、一个神经网络例子"><a href="#2、一个神经网络例子" class="headerlink" title="2、一个神经网络例子"></a>2、一个神经网络例子</h3><ul><li><p>制造数据</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">&apos;&apos;&apos;制造假数据，并显示&apos;&apos;&apos;</div><div class="line">x_data = np.linspace(-1,1,300)[:,np.newaxis]  # [:,np.newaxis]是将(300,)转为(300,1),增加一个维度，将列表转化为矩阵</div><div class="line">noise = np.random.normal(0,0.05,x_data.shape)</div><div class="line">y_data = np.square(x_data) -0.5 + noise</div><div class="line">plt.scatter(x_data,y_data)</div><div class="line">plt.show()</div></pre></td></tr></table></figure></li><li><p>定义输入，相当于TensorFlow中的placeholder，后面传入真实数据</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">x = T.dmatrix(&apos;x&apos;)</div><div class="line">y = T.dmatrix(&apos;y&apos;)</div></pre></td></tr></table></figure></li><li><p>定义网络，使用上面定义的<code>Layer</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">l1 = Layer(x, 1, 10,T.nnet.relu)</div><div class="line">l2 = Layer(l1.outputs,10,1,None)</div></pre></td></tr></table></figure></li><li><p>定义<strong>cost</strong>,并且计算其<strong>梯度</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">cost = T.mean(T.square(l2.outputs-y))</div><div class="line">g_w1,g_b1,g_w2,g_b2 = T.grad(cost, [l1.W,l1.b,l2.W,l2.b])</div></pre></td></tr></table></figure></li><li><p>使用<code>theano</code>中的<strong>function</strong>进行梯度下降</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">learning_rate = 0.05</div><div class="line">&apos;&apos;&apos;调用function，使用梯度下降求解&apos;&apos;&apos;</div><div class="line">train = theano.function(inputs=[x,y],</div><div class="line">                        outputs=cost,</div><div class="line">                        updates=[(l1.W, l1.W - learning_rate * g_w1),</div><div class="line">                                 (l1.b, l1.b - learning_rate * g_b1),</div><div class="line">                                 (l2.W, l2.W - learning_rate * g_w2),</div><div class="line">                                 (l2.b, l2.b - learning_rate * g_b2)]</div><div class="line">                        )</div></pre></td></tr></table></figure></li><li><p>传入之前制造的数据训练</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">for i in range(1000):</div><div class="line">    err = train(x_data,y_data)  # 训练，传入真实数据</div><div class="line">    if i%50 == 0:</div><div class="line">        print(err)</div></pre></td></tr></table></figure></li><li><p>预测</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">&apos;&apos;&apos;预测，输入为x，输出为layer2的输出&apos;&apos;&apos;</div><div class="line">prediction = theano.function([x],l2.outputs)</div></pre></td></tr></table></figure></li><li><p>计算准确度函数</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">def compute_accuracy(y_target,y_predict):</div><div class="line">    correct_prediction = np.equal(y_target,y_predict)</div><div class="line">    accuracy = np.sum(correct_prediction)/len(correct_prediction)</div><div class="line">    return accuracy</div></pre></td></tr></table></figure></li></ul><h3 id="3、保存和提取模型"><a href="#3、保存和提取模型" class="headerlink" title="3、保存和提取模型"></a>3、保存和提取模型</h3><ul><li>导入包：<code>import pickle</code></li><li><p>保存模型–即学到的参数<strong>权重</strong>和<strong>偏置</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">&apos;&apos;&apos;保存神经网络--保存学到的参数&apos;&apos;&apos;</div><div class="line">with open(&apos;model.pickle&apos;, mode=&apos;wb&apos;) as file:</div><div class="line">    #model = [l1.W.get_value(),l1.b.get_value(),</div><div class="line">    #         l2.W.get_value(),l2.b.get_value()]</div><div class="line">    model = &#123;&apos;layer1_w&apos;:l1.W.get_value(),&apos;layer1_b&apos;:l1.b.get_value(),</div><div class="line">             &apos;layer2_w&apos;:l2.W.get_value(),&apos;layer2_b&apos;:l2.b.get_value()&#125;    # 保存为字典形式，通过get_value()获取值</div><div class="line">    pickle.dump(model, file)</div><div class="line">    print(model[&apos;layer1_w&apos;])</div></pre></td></tr></table></figure></li><li><p>提取模型</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">with open(&apos;model.pickle&apos;,&apos;rb&apos;) as file:</div><div class="line">    model = pickle.load(file)</div><div class="line">    l1.W.set_value(model[&apos;layer1.w&apos;])    # 通过set_value()将值设置进去</div><div class="line">    ...</div></pre></td></tr></table></figure></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;一、Theano概述&quot;&gt;&lt;a href=&quot;#一、Theano概述&quot; class=&quot;headerlink&quot; title=&quot;一、Theano概述&quot;&gt;&lt;/a&gt;一、Theano概述&lt;/h2&gt;&lt;h3 id=&quot;1、介绍&quot;&gt;&lt;a href=&quot;#1、介绍&quot; class=&quot;headerlink&quot; title=&quot;1、介绍&quot;&gt;&lt;/a&gt;1、介绍&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;code&gt;Theano&lt;/code&gt; 是神经网络python机器学习的模块，和 &lt;code&gt;Tensowflow&lt;/code&gt; 类似&lt;/li&gt;
&lt;li&gt;可以在MacOS、Linux、Windows上运行&lt;/li&gt;
&lt;li&gt;theano 可以使用 GPU 进行运算&lt;/li&gt;
&lt;li&gt;网址：&lt;a href=&quot;http://deeplearning.net/software/theano/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://deeplearning.net/software/theano/&lt;/a&gt; &lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;2、安装&quot;&gt;&lt;a href=&quot;#2、安装&quot; class=&quot;headerlink&quot; title=&quot;2、安装&quot;&gt;&lt;/a&gt;2、安装&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;Windows上直接：&lt;code&gt;pip install theano&lt;/code&gt;&lt;ul&gt;
&lt;li&gt;可能提示个警告：&lt;code&gt;WARNING (theano.configdefaults): g++ not detected ! Theano will be unable to execute optimized C-implementations (for both CPU and GPU) and will default to Python implementations. Performance will be severely degraded. To remove this warning, set Theano flags cxx to an empty string.&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="Python" scheme="http://lawlite.cn/tags/Python/"/>
    
      <category term="DeepLearning" scheme="http://lawlite.cn/tags/DeepLearning/"/>
    
  </entry>
  
  <entry>
    <title>论文记录-Batch-Normalization</title>
    <link href="http://lawlite.cn/2017/01/09/%E8%AE%BA%E6%96%87%E8%AE%B0%E5%BD%95-Batch-Normalization/"/>
    <id>http://lawlite.cn/2017/01/09/论文记录-Batch-Normalization/</id>
    <published>2017-01-09T13:37:38.000Z</published>
    <updated>2017-06-25T08:50:34.000Z</updated>
    
    <content type="html"><![CDATA[<ul><li>参考论文：<a href="http://jmlr.org/proceedings/papers/v37/ioffe15.pdf" target="_blank" rel="external">http://jmlr.org/proceedings/papers/v37/ioffe15.pdf</a></li></ul><h2 id="一、论文概述"><a href="#一、论文概述" class="headerlink" title="一、论文概述"></a>一、论文概述</h2><ul><li><code>2015</code>年<code>Google</code>提出的<code>Batch Normalization</code></li><li>训练深层的神经网络很复杂，因为训练时每一层输入的分布在变化，导致训练过程中的饱和，称这种现象为：<code>internal covariate shift</code></li><li>需要降低<strong>学习率Learning Rate</strong>和注意<strong>参数的初始化</strong></li></ul><a id="more"></a><ul><li>论文中提出的方法是对于每一个小的训练<code>batch</code>都进行<strong>标准化（正态化）</strong><ul><li>允许使用较大的<strong>学习率</strong></li><li>不必太关心<strong>初始化的问题</strong></li><li>同时一些例子中不需要使用<code>Dropout</code>方法避免过拟合</li><li>此方法在<code>ImageNet classification</code>比赛中获得<code>4.82% top-5</code>的测试错误率</li></ul></li></ul><h2 id="二、BN思路"><a href="#二、BN思路" class="headerlink" title="二、BN思路"></a>二、<code>BN</code>思路</h2><h3 id="1、问题"><a href="#1、问题" class="headerlink" title="1、问题"></a>1、问题</h3><ul><li><p>如果输入数据是<strong>白化</strong>的（whitened），网络会更快的<strong>收敛</strong></p><ul><li>白化<strong>目的</strong>是<strong>降低数据的冗余性和特征的相关性</strong>，例如通过<strong>线性变换</strong>使数据为<strong>0均值</strong>和<strong>单位方差</strong></li></ul></li><li><p><strong>并非直接标准化每一层</strong>那么简单，如果不考虑归一化的影响，可能会降低梯度下降的影响</p></li><li>标准化与某个样本和所有样本都有关系<ul><li>解决上面的问题，我们希望对于任何参数值，都要满足想要的分布；$$\widehat x Norm(x,\chi )$$</li><li>对于反向传播，需要计算:${\partial Norm(x,\chi )} \over {\partial x}$和${\partial Norm({x},\chi )} \over {\partial \chi }$</li><li>这样做的<strong>计算代价</strong>是非常大的，因为需要计算x的<strong>协方差矩阵</strong> </li><li>然后<strong>白化</strong>操作：$${x - E[x]} \over {\sqrt {Cov[x]} }$$</li></ul></li><li>上面两种都不行或是不好，进而得到了<strong>BN</strong>的方法</li><li>既然<strong>白化每一层</strong>的<strong>输入代价非常大</strong>，我们可以进行简化</li></ul><h3 id="2、简化1"><a href="#2、简化1" class="headerlink" title="2、简化1"></a>2、简化1</h3><ul><li>标准化特征的<strong>每一个维度</strong>而不是去标准化<strong>所有的特征</strong>，这样就不用求<strong>协方差矩阵</strong>了<ul><li>例如<code>d</code>维的输入：$$x = ({x^{(1)}},{x^{(2)}}, \cdots ,{x^{(d)}})$$</li><li>标准化操作：<br>$${\widehat x^k} = {x^{(k) - E[x^{(k)}]} \over {\sqrt {Var[x^{(k)}]} }}$$</li></ul></li><li><p>需要注意的是标准化操作可能会<strong>降低数据的表达能力</strong>,例如我们之前提到的<strong>Sigmoid函数</strong>，标准化之后<strong>均值为0</strong>，<strong>方差为1</strong>，数据就会落在<strong>近似线性</strong>的函数区域内，这样激活函数的意义就不明显<br><img src="/assets/blog_images/BN/sigmoid.png" alt="Sigmoid激励函数" title="sigmoid"></p></li><li><p>所以对于每个标准化之后的$\widehat x^{(k)}$，对应一对<strong>参数</strong>：${\gamma ^{(k)}},{\beta ^{(k)}}$ ，然后令：${y^{(k)}} = {\gamma ^{(k)}}{\widehat x^{(k)}} + {\beta ^{(k)}}$</p></li><li>从式子来看就是对标准化的数据进行<strong>缩放和平移</strong>，不至于使数据落在线性区域内，增加数据的表达能力（式子中如果：${\gamma ^{(k)}} = \sqrt {Var[x^{(k)}]},  {\beta ^{(k)}} = E[x^{(k)}]$ ，就会使<strong>恢复到原来的值</strong>了）</li><li>但是这里还是使用的<strong>全部的数据集</strong>，但是如果使用<strong>随机梯度下降</strong>，可以选取一个<strong>batch</strong>进行训练</li></ul><h3 id="3、简化2"><a href="#3、简化2" class="headerlink" title="3、简化2"></a>3、简化2</h3><ul><li>第二种简化就是使用<code>mini-batch</code>进行<code>随机梯度下降</code></li><li>注意这里使用<code>mini-batch</code>也是标准化<strong>每一个维度</strong>上的特征，而不是所有的特征一起，因为若果<code>mini-batch</code>中的数据量小于特征的维度时，会产生<strong>奇异协方差矩阵</strong>， 对应的<strong>行列式</strong>的值为0，非满秩</li><li>假设<code>mini-batch</code> 大小为<code>m</code>的<code>B</code></li><li>$B = \{ {x_{1 \ldots m}}\}$对应的变换操作为：$$B{N_{\gamma ,\beta }}:{x_{1 \ldots m}} \to {y_{1 \ldots m}}$$</li><li>作者给出的批标准化的算法如下：<br><img src="/assets/blog_images/BN/bn-algorithm.png" alt="BN算法" title="bn-algorithm"></li><li>算法中的<code>ε</code>是一个<strong>很小的常量</strong>，为了保证数值的稳定性（就是<strong>防止除数为0</strong>）</li></ul><h3 id="4、反向传播求梯度："><a href="#4、反向传播求梯度：" class="headerlink" title="4、反向传播求梯度："></a>4、反向传播求梯度：</h3><ul><li>因为：$$y^{(k)} = \gamma ^{(k)}\widehat x^{(k)} + \beta ^{(k)}$$<ul><li>所以：$${\partial l \over \partial \widehat x_i} = {\partial l \over \partial y_i}\gamma $$</li></ul></li><li><p>因为：$$\widehat x_i = {x_i - \mu _B \over {\sqrt {\sigma _B^2 + \varepsilon } }}$$</p><ul><li>所以：$${\partial l \over \partial \sigma _B^2} = \sum\limits_{i=1}^m {\partial l \over \partial \widehat x_i} (x_i- \mu_B) {-1 \over 2}(\sigma_B^2 + \varepsilon)^{-{3\over2}}$$<br>$${\partial l \over \partial u_B} = \sum\limits_{i = 1}^m {\partial l \over \partial \widehat x_i} { - 1 \over \sqrt {\sigma _B^2 + \varepsilon }}$$</li></ul></li><li><p>因为：${\mu _B} = {1 \over m}\sum\limits_{i = 1}^m $和$\sigma _B^2 = {1 \over m}\sum\limits_{i = 1}^m {({x_i}}  - {\mu _B}{)^2}$</p><ul><li>所以：$${\partial l \over \partial x_i} = {\partial l \over \partial \widehat x_i}{1 \over \sqrt {\sigma _B^2 + \varepsilon } } + {\partial l \over \partial \sigma _B^2}{2(x_i - \mu _B) \over m} + {\partial l \over \partial u_B}{1 \over m}$$<ul><li>所以：$${\partial l \over \partial \gamma } = \sum\limits_{i = 1}^m {\partial l \over \partial y_i} {\widehat x_i}$$<br>$${\partial l \over \partial \beta } = \sum\limits_{i = 1}^m {\partial l \over \partial y_i} $$</li></ul></li></ul></li><li>对于<strong>BN变换</strong>是<strong>可微分</strong>的，随着网络的训练，网络层可以持续学到输入的分布。</li></ul><h2 id="三、BN网络的训练和推断（预测）"><a href="#三、BN网络的训练和推断（预测）" class="headerlink" title="三、BN网络的训练和推断（预测）"></a>三、<code>BN</code>网络的训练和推断（预测）</h2><h3 id="1、预测的问题"><a href="#1、预测的问题" class="headerlink" title="1、预测的问题"></a>1、预测的问题</h3><ul><li>按照<code>BN</code>方法，输入数据<code>x</code>会经过变化得到<code>BN（x）</code>，然后可以通过<strong>随机梯度下降</strong>进行训练，标准化是在<code>mini-batch</code>上所以是非常高效的。</li><li>但是对于推断我们希望输出只取决于输入，而对于输入<strong>只有一个实例数据</strong>，无法得到<code>mini-batch</code>的其他实例，就<strong>无法求对应的均值和方差</strong>了。</li></ul><h3 id="2、解决方法"><a href="#2、解决方法" class="headerlink" title="2、解决方法"></a>2、解决方法</h3><ul><li>可以通过从所有<strong>训练实例中获得的统计量</strong>来代替<code>mini-batch</code>中<code>m</code>个训练实例获得<strong>统计量均值和方差</strong><ul><li>比如我们机器学习算法，在训练集上进行了<strong>标准化</strong>，在测试集上的标准化操作时利用的训练集上的数据(<code>Standarscaler</code>中的<code>mean</code>和<code>variance</code>)</li></ul></li><li>我们对每个<code>mini-batch</code>做标准化，可以对记住每个<code>mini-batch</code>的B，然后得到<strong>全局统计量</strong></li><li>$$E[x] \leftarrow E_B[{\mu _B}]$$</li><li>$$Var[x] \leftarrow {m \over {m - 1}}E_B[\sigma _B^2]$$（这里方差采用的是<strong>无偏</strong>方差估计，所以是<code>m-1</code>）</li><li>所以<strong>推断</strong>采用<code>BN</code>的方式为：<br>$$\eqalign{<br>&amp; y = \gamma {x - E(x) \over \sqrt {Var[x] + \varepsilon }} + \beta   \cr<br>&amp; \quad= {\gamma  \over \sqrt {Var[x] + \varepsilon }}x + (\beta  - {\gamma E[x] \over \sqrt {Var[x] + \varepsilon }})} $$<h3 id="3、完整算法"><a href="#3、完整算法" class="headerlink" title="3、完整算法"></a>3、完整算法</h3></li><li>作者给出的完整算法：<br><img src="/assets/blog_images/BN/bn-algorithm-complete.png" alt="BN 完整算法" title="bn-algorithm-complete"></li></ul><h2 id="四、实验"><a href="#四、实验" class="headerlink" title="四、实验"></a>四、实验</h2><ul><li>最后给出的实验可以看出使用BN的方式训练<strong>精准度</strong>很高而且很<strong>稳定</strong>。<br><img src="/assets/blog_images/BN/experience-result.png" alt="实验结果" title="experience-result"></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;ul&gt;
&lt;li&gt;参考论文：&lt;a href=&quot;http://jmlr.org/proceedings/papers/v37/ioffe15.pdf&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://jmlr.org/proceedings/papers/v37/ioffe15.pdf&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;一、论文概述&quot;&gt;&lt;a href=&quot;#一、论文概述&quot; class=&quot;headerlink&quot; title=&quot;一、论文概述&quot;&gt;&lt;/a&gt;一、论文概述&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;code&gt;2015&lt;/code&gt;年&lt;code&gt;Google&lt;/code&gt;提出的&lt;code&gt;Batch Normalization&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;训练深层的神经网络很复杂，因为训练时每一层输入的分布在变化，导致训练过程中的饱和，称这种现象为：&lt;code&gt;internal covariate shift&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;需要降低&lt;strong&gt;学习率Learning Rate&lt;/strong&gt;和注意&lt;strong&gt;参数的初始化&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="DeepLearning" scheme="http://lawlite.cn/tags/DeepLearning/"/>
    
      <category term="Paper阅读记录" scheme="http://lawlite.cn/tags/Paper%E9%98%85%E8%AF%BB%E8%AE%B0%E5%BD%95/"/>
    
  </entry>
  
  <entry>
    <title>论文记录-Relu激励函数权重初始化</title>
    <link href="http://lawlite.cn/2017/01/09/%E8%AE%BA%E6%96%87%E8%AE%B0%E5%BD%95-Relu%E6%BF%80%E5%8A%B1%E5%87%BD%E6%95%B0%E6%9D%83%E9%87%8D%E5%88%9D%E5%A7%8B%E5%8C%96/"/>
    <id>http://lawlite.cn/2017/01/09/论文记录-Relu激励函数权重初始化/</id>
    <published>2017-01-09T07:20:00.000Z</published>
    <updated>2018-10-19T02:44:38.935Z</updated>
    
    <content type="html"><![CDATA[<ul><li>参考论文：<a href="https://arxiv.org/pdf/1502.01852v1.pdf" target="_blank" rel="external">点击这里查看</a></li><li><a href="http://lawlite.me/2016/12/20/%E8%AE%BA%E6%96%87%E8%AE%B0%E5%BD%95-UnderstandingTheDifficultyOfTrainingDeepFeedforwardNeuralNetworks/" target="_blank" rel="external">上一篇博客</a>谈到了关于<code>Sigmoid，tanh</code>激励函数的<strong>权重初始化方法</strong>，以及深度神经网络为什么难训练</li><li>这篇博客主要推导关于<code>Relu</code>类激励函数的<strong>权重初始化方法</strong></li></ul><a id="more"></a><h2 id="一、ReLu-PReLu激励函数"><a href="#一、ReLu-PReLu激励函数" class="headerlink" title="一、ReLu/PReLu激励函数"></a>一、<code>ReLu/PReLu</code>激励函数</h2><ul><li>目前<code>ReLu</code>激活函数使用比较多，而<a href="http://lawlite.me/2016/12/20/%E8%AE%BA%E6%96%87%E8%AE%B0%E5%BD%95-UnderstandingTheDifficultyOfTrainingDeepFeedforwardNeuralNetworks/" target="_blank" rel="external">上一篇论文博客</a>中没有讨论，如果还是使用同样初始化权重的方法（<strong>Xavier初始化</strong>）会有问题</li><li><code>PReLu</code>函数定义如下：<ul><li><img src="/assets/blog_images/Weights-Initialization/Weights_initialization_06.png" alt="PRelu激励函数" title="Weights_initialization_06"></li><li>等价于：$$f(y_i) = \max (0,y_i) + a_i\min (0,y_i)$$</li></ul></li><li><code>ReLu</code>（左）和<code>PReLu</code>（右）激活函数图像<br><img src="/assets/blog_images/Weights-Initialization/Weights_initialization_07.png" alt="Relu和PRelu激励函数" title="Weights_initialization_07"></li></ul><h2 id="二、前向传播推导"><a href="#二、前向传播推导" class="headerlink" title="二、前向传播推导"></a>二、前向传播推导</h2><h3 id="1、符号说明"><a href="#1、符号说明" class="headerlink" title="1、符号说明"></a>1、符号说明</h3><p>$\varepsilon$……………………………………目标函数<br>$\mu$……………………………………动量<br>$\alpha$……………………………………学习率<br>$f()$…………………………………激励函数<br>$l$……………………………………当前层<code>layer</code><br>$L$……………………………………神经网络总层数<br>$b$…………………………..…………偏置向量</p><h3 id="2、推导过程"><a href="#2、推导过程" class="headerlink" title="2、推导过程"></a>2、推导过程</h3><ul><li><p>可以得到：$$y_l = W_l x_l + b_l……………………………………..(1)$$<br>$$x_l= f(y_{l - 1})$$</p></li><li><p>根据式<code>(1)</code>得：<br>$$Var[y_l] = n_lVar[w_lx_l]………………………………….(2)$$</p></li><li>因为初始化权重<code>w</code>均值为<code>0</code>，所以<ul><li><strong>期望</strong>：$$E(w_l) = 0$$</li><li><strong>方差</strong>：$$Var[w_l] = E(w_l^2) - E^2(w_l) = E(w_l^2)$$</li></ul></li><li><p>根据 <strong>公式(2)</strong> 继续推导：</p><ul><li><p>$Var[y_l] = n_l Var[w_l x_l]\\<br>\quad \quad\quad = n_l[E(w_l^2 x_l^2) - E^2(w_l x_l)]\\<br>\quad \quad\quad = n_lE(w_l^2)E(x_l^2)\\<br>\quad \quad\quad = n_lVar[w_l]E(x_l^2)……………………………………..(3)$</p></li><li><p>对于<code>x</code>来说：$Var[x_l] \ne E[x_l^2]$，除非<code>x</code>的均值也是<code>0</code>,</p></li><li>对于<code>ReLu</code>函数来说：$x_l = \max (0,y_{l - 1})$，所以<strong>不可能均值为0</strong></li></ul></li><li><p><code>w</code>满足对称区间的分布，并且偏置${b_{l - 1}} = 0$，所以${y_{l - 1}}$也满足<strong>对称区间的分布</strong>，所以： </p><ul><li>$E(x_l^2) = E[max(0, y_{l-1})^2]\\<br>\quad \quad\quad= {1\over 2} [E(y_{l-1}^2)]\\<br>\quad \quad\quad= {1 \over 2} [E(y_{l-1}^2) - E^2(y_{l-1})]……………………………………(4)$</li></ul></li><li>将上式<code>(4)</code>代入<code>(3)</code>中得：<br>$$Var[y_l] = {1 \over 2}{n_l}Var[w_l]Var[y_{l - 1}]……………………………………….(5)$$</li><li>所以对于<code>L</code>层:<br>$$Var[y_L] = Var[y_1]\prod\limits_{l = 2}^L {1 \over 2}n_lVar[w_l] …………………………………..(6)$$<ul><li>从上式可以看出，因为<strong>累乘</strong>的存在，若是$${1 \over 2}n_lVar[w_l] &lt; 1$$，每次累乘都会使方差缩小，若是大于<code>1</code>，每次会使方差当大。</li><li>所以我们希望：<br>$${1 \over 2}n_lVar[w_l] = 1$$</li></ul></li><li>所以<strong>初始化方法</strong>为：是<code>w</code>满足<strong>均值为0</strong>，<strong>标准差</strong>为$\sqrt {2 \over n_l}$的<strong>高斯分布</strong>，同时<strong>偏置</strong>初始化为<code>0</code></li></ul><h2 id="三、反向传播推导"><a href="#三、反向传播推导" class="headerlink" title="三、反向传播推导"></a>三、反向传播推导</h2><ul><li><p>$\Delta x_l = \widehat W_l\Delta y_l…………………………………………….(7)$</p><ul><li>假设$\widehat W_l$和$\Delta y_l$相互独立</li><li>当$\widehat W_l$初始化为对称区间的分布时，可以得到：$\Delta x_l$的<strong>均值</strong>为<code>0</code></li><li><code>△x,△y</code>都表示梯度，即：<br>$$\Delta x = {\partial \varepsilon  \over \partial x}$$，$$\Delta y = {\partial \varepsilon  \over \partial y}$$</li></ul></li><li><p>根据<strong>反向传播</strong>：<br>$$\Delta {y_l} = f^{‘}(y_l)\Delta x_{l + 1}$$</p><ul><li>对于<code>ReLu</code>函数，<strong>f的导数</strong>为<code>0</code>或<code>1</code>，且<strong>概率是相等的</strong>，假设$f^{‘}(y_l)$和$\Delta x_{l + 1}$是相互独立的，</li><li>所以：$$E[\Delta y_l] = E[\Delta x_{l + 1}]/2 = 0$$</li></ul></li><li>所以：$$E[(\Delta y_l)^2] = Var[\Delta y_l] = {1 \over 2}Var[\Delta x_{l + 1}]……………………………………………(8)$$</li><li>根据<code>(7)</code>可以得到：              <ul><li>$Var[\Delta x_l] = \widehat n_l Var[w_l] Var[\Delta y_l] \\<br>\quad\quad\quad\quad= {1\over2} {\widehat n_l Var[w_l]Var[\Delta x_{l+1}]}$</li></ul></li><li>将<code>L</code>层展开得：<br>$$Var[\Delta x_2] = Var[\Delta x_{L + 1}]\prod\limits_{l = 2}^L {1 \over 2}\widehat n_lVar[w_l]…………………………………………………..(9)$$</li><li><p>同样令：$${1 \over 2}\widehat n_lVar[w_l] = 1$$</p><ul><li>注意这里：$\widehat n_l = k_l^2d_l$，而$n_l = k_l^2c_l = k_l^2d_{l - 1}$</li></ul></li><li><p>所以$w_l$应满足均值为0，标准差为$\sqrt {2 \over \widehat n_l}$的的分布</p></li></ul><h2 id="四、正向和反向传播讨论、实验和PReLu函数"><a href="#四、正向和反向传播讨论、实验和PReLu函数" class="headerlink" title="四、正向和反向传播讨论、实验和PReLu函数"></a>四、正向和反向传播讨论、实验和<strong>PReLu</strong>函数</h2><h2 id="1、正向和方向传播"><a href="#1、正向和方向传播" class="headerlink" title="1、正向和方向传播"></a>1、正向和方向传播</h2><ul><li>对于<strong>正向和反向</strong>两种初始化权重的方式都是可以的，论文中的模型都能够<strong>收敛</strong></li><li>比如利用<strong>反向传播</strong>得到的初始化得到：$$\prod\limits_{l = 2}^L {1 \over 2}\widehat n_lVar[{w_l}]  = 1$$</li><li><p>对应到<strong>正向传播</strong>中得到：</p><ul><li>$\prod\limits_{l=2}^L{1\over2} {n_lVar[w_l]} = \prod\limits_{l=2}^L {n_l \over \widehat n_l}\\<br>\quad\quad\quad\quad\quad\quad= {k_2^2 c_2 \over k_2^2 d_2} \cdot {k_3^2 d_2 \over k_3^2d_3} \cdot {k_L^2d_L \over K_L^2 d_L} \\<br>\quad\quad\quad\quad\quad\quad= {c_2 \over d_L}$</li></ul></li><li><p>所以也不是逐渐缩小的</p></li><li><p>实验给出了与<strong>第一篇论文</strong>的比较，如下图所示，当神经网络有30层时，<strong>Xavier初始化权重</strong>的方法（第一篇论文中的方法）已经不能收敛。<br><img src="/assets/blog_images/Weights-Initialization/Weights_initialization_12.png" alt="实验对比" title="Weights_initialization_12"></p><h3 id="2、PRelu对应方差"><a href="#2、PRelu对应方差" class="headerlink" title="2、PRelu对应方差"></a>2、<code>PRelu</code>对应方差</h3></li><li><p>对于<strong>PReLu激励函数</strong>可以得到：$${1 \over 2}(1 + a^2)n_lVar[w_l] = 1$$</p><ul><li>当<code>a=0</code>时就是对应的<strong>ReLu激励函数</strong></li><li>当<code>a=1</code>是就是对应<strong>线性函数</strong></li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;ul&gt;
&lt;li&gt;参考论文：&lt;a href=&quot;https://arxiv.org/pdf/1502.01852v1.pdf&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;点击这里查看&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://lawlite.me/2016/12/20/%E8%AE%BA%E6%96%87%E8%AE%B0%E5%BD%95-UnderstandingTheDifficultyOfTrainingDeepFeedforwardNeuralNetworks/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;上一篇博客&lt;/a&gt;谈到了关于&lt;code&gt;Sigmoid，tanh&lt;/code&gt;激励函数的&lt;strong&gt;权重初始化方法&lt;/strong&gt;，以及深度神经网络为什么难训练&lt;/li&gt;
&lt;li&gt;这篇博客主要推导关于&lt;code&gt;Relu&lt;/code&gt;类激励函数的&lt;strong&gt;权重初始化方法&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="DeepLearning" scheme="http://lawlite.cn/tags/DeepLearning/"/>
    
      <category term="Paper阅读记录" scheme="http://lawlite.cn/tags/Paper%E9%98%85%E8%AF%BB%E8%AE%B0%E5%BD%95/"/>
    
  </entry>
  
  <entry>
    <title>Python机器学习</title>
    <link href="http://lawlite.cn/2017/01/08/Python%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    <id>http://lawlite.cn/2017/01/08/Python机器学习/</id>
    <published>2017-01-08T15:01:58.000Z</published>
    <updated>2017-06-25T08:48:51.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="机器学习算法Python实现"><a href="#机器学习算法Python实现" class="headerlink" title="机器学习算法Python实现"></a>机器学习算法Python实现</h1><h2 id="说明"><a href="#说明" class="headerlink" title="说明"></a>说明</h2><ul><li>github地址：<a href="https://github.com/lawlite19/MachineLearning_Python" target="_blank" rel="external">https://github.com/lawlite19/MachineLearning_Python</a></li><li><strong>因为里面的公式加载出现问题，这里只给出了目录，可以去github中查看</strong><h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2></li></ul><ul><li><a href="#机器学习算法python实现">机器学习算法Python实现</a><ul><li><a href="#一-线性回归">一、线性回归</a><ul><li><a href="#1-代价函数">1、代价函数</a></li><li><a href="#2-梯度下降算法">2、梯度下降算法</a></li><li><a href="#3-均值归一化">3、均值归一化</a></li><li><a href="#4-最终运行结果">4、最终运行结果</a></li><li><a href="#5-使用scikit-learn库中的线性模型实现">5、使用scikit-learn库中的线性模型实现</a><a id="more"></a></li></ul></li><li><a href="#二-逻辑回归">二、逻辑回归</a><ul><li><a href="#1-代价函数">1、代价函数</a></li><li><a href="#2-梯度">2、梯度</a></li><li><a href="#3-正则化">3、正则化</a></li><li><a href="#4-s型函数即">4、S型函数（即）</a></li><li><a href="#5-映射为多项式">5、映射为多项式</a></li><li><a href="#6-使用的优化方法">6、使用的优化方法</a></li><li><a href="#7-运行结果">7、运行结果</a></li><li><a href="#8-使用scikit-learn库中的逻辑回归模型实现">8、使用scikit-learn库中的逻辑回归模型实现</a></li></ul></li><li><a href="#逻辑回归_手写数字识别_onevsall">逻辑回归_手写数字识别_OneVsAll</a><ul><li><a href="#1-随机显示100个数字">1、随机显示100个数字</a></li><li><a href="#2-onevsall">2、OneVsAll</a></li><li><a href="#3-手写数字识别">3、手写数字识别</a></li><li><a href="#4-预测">4、预测</a></li><li><a href="#5-运行结果">5、运行结果</a></li><li><a href="#6-使用scikit-learn库中的逻辑回归模型实现">6、使用scikit-learn库中的逻辑回归模型实现</a></li></ul></li><li><a href="#三-bp神经网络">三、BP神经网络</a><ul><li><a href="#1-神经网络model">1、神经网络model</a></li><li><a href="#2-代价函数">2、代价函数</a></li><li><a href="#3-正则化">3、正则化</a></li><li><a href="#4-反向传播bp">4、反向传播BP</a></li><li><a href="#5-bp可以求梯度的原因">5、BP可以求梯度的原因</a></li><li><a href="#6-梯度检查">6、梯度检查</a></li><li><a href="#7-权重的随机初始化">7、权重的随机初始化</a></li><li><a href="#8-预测">8、预测</a></li><li><a href="#9-输出结果">9、输出结果</a></li></ul></li><li><a href="#四-svm支持向量机">四、SVM支持向量机</a><ul><li><a href="#1-代价函数">1、代价函数</a></li><li><a href="#2-large-margin">2、Large Margin</a></li><li><a href="#3-svm-kernel核函数">3、SVM Kernel（核函数）</a></li><li><a href="#4-使用中的模型代码">4、使用中的模型代码</a></li><li><a href="#5-运行结果">5、运行结果</a></li></ul></li><li><a href="#五-k-means聚类算法">五、K-Means聚类算法</a><ul><li><a href="#1-聚类过程">1、聚类过程</a></li><li><a href="#2-目标函数">2、目标函数</a></li><li><a href="#3-聚类中心的选择">3、聚类中心的选择</a></li><li><a href="#4-聚类个数k的选择">4、聚类个数K的选择</a></li><li><a href="#5-应用图片压缩">5、应用——图片压缩</a></li><li><a href="#6-使用scikit-learn库中的线性模型实现聚类">6、使用scikit-learn库中的线性模型实现聚类</a></li><li><a href="#7-运行结果">7、运行结果</a></li></ul></li><li><a href="#六-pca主成分分析降维">六、PCA主成分分析（降维）</a><ul><li><a href="#1-用处">1、用处</a></li><li><a href="#2-2d-1dnd-kd">2、2D–&gt;1D，nD–&gt;kD</a></li><li><a href="#3-主成分分析pca与线性回归的区别">3、主成分分析PCA与线性回归的区别</a></li><li><a href="#4-pca降维过程">4、PCA降维过程</a></li><li><a href="#5-数据恢复">5、数据恢复</a></li><li><a href="#6-主成分个数的选择即要降的维度">6、主成分个数的选择（即要降的维度）</a></li><li><a href="#7-使用建议">7、使用建议</a></li><li><a href="#8-运行结果">8、运行结果</a></li><li><a href="#9-使用scikit-learn库中的pca实现降维">9、使用scikit-learn库中的PCA实现降维</a></li></ul></li><li><a href="#七-异常检测-anomaly-detection">七、异常检测 Anomaly Detection</a><ul><li><a href="#1-高斯分布正态分布">1、高斯分布（正态分布）</a></li><li><a href="#2-异常检测算法">2、异常检测算法</a></li><li><a href="#3-评价的好坏以及的选取">3、评价的好坏，以及的选取</a></li><li><a href="#4-选择使用什么样的feature单元高斯分布">4、选择使用什么样的feature（单元高斯分布）</a></li><li><a href="#5-多元高斯分布">5、多元高斯分布</a></li><li><a href="#6-单元和多元高斯分布特点">6、单元和多元高斯分布特点</a></li><li><a href="#7-程序运行结果">7、程序运行结果</a></li></ul></li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;机器学习算法Python实现&quot;&gt;&lt;a href=&quot;#机器学习算法Python实现&quot; class=&quot;headerlink&quot; title=&quot;机器学习算法Python实现&quot;&gt;&lt;/a&gt;机器学习算法Python实现&lt;/h1&gt;&lt;h2 id=&quot;说明&quot;&gt;&lt;a href=&quot;#说明&quot; class=&quot;headerlink&quot; title=&quot;说明&quot;&gt;&lt;/a&gt;说明&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;github地址：&lt;a href=&quot;https://github.com/lawlite19/MachineLearning_Python&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://github.com/lawlite19/MachineLearning_Python&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;因为里面的公式加载出现问题，这里只给出了目录，可以去github中查看&lt;/strong&gt;&lt;h2 id=&quot;目录&quot;&gt;&lt;a href=&quot;#目录&quot; class=&quot;headerlink&quot; title=&quot;目录&quot;&gt;&lt;/a&gt;目录&lt;/h2&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#机器学习算法python实现&quot;&gt;机器学习算法Python实现&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#一-线性回归&quot;&gt;一、线性回归&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#1-代价函数&quot;&gt;1、代价函数&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#2-梯度下降算法&quot;&gt;2、梯度下降算法&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#3-均值归一化&quot;&gt;3、均值归一化&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#4-最终运行结果&quot;&gt;4、最终运行结果&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#5-使用scikit-learn库中的线性模型实现&quot;&gt;5、使用scikit-learn库中的线性模型实现&lt;/a&gt;
    
    </summary>
    
    
      <category term="Python" scheme="http://lawlite.cn/tags/Python/"/>
    
      <category term="机器学习" scheme="http://lawlite.cn/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>论文记录-UnderstandingTheDifficultyOfTrainingDeepFeedforwardNeuralNetworks</title>
    <link href="http://lawlite.cn/2016/12/20/%E8%AE%BA%E6%96%87%E8%AE%B0%E5%BD%95-UnderstandingTheDifficultyOfTrainingDeepFeedforwardNeuralNetworks/"/>
    <id>http://lawlite.cn/2016/12/20/论文记录-UnderstandingTheDifficultyOfTrainingDeepFeedforwardNeuralNetworks/</id>
    <published>2016-12-20T11:03:24.000Z</published>
    <updated>2017-06-25T08:50:53.000Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1、说明"><a href="#1、说明" class="headerlink" title="1、说明"></a>1、说明</h3><ul><li><code>2010</code>年的一篇论文，说明<strong>深度神经网络为什么难以训练</strong>，当时只讨论了<strong>Sigmoid，tanh和Softsign激活函数</strong></li><li>提出了一种<strong>初始化权重weights</strong>的方法，能够解决训练中梯度消失的问题<ul><li>但是使用现在的<code>ReLu</code>激活函数，同样使用此初始化方法就会出现问题。</li></ul></li></ul><a id="more"></a><h3 id="2、Sigmoid激励函数实验"><a href="#2、Sigmoid激励函数实验" class="headerlink" title="2、Sigmoid激励函数实验"></a>2、<code>Sigmoid</code>激励函数实验</h3><ul><li>说明<ul><li>论文首先通过实验观察激活函数的影响，指出<code>Sigmoid</code>函数是<strong>不适合</strong>作为深度神经网络激活函数的</li><li>因为它的<strong>均值总是大于0</strong>的，如下图，导致后面的隐藏层<code>hidden layer</code>趋于饱和，并且发现饱和的神经元可以自发移出饱和趋于，<strong>但是非常慢</strong>。接着发现一个新的非线性的激活函数是非常有益的。<br><img src="/assets/blog_images/Weights-Initialization/01.png" alt="Sigmoid函数" title="01"></li><li>最后观察每一层激活值和梯度的变化，给出了一种<strong>新的初始化权重的方法</strong>。</li></ul></li><li>实验部分<ul><li>初始化偏置<code>biases</code>为<code>0</code>，权重<code>w</code>服从<strong>均匀分布</strong>，即：<br>$${{W_{ij}} \sim U[ - {1 \over {\sqrt n }},{1 \over {\sqrt n }}]}$$</li><li>其中<code>n</code>为前一层的神经元个数。然后构建了一个含有<strong>4个隐含层</strong>的神经网络，激活函数使用的是<code>Sigmoid</code></li><li>观察每一层的<strong>激活值的均值和标准差</strong>随着<strong>训练次数</strong>的变化，<code>layer1</code>表示<strong>第一个隐含层的输出</strong>，以此类推。如图所示：<strong>实线表示均值mean value，垂直的条表示标准差。</strong><br><img src="/assets/blog_images/Weights-Initialization/02.png" alt="均值和方差的变化" title="02"></li></ul></li><li>实验的直观理解<ul><li>最后我们使用 ${Softmax(b+Wh)}$ 作为输出预测的，刚开始训练的时候不能够很好的预测<code>y</code>的值，因此误差梯度会迫使<code>Wh</code>趋于<code>0</code>，所以会是<code>h</code>的值趋于<code>0</code>，<code>h</code>就是上一层的输出，所以激活值很快为<code>0</code>。</li><li>但是对于<code>tanh</code>函数是关于<strong>原点对称</strong>的，图像如下，值趋于<code>0</code>是好的，因为梯度能够<strong>反向传播</strong>回去，但是对于<code>sigmoid</code>函数来说就<strong>趋于饱和</strong>的位置了，梯度很难反向传回去，也就学习不到东西了。<br><img src="/assets/blog_images/Weights-Initialization/03.png" alt="Sigmoid和tanh函数" title="03"></li></ul></li></ul><h3 id="3、梯度计算和公式推导"><a href="#3、梯度计算和公式推导" class="headerlink" title="3、梯度计算和公式推导"></a>3、梯度计算和公式推导</h3><h4 id="1-代价函数"><a href="#1-代价函数" class="headerlink" title="1) 代价函数"></a>1) 代价函数</h4><ul><li>代价函数使用的是<strong>交叉熵代价函数</strong>，相比对于<strong>二次代价函数</strong>会更好<br><img src="/assets/blog_images/Weights-Initialization/04.png" alt="交叉熵和二次代价函数图像" title="04"> </li><li>二次代价函数较为平坦，所以使用梯度下降会比较慢。<h4 id="2-公式推导"><a href="#2-公式推导" class="headerlink" title="2) 公式推导"></a>2) 公式推导</h4></li><li>符号说明<br>${z^i}$………………………………第i层的激活值向量<br>${s^i}$………………………………第i+1层的输入<br>$X$………………………………输入<br>${n_i}$………………………………第i层神经元个数<br>$W$………………………………权重</li><li>可以得到：<br>$${s^i = {z^i}{W^i} + {b^i}}$$<br>$${z^{i + 1} = f({s^i})}$$</li><li>所以分别对上面两式求偏导可以得到：<br>$${{\partial Cost \over \partial s_k^i}=f^{'}W_{k,\bullet}^{i+1}}{ \partial Cost \over \partial s^{i+1}}…………………………….(1)$$</li></ul><p>$${{\partial Cost \over \partial w_{l,k}^i} = z_l^i}{\partial Cost \over \partial s_k^i}........................................(2)$$推导如下![BP推导][5]- 上面公式推导说明  - 其中 $${{\partial Cost \over \partial s^{i-1}}={\delta ^{i-1}}}$$</p><ul><li>这里<code>W</code>从<code>1</code>开始，上面给出的最终公式是从<code>0</code>开始。</li><li>对权重的偏导（梯度）再乘以输入 ${z^i}$ 即可。</li><li>因为我们使用均匀分布进行初始化，所以方差是一样的，对于<code>tanh</code>函数的导数，$${[\tanh (x)]^{‘}} = 1 - {[\tanh (x)]^2}$$</li><li>所以：$${f^,}(s_k^i) \approx 1$$</li><li>实际这里作者假设了<strong>这个区间内激活函数是线性的</strong>，第二篇论文中也有提到。（下面会给出）<ul><li>根据方差的公式： $$Var(x) = E({x^2}) - {E^2}(x)$$</li></ul></li><li><p>可以得到: $${Var[z^i] = Var[x] \prod\limits_{j=0}^{i-1}n_j Var[W^j]}…………………………(3)$$</p></li><li><p>推导如下：</p><ul><li>${Var(s) = Var(\sum\limits_i^n w_i x_i)}=\sum\limits_i^n Var(w_ix_i)$</li><li>${Var(wx) = E(w^2x^2) - E^2(wx)} \\<br>\quad\quad\quad\quad=E(w^2)E(x^2) - E^2(w)E^2(x) \\<br>\quad\quad\quad\quad=[E(w^2)-E^2(w)][E(x^2)-E^2(x)] + E^2(w)[E(x^2)-E^2(x)] + E^2(x)[E(w^2)-E^2(w)] \\<br>\quad\quad\quad\quad=Var(w)Var(x)+E^2Var(x)+E^2Var(w)$</li><li>因为输入的<strong>均值为0</strong>，所以$${E(w) = E(x) = 0}$$</li><li>所以：$${Var(wx) = Var(w)Var(x)}$$</li><li>又因为${f^{‘}(s_k^i) \approx 1}$成立，然后代入上面的式子即可</li><li>根据<strong>公式（1）</strong>，所以对${S^i}$偏导数的方差为：$${Var[{\partial Cost \over \partial s^i}]} = {Var[{\partial Cost \over \partial s^n}]}{\prod\limits_{j=i}^n}{n_{j+1}Var[W^j]}$$</li></ul></li></ul><ul><li><p>根据<strong>公式（2）</strong>，代入到对权重<code>w</code>偏导（即为梯度）的方差为: $${Var[{\partial Cost \over \partial w^i}]} = {\prod \limits_{j=0}^{i-1} n_j Var[W^j]}{\prod \limits_{j=i}^{n-1} n_{j+1}Var[W^j] \ast Var[x] Var[{\partial Cost \over \partial s^n}]}$$</p></li><li><p>对于正向传播，希望：$$\forall (i,j),Var[{z^i}] = Var[{z^j}]$$</p><ul><li>从反向传播的角度同样可以有：$${\forall (i,j), Var[{\partial Cost \over \partial s^i}]} = Var[{\partial Cost \over \partial s^j}]$$</li><li>就可以转化为：$$\left\{ {\matrix{<br>{n_iVar[w^i]}=1  \cr<br>{n_{i+1}Var[w^i]}=1  \cr<br>} }…………………………(4) \right.$$<ul><li>比如第一种(公式（3）)： $${Var[z^i] = Var[x] \prod\limits_{j=0}^{i-1}n_j Var[W^j]}$$ $$Var(z^i) = Var(x)$$</li><li>所以${n_i}Var[{w^i}] = 1$ ，第二种情况同理</li></ul></li></ul></li><li>所以将 <strong>（4）</strong> 中的两式相加可得：$${Var[{W^i}]}={2 \over {n_i + n_{i+1}}}$$<ul><li>如果所有层的神经元个数一样时：$$\left\{ {\matrix{<br>{Var[{\partial Cost \over \partial s^i}] = [nVar[W]]^{d-i}Var[x]} \cr<br>{Var[{\partial Cost \over \partial w^i}] = [nVar[w]]^{d}Var[x]Var[{\partial Cost \over \partial w^n}]} \cr<br>}} \right.$$</li><li>可以看到，所有层的梯度的方差都是一样的。但是对于很深层的神经网络还是有可能导致梯度消失。</li></ul></li></ul><h3 id="4、初始化权重方法"><a href="#4、初始化权重方法" class="headerlink" title="4、初始化权重方法"></a>4、初始化权重方法</h3><ul><li>最后提出了一个归一化的初始化方法，因为<code>W</code>服从均匀分布，根据均匀分布的方差公式可以得到：$${[c-(-c)]^2 \over 12} = {c^2 \over 3}$$</li><li>所以得到：$${2 \over {n_i + n_{i+1}} =}{ c^2 \over 3}$$</li><li>求出: <code> </code>$$c={\sqrt 6 \over \sqrt {n_i + n_{i+1}}}$$</li><li>所以最终给出初始化权重的方法为：$${W \sim U[-{{\sqrt 6}\over {\sqrt n_i+n_{i+1}}},{\sqrt 6 \over \sqrt {n_i + n_{i+1}}}]}$$</li></ul><h3 id="5、总结"><a href="#5、总结" class="headerlink" title="5、总结"></a>5、总结</h3><ul><li>论文讨论了<code>Sigmoid，tanh</code>激励函数权重初始化的问题，并给出了初始化的方法，- 但是针对<code>ReLu</code>这种激励函数是不适用的，第二篇会提到。</li><li>并且推导过程中假设了激励函数在初始化对应区间上是<strong>线性的</strong>，即假设导数恒为1我感觉也是存在问题的。</li><li>作者给出的实验部分网络的深度还是很有限的，随着网络的不断加深，对应的初始化权重的分布范围还是会不断减小的。可以通过控制学习率参数等方式来进行对应处理。</li></ul><h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><ul><li><a href="http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf" target="_blank" rel="external">http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;1、说明&quot;&gt;&lt;a href=&quot;#1、说明&quot; class=&quot;headerlink&quot; title=&quot;1、说明&quot;&gt;&lt;/a&gt;1、说明&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;code&gt;2010&lt;/code&gt;年的一篇论文，说明&lt;strong&gt;深度神经网络为什么难以训练&lt;/strong&gt;，当时只讨论了&lt;strong&gt;Sigmoid，tanh和Softsign激活函数&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;提出了一种&lt;strong&gt;初始化权重weights&lt;/strong&gt;的方法，能够解决训练中梯度消失的问题&lt;ul&gt;
&lt;li&gt;但是使用现在的&lt;code&gt;ReLu&lt;/code&gt;激活函数，同样使用此初始化方法就会出现问题。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="DeepLearning" scheme="http://lawlite.cn/tags/DeepLearning/"/>
    
      <category term="Paper阅读记录" scheme="http://lawlite.cn/tags/Paper%E9%98%85%E8%AF%BB%E8%AE%B0%E5%BD%95/"/>
    
  </entry>
  
  <entry>
    <title>Tensorflow学习</title>
    <link href="http://lawlite.cn/2016/12/08/Tensorflow%E5%AD%A6%E4%B9%A0/"/>
    <id>http://lawlite.cn/2016/12/08/Tensorflow学习/</id>
    <published>2016-12-08T08:07:33.000Z</published>
    <updated>2017-06-25T08:50:03.000Z</updated>
    
    <content type="html"><![CDATA[<ul><li>github地址：<a href="https://github.com/lawlite19/MachineLearning_TensorFlow" target="_blank" rel="external">https://github.com/lawlite19/MachineLearning_TensorFlow</a></li></ul><h2 id="一、TensorFlow介绍"><a href="#一、TensorFlow介绍" class="headerlink" title="一、TensorFlow介绍"></a>一、TensorFlow介绍</h2><a id="more"></a><h3 id="1、什么是TensorFlow"><a href="#1、什么是TensorFlow" class="headerlink" title="1、什么是TensorFlow"></a>1、什么是TensorFlow</h3><ul><li>官网：<a href="https://www.tensorflow.org/" target="_blank" rel="external">https://www.tensorflow.org/</a></li><li>TensorFlow是Google开发的一款神经网络的Python外部的结构包, 也是一个采用数据流图来进行数值计算的开源软件库.</li><li>先绘制计算结构图, 也可以称是一系列可人机交互的计算操作, 然后把编辑好的Python文件 转换成 更高效的C++, 并在后端进行计算.</li></ul><h3 id="2、TensorFlow强大之处"><a href="#2、TensorFlow强大之处" class="headerlink" title="2、TensorFlow强大之处"></a>2、TensorFlow强大之处</h3><ul><li>擅长的任务就是训练深度神经网络</li><li>快速的入门神经网络,大大降低了深度学习（也就是深度神经网络）的开发成本和开发难度</li><li>TensorFlow 的开源性, 让所有人都能使用并且维护</li></ul><h3 id="3、安装TensorFlow"><a href="#3、安装TensorFlow" class="headerlink" title="3、安装TensorFlow"></a>3、安装TensorFlow</h3><ul><li>暂不支持Windows下安装TensorFlow,可以在虚拟机里使用或者安装Docker安装</li><li>这里在CentOS6.5下进行安装</li><li><p>安装<strong>Python2.7</strong>，默认CentOS中安装的是<strong>Python2.6</strong></p><ul><li><p>先安装<strong>zlib</strong>的依赖，下面安装<strong>easy_install</strong>时会用到</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">yum install zlib</div><div class="line">yum install zlib-devel</div></pre></td></tr></table></figure></li><li><p>在安装<strong>openssl</strong>的依赖，下面安装<strong>pip</strong>时会用到</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">yum install openssl</div><div class="line">yum install openssl-devel</div></pre></td></tr></table></figure></li><li><p>下载安装包，我传到<code>github</code>上的安装包，<code>https</code>协议后面加上<code>--no-check-certificate</code>，：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">wget https://raw.githubusercontent.com/lawlite19/LinuxSoftware/master/python/Python-2.7.12.tgz --no-check-certificate</div></pre></td></tr></table></figure></li><li><p>解压缩：<code>tar -zxvf xxx</code></p></li><li>进入，配置：<code>./configure --prefix=/usr/local/python2.7</code></li><li>编译并安装：<code>make &amp;&amp; make install</code></li><li>创建链接来使系统默认python变为python2.7,<br><code>ln -fs /usr/local/python2.7/bin/python2.7 /usr/bin/python</code></li><li>修改一下<strong>yum</strong>，因为yum的执行文件还是需要原来的<strong>python2.6</strong>,<code>vim /usr/bin/yum</code><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">#!/usr/bin/python</div></pre></td></tr></table></figure></li></ul><p>修改为系统原有的python版本地址</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">#!/usr/bin/python2.6</div></pre></td></tr></table></figure></li></ul><ul><li><p>安装<strong>easy_install</strong></p><ul><li>下载：<code>wget https://raw.githubusercontent.com/lawlite19/LinuxSoftware/blob/master/python/setuptools-26.1.1.tar.gz --no-check-certificate</code></li><li>解压缩：<code>tar -zxvf xxx</code></li><li><code>python setup.py build</code>  #注意这里python是新的python2.7</li><li><code>python setup.py install</code></li><li>到<code>/usr/local/python2.7/bin</code>目录下查看就会看到<code>easy_install</code>了</li><li>创建一个软连接：<code>ln -s /usr/local/python2.7/bin/easy_install /usr/local/bin/easy_install</code></li><li>就可以使用<code>easy_install 包名</code> 进行安装</li></ul></li><li><p>安装<strong>pip</strong></p><ul><li>下载:</li><li>解压缩：<code>tar -zxvf xxx</code></li><li>安装：<code>python setup.py install</code></li><li>到<code>/usr/local/python2.7/bin</code>目录下查看就会看到<code>pip</code>了</li><li>同样创建软连接：<code>ln -s /usr/local/python2.7/bin/pip /usr/local/bin/pip</code></li><li>就可以使用<code>pip install 包名</code>进行安装包了</li></ul></li><li><p>安装<strong>wingIDE</strong></p><ul><li>默认安装到<code>/usr/local/lib</code>下，进入，执行<code>./wing</code>命令即可执行</li><li>创建软连接：<code>ln -s /usr/local/lib/wingide5.1/wing /usr/local/bin/wing</code></li><li>破解：</li></ul></li></ul><ul><li><p>[另]安装<strong>VMwareTools</strong>，可以在windows和Linux之间复制粘贴</p><ul><li>启动CentOS</li><li>选择VMware中的虚拟机–&gt;安装VMware Tools</li><li>会自动弹出VMware Tools的文件夹</li><li>拷贝一份到root目录下 <code>cp VMwareTools-9.9.3-2759765.tar.gz /root</code></li><li>解压缩 <code>tar -zxvf VMwareTools-9.9.3-2759765.tar.gz</code></li><li>进入目录执行，<code>vmware-install.pl</code>，一路回车下去即可</li><li>重启CentOS即可</li></ul></li><li><p>安装<strong>numpy</strong></p><ul><li>直接安装没有出错</li></ul></li><li><p>安装<strong>scipy</strong></p><ul><li>安装依赖：<code>yum install bzip2-devel pcre-devel ncurses-devel  readline-devel tk-devel gcc-c++ lapack-devel</code></li><li>安装即可：<code>pip install scipy</code></li></ul></li><li><p>安装<strong>matplotlib</strong></p><ul><li>安装依赖：<code>yum install libpng-devel</code></li><li>安装即可：<code>pip install matplotlib</code></li><li>运行可能有以下的错误：<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">ImportError: No module named _tkinter</div></pre></td></tr></table></figure></li></ul><p>安装：<code>tcl8.5.9-src.tar.gz</code></p><ul><li>进入安装即可,<code>./confgiure  make  make install</code><br>安装：<code>tk8.5.9-src.tar.gz</code></li><li>进入安装即可。</li><li><strong>[注意]</strong>要重新安装一下<strong>Pyhton2.7</strong>才能链接到<code>tkinter</code></li></ul></li><li><p>安装<strong>scikit-learn</strong></p><ul><li>直接安装没有出错，但是缺少包<code>bz2</code></li><li>将系统中<code>python2.6</code>的<code>bz2</code>复制到<code>python2.7</code>对应文件夹下<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">cp /usr/lib/python2.6/lib-dynload/bz2.so /usr/local/python2.7/lib/python2.7/lib-dynload</div></pre></td></tr></table></figure></li></ul></li><li><p>安装<strong>TensorFlow</strong></p><ul><li><a href="https://www.tensorflow.org/" target="_blank" rel="external">官网点击</a></li><li><p>选择对应的版本</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div></pre></td><td class="code"><pre><div class="line"> # Ubuntu/Linux 64-bit, CPU only, Python 2.7</div><div class="line">$ export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.12.0rc0-cp27-none-linux_x86_64.whl</div><div class="line"></div><div class="line"># Ubuntu/Linux 64-bit, GPU enabled, Python 2.7</div><div class="line"># Requires CUDA toolkit 8.0 and CuDNN v5. For other versions, see &quot;Installing from sources&quot; below.</div><div class="line">$ export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-0.12.0rc0-cp27-none-linux_x86_64.whl</div><div class="line"></div><div class="line"># Mac OS X, CPU only, Python 2.7:</div><div class="line">$ export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-0.12.0rc0-py2-none-any.whl</div><div class="line"></div><div class="line"># Mac OS X, GPU enabled, Python 2.7:</div><div class="line">$ export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/mac/gpu/tensorflow_gpu-0.12.0rc0-py2-none-any.whl</div><div class="line"></div><div class="line"># Ubuntu/Linux 64-bit, CPU only, Python 3.4</div><div class="line">$ export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.12.0rc0-cp34-cp34m-linux_x86_64.whl</div><div class="line"></div><div class="line"># Ubuntu/Linux 64-bit, GPU enabled, Python 3.4</div><div class="line"># Requires CUDA toolkit 8.0 and CuDNN v5. For other versions, see &quot;Installing from sources&quot; below.</div><div class="line">$ export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-0.12.0rc0-cp34-cp34m-linux_x86_64.whl</div><div class="line"></div><div class="line"># Ubuntu/Linux 64-bit, CPU only, Python 3.5</div><div class="line">$ export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.12.0rc0-cp35-cp35m-linux_x86_64.whl</div><div class="line"></div><div class="line"># Ubuntu/Linux 64-bit, GPU enabled, Python 3.5</div><div class="line"># Requires CUDA toolkit 8.0 and CuDNN v5. For other versions, see &quot;Installing from sources&quot; below.</div><div class="line">$ export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-0.12.0rc0-cp35-cp35m-linux_x86_64.whl</div><div class="line"></div><div class="line"># Mac OS X, CPU only, Python 3.4 or 3.5:</div><div class="line">$ export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-0.12.0rc0-py3-none-any.whl</div><div class="line"></div><div class="line"># Mac OS X, GPU enabled, Python 3.4 or 3.5:</div><div class="line">$ export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/mac/gpu/tensorflow_gpu-0.12.0rc0-py3-none-any.whl</div></pre></td></tr></table></figure></li><li><p>对应<code>python</code>版本</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"> # Python 2</div><div class="line">$ sudo pip install --upgrade $TF_BINARY_URL</div><div class="line"></div><div class="line"># Python 3</div><div class="line">$ sudo pip3 install --upgrade $TF_BINARY_URL</div></pre></td></tr></table></figure></li><li><p>可能缺少依赖<code>glibc</code>,看对应提示的版本，</p></li><li>还有可能报错<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">ImportError: /usr/lib64/libstdc++.so.6: version `GLIBCXX_3.4.19&apos; not found (required by /usr/local/python2.7/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so)</div></pre></td></tr></table></figure></li></ul></li><li><p>安装对应版本的<strong>glibc</strong></p><ul><li>查看现有版本的glibc, <code>strings /lib64/libc.so.6 |grep GLIBC</code></li><li>下载对应版本：<code>wget http://ftp.gnu.org/gnu/glibc/glibc-2.17.tar.gz</code></li><li>解压缩：<code>tar -zxvf glibc-2.17</code></li><li>进入文件夹创建<code>build</code>文件夹<code>cd glibc-2.17 &amp;&amp; mkdir build</code></li><li><p>配置：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">../configure  \</div><div class="line">   --prefix=/usr          \</div><div class="line">   --disable-profile      \</div><div class="line">   --enable-add-ons       \</div><div class="line">   --enable-kernel=2.6.25 \</div><div class="line">   --libexecdir=/usr/lib/glibc</div></pre></td></tr></table></figure></li><li><p>编译安装：<code>make &amp;&amp; make install</code></p></li><li>可以再用命令：<code>strings /lib64/libc.so.6 |grep GLIBC</code>查看</li></ul></li><li><p>添加<strong>GLIBCXX_3.4.19</strong>的支持</p><ul><li>下载：<code>wget https://raw.githubusercontent.com/lawlite19/LinuxSoftware/master/python2.7_tensorflow/libstdc++.so.6.0.20</code> </li><li>复制到<code>/usr/lib64</code>文件夹下：<code>cp libstdc++.so.6.0.20 /usr/lib64/</code></li><li>添加执行权限：<code>chmod +x /usr/lib64/libstdc++.so.6.0.20</code></li><li>删除原来的：<code>rm -rf /usr/lib64/libstdc++.so.6</code></li><li>创建软连接：<code>ln -s /usr/lib64/libstdc++.so.6.0.20 /usr/lib64/libstdc++.so.6</code></li><li>可以查看是否有个版本：<code>strings /usr/lib64/libstdc++.so.6 | grep GLIBCXX</code></li></ul></li></ul><ul><li><p>运行还可能报错编码的问题，这里安装<code>0.10.0</code>版本:<code>pip install --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.10.0rc0-cp27-none-linux_x86_64.whl</code></p></li><li><p>安装<code>pandas</code></p><ul><li><code>pip install pandas</code>没有问题</li></ul></li></ul><h2 id="二、TensorFlow基础架构"><a href="#二、TensorFlow基础架构" class="headerlink" title="二、TensorFlow基础架构"></a>二、TensorFlow基础架构</h2><h3 id="1、处理结构"><a href="#1、处理结构" class="headerlink" title="1、处理结构"></a>1、处理结构</h3><ul><li>Tensorflow 首先要定义神经网络的结构,然后再把数据放入结构当中去运算和 training<br><img src="https://raw.githubusercontent.com/lawlite19/MachineLearning_TensorFlow/master/images/tensors_flowing.gif" alt="enter description here" title="tensors_flowing.gif"></li><li>TensorFlow是采用数据流图（data　flow　graphs）来计算</li><li>首先我们得创建一个数据流流图</li><li>然后再将我们的数据（数据以张量(tensor)的形式存在）放在数据流图中计算</li><li>张量（tensor):<ul><li>张量有多种. 零阶张量为 纯量或标量 (scalar) 也就是一个数值. 比如 <a href="https://raw.githubusercontent.com/lawlite19/MachineLearning_TensorFlow/master/images/tensors_flowing.gif" title="tensors_flowing.gif" target="_blank" rel="external">1</a></li><li>一阶张量为 向量 (vector), 比如 一维的 [1, 2, 3]</li><li>二阶张量为 矩阵 (matrix), 比如 二维的 [[1, 2, 3],[4, 5, 6],[7, 8, 9]]</li><li>以此类推, 还有 三阶 三维的 …</li></ul></li></ul><h3 id="2、一个例子"><a href="#2、一个例子" class="headerlink" title="2、一个例子"></a>2、一个例子</h3><ul><li><p>求<code>y=1*x+3</code>中的权重<code>1</code>和偏置<code>3</code></p><ul><li><p>定义这个函数</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">x_data = np.random.rand(100).astype(np.float32)</div><div class="line">y_data = x_data*1.0+3.0</div></pre></td></tr></table></figure></li><li><p>创建TensorFlow结构</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">Weights = tf.Variable(tf.random_uniform([1], -1.0, 1.0)) # 创建变量Weight是，范围是 -1.0~1.0</div><div class="line">biases = tf.Variable(tf.zeros([1]))                      # 创建偏置，初始值为0</div><div class="line">y = Weights*x_data+biases                                # 定义方程</div><div class="line">loss = tf.reduce_mean(tf.square(y-y_data))               # 定义损失，为真实值减去我们每一步计算的值</div><div class="line">optimizer = tf.train.GradientDescentOptimizer(0.5)       # 0.5 是学习率</div><div class="line">train = optimizer.minimize(loss)                         # 使用梯度下降优化</div><div class="line">init = tf.initialize_all_variables()                     # 初始化所有变量</div></pre></td></tr></table></figure></li><li><p>定义<code>Session</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">sess = tf.Session()</div><div class="line">sess.run(init)</div></pre></td></tr></table></figure></li><li><p>输出结果</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">for i in range(201):</div><div class="line">   sess.run(train)</div><div class="line">   if i%20 == 0:</div><div class="line">       print i,sess.run(Weights),sess.run(biases)</div></pre></td></tr></table></figure></li></ul><p>结果为：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"> 0 [ 1.60895896] [ 3.67376709]</div><div class="line">20 [ 1.04673827] [ 2.97489643]</div><div class="line">40 [ 1.011392] [ 2.99388123]</div><div class="line">60 [ 1.00277638] [ 2.99850869]</div><div class="line">80 [ 1.00067675] [ 2.99963641]</div><div class="line">100 [ 1.00016499] [ 2.99991131]</div><div class="line">120 [ 1.00004005] [ 2.99997854]</div><div class="line">140 [ 1.00000978] [ 2.99999475]</div><div class="line">160 [ 1.0000025] [ 2.99999857]</div><div class="line">180 [ 1.00000119] [ 2.99999928]</div><div class="line">200 [ 1.00000119] [ 2.99999928]</div></pre></td></tr></table></figure></li></ul><h3 id="3、Session会话控制"><a href="#3、Session会话控制" class="headerlink" title="3、Session会话控制"></a>3、Session会话控制</h3><ul><li>运行 <code>session.run()</code> 可以获得你要得知的运算结果, 或者是你所要运算的部分</li><li>定义常量矩阵：<code>tf.constant([[3,3]])</code></li><li>矩阵乘法 ：<code>tf.matmul(matrix1,matrix2)</code></li><li><p>运行Session的两种方法：</p><ul><li><p>手动关闭</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">sess = tf.Session()</div><div class="line">print sess.run(product)</div><div class="line">sess.close()</div></pre></td></tr></table></figure></li><li><p>使用<code>with</code>，执行完会自动关闭</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">with tf.Session() as sess:</div><div class="line">print sess.run(product)</div></pre></td></tr></table></figure></li></ul></li></ul><h3 id="4、Variable变量"><a href="#4、Variable变量" class="headerlink" title="4、Variable变量"></a>4、<code>Variable</code>变量</h3><ul><li>定义变量：<code>tf.Variable()</code></li><li>初始化所有变量：<code>init = tf.initialize_all_variables()</code> </li><li>需要再在 sess 里, <code>sess.run(init)</code> , 激活变量</li><li>输出时，一定要把 sess 的指针指向变量再进行 <code>print</code> 才能得到想要的结果</li></ul><h3 id="5、Placeholder传入值"><a href="#5、Placeholder传入值" class="headerlink" title="5、Placeholder传入值"></a>5、<code>Placeholder</code>传入值</h3><ul><li>首先定义<code>Placeholder</code>，然后在<code>Session.run()</code>的时候输入值</li><li><code>placeholder</code> 与 <code>feed_dict={}</code> 是绑定在一起出现的<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">input1 = tf.placeholder(tf.float32) #在 Tensorflow 中需要定义 placeholder 的 type ，一般为 float32 形式</div><div class="line">input2 = tf.placeholder(tf.float32)</div><div class="line"></div><div class="line">output = tf.mul(input1,input2)  # 乘法运算</div><div class="line"></div><div class="line">with tf.Session() as sess:</div><div class="line">    print sess.run(output,feed_dict=&#123;input1:7.,input2:2.&#125;) # placeholder 与 feed_dict=&#123;&#125; 是绑定在一起出现的</div></pre></td></tr></table></figure></li></ul><h2 id="三、定义一个神经网络"><a href="#三、定义一个神经网络" class="headerlink" title="三、定义一个神经网络"></a>三、定义一个神经网络</h2><h3 id="1、添加层函数add-layer"><a href="#1、添加层函数add-layer" class="headerlink" title="1、添加层函数add_layer()"></a>1、添加层函数<code>add_layer()</code></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">&apos;&apos;&apos;参数：输入数据，前一层size，当前层size，激活函数&apos;&apos;&apos;</div><div class="line">def add_layer(inputs,in_size,out_size,activation_function=None):</div><div class="line">    Weights = tf.Variable(tf.random_normal([in_size,out_size]))  #随机初始化权重</div><div class="line">    biases = tf.Variable(tf.zeros([1,out_size]) + 0.1)  # 初始化偏置，+0.1</div><div class="line">    Ws_plus_b = tf.matmul(inputs,Weights) + biases      # 未使用激活函数的值</div><div class="line">    if activation_function is None:</div><div class="line">        outputs = Ws_plus_b</div><div class="line">    else:</div><div class="line">        outputs = activation_function(Ws_plus_b)   # 使用激活函数激活</div><div class="line">    return outputs</div></pre></td></tr></table></figure><h3 id="2、构建神经网络"><a href="#2、构建神经网络" class="headerlink" title="2、构建神经网络"></a>2、构建神经网络</h3><ul><li><p>定义二次函数</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">x_data = np.linspace(-1,1,300,dtype=np.float32)[:,np.newaxis]</div><div class="line">noise = np.random.normal(0,0.05,x_data.shape).astype(np.float32)</div><div class="line">y_data = np.square(x_data)-0.5+noise</div></pre></td></tr></table></figure></li><li><p>定义<code>Placeholder</code>,用于后期输入数据</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">xs = tf.placeholder(tf.float32,[None,1]) # None代表无论输入有多少都可以,只有一个特征，所以这里是1</div><div class="line">ys = tf.placeholder(tf.float32,[None,1])</div></pre></td></tr></table></figure></li><li><p>定义神经层<code>layer</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">layer1 = add_layer(xs, 1, 10, activation_function=tf.nn.relu) # 第一层，输入层为1，隐含层为10个神经元，Tensorflow 自带的激励函数tf.nn.relu</div></pre></td></tr></table></figure></li><li><p>定义输出层</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">prediction = add_layer(layer1, 10, 1) # 利用上一层作为输入</div></pre></td></tr></table></figure></li><li><p>计算<code>loss</code>损失</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">loss = tf.reduce_mean(tf.reduce_sum(tf.square(ys-prediction),reduction_indices=[1])) # 对二者差的平方求和再取平均</div></pre></td></tr></table></figure></li><li><p>梯度下降最小化损失</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">train = tf.train.GradientDescentOptimizer(0.1).minimize(loss)</div></pre></td></tr></table></figure></li><li><p>初始化所有变量</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">init = tf.initialize_all_variables()</div></pre></td></tr></table></figure></li><li><p>定义Session</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">sess = tf.Session()</div><div class="line">sess.run(init)</div></pre></td></tr></table></figure></li><li><p>输出</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">for i in range(1000):</div><div class="line">    sess.run(train,feed_dict=&#123;xs:x_data,ys:y_data&#125;)</div><div class="line">    if i%50==0:</div><div class="line">        print sess.run(loss,feed_dict=&#123;xs:x_data,ys:y_data&#125;)</div></pre></td></tr></table></figure></li></ul><p>结果：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line">0.45402</div><div class="line">0.0145364</div><div class="line">0.00721318</div><div class="line">0.0064215</div><div class="line">0.00614493</div><div class="line">0.00599307</div><div class="line">0.00587578</div><div class="line">0.00577039</div><div class="line">0.00567172</div><div class="line">0.00558008</div><div class="line">0.00549546</div><div class="line">0.00541595</div><div class="line">0.00534059</div><div class="line">0.00526139</div><div class="line">0.00518873</div><div class="line">0.00511403</div><div class="line">0.00504063</div><div class="line">0.0049613</div><div class="line">0.0048874</div><div class="line">0.004819</div></pre></td></tr></table></figure></p><h3 id="3、可视化结果"><a href="#3、可视化结果" class="headerlink" title="3、可视化结果"></a>3、可视化结果</h3><ul><li>显示数据<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">fig = plt.figure()</div><div class="line">ax = fig.add_subplot(111)</div><div class="line">ax.scatter(x_data,y_data)</div><div class="line">plt.ion()   # 绘画之后不暂停</div><div class="line">plt.show()</div></pre></td></tr></table></figure></li></ul><p><img src="https://raw.githubusercontent.com/lawlite19/MachineLearning_TensorFlow/master/images/example_01.png" alt="enter description here" title="example_01.png">  </p><ul><li>动态绘画<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">        try:</div><div class="line">            ax.lines.remove(lines[0])   # 每次绘画需要移除上次绘画的结果，放在try catch里因为第一次执行没有，所以直接pass</div><div class="line">        except Exception:</div><div class="line">            pass</div><div class="line">        prediction_value = sess.run(prediction, feed_dict=&#123;xs: x_data&#125;)</div><div class="line">        # plot the prediction</div><div class="line">        lines = ax.plot(x_data, prediction_value, &apos;r-&apos;, lw=3)  # 绘画</div><div class="line">        plt.pause(0.1)  # 停0.1s</div><div class="line">```    </div><div class="line">![enter description here][3]</div><div class="line"></div><div class="line">## 四、TensorFlow可视化</div><div class="line"></div><div class="line">### 1、TensorFlow的可视化工具`tensorboard`，可视化神经网路额结构</div><div class="line">- 输入`input`</div></pre></td></tr></table></figure></li></ul><p>with tf.name_scope(‘input’):<br>    xs = tf.placeholder(tf.float32,[None,1],name=’x_in’)  #<br>    ys = tf.placeholder(tf.float32,[None,1],name=’y_in’)<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">![enter description here][4]</div><div class="line"></div><div class="line">- `layer`层</div></pre></td></tr></table></figure></p><p>def add_layer(inputs,in_size,out_size,activation_function=None):<br>    with tf.name_scope(‘layer’):<br>        with tf.name_scope(‘Weights’):<br>            Weights = tf.Variable(tf.random_normal([in_size,out_size]),name=’W’)<br>        with tf.name_scope(‘biases’):<br>            biases = tf.Variable(tf.zeros([1,out_size]) + 0.1,name=’b’)<br>        with tf.name_scope(‘Ws_plus_b’):<br>            Ws_plus_b = tf.matmul(inputs,Weights) + biases<br>        if activation_function is None:                                       outputs = Ws_plus_b<br>        else:<br>            outputs = activation_function(Ws_plus_b)<br>        return outputs<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">![enter description here][5]</div><div class="line"></div><div class="line">- `loss`和`train`</div></pre></td></tr></table></figure></p><p>with tf.name_scope(‘loss’):<br>    loss = tf.reduce_mean(tf.reduce_sum(tf.square(ys-prediction),reduction_indices=<a href="https://raw.githubusercontent.com/lawlite19/MachineLearning_TensorFlow/master/images/tensors_flowing.gif" title="tensors_flowing.gif" target="_blank" rel="external">1</a>))</p><p>with tf.name_scope(‘train’):<br>    train = tf.train.GradientDescentOptimizer(0.1).minimize(loss)<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">![enter description here][6]</div><div class="line"></div><div class="line">- 写入文件中</div></pre></td></tr></table></figure></p><p>writer = tf.train.SummaryWriter(“logs/“, sess.graph)<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">- 浏览器中查看（chrome浏览器）</div><div class="line"> - 在终端输入：`tensorboard --logdir=&apos;logs/&apos;`，它会给出访问地址</div><div class="line"> - 浏览器中查看即可。</div><div class="line"> - `tensorboard`命令在安装**python**目录的**bin**目录下，可以创建一个软连接</div><div class="line"></div><div class="line">### 2、可视化训练过程</div><div class="line">- 可视化Weights权重和biases偏置</div><div class="line"> - 每一层起个名字</div></pre></td></tr></table></figure></p><p> layer_name = ‘layer%s’%n_layer<br> <figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">- tf.histogram_summary(name,value)</div></pre></td></tr></table></figure></p><p> def add_layer(inputs,in_size,out_size,n_layer,activation_function=None):<br>    layer_name = ‘layer%s’%n_layer<br>    with tf.name_scope(layer_name):<br>        with tf.name_scope(‘Weights’):<br>            Weights = tf.Variable(tf.random_normal([in_size,out_size]),name=’W’)<br>            tf.histogram_summary(layer_name+’/weights’, Weights)<br>        with tf.name_scope(‘biases’):<br>            biases = tf.Variable(tf.zeros([1,out_size]) + 0.1,name=’b’)<br>            tf.histogram_summary(layer_name+’/biases’,biases)<br>        with tf.name_scope(‘Ws_plus_b’):<br>            Ws_plus_b = tf.matmul(inputs,Weights) + biases</p><pre><code>if activation_function is None:                 outputs = Ws_plus_b else:                                                             outputs = activation_function(Ws_plus_b)      tf.histogram_summary(layer_name+&apos;/outputs&apos;,outputs)return outputs</code></pre> <figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">- merge所有的summary</div></pre></td></tr></table></figure><p> merged =tf.merge_all_summaries()<br> <figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">- 写入文件中</div></pre></td></tr></table></figure></p><p> writer = tf.train.SummaryWriter(“logs/“, sess.graph)<br> <figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">- 训练1000次，每50步显示一次：</div></pre></td></tr></table></figure></p><p> for i in range(1000):<br>    sess.run(train,feed_dict={xs:x_data,ys:y_data})<br>    if i%50==0:<br>        summary = sess.run(merged, feed_dict={xs: x_data, ys:y_data})<br>        writer.add_summary(summary, i)<br> <figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"> - 同样适用`tensorboard`查看   </div><div class="line"> ![enter description here][7]</div><div class="line"> </div><div class="line">- 可视化损失函数（代价函数）</div><div class="line"> - 添加：`tf.scalar_summary(&apos;loss&apos;,loss)`    </div><div class="line"> ![enter description here][8]</div><div class="line"></div><div class="line">## 五、手写数字识别_1</div><div class="line">### 1、说明</div><div class="line">- [全部代码](https://github.com/lawlite19/MachineLearning_TensorFlow/blob/master/Mnist_01/mnist.py)：`https://github.com/lawlite19/MachineLearning_TensorFlow/blob/master/Mnist_02/mnist.py`</div><div class="line">- 自己的数据集，没有使用tensorflow中mnist数据集，</div><div class="line">- 之前在机器学习中用Python实现过，地址：`https://github.com/lawlite19/MachineLearning_Python`,这里使用`tensorflow`实现</div><div class="line">- 神经网络只有两层</div><div class="line"></div><div class="line">### 2、代码实现</div><div class="line">- 添加一层</div></pre></td></tr></table></figure></p><p>‘’’添加一层神经网络’’’<br>def add_layer(inputs,in_size,out_size,activation_function=None):<br>    Weights = tf.Variable(tf.random_normal([in_size,out_size]))    # 权重，in*out<br>    biases = tf.Variable(tf.zeros([1,out_size]) + 0.1)<br>    Ws_plus_b = tf.matmul(inputs,Weights) + biases   # 计算权重和偏置之后的值<br>    if activation_function is None:<br>        outputs = Ws_plus_b<br>    else:<br>        outputs = activation_function(Ws_plus_b)    # 调用激励函数运算<br>    return outputs<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">- 运行函数</div></pre></td></tr></table></figure></p><p>‘’’运行函数’’’<br>def NeuralNetwork():<br>    data_digits = spio.loadmat(‘data_digits.mat’)<br>    X = data_digits[‘X’]<br>    y = data_digits[‘y’]<br>    m,n = X.shape<br>    class_y = np.zeros((m,10))      # y是0,1,2,3…9,需要映射0/1形式<br>    for i in range(10):<br>        class_y[:,i] = np.float32(y==i).reshape(1,-1) </p><pre><code>xs = tf.placeholder(tf.float32, shape=[None,400])  # 像素是20x20=400，所以有400个featureys = tf.placeholder(tf.float32, shape=[None,10])   # 输出有10个prediction = add_layer(xs, 400, 10, activation_function=tf.nn.softmax) # 两层神经网络，400x10#prediction = add_layer(layer1, 25, 10, activation_function=tf.nn.softmax)#loss = tf.reduce_mean(tf.reduce_sum(tf.square(ys-prediction),reduction_indices=[1]))loss = tf.reduce_mean(-tf.reduce_sum(ys*tf.log(prediction),reduction_indices=[1]))  # 定义损失函数（代价函数），train = tf.train.GradientDescentOptimizer(learning_rate=0.5).minimize(loss)     # 使用梯度下降最小化损失init = tf.initialize_all_variables()   # 初始化所有变量sess = tf.Session()  # 创建Sessionsess.run(init)for i in range(4000): # 迭代训练4000次    sess.run(train, feed_dict={xs:X,ys:class_y})  # 训练train，填入数据    if i%50==0:  # 每50次输出当前的准确度        print(compute_accuracy(xs,ys,X,class_y,sess,prediction))</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">- 计算准确度</div></pre></td></tr></table></figure><p>‘’’计算预测准确度’’’<br>def compute_accuracy(xs,ys,X,y,sess,prediction):<br>    y_pre = sess.run(prediction,feed_dict={xs:X})<br>    correct_prediction = tf.equal(tf.argmax(y_pre,1),tf.argmax(y,1))  #tf.argmax 给出某个tensor对象在某一维上的其数据最大值所在的索引值,即为对应的数字，tf.equal 来检测我们的预测是否真实标签匹配<br>    accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32)) # 平均值即为准确度<br>    result = sess.run(accuracy,feed_dict={xs:X,ys:y})<br>    return result<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">- 输出每一次预测的结果准确度    </div><div class="line">![enter description here][9]</div><div class="line"></div><div class="line">## 六、手写数字识别_2</div><div class="line">### 1、说明</div><div class="line">- [全部代码](https://github.com/lawlite19/MachineLearning_TensorFlow/blob/master/Mnist_02/mnist.py)：`https://github.com/lawlite19/MachineLearning_TensorFlow/blob/master/Mnist_02/mnist.py`</div><div class="line">- 采用TensorFlow中的mnist数据集（可以取网站下载它的数据集，http://yann.lecun.com/exdb/mnist/）</div><div class="line">- 实现代码与上面类似，它有专门的测试集</div><div class="line"></div><div class="line">### 2、代码</div><div class="line">- 随机梯度下降`SGD`,每次选出`100`个数据进行训练</div></pre></td></tr></table></figure></p><p>for i in range(2000):<br>        batch_xs, batch_ys = minist.train.next_batch(100)<br>        sess.run(train_step,feed_dict={xs:batch_xs,ys:batch_ys})<br>        if i%50==0:<br>            print(compute_accuracy(xs,ys,minist.test.images, minist.test.labels,sess,prediction))</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">- 输出每一次预测的结果准确度     </div><div class="line">![enter description here][10]</div><div class="line"></div><div class="line">## 七、手写数字识别_3_CNN卷积神经网络</div><div class="line">### 1、说明</div><div class="line">- 关于**卷积神经网络CNN**可以查看[我的博客](http://blog.csdn.net/u013082989/article/details/53673602)：http://blog.csdn.net/u013082989/article/details/53673602</div><div class="line"> - 或者[github](https://github.com/lawlite19/DeepLearning_Python)：https://github.com/lawlite19/DeepLearning_Python</div><div class="line">- [全部代码](https://github.com/lawlite19/MachineLearning_TensorFlow/blob/master/Mnist_03_CNN/mnist_cnn.py)：`https://github.com/lawlite19/MachineLearning_TensorFlow/blob/master/Mnist_03_CNN/mnist_cnn.py`</div><div class="line">- 采用TensorFlow中的mnist数据集（可以取网站下载它的数据集，http://yann.lecun.com/exdb/mnist/）</div><div class="line"></div><div class="line">### 2、代码实现</div><div class="line">- 权重和偏置初始化函数</div><div class="line"> - 权重使用的`truncated_normal`进行初始化,`stddev`标准差定义为0.1</div><div class="line"> - 偏置初始化为常量0.1</div></pre></td></tr></table></figure><p>‘’’权重初始化函数’’’<br>def weight_variable(shape):<br>    inital = tf.truncated_normal(shape, stddev=0.1)  # 使用truncated_normal进行初始化<br>    return tf.Variable(inital)</p><p>‘’’偏置初始化函数’’’<br>def bias_variable(shape):<br>    inital = tf.constant(0.1,shape=shape)  # 偏置定义为常量<br>    return tf.Variable(inital)<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">- 卷积函数</div><div class="line"> - `strides[0]`和`strides[3]`的两个1是默认值，中间两个1代表padding时在x方向运动1步，y方向运动1步</div><div class="line"> - `padding=&apos;SAME&apos;`代表经过卷积之后的输出图像和原图像大小一样</div></pre></td></tr></table></figure></p><p>‘’’卷积函数’’’<br>def conv2d(x,W):#x是图片的所有参数，W是此卷积层的权重<br>    return tf.nn.conv2d(x,W,strides=[1,1,1,1],padding=’SAME’)#strides[0]和strides<a href="https://raw.githubusercontent.com/lawlite19/MachineLearning_TensorFlow/master/images/example_02.gif" title="example_02.gif" target="_blank" rel="external">3</a>的两个1是默认值，中间两个1代表padding时在x方向运动1步，y方向运动1步<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"></div><div class="line"></div><div class="line">- 池化函数</div><div class="line"> - `ksize`指定池化核函数的大小</div><div class="line"> - 根据池化核函数的大小定义`strides`的大小</div></pre></td></tr></table></figure></p><p>‘’’池化函数’’’<br>def max_pool_2x2(x):<br>    return tf.nn.max_pool(x,ksize=[1,2,2,1],<br>                          strides=[1,2,2,1],                          padding=’SAME’)#池化的核函数大小为2x2，因此ksize=[1,2,2,1]，步长为2，因此strides=[1,2,2,1]<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">- 加载`mnist`数据和定义`placeholder`</div><div class="line"> - 输入数据`x_image`最后一个`1`代表`channel`的数量,若是`RGB`3个颜色通道就定义为3</div><div class="line"> - `keep_prob` 用于**dropout**防止过拟合</div></pre></td></tr></table></figure></p><pre><code>mnist = input_data.read_data_sets(&apos;MNIST_data&apos;, one_hot=True)  # 下载数据xs = tf.placeholder(tf.float32,[None,784])  # 输入图片的大小，28x28=784ys = tf.placeholder(tf.float32,[None,10])   # 输出0-9共10个数字keep_prob = tf.placeholder(tf.float32)      # 用于接收dropout操作的值，dropout为了防止过拟合x_image = tf.reshape(xs,[-1,28,28,1])       #-1代表先不考虑输入的图片例子多少这个维度，后面的1是channel的数量，因为我们输入的图片是黑白的，因此channel是1，例如如果是RGB图像，那么channel就是3</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">- 第一层卷积和池化</div><div class="line">  - 使用**ReLu**激活函数</div></pre></td></tr></table></figure><pre><code>&apos;&apos;&apos;第一层卷积，池化&apos;&apos;&apos;W_conv1 = weight_variable([5,5,1,32])  # 卷积核定义为5x5,1是输入的通道数目，32是输出的通道数目b_conv1 = bias_variable([32])          # 每个输出通道对应一个偏置h_conv1 = tf.nn.relu(conv2d(x_image,W_conv1)+b_conv1) # 卷积运算，并使用ReLu激活函数激活h_pool1 = max_pool_2x2(h_conv1)        # pooling操作 </code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"></div><div class="line"></div><div class="line">- 第二层卷积和池化</div></pre></td></tr></table></figure><pre><code>&apos;&apos;&apos;第二层卷积，池化&apos;&apos;&apos;W_conv2 = weight_variable([5,5,32,64]) # 卷积核还是5x5,32个输入通道，64个输出通道b_conv2 = bias_variable([64])          # 与输出通道一致h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2)+b_conv2)h_pool2 = max_pool_2x2(h_conv2)</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"></div><div class="line"></div><div class="line">- 全连接第一层</div></pre></td></tr></table></figure><pre><code>&apos;&apos;&apos;全连接层&apos;&apos;&apos;h_pool2_flat = tf.reshape(h_pool2, [-1,7*7*64])   # 将最后操作的数据展开W_fc1 = weight_variable([7*7*64,1024])            # 下面就是定义一般神经网络的操作了，继续扩大为1024b_fc1 = bias_variable([1024])                     # 对应的偏置h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat,W_fc1)+b_fc1)  # 运算、激活（这里不是卷积运算了，就是对应相乘）</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"></div><div class="line"></div><div class="line">- `dropout`防止过拟合</div></pre></td></tr></table></figure><pre><code>&apos;&apos;&apos;dropout&apos;&apos;&apos;h_fc1_drop = tf.nn.dropout(h_fc1,keep_prob)       # dropout操作</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"></div><div class="line"></div><div class="line">- 最后一层全连接预测,使用梯度下降优化**交叉熵损失函数**</div><div class="line"> - 使用**softmax**分类器分类</div></pre></td></tr></table></figure><pre><code>&apos;&apos;&apos;最后一层全连接&apos;&apos;&apos;W_fc2 = weight_variable([1024,10])                # 最后一层权重初始化b_fc2 = bias_variable([10])                       # 对应偏置prediction = tf.nn.softmax(tf.matmul(h_fc1_drop,W_fc2)+b_fc2)  # 使用softmax分类器cross_entropy = tf.reduce_mean(-tf.reduce_sum(ys*tf.log(prediction),reduction_indices=[1]))  # 交叉熵损失函数来定义cost functiontrain_step = tf.train.AdamOptimizer(1e-3).minimize(cross_entropy)  # 调用梯度下降</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"></div><div class="line"></div><div class="line">- 定义Session，使用`SGD`训练</div></pre></td></tr></table></figure><pre><code>&apos;&apos;&apos;下面就是tf的一般操作，定义Session，初始化所有变量，placeholder传入值训练&apos;&apos;&apos;sess = tf.Session()sess.run(tf.initialize_all_variables())for i in range(1000):    batch_xs, batch_ys = mnist.train.next_batch(100)  # 使用SGD，每次选取100个数据训练    sess.run(train_step, feed_dict={xs: batch_xs, ys: batch_ys, keep_prob: 0.5})  # dropout值定义为0.5    if i % 50 == 0:        print compute_accuracy(xs,ys,mnist.test.images, mnist.test.labels,keep_prob,sess,prediction)  # 每50次输出一下准确度</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">- 计算准确度函数</div><div class="line">  - 和上面的两个计算准确度的函数一致，就是多了个**dropout**的参数`keep_prob`</div></pre></td></tr></table></figure><p>‘’’计算准确度函数’’’<br>def compute_accuracy(xs,ys,X,y,keep_prob,sess,prediction):<br>    y_pre = sess.run(prediction,feed_dict={xs:X,keep_prob:1.0})       # 预测，这里的keep_prob是dropout时用的，防止过拟合<br>    correct_prediction = tf.equal(tf.argmax(y_pre,1),tf.argmax(y,1))  #tf.argmax 给出某个tensor对象在某一维上的其数据最大值所在的索引值,即为对应的数字，tf.equal 来检测我们的预测是否真实标签匹配<br>    accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32)) # 平均值即为准确度<br>    result = sess.run(accuracy,feed_dict={xs:X,ys:y,keep_prob:1.0})<br>    return result<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">### 3、运行结果</div><div class="line">- 测试集上准确度   </div><div class="line">![enter description here][11]   </div><div class="line">- 使用`top`命令查看占用的CPU和内存，还是很消耗CPU和内存的，所以上面只输出了四次我就终止了</div><div class="line">![enter description here][12]   </div><div class="line">- 由于我在虚拟机里运行的`TensorFlow`程序，分配了`5G`的内存，若是内存不够会报一个错误。</div><div class="line"></div><div class="line">-------------------------------------------------------------</div><div class="line"></div><div class="line">## 八、保存和提取神经网络</div><div class="line">### 1、保存</div><div class="line">- 定义要保存的数据</div></pre></td></tr></table></figure></p><p>W = tf.Variable(initial_value=[[1,2,3],[3,4,5]],<br>               name=’weights’, dtype=tf.float32)   # 注意需要指定name和dtype<br>b = tf.Variable(initial_value=[1,2,3],<br>               name=’biases’, dtype=tf.float32)<br>init = tf.initialize_all_variables()<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">- 保存</div></pre></td></tr></table></figure></p><p>saver = tf.train.Saver()<br>with tf.Session() as sess:<br>    sess.run(init)<br>    save_path = saver.save(sess, ‘my_network/save_net.ckpt’) # 保存目录，注意要在当前项目下建立my_network的目录<br>    print (‘保存到 :’,save_path)<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">### 2、提取</div><div class="line">- 定义数据</div></pre></td></tr></table></figure></p><p>W = tf.Variable(np.arange(6).reshape((2,3)),<br>               name=’weights’, dtype=tf.float32) # 注意与之前保存的一致<br>b = tf.Variable(np.arange((3)),<br>               name=’biases’, dtype=tf.float32)<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">- `restore`提取</div></pre></td></tr></table></figure></p><p>saver = tf.train.Saver()<br>with tf.Session() as sess:<br>    saver.restore(sess,’my_network/save_net.ckpt’)<br>    print(‘weights:’,sess.run(W))  # 输出一下结果<br>    print(‘biases:’,sess.run(b))<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">-------------------------------------------------</div><div class="line"></div><div class="line">- 以下来自`tensorflow-turorial`，使用`python3.5`</div><div class="line"></div><div class="line"></div><div class="line">## 九、线性模型Linear Model</div><div class="line">- [全部代码][13]</div><div class="line">- 使用`MNIST`数据集</div><div class="line"></div><div class="line">### 1、加载MNIST数据集，并输出信息</div><div class="line"></div><div class="line">``` stylus</div><div class="line">&apos;&apos;&apos;Load MNIST data and print some information&apos;&apos;&apos;</div><div class="line">data = input_data.read_data_sets(&quot;MNIST_data&quot;, one_hot = True)</div><div class="line">print(&quot;Size of:&quot;)</div><div class="line">print(&quot;\t training-set:\t\t&#123;&#125;&quot;.format(len(data.train.labels)))</div><div class="line">print(&quot;\t test-set:\t\t\t&#123;&#125;&quot;.format(len(data.test.labels)))</div><div class="line">print(&quot;\t validation-set:\t&#123;&#125;&quot;.format(len(data.validation.labels)))</div><div class="line">print(data.test.labels[0:5])</div><div class="line">data.test.cls = np.array([label.argmax() for label in data.test.labels])   # get the actual value</div><div class="line">print(data.test.cls[0:5])</div></pre></td></tr></table></figure></p><h3 id="2、绘制9张图像"><a href="#2、绘制9张图像" class="headerlink" title="2、绘制9张图像"></a>2、绘制9张图像</h3><ul><li><p>实现函数</p><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line">'''define a funciton to plot 9 images'''</div><div class="line">def plot_images(images, cls_true, cls_pred = None):</div><div class="line">    '''</div><div class="line">    @parameter images:   the images info</div><div class="line">    @parameter cls_true: the true value of image</div><div class="line">    @parameter cls_pred: the prediction value, default is None</div><div class="line">    '''</div><div class="line">    assert len(images) == len(cls_true) == 9  # only show 9 images</div><div class="line">    fig, axes = plt.subplots(nrows=3, ncols=3)</div><div class="line">    for i, ax in enumerate(axes.flat):</div><div class="line">        ax.imshow(images[i].reshape(img_shape), cmap="binary")  # binary means black_white image</div><div class="line">        # show the true and pred values</div><div class="line">        if cls_pred is None:</div><div class="line">            xlabel = "True: &#123;0&#125;".format(cls_true[i])</div><div class="line">        else:</div><div class="line">            xlabel = "True: &#123;0&#125;,Pred: &#123;1&#125;".format(cls_true[i],cls_pred[i])</div><div class="line">        ax.set_xlabel(xlabel)</div><div class="line">        ax.set_xticks([])  # remove the ticks</div><div class="line">        ax.set_yticks([])</div><div class="line">    plt.show()</div></pre></td></tr></table></figure></li><li><p>选择测试集中的9张图显示</p></li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">'''show 9 images'''</div><div class="line">images = data.test.images[0:9]</div><div class="line">cls_true = data.test.cls[0:9]</div><div class="line">plot_images(images, cls_true)</div><div class="line">```                   </div><div class="line">![enter description here][14]</div><div class="line"></div><div class="line">### 3、定义要训练的模型</div><div class="line">- 定义`placeholder`</div><div class="line">``` stylus</div><div class="line">'''define the placeholder'''</div><div class="line">X = tf.placeholder(tf.float32, [None, img_size_flat])    # None means the arbitrary number of labels, the features size is img_size_flat </div><div class="line">y_true = tf.placeholder(tf.float32, [None, num_classes]) # output size is num_classes</div><div class="line">y_true_cls = tf.placeholder(tf.int64, [None])</div></pre></td></tr></table></figure><ul><li>定义<code>weights</code>和<code>biases</code></li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">'''define weights and biases'''</div><div class="line">weights = tf.Variable(tf.zeros([img_size_flat, num_classes]))  # img_size_flat*num_classes</div><div class="line">biases = tf.Variable(tf.zeros([num_classes]))</div></pre></td></tr></table></figure><ul><li>定义模型</li></ul><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="string">''</span><span class="string">'define the model'</span><span class="string">''</span></div><div class="line">logits = tf.matmul(X,weights) + biases </div><div class="line">y_pred = tf<span class="selector-class">.nn</span><span class="selector-class">.softmax</span>(logits)</div><div class="line">y_pred_cls = tf.argmax(y_pred, dimension=<span class="number">1</span>)</div><div class="line">cross_entropy = tf<span class="selector-class">.nn</span><span class="selector-class">.softmax_cross_entropy_with_logits</span>(labels=y_true, </div><div class="line">                                                       logits=logits)</div><div class="line">cost = tf.reduce_mean(cross_entropy)</div><div class="line"><span class="string">''</span><span class="string">'define the optimizer'</span><span class="string">''</span></div><div class="line">optimizer = tf<span class="selector-class">.train</span><span class="selector-class">.GradientDescentOptimizer</span>(learning_rate=<span class="number">0.5</span>).minimize(cost)</div></pre></td></tr></table></figure><ul><li>定义求准确度</li></ul><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="string">''</span><span class="string">'define the accuracy'</span><span class="string">''</span></div><div class="line">correct_prediction = tf.equal(y_pred_cls, y_true_cls)</div><div class="line">accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</div></pre></td></tr></table></figure><ul><li>定义<code>session</code></li></ul><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="string">''</span><span class="string">'run the datagraph and use batch gradient descent'</span><span class="string">''</span></div><div class="line">session = tf.Session()</div><div class="line">session.run(tf.global_variables_initializer())</div><div class="line">batch_size = <span class="number">100</span></div></pre></td></tr></table></figure><h3 id="4、定义函数optimize进行bgd训练"><a href="#4、定义函数optimize进行bgd训练" class="headerlink" title="4、定义函数optimize进行bgd训练"></a>4、定义函数<code>optimize</code>进行bgd训练</h3><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">'''define a function to run the optimizer'''</div><div class="line">def optimize(num_iterations):</div><div class="line">    '''</div><div class="line">    @parameter num_iterations: the traning times</div><div class="line">    '''</div><div class="line">    for i in range(num_iterations):</div><div class="line">        x_batch, y_true_batch = data.train.next_batch(batch_size)</div><div class="line">        feed_dict_train = &#123;X: x_batch,y_true: y_true_batch&#125;</div><div class="line">        session.run(optimizer, feed_dict=feed_dict_train)</div></pre></td></tr></table></figure><h3 id="5、定义输出准确度的函数"><a href="#5、定义输出准确度的函数" class="headerlink" title="5、定义输出准确度的函数"></a>5、定义输出准确度的函数</h3><ul><li><p>代码</p><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">feed_dict_test = &#123;X: data.test.images, </div><div class="line">                  y_true: data.test.labels, </div><div class="line">                  y_true_cls: data.test.cls&#125;        </div><div class="line">'''define a function to print the accuracy'''    </div><div class="line">def print_accuracy():</div><div class="line">    acc = session.run(accuracy, feed_dict=feed_dict_test)</div><div class="line">    print("Accuracy on test-set:&#123;0:.1%&#125;".format(acc))</div></pre></td></tr></table></figure></li><li><p>输出：<code>Accuracy on test-set:89.4%</code></p></li></ul><h3 id="6、定义绘制错误预测的图片函数"><a href="#6、定义绘制错误预测的图片函数" class="headerlink" title="6、定义绘制错误预测的图片函数"></a>6、定义绘制错误预测的图片函数</h3><ul><li><p>代码</p><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">'''define a function to plot the error prediciton'''    </div><div class="line">def plot_example_errors():</div><div class="line">    correct, cls_pred = session.run([correct_prediction, y_pred_cls], feed_dict=feed_dict_test) </div><div class="line">    incorrect = (correct == False)</div><div class="line">    images = data.test.images[incorrect]  # get the prediction error images</div><div class="line">    cls_pred = cls_pred[incorrect]        # get prediction value</div><div class="line">    cls_true = data.test.cls[incorrect]   # get true value</div><div class="line">    plot_images(images[0:9], cls_true[0:9], cls_pred[0:9])</div></pre></td></tr></table></figure></li><li><p>输出：<br><img src="https://raw.githubusercontent.com/lawlite19/MachineLearning_TensorFlow/master/images/LinearModel_02.png" alt="enter description here" title="LinearModel_02"></p><h3 id="7、定义可视化权重的函数"><a href="#7、定义可视化权重的函数" class="headerlink" title="7、定义可视化权重的函数"></a>7、定义可视化权重的函数</h3></li><li><p>代码</p><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">'''define a fucntion to plot weights'''</div><div class="line">def plot_weights():</div><div class="line">    w = session.run(weights)</div><div class="line">    w_min = np.min(w)</div><div class="line">    w_max = np.max(w)</div><div class="line">    fig, axes = plt.subplots(3, 4)</div><div class="line">    fig.subplots_adjust(0.3, 0.3)</div><div class="line">    for i, ax in enumerate(axes.flat):</div><div class="line">        if i&lt;10:</div><div class="line">            image = w[:,i].reshape(img_shape)</div><div class="line">            ax.set_xlabel("Weights: &#123;0&#125;".format(i))</div><div class="line">            ax.imshow(image, vmin=w_min,vmax=w_max,cmap="seismic")</div><div class="line">        ax.set_xticks([])</div><div class="line">        ax.set_yticks([])</div><div class="line">    plt.show()</div></pre></td></tr></table></figure></li><li><p>输出：<br><img src="https://raw.githubusercontent.com/lawlite19/MachineLearning_TensorFlow/master/images/LinearModel_03.png" alt="enter description here" title="LinearModel_03"></p><h3 id="8、定义输出confusion-matrix的函数"><a href="#8、定义输出confusion-matrix的函数" class="headerlink" title="8、定义输出confusion_matrix的函数"></a>8、定义输出<code>confusion_matrix</code>的函数</h3></li><li><p>代码：</p><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">'''define a function to printand plot the confusion matrix using scikit-learn.'''   </div><div class="line">def print_confusion_martix():</div><div class="line">    cls_true = data.test.cls  # test set actual value </div><div class="line">    cls_pred = session.run(y_pred_cls, feed_dict=feed_dict_test)  # test set predict value</div><div class="line">    cm = confusion_matrix(y_true=cls_true,y_pred=cls_pred)        # use sklearn confusion_matrix</div><div class="line">    print(cm)</div><div class="line">    plt.imshow(cm, interpolation='nearest',cmap=plt.cm.Blues) # Plot the confusion matrix as an image.</div><div class="line">    plt.tight_layout()</div><div class="line">    plt.colorbar()</div><div class="line">    tick_marks = np.arange(num_classes)</div><div class="line">    tick_marks = np.arange(num_classes)</div><div class="line">    plt.xticks(tick_marks, range(num_classes))</div><div class="line">    plt.yticks(tick_marks, range(num_classes))</div><div class="line">    plt.xlabel('Predicted')</div><div class="line">    plt.ylabel('True')    </div><div class="line">    plt.show()</div></pre></td></tr></table></figure></li><li><p>输出：<br><img src="https://raw.githubusercontent.com/lawlite19/MachineLearning_TensorFlow/master/images/LinearModel_04.png" alt="enter description here" title="LinearModel_04"></p></li></ul><h2 id="十：CNN"><a href="#十：CNN" class="headerlink" title="十：CNN"></a>十：CNN</h2><ul><li><a href="https://github.com/lawlite19/MachineLearning_TensorFlow/blob/master/CNNModel/CNN_Model.py" target="_blank" rel="external">全部代码</a></li><li>使用<code>MNIST</code>数据集</li><li>加载数据，绘制9张图等函数与上面一致，<code>readme</code>中不再写出</li></ul><h3 id="1、定义CNN所需要的变量"><a href="#1、定义CNN所需要的变量" class="headerlink" title="1、定义CNN所需要的变量"></a>1、定义CNN所需要的变量</h3><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">'''define cnn description'''</div><div class="line">filter_size1 = 5     # the first conv filter size is 5x5 </div><div class="line">num_filters1 = 32    # there are 32 filters</div><div class="line">filter_size2 = 5     # the second conv filter size</div><div class="line">num_filters2 = 64    # there are 64 filters</div><div class="line">fc_size = 1024       # fully-connected layer</div></pre></td></tr></table></figure><h3 id="2、初始化weights和biases的函数"><a href="#2、初始化weights和biases的函数" class="headerlink" title="2、初始化weights和biases的函数"></a>2、初始化weights和biases的函数</h3><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">'''define a function to intialize weights'''</div><div class="line">def initialize_weights(shape):</div><div class="line">    '''</div><div class="line">    @param shape：the shape of weights</div><div class="line">    '''</div><div class="line">    return tf.Variable(tf.truncated_normal(shape=shape, stddev=0.1))</div><div class="line">'''define a function to intialize biases'''</div><div class="line">def initialize_biases(length):</div><div class="line">    '''</div><div class="line">    @param length: the length of biases, which is a vector</div><div class="line">    '''</div><div class="line">    return tf.Variable(tf.constant(0.1,shape=[length]))</div></pre></td></tr></table></figure><h3 id="3、定义卷积操作和池化（如果使用的话）的函数"><a href="#3、定义卷积操作和池化（如果使用的话）的函数" class="headerlink" title="3、定义卷积操作和池化（如果使用的话）的函数"></a>3、定义卷积操作和池化（如果使用的话）的函数</h3><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line">'''define a function to do conv and pooling if used'''</div><div class="line">def conv_layer(input, </div><div class="line">               num_input_channels,</div><div class="line">               filter_size,</div><div class="line">               num_output_filters,</div><div class="line">               use_pooling=True):</div><div class="line">    '''</div><div class="line">    @param input: the input of previous layer's output</div><div class="line">    @param num_input_channels: input channels</div><div class="line">    @param filter_size: the weights filter size</div><div class="line">    @param num_output_filters: the output number channels</div><div class="line">    @param use_pooling: if use pooling operation</div><div class="line">    '''</div><div class="line">    shape = [filter_size, filter_size, num_input_channels, num_output_filters]</div><div class="line">    weights = initialize_weights(shape=shape)</div><div class="line">    biases = initialize_biases(length=num_output_filters)   # one for each filter</div><div class="line">    layer = tf.nn.conv2d(input=input, filter=weights, strides=[1,1,1,1], padding='SAME')</div><div class="line">    layer += biases</div><div class="line">    if use_pooling:</div><div class="line">        layer = tf.nn.max_pool(value=layer,</div><div class="line">                               ksize=[1,2,2,1],</div><div class="line">                               strides=[1,2,2,1],</div><div class="line">                               padding="SAME")   # the kernel function size is 2x2,so the ksize=[1,2,2,1]</div><div class="line">    layer = tf.nn.relu(layer)</div><div class="line">    return layer, weights</div></pre></td></tr></table></figure><h3 id="4、定义将卷积层展开的函数"><a href="#4、定义将卷积层展开的函数" class="headerlink" title="4、定义将卷积层展开的函数"></a>4、定义将卷积层展开的函数</h3><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">'''define a function to flat conv layer'''</div><div class="line">def flatten_layer(layer):</div><div class="line">    '''</div><div class="line">    @param layer: the conv layer</div><div class="line">    '''</div><div class="line">    layer_shape = layer.get_shape() # get the shape of the layer(layer_shape == [num_images, img_height, img_width, num_channels])</div><div class="line">    num_features = layer_shape[1:4].num_elements()  # [1:4] means the last three demension, namely the flatten size</div><div class="line">    layer_flat = tf.reshape(layer, [-1, num_features])   # reshape to flat,-1 means don't care about the number of images</div><div class="line">    return layer_flat, num_features</div></pre></td></tr></table></figure><h3 id="5、定义全连接层的函数"><a href="#5、定义全连接层的函数" class="headerlink" title="5、定义全连接层的函数"></a>5、定义全连接层的函数</h3><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">'''define a function to do fully-connected'''</div><div class="line">def fc_layer(input, num_inputs, num_outputs, use_relu=True):</div><div class="line">    '''</div><div class="line">    @param input: the input</div><div class="line">    @param num_inputs: the input size</div><div class="line">    @param num_outputs: the output size</div><div class="line">    @param use_relu: if use relu activation function</div><div class="line">    '''</div><div class="line">    weights = initialize_weights(shape=[num_inputs, num_outputs])</div><div class="line">    biases = initialize_biases(num_outputs)</div><div class="line">    layer = tf.matmul(input, weights) + biases</div><div class="line">    if use_relu:</div><div class="line">        layer = tf.nn.relu(layer)</div><div class="line">    return layer</div></pre></td></tr></table></figure><h3 id="6、定义模型"><a href="#6、定义模型" class="headerlink" title="6、定义模型"></a>6、定义模型</h3><ul><li>定义<code>placeholder</code></li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">'''define the placeholder'''</div><div class="line">X = tf.placeholder(tf.float32, shape=[None, img_flat_size], name="X")</div><div class="line">X_image = tf.reshape(X, shape=[-1, img_size, img_size, num_channels])  # reshape to the image shape</div><div class="line">y_true = tf.placeholder(tf.float32, [None, num_classes], name="y_true")</div><div class="line">y_true_cls = tf.argmax(y_true, axis=1)</div><div class="line">keep_prob = tf.placeholder(tf.float32)  # drop out placeholder</div></pre></td></tr></table></figure><ul><li>定义卷积、dropout、和全连接</li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line">'''define the cnn model'''</div><div class="line">layer_conv1, weights_conv1 = conv_layer(input=X_image, num_input_channels=num_channels, </div><div class="line">                                       filter_size=filter_size1, </div><div class="line">                                       num_output_filters=num_filters1,</div><div class="line">                                       use_pooling=True)</div><div class="line">print("conv1:",layer_conv1)</div><div class="line">layer_conv2, weights_conv2 = conv_layer(input=layer_conv1, num_input_channels=num_filters1, </div><div class="line">                                        filter_size=filter_size2,</div><div class="line">                                        num_output_filters=num_filters2,</div><div class="line">                                        use_pooling=True)</div><div class="line">print("conv2:",layer_conv2)</div><div class="line">layer_flat, num_features = flatten_layer(layer_conv2) # the num_feature is 7x7x36=1764</div><div class="line">print("flatten layer:", layer_flat)  </div><div class="line">layer_fc1 = fc_layer(layer_flat, num_features, fc_size, use_relu=True)</div><div class="line">print("fully-connected layer1:", layer_fc1)</div><div class="line">layer_drop_out = tf.nn.dropout(layer_fc1, keep_prob)   # dropout operation</div><div class="line">layer_fc2 = fc_layer(layer_drop_out, fc_size, num_classes,use_relu=False)</div><div class="line">print("fully-connected layer2:", layer_fc2)</div><div class="line">y_pred = tf.nn.softmax(layer_fc2)</div><div class="line">y_pred_cls = tf.argmax(y_pred, axis=1)</div><div class="line">cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=y_true, </div><div class="line">                                                       logits=layer_fc2)</div><div class="line">cost = tf.reduce_mean(cross_entropy)</div><div class="line">optimizer = tf.train.AdamOptimizer(learning_rate=1e-3).minimize(cost)  # use AdamOptimizer优化</div></pre></td></tr></table></figure><ul><li>定义求准确度</li></ul><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="string">''</span><span class="string">'define accuracy'</span><span class="string">''</span></div><div class="line">correct_prediction = tf.equal(y_true_cls, y_pred_cls)</div><div class="line">accuracy = tf.reduce_mean(tf.cast(correct_prediction,dtype=tf.float32))</div></pre></td></tr></table></figure><h3 id="7、定义训练的函数optimize，使用bgd"><a href="#7、定义训练的函数optimize，使用bgd" class="headerlink" title="7、定义训练的函数optimize，使用bgd"></a>7、定义训练的函数<code>optimize</code>，使用bgd</h3><ul><li><p>代码：</p><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line">'''define a function to run train the model with bgd'''</div><div class="line">total_iterations = 0  # record the total iterations</div><div class="line">def optimize(num_iterations):</div><div class="line">    '''</div><div class="line">    @param num_iterations: the total interations of train batch_size operation</div><div class="line">    '''</div><div class="line">    global total_iterations</div><div class="line">    start_time = time.time()</div><div class="line">    for i in range(total_iterations,total_iterations + num_iterations):</div><div class="line">        x_batch, y_batch = data.train.next_batch(batch_size)</div><div class="line">        feed_dict = &#123;X: x_batch, y_true: y_batch, keep_prob: 0.5&#125;</div><div class="line">        session.run(optimizer, feed_dict=feed_dict)</div><div class="line">        if i % 10 == 0:</div><div class="line">            acc = session.run(accuracy, feed_dict=feed_dict)</div><div class="line">            msg = "Optimization Iteration: &#123;0:&gt;6&#125;, Training Accuracy: &#123;1:&gt;6.1%&#125;"    # &#123;:&gt;6&#125;means the fixed width,&#123;1:&gt;6.1%&#125;means the fixed width is 6 and keep 1 decimal place         </div><div class="line">            print(msg.format(i + 1, acc))</div><div class="line">    total_iterations += num_iterations</div><div class="line">    end_time = time.time()</div><div class="line">    time_dif = end_time-start_time</div><div class="line">    print("time usage:"+str(timedelta(seconds=int(round(time_dif)))))</div></pre></td></tr></table></figure></li><li><p>输出：</p></li></ul><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div></pre></td><td class="code"><pre><div class="line">Optimization Iteration:    <span class="number">651</span>, Training Accuracy:  <span class="number">99.0%</span></div><div class="line">Optimization Iteration:    <span class="number">661</span>, Training Accuracy:  <span class="number">99.0%</span></div><div class="line">Optimization Iteration:    <span class="number">671</span>, Training Accuracy:  <span class="number">99.0%</span></div><div class="line">Optimization Iteration:    <span class="number">681</span>, Training Accuracy:  <span class="number">99.0%</span></div><div class="line">Optimization Iteration:    <span class="number">691</span>, Training Accuracy:  <span class="number">99.0%</span></div><div class="line">Optimization Iteration:    <span class="number">701</span>, Training Accuracy:  <span class="number">99.0%</span></div><div class="line">Optimization Iteration:    <span class="number">711</span>, Training Accuracy:  <span class="number">99.0%</span></div><div class="line">Optimization Iteration:    <span class="number">721</span>, Training Accuracy:  <span class="number">99.0%</span></div><div class="line">Optimization Iteration:    <span class="number">731</span>, Training Accuracy:  <span class="number">99.0%</span></div><div class="line">Optimization Iteration:    <span class="number">741</span>, Training Accuracy: <span class="number">100.0%</span></div><div class="line">Optimization Iteration:    <span class="number">751</span>, Training Accuracy:  <span class="number">99.0%</span></div><div class="line">Optimization Iteration:    <span class="number">761</span>, Training Accuracy:  <span class="number">99.0%</span></div><div class="line">Optimization Iteration:    <span class="number">771</span>, Training Accuracy:  <span class="number">97.0%</span></div><div class="line">Optimization Iteration:    <span class="number">781</span>, Training Accuracy:  <span class="number">96.0%</span></div><div class="line">Optimization Iteration:    <span class="number">791</span>, Training Accuracy:  <span class="number">98.0%</span></div><div class="line">Optimization Iteration:    <span class="number">801</span>, Training Accuracy: <span class="number">100.0%</span></div><div class="line">Optimization Iteration:    <span class="number">811</span>, Training Accuracy: <span class="number">100.0%</span></div><div class="line">Optimization Iteration:    <span class="number">821</span>, Training Accuracy:  <span class="number">97.0%</span></div><div class="line">Optimization Iteration:    <span class="number">831</span>, Training Accuracy:  <span class="number">98.0%</span></div><div class="line">Optimization Iteration:    <span class="number">841</span>, Training Accuracy:  <span class="number">99.0%</span></div><div class="line">Optimization Iteration:    <span class="number">851</span>, Training Accuracy:  <span class="number">99.0%</span></div><div class="line">Optimization Iteration:    <span class="number">861</span>, Training Accuracy:  <span class="number">99.0%</span></div><div class="line">Optimization Iteration:    <span class="number">871</span>, Training Accuracy:  <span class="number">96.0%</span></div><div class="line">Optimization Iteration:    <span class="number">881</span>, Training Accuracy:  <span class="number">99.0%</span></div><div class="line">Optimization Iteration:    <span class="number">891</span>, Training Accuracy:  <span class="number">99.0%</span></div><div class="line">Optimization Iteration:    <span class="number">901</span>, Training Accuracy:  <span class="number">98.0%</span></div><div class="line">Optimization Iteration:    <span class="number">911</span>, Training Accuracy:  <span class="number">99.0%</span></div><div class="line">Optimization Iteration:    <span class="number">921</span>, Training Accuracy:  <span class="number">99.0%</span></div><div class="line">Optimization Iteration:    <span class="number">931</span>, Training Accuracy:  <span class="number">99.0%</span></div><div class="line">Optimization Iteration:    <span class="number">941</span>, Training Accuracy:  <span class="number">98.0%</span></div><div class="line">Optimization Iteration:    <span class="number">951</span>, Training Accuracy: <span class="number">100.0%</span></div><div class="line">Optimization Iteration:    <span class="number">961</span>, Training Accuracy:  <span class="number">99.0%</span></div><div class="line">Optimization Iteration:    <span class="number">971</span>, Training Accuracy:  <span class="number">98.0%</span></div><div class="line">Optimization Iteration:    <span class="number">981</span>, Training Accuracy:  <span class="number">99.0%</span></div><div class="line">Optimization Iteration:    <span class="number">991</span>, Training Accuracy: <span class="number">100.0%</span></div><div class="line"><span class="selector-tag">time</span> usage:<span class="number">0</span>:<span class="number">07</span>:<span class="number">07</span></div></pre></td></tr></table></figure><h3 id="8、定义批量预测的函数，方便输出训练错的图像"><a href="#8、定义批量预测的函数，方便输出训练错的图像" class="headerlink" title="8、定义批量预测的函数，方便输出训练错的图像"></a>8、定义批量预测的函数，方便输出训练错的图像</h3><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line">batch_size_test = 256</div><div class="line">def print_test_accuracy(print_error=False,print_confusion_matrix=False):</div><div class="line">    '''</div><div class="line">    @param print_error: whether plot the error images</div><div class="line">    @param print_confusion_matrix: whether plot the confusion_matrix</div><div class="line">    '''</div><div class="line">    num_test = len(data.test.images)   </div><div class="line">    cls_pred = np.zeros(shape=num_test, dtype=np.int)  # declare the cls_pred</div><div class="line">    i = 0</div><div class="line">    #predict the test set using batch_size</div><div class="line">    while i &lt; num_test:</div><div class="line">        j = min(i + batch_size_test, num_test)</div><div class="line">        images = data.test.images[i:j,:]</div><div class="line">        labels = data.test.labels[i:j,:]</div><div class="line">        feed_dict = &#123;X:images,y_true:labels,keep_prob:0.5&#125;</div><div class="line">        cls_pred[i:j] = session.run(y_pred_cls,feed_dict=feed_dict)</div><div class="line">        i = j</div><div class="line">    cls_true = data.test.cls</div><div class="line">    correct = (cls_true == cls_pred)</div><div class="line">    correct_sum = correct.sum()   # correct predictions</div><div class="line">    acc = float(correct_sum)/num_test</div><div class="line">    msg = "Accuracy on Test-Set: &#123;0:.1%&#125; (&#123;1&#125; / &#123;2&#125;)"</div><div class="line">    print(msg.format(acc, correct_sum, num_test))    </div><div class="line">    if print_error:</div><div class="line">        plot_error_pred(cls_pred,correct)</div><div class="line">    if print_confusion_matrix:</div><div class="line">        plot_confusin_martrix(cls_pred)</div></pre></td></tr></table></figure><h3 id="9、定义可视化卷积核权重的函数"><a href="#9、定义可视化卷积核权重的函数" class="headerlink" title="9、定义可视化卷积核权重的函数"></a>9、定义可视化卷积核权重的函数</h3><ul><li><p>代码：</p><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line">'''define a function to plot conv weights'''</div><div class="line">def plot_conv_weights(weights,input_channel=0):</div><div class="line">    '''</div><div class="line">    @param weights: the conv filter weights, for example: the weights_conv1 and weights_conv2, which are 4 dimension [filter_size, filter_size, num_input_channels, num_output_filters]</div><div class="line">    @param input_channel: the input_channels</div><div class="line">    '''</div><div class="line">    w = session.run(weights)</div><div class="line">    w_min = np.min(w)</div><div class="line">    w_max = np.max(w)</div><div class="line">    num_filters = w.shape[3]   # get the number of filters</div><div class="line">    num_grids = math.ceil(math.sqrt(num_filters))</div><div class="line">    fig, axes = plt.subplots(num_grids, num_grids)</div><div class="line">    for i, ax in enumerate(axes.flat):</div><div class="line">        if i &lt; num_filters:</div><div class="line">            img = w[:,:,input_channel,i]   # the ith weight</div><div class="line">            ax.imshow(img,vmin=w_min,vmax=w_max,interpolation="nearest",cmap='seismic')</div><div class="line">        ax.set_xticks([])</div><div class="line">        ax.set_yticks([])</div><div class="line">    plt.show()</div></pre></td></tr></table></figure></li><li><p>输出：</p><ul><li>第一层：<br><img src="https://raw.githubusercontent.com/lawlite19/MachineLearning_TensorFlow/master/images/CNNModel_01.png" alt="enter description here" title="CNNModel_01"></li><li>第二层：<br><img src="https://raw.githubusercontent.com/lawlite19/MachineLearning_TensorFlow/master/images/CNNModel_03.png" alt="enter description here" title="CNNModel_03"><h3 id="10、定义可视化卷积层输出的函数"><a href="#10、定义可视化卷积层输出的函数" class="headerlink" title="10、定义可视化卷积层输出的函数"></a>10、定义可视化卷积层输出的函数</h3></li></ul></li><li><p>代码：</p><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line">'''define a function to plot conv output layer'''</div><div class="line">def plot_conv_layer(layer, image):</div><div class="line">    '''</div><div class="line">    @param layer: the conv layer, which is also a image after conv</div><div class="line">    @param image: the image info</div><div class="line">    '''</div><div class="line">    feed_dict = &#123;X:[image]&#125;</div><div class="line">    values = session.run(layer, feed_dict=feed_dict)</div><div class="line">    num_filters = values.shape[3]   # get the number of filters</div><div class="line">    num_grids = math.ceil(math.sqrt(num_filters))</div><div class="line">    fig, axes = plt.subplots(num_grids,num_grids)</div><div class="line">    for i, ax in enumerate(axes.flat):</div><div class="line">        if i &lt; num_filters:</div><div class="line">            img = values[0,:,:,i]</div><div class="line">            ax.imshow(img, interpolation="nearest",cmap="binary")</div><div class="line">        ax.set_xticks([])</div><div class="line">        ax.set_yticks([])</div><div class="line">    plt.show()</div></pre></td></tr></table></figure></li><li><p>输出：</p><ul><li>第一层：<br><img src="https://raw.githubusercontent.com/lawlite19/MachineLearning_TensorFlow/master/images/CNNModel_02.png" alt="enter description here" title="CNNModel_02"></li><li>第二层：<br><img src="https://raw.githubusercontent.com/lawlite19/MachineLearning_TensorFlow/master/images/CNNModel_04.png" alt="enter description here" title="CNNModel_04"></li></ul></li></ul><h2 id="十一：使用prettytensor实现CNNModel"><a href="#十一：使用prettytensor实现CNNModel" class="headerlink" title="十一：使用prettytensor实现CNNModel"></a>十一：使用prettytensor实现CNNModel</h2><ul><li><a href="https://github.com/lawlite19/MachineLearning_TensorFlow/blob/master/CNNModel_PrettyTensor/CNNModel_prettytensor.py" target="_blank" rel="external">全部代码</a></li><li>使用<code>MNIST</code>数据集</li><li>加载数据，绘制9张图等函数与<strong>九</strong>一致，<code>readme</code>中不再写出<h3 id="1、定义模型"><a href="#1、定义模型" class="headerlink" title="1、定义模型"></a>1、定义模型</h3></li><li>定义<code>placeholder</code>,与之前的一致</li></ul><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="string">''</span><span class="string">'declare the placeholder'</span><span class="string">''</span></div><div class="line">X = tf.placeholder(tf<span class="selector-class">.float32</span>, [None, img_flat_size], name=<span class="string">"X"</span>)</div><div class="line">X_img = tf.reshape(X, shape=[-<span class="number">1</span>,img_size,img_size, num_channels])</div><div class="line">y_true = tf.placeholder(tf<span class="selector-class">.float32</span>, shape=[None, num_classes], name=<span class="string">"y_true"</span>)</div><div class="line">y_true_cls = tf.argmax(y_true,<span class="number">1</span>)</div></pre></td></tr></table></figure><ul><li>使用<code>prettytensor</code>实现CNN模型</li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">'''define the cnn model with prettytensor'''</div><div class="line">x_pretty = pt.wrap(X_img)</div><div class="line">with pt.defaults_scope():   # or pt.defaults_scope(activation_fn=tf.nn.relu) if just use one activation function</div><div class="line">    y_pred, loss = x_pretty.\</div><div class="line">        conv2d(kernel=5, depth=16, activation_fn=tf.nn.relu, name="conv_layer1").\</div><div class="line">        max_pool(kernel=2, stride=2).\</div><div class="line">        conv2d(kernel=5, depth=36, activation_fn=tf.nn.relu, name="conv_layer2").\</div><div class="line">        max_pool(kernel=2, stride=2).\</div><div class="line">        flatten().\</div><div class="line">        fully_connected(size=128, activation_fn=tf.nn.relu, name="fc_layer1").\</div><div class="line">        softmax_classifier(num_classes=num_classes, labels=y_true)</div></pre></td></tr></table></figure><ul><li>获取卷积核的权重(后续可视化)</li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">'''define a function to get weights'''</div><div class="line">def get_weights_variable(layer_name):</div><div class="line">    with tf.variable_scope(layer_name, reuse=True):</div><div class="line">        variable = tf.get_variable("weights")</div><div class="line">    return variable</div><div class="line">conv1_weights = get_weights_variable("conv_layer1")</div><div class="line">conv2_weights = get_weights_variable("conv_layer2")</div></pre></td></tr></table></figure><ul><li>定义<code>optimizer</code>训练，和之前的一样了</li></ul><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="string">''</span><span class="string">'define optimizer to train'</span><span class="string">''</span></div><div class="line">optimizer = tf<span class="selector-class">.train</span><span class="selector-class">.AdamOptimizer</span>().minimize(loss)</div><div class="line">y_pred_cls = tf.argmax(y_pred,<span class="number">1</span>)</div><div class="line">correct_prediction = tf.equal(y_pred_cls, y_true_cls)</div><div class="line">accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</div><div class="line">session = tf.Session()</div><div class="line">session.run(tf.global_variables_initializer())</div></pre></td></tr></table></figure><h2 id="十二：CNN-保存和加载模型，使用Early-Stopping"><a href="#十二：CNN-保存和加载模型，使用Early-Stopping" class="headerlink" title="十二：CNN,保存和加载模型，使用Early Stopping"></a>十二：CNN,保存和加载模型，使用Early Stopping</h2><ul><li><a href="https://github.com/lawlite19/MachineLearning_TensorFlow/blob/master/CNNModel_EarlyStopping_Save_Restore/CNNModel_EarlyStopping_Save_Restore.py" target="_blank" rel="external">全部代码</a></li><li>使用<code>MNIST</code>数据集</li><li>加载数据，绘制9张图等函数与<strong>九</strong>一致，<code>readme</code>中不再写出</li><li>CNN模型的定义和<strong>十一</strong>中的一致，<code>readme</code>中不再写出<h3 id="1、保存模型"><a href="#1、保存模型" class="headerlink" title="1、保存模型"></a>1、保存模型</h3></li><li>创建saver,和保存的目录</li></ul><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="string">''</span><span class="string">'define a Saver to save the network'</span><span class="string">''</span></div><div class="line">saver = tf<span class="selector-class">.train</span><span class="selector-class">.Saver</span>()</div><div class="line">save_dir = <span class="string">"checkpoints/"</span></div><div class="line"><span class="keyword">if</span> not os<span class="selector-class">.path</span><span class="selector-class">.exists</span>(save_dir):</div><div class="line">    os.makedirs(save_dir)</div><div class="line">save_path = os<span class="selector-class">.path</span><span class="selector-class">.join</span>(save_dir, <span class="string">'best_validation'</span>)</div></pre></td></tr></table></figure><ul><li>保存session,对应到下面2中的Early Stopping，将最好的模型保存</li></ul><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">saver.save(sess=session, save_path=save_path)</div></pre></td></tr></table></figure><h3 id="2、Early-Stopping"><a href="#2、Early-Stopping" class="headerlink" title="2、Early Stopping"></a>2、Early Stopping</h3><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div></pre></td><td class="code"><pre><div class="line">'''declear the train info'''</div><div class="line">train_batch_size = 64</div><div class="line">best_validation_accuracy = 0.0</div><div class="line">last_improvement = 0</div><div class="line">require_improvement_iterations = 1000</div><div class="line">total_iterations = 0</div><div class="line">'''define a function to optimize the optimizer'''</div><div class="line">def optimize(num_iterations):</div><div class="line">    global total_iterations</div><div class="line">    global best_validation_accuracy</div><div class="line">    global last_improvement</div><div class="line">    start_time = time.time()</div><div class="line">    for i in range(num_iterations):</div><div class="line">        total_iterations += 1</div><div class="line">        X_batch, y_true_batch = data.train.next_batch(train_batch_size)</div><div class="line">        feed_dict_train = &#123;X: X_batch,</div><div class="line">                     y_true: y_true_batch&#125;</div><div class="line">        session.run(optimizer, feed_dict=feed_dict_train)</div><div class="line">        if (total_iterations%100 == 0) or (i == num_iterations-1):</div><div class="line">            acc_train = session.run(accuracy, feed_dict=feed_dict_train)</div><div class="line">            acc_validation, _ = validation_accuracy()</div><div class="line">            if acc_validation &gt; best_validation_accuracy:</div><div class="line">                best_validation_accuracy = acc_validation</div><div class="line">                last_improvement = total_iterations</div><div class="line">                saver.save(sess=session, save_path=save_path)</div><div class="line">                improved_str = "*"</div><div class="line">            else:</div><div class="line">                improved_str = ""</div><div class="line">            msg = "Iter: &#123;0:&gt;6&#125;, Train_batch accuracy:&#123;1:&gt;6.1%&#125;, validation acc:&#123;2:&gt;6.1%&#125; &#123;3&#125;"</div><div class="line">            print(msg.format(i+1, acc_train, acc_validation, improved_str))</div><div class="line">        if total_iterations-last_improvement &gt; require_improvement_iterations:</div><div class="line">            print('No improvement found in a while, stop running')</div><div class="line">            break</div><div class="line">    end_time = time.time()</div><div class="line">    time_diff = end_time-start_time</div><div class="line">    print("Time usage:" + str(timedelta(seconds=int(round(time_diff)))))</div></pre></td></tr></table></figure><ul><li>调用<code>optimize(10000)</code>输出信息</li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line">Iter:   5100, Train_batch accuracy:100.0%, validation acc: 98.8% *</div><div class="line">Iter:   5200, Train_batch accuracy:100.0%, validation acc: 98.3% </div><div class="line">Iter:   5300, Train_batch accuracy:100.0%, validation acc: 98.7% </div><div class="line">Iter:   5400, Train_batch accuracy: 98.4%, validation acc: 98.6% </div><div class="line">Iter:   5500, Train_batch accuracy: 98.4%, validation acc: 98.6% </div><div class="line">Iter:   5600, Train_batch accuracy:100.0%, validation acc: 98.7% </div><div class="line">Iter:   5700, Train_batch accuracy: 96.9%, validation acc: 98.9% *</div><div class="line">Iter:   5800, Train_batch accuracy:100.0%, validation acc: 98.6% </div><div class="line">Iter:   5900, Train_batch accuracy:100.0%, validation acc: 98.6% </div><div class="line">Iter:   6000, Train_batch accuracy: 98.4%, validation acc: 98.7% </div><div class="line">Iter:   6100, Train_batch accuracy:100.0%, validation acc: 98.7% </div><div class="line">Iter:   6200, Train_batch accuracy:100.0%, validation acc: 98.7% </div><div class="line">Iter:   6300, Train_batch accuracy: 98.4%, validation acc: 98.8% </div><div class="line">Iter:   6400, Train_batch accuracy: 98.4%, validation acc: 98.8% </div><div class="line">Iter:   6500, Train_batch accuracy:100.0%, validation acc: 98.7% </div><div class="line">Iter:   6600, Train_batch accuracy:100.0%, validation acc: 98.7% </div><div class="line">Iter:   6700, Train_batch accuracy:100.0%, validation acc: 98.8% </div><div class="line">No improvement found in a while, stop running</div><div class="line">Time usage:0:18:43</div></pre></td></tr></table></figure><p>可以看到最后10次输出（每100次输出一次）在验证集上准确度都没有提高，停止执行</p><h3 id="3、-小批量预测并计算准确率"><a href="#3、-小批量预测并计算准确率" class="headerlink" title="3、 小批量预测并计算准确率"></a>3、 小批量预测并计算准确率</h3><ul><li><p>因为需要预测<strong>测试集和验证集</strong>，这里参数指定需要的images</p><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">'''define a function to predict using batch'''</div><div class="line">batch_size_predict = 256</div><div class="line">def predict_cls(images, labels, cls_true):</div><div class="line">    num_images = len(images)</div><div class="line">    cls_pred = np.zeros(shape=num_images, dtype=np.int)</div><div class="line">    i = 0</div><div class="line">    while i &lt; num_images:</div><div class="line">        j = min(i+batch_size_predict, num_images)</div><div class="line">        feed_dict = &#123;X: images[i:j,:],</div><div class="line">                     y_true: labels[i:j,:]&#125;</div><div class="line">        cls_pred[i:j] = session.run(y_pred_cls, feed_dict=feed_dict)</div><div class="line">        i = j</div><div class="line">    correct = (cls_true==cls_pred)</div><div class="line">    return correct, cls_pred</div></pre></td></tr></table></figure></li><li><p>测试集和验证集直接调用即可</p></li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">def predict_cls_test():</div><div class="line">    return predict_cls(data.test.images, data.test.labels, data.test.cls)</div><div class="line"></div><div class="line">def predict_cls_validation():</div><div class="line">    return predict_cls(data.validation.images, data.validation.labels, data.validation.cls)</div></pre></td></tr></table></figure><ul><li>计算验证集准确率（上面optimize函数中需要用到）</li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">'''calculate the acc'''</div><div class="line">def cls_accuracy(correct):</div><div class="line">    correct_sum = correct.sum()</div><div class="line">    acc = float(correct_sum)/len(correct)</div><div class="line">    return acc, correct_sum</div><div class="line">'''define a function to calculate the validation acc'''</div><div class="line">def validation_accuracy():</div><div class="line">    correct, _ = predict_cls_validation()</div><div class="line">    return cls_accuracy(correct)</div></pre></td></tr></table></figure><ul><li>计算测试集准确率，并且输出错误的预测和confusion matrix</li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line">'''define a function to calculate test acc'''</div><div class="line">def print_test_accuracy(show_example_errors=False,</div><div class="line">                        show_confusion_matrix=False):</div><div class="line">    correct, cls_pred = predict_cls_test()</div><div class="line">    acc, num_correct = cls_accuracy(correct)</div><div class="line">    num_images = len(correct)</div><div class="line">    msg = "Accuracy on Test-Set: &#123;0:.1%&#125; (&#123;1&#125; / &#123;2&#125;)"</div><div class="line">    print(msg.format(acc, num_correct, num_images))</div><div class="line"></div><div class="line">    # Plot some examples of mis-classifications, if desired.</div><div class="line">    if show_example_errors:</div><div class="line">        print("Example errors:")</div><div class="line">        plot_example_errors(cls_pred=cls_pred, correct=correct)</div><div class="line"></div><div class="line">    # Plot the confusion matrix, if desired.</div><div class="line">    if show_confusion_matrix:</div><div class="line">        print("Confusion Matrix:")</div><div class="line">        plot_confusion_matrix(cls_pred=cls_pred)</div></pre></td></tr></table></figure><h2 id="十二：模型融合"><a href="#十二：模型融合" class="headerlink" title="十二：模型融合"></a>十二：模型融合</h2><ul><li><a href="https://github.com/lawlite19/MachineLearning_TensorFlow/blob/master/Ensemble_Learning/ensemble_learning.py" target="_blank" rel="external">全部代码</a></li><li>使用<code>MNIST</code>数据集</li><li>一些方法和之前的一致，不在给出</li><li>其中训练了多个CNN 模型，然后取预测的平均值作为最后的预测结果<h3 id="1、将测试集和验证集合并后，并重新划分"><a href="#1、将测试集和验证集合并后，并重新划分" class="headerlink" title="1、将测试集和验证集合并后，并重新划分"></a>1、将测试集和验证集合并后，并重新划分</h3></li><li>主要是希望训练时数据集有些变换，否则都是一样的数据去训练了，最后再融合意义不大<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line">'''将training set和validation set合并，并重新划分'''</div><div class="line">combine_images = np.concatenate([data.train.images, data.validation.images], axis=0)</div><div class="line">combine_labels = np.concatenate([data.train.labels, data.validation.labels], axis=0)</div><div class="line">print("合并后图片：", combine_images.shape)</div><div class="line">print("合并后label：", combine_labels.shape)</div><div class="line">combined_size = combine_labels.shape[0]</div><div class="line">train_size = int(0.8*combined_size)</div><div class="line">validation_size = combined_size - train_size</div><div class="line">'''函数：将合并后的重新随机划分'''</div><div class="line">def random_training_set():</div><div class="line">    idx = np.random.permutation(combined_size)   # 将0-combined_size数字随机排列</div><div class="line">    idx_train = idx[0:train_size]</div><div class="line">    idx_validation = idx[train_size:]</div><div class="line">    x_train = combine_images[idx_train, :]</div><div class="line">    y_train = combine_labels[idx_train, :]</div><div class="line">    </div><div class="line">    x_validation = combine_images[idx_validation, :]</div><div class="line">    y_validation = combine_images[idx_validation, :]</div><div class="line">    return x_train, y_train, x_validation, y_validation</div></pre></td></tr></table></figure></li></ul><h3 id="2、融合模型"><a href="#2、融合模型" class="headerlink" title="2、融合模型"></a>2、融合模型</h3><ul><li><p>加载训练好的模型，并输出每个模型在测试集的预测结果等</p><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">def ensemble_predictions():</div><div class="line">    pred_labels = []</div><div class="line">    test_accuracies = []</div><div class="line">    validation_accuracies = []</div><div class="line">    for i in range(num_networks):</div><div class="line">        saver.restore(sess=session, save_path=get_save_path(i))</div><div class="line">        test_acc = test_accuracy()</div><div class="line">        test_accuracies.append(test_acc)</div><div class="line">        validation_acc = validation_accuracy()</div><div class="line">        validation_accuracies.append(validation_acc)</div><div class="line">        msg = "网络：&#123;0&#125;，验证集：&#123;1:.4f&#125;，测试集&#123;2:.4f&#125;"</div><div class="line">        print(msg.format(i, validation_acc, test_acc))</div><div class="line">        pred = predict_labels(data.test.images)</div><div class="line">        pred_labels.append(pred)</div><div class="line">    return np.array(pred_labels),\</div><div class="line">           np.array(test_accuracies),\</div><div class="line">           np.array(validation_accuracies)</div></pre></td></tr></table></figure></li><li><p>调用<code>pred_labels, test_accuracies, val_accuracies = ensemble_predictions()</code></p></li><li>取均值：<code>ensemble_pred_labels = np.mean(pred_labels, axis=0)</code></li><li>融合后的真实结果：<code>ensemble_cls_pred = np.argmax(ensemble_pred_labels, axis=1)</code></li><li>其他一些信息：</li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">ensemble_correct = (ensemble_cls_pred == data.test.cls)</div><div class="line">ensemble_incorrect = np.logical_not(ensemble_correct)</div><div class="line">print(test_accuracies)</div><div class="line">best_net = np.argmax(test_accuracies)</div><div class="line">print(best_net)</div><div class="line">print(test_accuracies[best_net])</div><div class="line">best_net_pred_labels = pred_labels[best_net, :, :]</div><div class="line">best_net_cls_pred = np.argmax(best_net_pred_labels, axis=1)</div><div class="line">best_net_correct = (best_net_cls_pred == data.test.cls)</div><div class="line">best_net_incorrect = np.logical_not(best_net_correct)</div><div class="line">print("融合后预测对的：", np.sum(ensemble_correct))</div><div class="line">print("单个最好模型预测对的", np.sum(best_net_correct))</div><div class="line">ensemble_better = np.logical_and(best_net_incorrect, ensemble_correct)  # 融合之后好于单个的个数</div><div class="line">print(ensemble_better.sum())</div><div class="line">best_net_better = np.logical_and(best_net_correct, ensemble_incorrect)  # 单个好于融合之后的个数</div><div class="line">print(best_net_better.sum())</div></pre></td></tr></table></figure><h2 id="十二：Cifar-10数据集，使用variable-scope重复使用变量"><a href="#十二：Cifar-10数据集，使用variable-scope重复使用变量" class="headerlink" title="十二：Cifar-10数据集，使用variable_scope重复使用变量"></a>十二：Cifar-10数据集，使用<code>variable_scope</code>重复使用变量</h2><ul><li><a href="https://github.com/lawlite19/MachineLearning_TensorFlow/blob/master/Ensemble_Learning/CNN_for_CIFAR-10" target="_blank" rel="external">全部代码</a></li><li>使用<code>CIFAR-10</code>数据集</li><li>创建了<strong>两个网络</strong>，一个用于训练，一个用于测试，测试使用的是训练好的权重参数，所以用到<strong>参数重用</strong></li><li>网络结构</li></ul><p><img src="https://raw.githubusercontent.com/lawlite19/MachineLearning_TensorFlow/master/images/06_network_flowchart.png" alt="cifar-10结构" title="06_network_flowchart"></p><h3 id="1、数据集"><a href="#1、数据集" class="headerlink" title="1、数据集"></a>1、数据集</h3><ul><li><p>导入包：</p><ul><li>这是别人实现好的下载和处理<code>cifar-10</code>数据集的diamante<figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">import cifar10</div><div class="line">from cifar10 import img_size, num_channels, num_classes</div></pre></td></tr></table></figure></li></ul></li><li><p>输出一些数据集信息</p></li></ul><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="string">''</span><span class="string">'下载cifar10数据集, 大概163M'</span><span class="string">''</span></div><div class="line">cifar10.maybe_download_and_extract()</div><div class="line"><span class="string">''</span><span class="string">'加载数据集'</span><span class="string">''</span></div><div class="line">images_train, cls_train, labels_train = cifar10.load_training_data()</div><div class="line">images_test,  cls_test,  labels_test  = cifar10.load_test_data()</div><div class="line"></div><div class="line"><span class="string">''</span><span class="string">'打印一些信息'</span><span class="string">''</span></div><div class="line">class_names = cifar10.load_class_names()</div><div class="line"><span class="function"><span class="title">print</span><span class="params">(class_names)</span></span></div><div class="line"><span class="function"><span class="title">print</span><span class="params">(<span class="string">"Size of:"</span>)</span></span></div><div class="line"><span class="function"><span class="title">print</span><span class="params">(<span class="string">"training set:\t\t&#123;&#125;"</span>.format(len(images_train)</span></span>))</div><div class="line"><span class="function"><span class="title">print</span><span class="params">(<span class="string">"test set:\t\t\t&#123;&#125;"</span>.format(len(images_test)</span></span>))</div></pre></td></tr></table></figure><ul><li>显示9张图片函数<ul><li>相比之前的，加入了<code>smooth</code></li></ul></li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line">'''显示9张图片函数'''</div><div class="line">def plot_images(images, cls_true, cls_pred=None, smooth=True):   # smooth是否平滑显示</div><div class="line">    assert len(images) == len(cls_true) == 9</div><div class="line">    fig, axes = plt.subplots(3,3)</div><div class="line">    </div><div class="line">    for i, ax in enumerate(axes.flat):</div><div class="line">        if smooth:</div><div class="line">            interpolation = 'spline16'</div><div class="line">        else:</div><div class="line">            interpolation = 'nearest'</div><div class="line">        ax.imshow(images[i, :, :, :], interpolation=interpolation)</div><div class="line">        cls_true_name = class_names[cls_true[i]]</div><div class="line">        if cls_pred is None:</div><div class="line">            xlabel = "True:&#123;0&#125;".format(cls_true_name)</div><div class="line">        else:</div><div class="line">            cls_pred_name = class_names[cls_pred[i]]</div><div class="line">            xlabel = "True:&#123;0&#125;, Pred:&#123;1&#125;".format(cls_true_name, cls_pred_name)</div><div class="line">        ax.set_xlabel(xlabel)</div><div class="line">        ax.set_xticks([])</div><div class="line">        ax.set_yticks([])</div><div class="line">    plt.show()</div></pre></td></tr></table></figure><h3 id="2、定义placeholder"><a href="#2、定义placeholder" class="headerlink" title="2、定义placeholder"></a>2、定义<code>placeholder</code></h3><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">X = tf.placeholder(tf<span class="selector-class">.float32</span>, shape=[None, img_size, img_size, num_channels], name=<span class="string">"X"</span>)</div><div class="line">y_true = tf.placeholder(tf<span class="selector-class">.float32</span>, shape=[None, num_classes], name=<span class="string">"y"</span>)</div><div class="line">y_true_cls = tf.argmax(y_true, axis=<span class="number">1</span>)</div></pre></td></tr></table></figure><h3 id="3、图片处理"><a href="#3、图片处理" class="headerlink" title="3、图片处理"></a>3、图片处理</h3><ul><li><p>单张图片处理</p><ul><li>原图是<code>32*32</code>像素的，裁剪成<code>24*24</code>像素的</li><li>如果是训练集进行一些裁剪，翻转，饱和度等处理</li><li>如果是测试集，只进行简单的裁剪处理</li><li>这也是为什么使用<code>variable_scope</code>定义两个网络<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">'''单个图片预处理, 测试集只需要裁剪就行了'''</div><div class="line">def pre_process_image(image, training):</div><div class="line">    if training:</div><div class="line">        image = tf.random_crop(image, size=[img_size_cropped, img_size_cropped, num_channels])  # 裁剪</div><div class="line">        image = tf.image.random_flip_left_right(image)                  # 左右翻转</div><div class="line">        image = tf.image.random_hue(image, max_delta=0.05)              # 色调调整</div><div class="line">        image = tf.image.random_brightness(image, max_delta=0.2)        # 曝光</div><div class="line">        image = tf.image.random_saturation(image, lower=0.0, upper=2.0) # 饱和度</div><div class="line">        '''上面的调整可能pixel值超过[0, 1], 所以约束一下'''        </div><div class="line">        image = tf.minimum(image, 1.0)</div><div class="line">        image = tf.maximum(image, 0.0)</div><div class="line">    else:</div><div class="line">        image = tf.image.resize_image_with_crop_or_pad(image, target_height=img_size_cropped, </div><div class="line">                                              target_width=img_size_cropped)</div><div class="line">    return image</div></pre></td></tr></table></figure></li></ul></li><li><p>多张图片处理</p><ul><li>因为训练和测试是都是使用<code>batch</code>的方式</li><li>调用上面处理单张图片的函数</li><li>tf.map_fn(fn, elems)函数，前面一般是<code>lambda</code>函数，后面是所有的数据<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">'''调用上面的函数，处理多个图片images'''</div><div class="line">def pre_process(images, training):</div><div class="line">    images = tf.map_fn(lambda image: pre_process_image(image, training), images)   # tf.map_fn()使用lambda函数</div><div class="line">    return images</div></pre></td></tr></table></figure></li></ul></li></ul><h3 id="4、定义tensorflow计算图"><a href="#4、定义tensorflow计算图" class="headerlink" title="4、定义tensorflow计算图"></a>4、定义tensorflow计算图</h3><ul><li>定义主网络图<ul><li>使用<code>prettytensor</code></li><li>分为<code>training</code>和<code>test</code>两个阶段</li></ul></li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line">'''定义主网络函数'''</div><div class="line">def main_network(images, training):</div><div class="line">    x_pretty = pt.wrap(images)</div><div class="line">    if training:</div><div class="line">        phase = pt.Phase.train</div><div class="line">    else:</div><div class="line">        phase = pt.Phase.infer</div><div class="line">    with pt.defaults_scope(activation_fn=tf.nn.relu, phase=phase):</div><div class="line">        y_pred, loss = x_pretty.\</div><div class="line">        conv2d(kernel=5, depth=64, name="layer_conv1", batch_normalize=True).\</div><div class="line">        max_pool(kernel=2, stride=2).\</div><div class="line">        conv2d(kernel=5, depth=64, name="layer_conv2").\</div><div class="line">        max_pool(kernel=2, stride=2).\</div><div class="line">        flatten().\</div><div class="line">        fully_connected(size=256, name="layer_fc1").\</div><div class="line">        fully_connected(size=128, name="layer_fc2").\</div><div class="line">        softmax_classifier(num_classes, labels=y_true)</div><div class="line">    return y_pred, loss</div></pre></td></tr></table></figure><ul><li><p>创建所有网络，包含<strong>预处理图片和主网络</strong></p><ul><li>需要使用<strong>variable_scope</strong>, 测试阶段需要<code>reuse</code>训练阶段的参数<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">'''创建所有网络, 包含预处理和主网络，'''</div><div class="line">def create_network(training):</div><div class="line">    # 使用variable_scope可以重复使用定义的变量，训练时创建新的，测试时重复使用</div><div class="line">    with tf.variable_scope("network", reuse=not training):</div><div class="line">        images = X</div><div class="line">        images = pre_process(images=images, training=training)</div><div class="line">        y_pred, loss = main_network(images=images, training=training)</div><div class="line">    return y_pred, loss</div></pre></td></tr></table></figure></li></ul></li><li><p>创建训练阶段网络</p><ul><li>定义一个<code>global_step</code>记录训练的次数，下面会将其保存到<code>checkpoint</code>,<code>trainable</code>为<code>False</code>就不会训练改变<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">'''训练阶段网络创建'''</div><div class="line">global_step = tf.Variable(initial_value=0, </div><div class="line">                          name="global_step",</div><div class="line">                          trainable=False) # trainable 在训练阶段不会改变</div><div class="line">_, loss = create_network(training=True)</div><div class="line">optimizer = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(loss, global_step)</div></pre></td></tr></table></figure></li></ul></li><li><p>定义测试阶段网络</p><ul><li>同时定义准确率</li></ul></li></ul><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="string">''</span><span class="string">'测试阶段网络创建'</span><span class="string">''</span></div><div class="line">y_pred, _ = create_network(training=False)</div><div class="line">y_pred_cls = tf.argmax(y_pred, dimension=<span class="number">1</span>)</div><div class="line">correct_prediction = tf.equal(y_pred_cls, y_true_cls)</div><div class="line">accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</div></pre></td></tr></table></figure><h3 id="5、获取权重和每层的输出值信息"><a href="#5、获取权重和每层的输出值信息" class="headerlink" title="5、获取权重和每层的输出值信息"></a>5、获取权重和每层的输出值信息</h3><ul><li>获取权重变量</li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">def get_weights_variable(layer_name):</div><div class="line">    with tf.variable_scope("network/" + layer_name, reuse=True):</div><div class="line">        variable = tf.get_variable("weights")</div><div class="line">    return variable </div><div class="line">weights_conv1 = get_weights_variable("layer_conv1")</div><div class="line">weights_conv2 = get_weights_variable("layer_conv2")</div></pre></td></tr></table></figure><ul><li>获取每层的输出变量</li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">def get_layer_output(layer_name):</div><div class="line">    tensor_name = "network/" + layer_name + "/Relu:0"</div><div class="line">    tensor = tf.get_default_graph().get_tensor_by_name(tensor_name)</div><div class="line">    return tensor</div><div class="line">output_conv1 = get_layer_output("layer_conv1")</div><div class="line">output_conv2 = get_layer_output("layer_conv2")</div></pre></td></tr></table></figure><h3 id="6、保存和加载计算图参数"><a href="#6、保存和加载计算图参数" class="headerlink" title="6、保存和加载计算图参数"></a>6、保存和加载计算图参数</h3><ul><li>因为第一次不会加载，所以放到<code>try</code>中判断</li></ul><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="string">''</span><span class="string">'执行tensorflow graph'</span><span class="string">''</span></div><div class="line">session = tf.Session()</div><div class="line">save_dir = <span class="string">"checkpoints/"</span></div><div class="line"><span class="keyword">if</span> not os<span class="selector-class">.path</span><span class="selector-class">.exists</span>(save_dir):</div><div class="line">    os.makedirs(save_dir)</div><div class="line">save_path = os<span class="selector-class">.path</span><span class="selector-class">.join</span>(save_dir, <span class="string">'cifat10_cnn'</span>)</div><div class="line"></div><div class="line"><span class="string">''</span><span class="string">'尝试存储最新的checkpoint, 可能会失败，比如第一次运行checkpoint不存在等'</span><span class="string">''</span></div><div class="line">try:</div><div class="line">    print(<span class="string">"开始存储最新的存储..."</span>)</div><div class="line">    last_chk_path = tf<span class="selector-class">.train</span><span class="selector-class">.latest_checkpoint</span>(save_dir)</div><div class="line">    saver.restore(session, save_path=last_chk_path)</div><div class="line">    print(<span class="string">"存储点来自："</span>, last_chk_path)</div><div class="line">except:</div><div class="line">    print(<span class="string">"存储错误, 初始化变量"</span>)</div><div class="line">    session.run(tf.global_variables_initializer())</div></pre></td></tr></table></figure><h3 id="7、训练"><a href="#7、训练" class="headerlink" title="7、训练"></a>7、训练</h3><ul><li>获取<code>batch</code></li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">'''SGD'''</div><div class="line">train_batch_size = 64</div><div class="line">def random_batch():</div><div class="line">    num_images = len(images_train)</div><div class="line">    idx = np.random.choice(num_images, size=train_batch_size, replace=False)</div><div class="line">    x_batch = images_train[idx, :, :, :]</div><div class="line">    y_batch = labels_train[idx, :]</div><div class="line">    return x_batch, y_batch</div></pre></td></tr></table></figure><ul><li>训练网络<ul><li>每1000次保存一下<code>checkpoint</code></li><li>因为上面会<code>restored</code>已经保存训练的网络，同时也保存了训练的次数，所以可以接着训练<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">def optimize(num_iterations):</div><div class="line">    start_time = time.time()</div><div class="line">    for i in range(num_iterations):</div><div class="line">        x_batch, y_batch = random_batch()</div><div class="line">        feed_dict_train = &#123;X: x_batch, y_true: y_batch&#125;</div><div class="line">        i_global, _ = session.run([global_step, optimizer], feed_dict=feed_dict_train)</div><div class="line">        if (i_global%100==0) or (i == num_iterations-1):</div><div class="line">            batch_acc = session.run(accuracy, feed_dict=feed_dict_train)</div><div class="line">            msg = "global step: &#123;0:&gt;6&#125;, training batch accuracy: &#123;1:&gt;6.1%&#125;"</div><div class="line">            print(msg.format(i_global, batch_acc))</div><div class="line">        if(i_global%1000==0) or (i==num_iterations-1):</div><div class="line">            saver.save(session, save_path=save_path,</div><div class="line">                       global_step=global_step)</div><div class="line">            print("保存checkpoint")</div><div class="line">    end_time = time.time()</div><div class="line">    time_diff = end_time-start_time</div><div class="line">    print("耗时：", str(timedelta(seconds=int(round(time_diff)))))</div></pre></td></tr></table></figure></li></ul></li></ul><h2 id="十三、Inception-model-GoogleNet"><a href="#十三、Inception-model-GoogleNet" class="headerlink" title="十三、Inception model (GoogleNet)"></a>十三、Inception model (GoogleNet)</h2><ul><li><a href="https://github.com/lawlite19/MachineLearning_TensorFlow/blob/master/Inception_model/InceptionModel_pretrained.py" target="_blank" rel="external">全部代码</a></li><li>使用训练好的<code>inception model</code>,因为模型很复杂，一般的电脑运行不起来的。</li><li>网络结构</li></ul><p><img src="https://raw.githubusercontent.com/lawlite19/MachineLearning_TensorFlow/master/images/07_inception_flowchart.png" alt="inception model(Google Net)" title="07_inception_flowchart"></p><h3 id="1、下载和加载inception-model"><a href="#1、下载和加载inception-model" class="headerlink" title="1、下载和加载inception model"></a>1、下载和加载inception model</h3><ul><li>因为是预训练好的模型，所以无需我们定义结构了</li><li><p>导入包</p><ul><li>这里 <code>inception</code>是别人实现好的下载的代码<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">import numpy as np</div><div class="line">import tensorflow as tf</div><div class="line">from matplotlib import pyplot as plt</div><div class="line">import inception # 第三方类加载inception model</div><div class="line">import os</div></pre></td></tr></table></figure></li></ul></li><li><p>下载和加载模型</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="string">''</span><span class="string">'下载和加载inception model'</span><span class="string">''</span></div><div class="line">inception.maybe_download()</div><div class="line">model = inception.Inception()</div></pre></td></tr></table></figure></li><li><p>预测和显示图片函数</p></li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">'''预测和显示图片'''</div><div class="line">def classify(image_path):</div><div class="line">    plt.imshow(plt.imread(image_path))</div><div class="line">    plt.show()</div><div class="line">    pred = model.classify(image_path=image_path)</div><div class="line">    model.print_scores(pred=pred, k=10, only_first_name=True)</div></pre></td></tr></table></figure><ul><li>显示调整后的图片<ul><li>因为 <code>inception model</code>要求输入图片为 <code>299*299</code> 像素的，所以它会<code>resize</code>成这个大小然后作为输入</li></ul></li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">'''显示处理后图片的样式'''</div><div class="line">def plot_resized_image(image_path):</div><div class="line">    resized_image = model.get_resized_image(image_path)</div><div class="line">    plt.imshow(resized_image, interpolation='nearest')</div><div class="line">    plt.show()</div><div class="line">plot_resized_image(image_path)</div></pre></td></tr></table></figure><h2 id="十四、迁移学习-Transfer-Learning"><a href="#十四、迁移学习-Transfer-Learning" class="headerlink" title="十四、迁移学习 Transfer Learning"></a>十四、迁移学习 Transfer Learning</h2><ul><li><a href="30">全部代码</a></li><li>网络结构还是使用上一节的<code>inception model</code>, 去掉最后的全连接层，然后重新构建全连接层进行训练<ul><li>因为<code>inception model</code> 是训练好的，前面的卷积层用于捕捉<strong>特征</strong>, 而后面的全连接层可用于<strong>分类</strong>，所以我们<strong>训练全连接层</strong>即可</li></ul></li><li>因为要计算每张图片的<code>transfer values</code>,所以使用一个<code>cache</code>缓存<code>transfer-values</code>，第一次计算完成后，后面重新运行直接读取存储的结果，这样比较节省时间<ul><li><code>transfer values</code>是<code>inception model</code>在<code>Softmax</code>层前一层的值</li><li><code>cifar-10</code>数据集, 我放在实验室电脑上运行了<strong>几个小时</strong>才得到<code>transfer values</code>，还是比较慢的</li></ul></li><li>总之最后相当于训练<strong>下面</strong>的神经网络，对应的 <code>transfer-values</code>作为输入<br><img src="https://raw.githubusercontent.com/lawlite19/MachineLearning_TensorFlow/master/images/08_transfer_learning_flowchart.png" alt="transfer learning-inception model" title="08_transfer_learning_flowchart"></li></ul><h3 id="1、准备工作"><a href="#1、准备工作" class="headerlink" title="1、准备工作"></a>1、准备工作</h3><ul><li><p>导入包</p><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">import numpy as np</div><div class="line">import tensorflow as tf</div><div class="line">import prettytensor as pt</div><div class="line">from matplotlib import pyplot as plt</div><div class="line">import time</div><div class="line">from datetime import timedelta</div><div class="line">import os</div><div class="line">import inception   # 第三方下载inception model的代码</div><div class="line">from inception import transfer_values_cache  # cache</div><div class="line">import cifar10     # 也是第三方的库，下载cifar-10数据集</div><div class="line">from cifar10 import num_classes</div></pre></td></tr></table></figure></li><li><p>下载<code>cifar-10</code>数据集</p></li></ul><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="string">''</span><span class="string">'下载cifar-10数据集'</span><span class="string">''</span></div><div class="line">cifar10.maybe_download_and_extract()</div><div class="line">class_names = cifar10.load_class_names()</div><div class="line"><span class="function"><span class="title">print</span><span class="params">(<span class="string">"所有类别是："</span>,class_names)</span></span></div><div class="line"><span class="string">''</span><span class="string">'训练和测试集'</span><span class="string">''</span></div><div class="line">images_train, cls_train, labels_train = cifar10.load_training_data()</div><div class="line">images_test,  cls_test,  labels_test  = cifar10.load_test_data()</div></pre></td></tr></table></figure><ul><li>下载和加载<code>inception model</code></li></ul><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="string">''</span><span class="string">'下载inception model'</span><span class="string">''</span></div><div class="line">inception.maybe_download()</div><div class="line">model = inception.Inception()</div></pre></td></tr></table></figure><ul><li><p>计算<code>cifar-10</code>训练集和测试集在<code>inception model</code>上的<code>transfer values</code></p><ul><li>因为计算非常耗时，这里第一次运行存储到本地，以后再运行直接读取即可</li><li><code>transfer values</code>的<code>shape</code>是<code>(dataset size, 2048)</code>，因为是<code>softmax</code>层的前一层<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">'''训练和测试的cache的路径'''</div><div class="line">file_path_cache_train = os.path.join(cifar10.data_path, 'inception_cifar10_train.pkl')</div><div class="line">file_path_cache_test = os.path.join(cifar10.data_path, 'inception_cifar10_test.pkl')</div><div class="line"></div><div class="line">print('处理训练集上的transfer-values.......... ')</div><div class="line">image_scaled = images_train * 255.0  # cifar-10的pixel是0-1的, shape=(50000, 32, 32, 3)</div><div class="line">transfer_values_train = transfer_values_cache(cache_path=file_path_cache_train,</div><div class="line">                                              images=image_scaled, </div><div class="line">                                              model=model)  # shape=(50000, 2048)</div><div class="line">print('处理测试集上的transfer-values.......... ')</div><div class="line">images_scaled = images_test * 255.0</div><div class="line">transfer_values_test = transfer_values_cache(cache_path=file_path_cache_test,</div><div class="line">                                             model=model,</div><div class="line">                                             images=images_scaled)</div><div class="line">print("transfer_values_train: ",transfer_values_train.shape)</div><div class="line">print("transfer_values_test: ",transfer_values_test.shape)</div></pre></td></tr></table></figure></li></ul></li><li><p>可视化一张图片对应的<code>transfer values</code></p></li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">'''显示transfer values'''</div><div class="line">def plot_transfer_values(i):</div><div class="line">    print("输入图片：")</div><div class="line">    plt.imshow(images_test[i], interpolation='nearest')</div><div class="line">    plt.show()</div><div class="line">    print('transfer values --&gt; 此图片在inception model上')</div><div class="line">    img = transfer_values_test[i]</div><div class="line">    img = img.reshape((32, 64))</div><div class="line">    plt.imshow(img, interpolation='nearest', cmap='Reds')</div><div class="line">    plt.show()</div><div class="line">plot_transfer_values(16)</div></pre></td></tr></table></figure><h3 id="2、分析transfer-values"><a href="#2、分析transfer-values" class="headerlink" title="2、分析transfer values"></a>2、分析<code>transfer values</code></h3><h4 id="1-使用PCA主成分分析"><a href="#1-使用PCA主成分分析" class="headerlink" title="(1) 使用PCA主成分分析"></a>(1) 使用PCA主成分分析</h4><ul><li>将数据<strong>降到2维</strong>，可视化，因为<code>transfer values</code>是已经捕捉到的<strong>特征</strong>，所以可视化应该是可以隐约看到<strong>不同类别的数据是有区别的</strong></li><li>取<code>3000</code>个数据观察（因为<code>PCA</code>也是比较耗时的）</li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">'''使用PCA分析transfer values'''</div><div class="line">from sklearn.decomposition import PCA</div><div class="line">pca = PCA(n_components=2)</div><div class="line">transfer_values = transfer_values_train[0:3000]  # 取3000个，大的话计算量太大</div><div class="line">cls = cls_train[0:3000]</div><div class="line">print(transfer_values.shape)</div><div class="line">transfer_values_reduced = pca.fit_transform(transfer_values)</div><div class="line">print(transfer_values_reduced.shape)</div></pre></td></tr></table></figure><ul><li>可视化降维后的数据</li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">## 显示降维后的transfer values</div><div class="line">def plot_scatter(values, cls):</div><div class="line">    from matplotlib import cm as cm</div><div class="line">    cmap = cm.rainbow(np.linspace(0.0, 1.0, num_classes))</div><div class="line">    colors = cmap[cls]</div><div class="line">    x = values[:, 0]</div><div class="line">    y = values[:, 1]</div><div class="line">    plt.scatter(x, y, color=colors)</div><div class="line">    plt.show()</div><div class="line">plot_scatter(transfer_values_reduced, cls)</div></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/lawlite19/MachineLearning_TensorFlow/master/images/08_transfer_learning_pca_visualize.png" alt="pca 降维后可视化transfer values" title="08_transfer_learning_pca_visualize"></p><h4 id="2-使用TSNE主成分分析"><a href="#2-使用TSNE主成分分析" class="headerlink" title="(2) 使用TSNE主成分分析"></a>(2) 使用TSNE主成分分析</h4><ul><li>因为<code>t-SNE</code>运行非常慢，所以这里先用<code>PCA</code>将到<strong>50维</strong></li></ul><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">from sklearn<span class="selector-class">.manifold</span> import TSNE</div><div class="line">pca = PCA(n_components=<span class="number">50</span>)</div><div class="line">transfer_values_50d = pca.fit_transform(transfer_values)</div><div class="line">tsne = TSNE(n_components=<span class="number">2</span>)</div><div class="line">transfer_values_reduced = tsne.fit_transform(transfer_values_50d)</div><div class="line"><span class="function"><span class="title">print</span><span class="params">(<span class="string">"最终降维后："</span>, transfer_values_reduced.shape)</span></span></div><div class="line"><span class="function"><span class="title">plot_scatter</span><span class="params">(transfer_values_reduced, cls)</span></span></div></pre></td></tr></table></figure><ul><li>数据区分还是比较明显的<br><img src="https://raw.githubusercontent.com/lawlite19/MachineLearning_TensorFlow/master/images/08_transfer_learning_pca_visualize_02.png" alt="t-SNE降维后可视化transfer values" title="08_transfer_learning_pca_visualize_02"></li></ul><h3 id="3、创建我们自己的网络"><a href="#3、创建我们自己的网络" class="headerlink" title="3、创建我们自己的网络"></a>3、创建我们自己的网络</h3><ul><li>使用<code>prettytensor</code>创建一个全连接层，使用<code>softmax</code>作为分类</li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">'''创建网络'''</div><div class="line">transfer_len = model.transfer_len   # 获取transfer values的大小，这里是2048</div><div class="line">x = tf.placeholder(tf.float32, shape=[None, transfer_len], name="x")</div><div class="line">y_true = tf.placeholder(tf.float32, shape=[None, num_classes], name="y")</div><div class="line">y_true_cls = tf.argmax(y_true, axis=1)</div><div class="line">x_pretty = pt.wrap(x)</div><div class="line">with pt.defaults_scope(activation_fn=tf.nn.relu):</div><div class="line">    y_pred, loss = x_pretty.\</div><div class="line">        fully_connected(1024, name="layer_fc1").\</div><div class="line">        softmax_classifier(num_classes, labels=y_true)</div></pre></td></tr></table></figure><ul><li>优化器</li></ul><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="string">''</span><span class="string">'优化器'</span><span class="string">''</span></div><div class="line">global_step = tf.Variable(initial_value=<span class="number">0</span>, name=<span class="string">"global_step"</span>, trainable=False)</div><div class="line">optimizer = tf<span class="selector-class">.train</span><span class="selector-class">.AdamOptimizer</span>(<span class="number">0.0001</span>).minimize(loss, global_step)</div></pre></td></tr></table></figure><ul><li>准确度</li></ul><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="string">''</span><span class="string">'accuracy'</span><span class="string">''</span></div><div class="line">y_pred_cls = tf.argmax(y_pred, axis=<span class="number">1</span>)</div><div class="line">correct_prediction = tf.equal(y_pred_cls, y_true_cls)</div><div class="line">accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</div></pre></td></tr></table></figure><ul><li><code>SGD</code>训练</li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line">'''SGD 训练'''</div><div class="line">session = tf.Session()</div><div class="line">session.run(tf.initialize_all_variables())</div><div class="line">train_batch_size = 64</div><div class="line">def random_batch():</div><div class="line">    num_images = len(images_train)</div><div class="line">    idx = np.random.choice(num_images, </div><div class="line">                           size=train_batch_size,</div><div class="line">                           replace=False)</div><div class="line">    x_batch = transfer_values_train[idx]</div><div class="line">    y_batch = labels_train[idx]</div><div class="line">    return x_batch, y_batch</div><div class="line">def optimize(num_iterations):</div><div class="line">    start_time = time.time()</div><div class="line">    for i in range(num_iterations):</div><div class="line">        x_batch, y_true_batch = random_batch()</div><div class="line">        feed_dict_train = &#123;x: x_batch,</div><div class="line">                           y_true: y_true_batch&#125;</div><div class="line">        i_global, _ = session.run([global_step, optimizer], feed_dict=feed_dict_train)</div><div class="line">        if (i_global % 100 == 0) or (i==num_iterations-1):</div><div class="line">            batch_acc = session.run(accuracy, feed_dict=feed_dict_train)</div><div class="line">            msg = "Global Step: &#123;0:&gt;6&#125;, Training Batch Accuracy: &#123;1:&gt;6.1%&#125;"</div><div class="line">            print(msg.format(i_global, batch_acc))            </div><div class="line">    end_time = time.time()</div><div class="line">    time_diff = end_time - start_time</div><div class="line">    print("耗时：", str(timedelta(seconds=int(round(time_diff)))))</div></pre></td></tr></table></figure><ul><li>使用<code>batch size</code>预测测试集数据</li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">'''batch 预测'''</div><div class="line">batch_size = 256</div><div class="line">def predict_cls(transfer_values, labels, cls_true):</div><div class="line">    num_images = len(images_test)</div><div class="line">    cls_pred = np.zeros(shape=num_images, dtype=np.int)</div><div class="line">    i = 0</div><div class="line">    while i &lt; num_images:</div><div class="line">        j = min(i + batch_size, num_images)</div><div class="line">        feed_dict = &#123;x: transfer_values[i:j],</div><div class="line">                     y_true: labels[i:j]&#125;</div><div class="line">        cls_pred[i:j] = session.run(y_pred_cls, feed_dict=feed_dict)</div><div class="line">        i = j</div><div class="line">    correct = (cls_true == cls_pred)</div><div class="line">    return correct, cls_pred</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;ul&gt;
&lt;li&gt;github地址：&lt;a href=&quot;https://github.com/lawlite19/MachineLearning_TensorFlow&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://github.com/lawlite19/MachineLearning_TensorFlow&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;一、TensorFlow介绍&quot;&gt;&lt;a href=&quot;#一、TensorFlow介绍&quot; class=&quot;headerlink&quot; title=&quot;一、TensorFlow介绍&quot;&gt;&lt;/a&gt;一、TensorFlow介绍&lt;/h2&gt;
    
    </summary>
    
    
      <category term="DeepLearning" scheme="http://lawlite.cn/tags/DeepLearning/"/>
    
      <category term="Tensorflow" scheme="http://lawlite.cn/tags/Tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>Python科学计算</title>
    <link href="http://lawlite.cn/2016/11/09/Python%E7%A7%91%E5%AD%A6%E8%AE%A1%E7%AE%97/"/>
    <id>http://lawlite.cn/2016/11/09/Python科学计算/</id>
    <published>2016-11-09T14:25:43.000Z</published>
    <updated>2017-06-25T08:49:02.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一、Numpy"><a href="#一、Numpy" class="headerlink" title="一、Numpy"></a>一、Numpy</h2><h3 id="1、Numpy特征和导入"><a href="#1、Numpy特征和导入" class="headerlink" title="1、Numpy特征和导入"></a>1、Numpy特征和导入</h3><ul><li>（1）用于多维数组的第三方Python包</li><li>（2）更接近于底层和硬件 (高效)</li><li>（3）专注于科学计算 (方便)</li><li>（4）导入包：<code>import numpy as np</code>     </li></ul><a id="more"></a><h3 id="2、list转为数组"><a href="#2、list转为数组" class="headerlink" title="2、list转为数组"></a>2、list转为数组</h3><ul><li>（1）<code>a = np.array([0,1,2,3])</code></li><li>（2）输出为：<code>[0 1 2 3]</code></li><li>（3）数据类型：<code>&lt;type &#39;numpy.ndarray&#39;&gt;</code></li></ul><h3 id="3、一维数组"><a href="#3、一维数组" class="headerlink" title="3、一维数组"></a>3、一维数组</h3><ul><li>（1）<code>a = np.array([1,2,3,4])</code>属性<br><code>a.ndim</code>–&gt;维度为1<br><code>a.shape</code>–&gt;形状，返回<code>(4,)</code><br><code>len(a)</code>–&gt;长度，4</li><li>（2）访问数组<br><code>a[1:5:2]</code>下标1-5，下标关系+2</li><li><p>（3）逆序</p><p>  <code>a[::-1]</code></p></li></ul><h3 id="4、多维数组"><a href="#4、多维数组" class="headerlink" title="4、多维数组"></a>4、多维数组</h3><ul><li><p>（1）二维：<code>a = np.array([[0,1,2,3],[1,2,3,4]])</code><br>输出为：</p><p>  [[0 1 2 3]<br>   [1 2 3 4]]<br>a.ndm   –&gt;2<br>a.shape –&gt;(2,4)–&gt;行数，列数<br>len(a)  –&gt;2–&gt;第一维大小</p></li><li>（2）三维：<code>a = np.array([[[0],[1]],[[2],[4]]])</code><br><code>a.shape</code>–&gt;(2,2,1)</li></ul><h3 id="5、用函数创建数组"><a href="#5、用函数创建数组" class="headerlink" title="5、用函数创建数组"></a>5、用函数创建数组</h3><ul><li><p>（1）<code>np.arange()</code></p><p>  a = np.arange(0, 10)<br>b = np.arange(10)<br>c = np.arange(0,10,2)<br>输出：</p><p>  [0 1 2 3 4 5 6 7 8 9]<br>[0 1 2 3 4 5 6 7 8 9]<br>[0 2 4 6 8]</p></li><li>（2）<code>np.linspace(start, stop, num=50, endpoint=True, retstep=False, dtype=None)</code><br>等距离产生num个数</li><li>（3）<code>np.logspace(start, stop, num=50, endpoint=True, base=10.0, dtype=None)</code><br>以log函数取</li></ul><h3 id="6、常用数组"><a href="#6、常用数组" class="headerlink" title="6、常用数组"></a>6、常用数组</h3><ul><li><p>（1）<code>a = np.ones((3,3))</code><br>输出：</p><p>  [[ 1.  1.  1.]<br>[ 1.  1.  1.]<br>[ 1.  1.  1.]]</p></li><li><p>（2）<code>np.zeros((3,3))</code></p></li><li>（3）<code>np.eye(2)</code>单位矩阵</li><li>（4）<code>np.diag([1,2,3],k=0)</code>对角矩阵，k为对角线的偏移</li></ul><h3 id="7、随机数矩阵"><a href="#7、随机数矩阵" class="headerlink" title="7、随机数矩阵"></a>7、随机数矩阵</h3><ul><li>（1）<code>a = np.random.rand(4)</code><br>输出：<code>[ 0.99890402  0.41171695  0.40725671  0.42501804]</code>范围在[0,1]之间</li><li>（2）<code>a = np.random.randn(4)</code> Gaussian函数，</li><li><p>（3）生成100个0-m的随机数:  <code>[t for t in [np.random.randint(x-x, m) for x in range(100)]]</code> </p><ul><li>也可以<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">m_arr = np.arange(0,m)      # 生成0-m-1</div><div class="line">np.random.shuffle(m_arr)    # 打乱m_arr顺序</div></pre></td></tr></table></figure></li></ul><p>然后取前100个即可</p></li></ul><h3 id="8、查看数据类型"><a href="#8、查看数据类型" class="headerlink" title="8、查看数据类型"></a>8、查看数据类型</h3><ul><li>（1）<code>a.dtype</code></li></ul><h3 id="9、数组复制"><a href="#9、数组复制" class="headerlink" title="9、数组复制"></a>9、数组复制</h3><ul><li>（1）共享内存<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">a = np.array([1,2,3,4,5])</div><div class="line">b = a</div><div class="line">print np.may_share_memory(a,b)</div></pre></td></tr></table></figure></li></ul><p>输出：True<br>说明使用的同一个存储区域，修改一个数组同时另外的也会修改</p><ul><li>（2）不共享内存<br><code>b = a.copy()</code></li></ul><h3 id="10、布尔型"><a href="#10、布尔型" class="headerlink" title="10、布尔型"></a>10、布尔型</h3><ul><li>（1）<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">a = np.random.random_integers(0,20,5)</div><div class="line">print a</div><div class="line">print a%3==0</div><div class="line">print a[a % 3 == 0]</div></pre></td></tr></table></figure></li></ul><p>输出：<br>    [14  3  6 15  4]<br>    [False  True  True  True False]<br>    [ 3  6 15]</p><h3 id="11、中间数、平均值"><a href="#11、中间数、平均值" class="headerlink" title="11、中间数、平均值"></a>11、中间数、平均值</h3><ul><li>（1）中间数<code>np.median(a)</code></li><li>（2）平均值<code>np.mean(a)</code>,<ul><li>若是矩阵，不指定<code>axis</code>默认求所有元素的均值</li><li><code>axis=0</code>,求列的均值</li><li><code>axis=1</code>，求行的均值</li></ul></li></ul><h3 id="12、矩阵操作"><a href="#12、矩阵操作" class="headerlink" title="12、矩阵操作"></a>12、矩阵操作</h3><ul><li>（1）乘积<code>np.dot(a,b)</code><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">a = np.array([[1,2,3],[2,3,4]])</div><div class="line">b = np.array([[1,2],[2,3],[2,2]])</div><div class="line">print np.dot(a,b)</div></pre></td></tr></table></figure></li></ul><p>或者使用<code>np.matrix()</code>生成矩阵，相乘需要满足矩阵相乘的条件</p><ul><li><p>（2）内积<code>np.inner(a,b)</code><br>行相乘</p></li><li><p>（3）逆矩阵<code>np.linalg.inv(a)</code></p></li><li>（4）列的最大值<code>np.max(a[:,0])</code>–&gt;返回第一列的最大值</li><li>（5）每列的和<code>np.sum(a,0)</code></li><li>（6）每行的平均数<code>np.mean(a,1)</code></li><li>（7）求交集<code>p.intersect1d(a,b)</code>，返回一维数组</li><li>（8）转置：<code>np.transpose(a)</code></li><li>（9）两个矩阵对应对应元素相乘（点乘）：<code>a*b</code></li></ul><h3 id="13、文件操作"><a href="#13、文件操作" class="headerlink" title="13、文件操作"></a>13、文件操作</h3><ul><li>（1）保存：<code>tofile()</code><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">a = np.arange(10)</div><div class="line">a.shape=2,5</div><div class="line">a.tofile(&quot;test.bin&quot;)</div></pre></td></tr></table></figure></li></ul><p>读取：（需要注意指定保存的数据类型）<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">a = np.fromfile(&quot;test.bin&quot;,dtype=np.int32)</div><div class="line">print a</div></pre></td></tr></table></figure></p><ul><li>（2）保存：<code>np.save(&quot;test&quot;,a)</code>–&gt;会保存成test.npy文件<br>读取：<code>a = np.load(&quot;test&quot;)</code></li></ul><h3 id="14、组合两个数组"><a href="#14、组合两个数组" class="headerlink" title="14、组合两个数组"></a>14、组合两个数组</h3><ul><li><p>（1）垂直组合</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">a = np.array([1,2,3])</div><div class="line">b = np.array([[1,2,3],[4,5,6]])</div><div class="line"></div><div class="line">c = np.vstack((b,a))</div></pre></td></tr></table></figure></li><li><p>（2）水平组合</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">a = np.array([[1,2],[3,4]])</div><div class="line">b = np.array([[1,2,3],[4,5,6]])</div><div class="line"></div><div class="line">c = np.hstack((a,b))</div></pre></td></tr></table></figure></li></ul><h3 id="15、读声音Wave文件"><a href="#15、读声音Wave文件" class="headerlink" title="15、读声音Wave文件"></a>15、读声音Wave文件</h3><ul><li>（1）<code>wave</code>     <figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div></pre></td><td class="code"><pre><div class="line">import wave</div><div class="line">from matplotlib import pyplot as plt</div><div class="line">import numpy as np</div><div class="line"></div><div class="line"># 打开WAV文档</div><div class="line">f = wave.open(r&quot;c:\WINDOWS\Media\ding.wav&quot;, &quot;rb&quot;)</div><div class="line"></div><div class="line"># 读取格式信息</div><div class="line"># (nchannels, sampwidth, framerate, nframes, comptype, compname)</div><div class="line">params = f.getparams()</div><div class="line">nchannels, sampwidth, framerate, nframes = params[:4]</div><div class="line"></div><div class="line"># 读取波形数据</div><div class="line">str_data = f.readframes(nframes)</div><div class="line">f.close()</div><div class="line"></div><div class="line">#将波形数据转换为数组</div><div class="line">wave_data = np.fromstring(str_data, dtype=np.short)</div><div class="line">wave_data.shape = -1, 2</div><div class="line">wave_data = wave_data.T</div><div class="line">time = np.arange(0, nframes) * (1.0 / framerate)</div><div class="line"></div><div class="line"># 绘制波形</div><div class="line">plt.subplot(211) </div><div class="line">plt.plot(time, wave_data[0])</div><div class="line">plt.subplot(212) </div><div class="line">plt.plot(time, wave_data[1], c=&quot;g&quot;)</div><div class="line">plt.xlabel(&quot;time (seconds)&quot;)</div><div class="line">plt.show()</div></pre></td></tr></table></figure></li></ul><h3 id="16、where"><a href="#16、where" class="headerlink" title="16、where"></a>16、<code>where</code></h3><ul><li>（1）找到y数组中=1的位置：<code>np.where(y==1)</code></li></ul><h3 id="17、np-ravel-y"><a href="#17、np-ravel-y" class="headerlink" title="17、np.ravel(y)"></a>17、<code>np.ravel(y)</code></h3><ul><li>将二维的转化为一维的，eg:<code>(5000,1)--&gt;(5000,)</code></li></ul><h3 id="18、ndarray-flat函数"><a href="#18、ndarray-flat函数" class="headerlink" title="18、ndarray.flat函数"></a>18、ndarray.flat函数</h3><ul><li>将数据展开对应的数组，可以进行访问</li><li>应用：0/1映射<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">def dense_to_one_hot(label_dense,num_classes):</div><div class="line">    num_labels = label_dense.shape[0]</div><div class="line">    index_offset = np.arange(num_labels)*num_classes</div><div class="line">    labels_one_hot = numpy.zeros((num_labels, num_classes))</div><div class="line">    labels_one_hot.flat[index_offset + labels_dense.ravel()] = 1</div><div class="line">    return labels_one_hot</div></pre></td></tr></table></figure></li></ul><h3 id="19、数组访问"><a href="#19、数组访问" class="headerlink" title="19、数组访问"></a>19、数组访问</h3><ul><li>X = np.array([[1,2],[3,4]])<ul><li><code>X[0:1]和X[0:1,:]</code>等价，都是系那是第一行数据</li></ul></li></ul><h3 id="20、np-c"><a href="#20、np-c" class="headerlink" title="20、np.c_()"></a>20、<code>np.c_()</code></h3><ul><li>按照第二维度，即列拼接数据<ul><li><code>np.c_[np.array([[1,2,3]]), 0, 0, np.array([[4,5,6]])]</code><br>输出：<code>array([[1, 2, 3, 0, 0, 4, 5, 6]])</code></li></ul></li><li>两个列表list拼接，长度要一致<ul><li><code>np.c_[[1,2,3],[2,3,4]]</code></li><li><code>np.c_[range(1,5),range(2,6)]</code></li></ul></li></ul><h2 id="二、Matplotlib"><a href="#二、Matplotlib" class="headerlink" title="二、Matplotlib"></a>二、Matplotlib</h2><h3 id="1、关于pyplot"><a href="#1、关于pyplot" class="headerlink" title="1、关于pyplot"></a>1、关于<code>pyplot</code></h3><ul><li>（1）matplotlib的pyplot子库提供了和matlab类似的绘图API，方便用户快速绘制2D图表。</li><li>（2）导入包：<code>from matplotlib import pyplot as plt</code></li></ul><h3 id="2、绘图基础"><a href="#2、绘图基础" class="headerlink" title="2、绘图基础"></a>2、绘图基础</h3><ul><li><p>（1）sin和cos</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">x = np.linspace(-np.pi, np.pi,256,endpoint=True)</div><div class="line">C,S = np.cos(x),np.sin(x)</div><div class="line">plt.plot(x,C)</div><div class="line">plt.plot(x,S)</div><div class="line">plt.xlabel(&quot;x&quot;)</div><div class="line">plt.ylabel(&quot;y&quot;)</div><div class="line">plt.show()</div></pre></td></tr></table></figure></li><li><p>（2）指定绘图的大小</p><p>  plt.figure(figsize=(8,6), dpi=80)</p></li><li><p>（3）指定线的颜色、粗细和类型</p><p>  plt.plot(x,C,color=”blue”,linewidth=2.0,linestyle=”-“,label=”cos”)<br>  蓝色、宽度、连续、label（使用legend会显示这个label）</p></li><li><p>（4）指定x坐标轴范围</p><p>  plt.xlim(-4.0,4.0)</p></li><li>（5）设置y抽刻度间隔<br><code>plt.yticks(np.linspace(-1, 1, 15, endpoint=True))</code></li><li><p>（6）显示图例</p><p>  plt.legend(loc=”upper left”)<br>  显示在左上方</p></li><li><p>（7）一个figure上画多个图subplot方式</p><p>  plt.subplot(1, 2, 1)<br>  plt.subplot(1, 2, 2)<br>  例如：plt.subplot(m, n, p)<br>  代表图共有的m行，n列，第p个图<br>  p是指第几个图，横向数<br>  上面代表有一行，两个图</p><ul><li>[更详细解释]：<br>231,232,233表示第一行的1,2,3个位置，接着的223表示把整个矩形分成4分，所以第3个位置就是第二行的第一个位置，但是相比第一行占了1.5列<br>（每次subplot划分都是整个图重新划分）</li></ul></li><li><p>（8）一个figure上画多个图，axes方式</p><p>  plt.axes([.1, .1, .8, .8])<br>  plt.axes([.2, .2, .3, .3])</p></li><li><p>（9）填充</p><p>  plt.fill_between(x, y1, y2=0, where=None, interpolate=False, step=None, </p><pre><code>hold=None, data=None)</code></pre><p>  eg:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">plt.fill_between(X, 1, C+1, C+1&gt;1,color=&quot;red&quot;)</div><div class="line">plt.fill_between(X, 1, C+1, C+1&lt;1,color=&quot;blue&quot;)</div></pre></td></tr></table></figure></li></ul><h3 id="3、散点图"><a href="#3、散点图" class="headerlink" title="3、散点图"></a>3、散点图</h3><ul><li><p>（1）</p><p>  <code>plt.scatter(X,Y)</code></p></li></ul><h3 id="4、条形图"><a href="#4、条形图" class="headerlink" title="4、条形图"></a>4、条形图</h3><ul><li><p>（1）</p><p>  <code>plt.bar(X, Y, facecolor=&quot;red&quot;, edgecolor=&quot;blue&quot; )</code><br>  填充颜色为facecolor,边界颜色为edgecolor</p></li></ul><h3 id="5、等高线图"><a href="#5、等高线图" class="headerlink" title="5、等高线图"></a>5、等高线图</h3><ul><li>（1）只显示等高线<code>contour</code></li><li>（2）显示表面<code>contourf</code></li><li>（3）注意三维图要用到<code>meshgrid</code>转化为网格<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">def f(x,y):</div><div class="line">    return (1 - x / 2 + x**5 + y**3) * np.exp(-x**2 -y**2)</div><div class="line"></div><div class="line">n = 256</div><div class="line">x = np.linspace(-3,3,n)</div><div class="line">y = np.linspace(-3,3,n)</div><div class="line">X,Y = np.meshgrid(x,y)</div><div class="line"></div><div class="line">plt.contourf(X,Y,f(X,Y),alpha=.5)</div><div class="line">C = plt.contour(X,Y,f(X, Y),colors=&quot;black&quot;,linewidth=.5)</div><div class="line">plt.clabel(C)</div><div class="line"></div><div class="line">plt.show()</div></pre></td></tr></table></figure></li></ul><h3 id="6、显示图片imshow"><a href="#6、显示图片imshow" class="headerlink" title="6、显示图片imshow"></a>6、显示图片<code>imshow</code></h3><ul><li>（1）<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">def f(x,y):</div><div class="line">return (1 - x / 2 + x ** 5 + y ** 3 ) * np.exp(-x ** 2 - y ** 2)</div><div class="line">n = 10</div><div class="line">x = np.linspace(-3, 3, 3.5 * n)</div><div class="line">y = np.linspace(-3, 3, 3.0 * n)</div><div class="line">X, Y = np.meshgrid(x, y)</div><div class="line">z = f(X,Y)</div><div class="line">plt.imshow(z)</div><div class="line">plt.show()</div></pre></td></tr></table></figure></li></ul><h3 id="7、饼图pie"><a href="#7、饼图pie" class="headerlink" title="7、饼图pie"></a>7、饼图<code>pie</code></h3><ul><li>（1）传入一个序列<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">plt.figure(figsize=(8,8))</div><div class="line">n = 20</div><div class="line">Z = np.arange(10)</div><div class="line">plt.pie(Z)</div><div class="line">plt.show()</div></pre></td></tr></table></figure></li></ul><h3 id="8、三维表面图"><a href="#8、三维表面图" class="headerlink" title="8、三维表面图*"></a>8、三维表面图*</h3><ul><li>（1）需要导入包：<code>from mpl_toolkits.mplot3d import Axes3D</code></li><li>（2）<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">fig = plt.figure()</div><div class="line">ax = Axes3D(fig)</div><div class="line">X = np.arange(-4, 4, 0.25)</div><div class="line">Y = np.arange(-4, 4, 0.25)</div><div class="line">X, Y = np.meshgrid(X, Y)</div><div class="line">R = np.sqrt(X ** 2 + Y ** 2)</div><div class="line">Z = np.sin(R)</div><div class="line">ax.plot_surface(X, Y, Z, rstride=1, cstride=1, cmap=plt.cm.hot)</div><div class="line">ax.contourf(X, Y, Z, zdir=&apos;z&apos;, offset=-2, cmap=plt.cm.hot)</div><div class="line">ax.set_zlim(-2, 2)</div><div class="line">plt.show()</div></pre></td></tr></table></figure></li></ul><h3 id="9、legend显示问题"><a href="#9、legend显示问题" class="headerlink" title="9、legend显示问题"></a>9、legend显示问题</h3><ul><li>（1）<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">p1, = plt.plot(np.ravel(X[pos,0]),np.ravel(X[pos,1]),&apos;ro&apos;,markersize=8)</div><div class="line">p2, = plt.plot(np.ravel(X[neg,0]),np.ravel(X[neg,1]),&apos;g^&apos;,markersize=8)</div><div class="line">plt.xlabel(&quot;X1&quot;)</div><div class="line">plt.ylabel(&quot;X2&quot;)</div><div class="line">plt.legend([p1,p2],[&quot;y==1&quot;,&quot;y==0&quot;])</div></pre></td></tr></table></figure></li></ul><p><strong>注意</strong> p1后要加上<code>,</code>逗号，里面的数据要是<strong>一维</strong>的，使用<code>np.ravel()</code>转化一下</p><h3 id="10、显示网格"><a href="#10、显示网格" class="headerlink" title="10、显示网格"></a>10、显示网格</h3><ul><li>（1）<code>plt.grid(True, linestyle = &quot;-.&quot;, color = &quot;b&quot;, linewidth = &quot;1&quot;)</code>  </li></ul><h3 id="11、显示正方形的坐标区域"><a href="#11、显示正方形的坐标区域" class="headerlink" title="11、显示正方形的坐标区域"></a>11、显示正方形的坐标区域</h3><ul><li>（1）<code>plt.axis(&#39;square&#39;)</code></li></ul><h2 id="三、Scipy"><a href="#三、Scipy" class="headerlink" title="三、Scipy"></a>三、Scipy</h2><h3 id="1、-Scipy特征"><a href="#1、-Scipy特征" class="headerlink" title="1、 Scipy特征"></a>1、 Scipy特征</h3><ul><li>（1）内置了图像处理， 优化，统计等等相关问题的子模块</li><li>（2）scipy 是Python科学计算环境的核心。 它被设计为利用 numpy 数组进行高效的运行。从这个角度来讲，scipy和numpy是密不可分的。</li></ul><h3 id="2、文件操作io"><a href="#2、文件操作io" class="headerlink" title="2、文件操作io"></a>2、文件操作<code>io</code></h3><ul><li>（1）导包：<code>from scipy import io as spio</code></li><li><p>（2）保存<code>mat</code>格式文件</p><p>  <code>spio.savemat(&quot;test.mat&quot;, {&#39;a&#39;:a})</code></p></li></ul><ul><li><p>（3）加载<code>mat</code>文件</p><p>  <code>data = spio.loadmat(&quot;test.mat&quot;)</code><br>  访问值：data[‘a’]–&gt;相当于map</p></li><li>（4）读取图片文件<br>导包：<code>from scipy import misc</code><br>读取：<code>data = misc.imread(&quot;123.png&quot;)</code><br>[注1]：与matplotlib中<code>plt.imread(&#39;fname.png&#39;)</code>类似<br>[注2]：执行<code>misc.imread</code>时可能提醒不存在这个模块，那就安装<code>pillow</code>的包</li></ul><h3 id="3、线性代数操作linalg"><a href="#3、线性代数操作linalg" class="headerlink" title="3、线性代数操作linalg"></a>3、线性代数操作<code>linalg</code></h3><ul><li><p>（1）求行列式<code>det</code></p><p>  <code>res = linalg.det(a)</code></p></li><li><p>（2）求逆矩阵<code>inv</code></p><p>  <code>res = linalg.inv(a)</code><br>  若是矩阵不可逆，则会抛异常<code>LinAlgError: singular matrix</code></p></li><li>（3）奇异值分解<code>svd</code><br>  <code>u,s,v = linalg.svd(a)</code><br>  [注1]：s为a的特征值（一维），降序排列，<br>  [注2]：a = u*s*v’（需要将s转换一下才能相乘）<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">t = np.diag(s)</div><div class="line">print u.dot(t).dot(v)</div></pre></td></tr></table></figure></li></ul><h3 id="4、梯度下降优化算法"><a href="#4、梯度下降优化算法" class="headerlink" title="4、梯度下降优化算法"></a>4、梯度下降优化算法</h3><ul><li><p>（1）<code>fmin_bfgs</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">def f(x):</div><div class="line">    return x**2-2*x</div><div class="line">initial_x = 0</div><div class="line">optimize.fmin_bfgs(f,initial_x)</div></pre></td></tr></table></figure><p>  [注]：initial_x为初始点（此方法可能会得到局部最小值）</p></li><li>（2）<code>fmin()</code>、<code>fmin_cg</code>等等方法</li></ul><h3 id="5、拟合（最小二乘法）"><a href="#5、拟合（最小二乘法）" class="headerlink" title="5、拟合（最小二乘法）"></a>5、拟合（最小二乘法）</h3><ul><li>（1）<code>curve_fit</code><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line">#产生数据</div><div class="line">def f(x):</div><div class="line">    return x**2 + 10*np.sin(x)</div><div class="line">xdata = np.linspace(-10, 10, num=20)</div><div class="line">ydata = f(xdata)+np.random.randn(xdata.size)</div><div class="line">plt.scatter(xdata, ydata, linewidths=3.0, </div><div class="line">           edgecolors=&quot;red&quot;)</div><div class="line">#plt.show()</div><div class="line">#拟合</div><div class="line">def f2(x,a,b):</div><div class="line">    return a*x**2 + b*np.sin(x)</div><div class="line">guess = [2,2]</div><div class="line">params, params_covariance = optimize.curve_fit(f2, xdata, ydata, guess)</div><div class="line">#画出拟合的曲线</div><div class="line">x1 = np.linspace(-10,10,256)</div><div class="line">y1 = f2(x1,params[0],params[1])</div><div class="line">plt.plot(x1,y1)</div><div class="line">plt.show()</div></pre></td></tr></table></figure></li></ul><h3 id="6、统计检验"><a href="#6、统计检验" class="headerlink" title="6、统计检验"></a>6、统计检验</h3><ul><li>（1）T-检验<code>stats.ttest_ind</code><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">a = np.random.normal(0, 1, size=10)</div><div class="line">b = np.random.normal(1, 1, size=10)</div><div class="line">print stats.ttest_ind(a, b)</div></pre></td></tr></table></figure></li></ul><p>输出：(-2.6694785119868358, 0.015631342180817954)<br>后面的是概率p: 两个过程相同的概率。如果其值接近1，那么两个过程几乎可以确定是相同的，如果其值接近0，那么它们很可能拥有不同的均值。</p><h3 id="7、插值"><a href="#7、插值" class="headerlink" title="7、插值"></a>7、插值</h3><ul><li>（1）导入包：<code>from scipy.interpolate import interp1d</code><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">#产生一些数据</div><div class="line">x = np.linspace(0, 1, 10)</div><div class="line">y = np.sin(2 * np.pi * x)</div><div class="line">computed_time = np.linspace(0, 1, 50)</div><div class="line">#线性插值</div><div class="line">linear_interp = interp1d(x, y)</div><div class="line">linear_results = linear_interp(computed_time)</div><div class="line">#三次方插值</div><div class="line">cubic_interp = interp1d(x, y, kind=&apos;cubic&apos;)</div><div class="line">cubic_results = cubic_interp(computed_time)</div><div class="line">#作图</div><div class="line">plt.plot(x, y, &apos;o&apos;, ms=6, label=&apos;y&apos;)</div><div class="line">plt.plot(computed_time, linear_results, label=&apos;linear interp&apos;)</div><div class="line">plt.plot(computed_time, cubic_results, label=&apos;cubic interp&apos;)</div><div class="line">plt.legend()</div><div class="line">plt.show()</div></pre></td></tr></table></figure></li></ul><h3 id="8、求解非线性方程组"><a href="#8、求解非线性方程组" class="headerlink" title="8、求解非线性方程组"></a>8、求解非线性方程组</h3><ul><li>（1）<code>optimize</code>中的<code>fsolve</code><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">from scipy.optimize import fsolve</div><div class="line">def func(x):</div><div class="line">    x0,x1,x2 = x.tolist()</div><div class="line">    return [5*x1-25,5*x0*x0-x1*x2,x2*x0-27]</div><div class="line">initial_x = [1,1,1]</div><div class="line">result = fsolve(func, initial_x)</div><div class="line">print result</div></pre></td></tr></table></figure></li></ul><h2 id="四、pandas"><a href="#四、pandas" class="headerlink" title="四、pandas"></a>四、pandas</h2><h3 id="1、pandas特征与导入"><a href="#1、pandas特征与导入" class="headerlink" title="1、pandas特征与导入"></a>1、pandas特征与导入</h3><ul><li>（1）包含高级的数据结构和精巧的工具</li><li>（2）pandas建造在NumPy之上</li><li>（3）导入：<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">from pandas import Series, DataFrame</div><div class="line">import pandas as pd</div></pre></td></tr></table></figure></li></ul><h3 id="2、pandas数据结构"><a href="#2、pandas数据结构" class="headerlink" title="2、pandas数据结构"></a>2、pandas数据结构</h3><h4 id="（1）Series"><a href="#（1）Series" class="headerlink" title="（1）Series"></a>（1）Series</h4><ul><li>一维的类似的数组对象</li><li><p>包含一个数组的数据（任何NumPy的数据类型）和一个与数组关联的索引</p><ul><li><p>不指定索引：<code>a = Series([1,2,3])</code> ，输出为</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">0    1</div><div class="line">1    2</div><div class="line">2    3</div></pre></td></tr></table></figure><p>包含属性<code>a.index,a.values</code>，对应索引和值</p></li><li>指定索引：<code>a = Series([1,2,3],index=[&#39;a&#39;,&#39;b&#39;,&#39;c&#39;])</code><br>可以通过索引访问<code>a[&#39;b&#39;]</code></li></ul></li><li>判断某个索引是否存在：<code>&#39;b&#39; in a</code></li><li>通过字典建立<code>Series</code><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">dict = &#123;&apos;china&apos;:10,&apos;america&apos;:30,&apos;indian&apos;:20&#125;</div><div class="line">print Series(dict)</div></pre></td></tr></table></figure></li></ul><p>输出：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">america    30</div><div class="line">china      10</div><div class="line">indian     20</div><div class="line">dtype: int64</div></pre></td></tr></table></figure></p><ul><li>判断哪个索引值缺失：<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">dict = &#123;&apos;china&apos;:10,&apos;america&apos;:30,&apos;indian&apos;:20&#125;</div><div class="line">state = [&apos;china&apos;,&apos;america&apos;,&apos;test&apos;]</div><div class="line">a = Series(dict,state)</div><div class="line">print a.isnull()</div></pre></td></tr></table></figure></li></ul><p>输出：（test索引没有对应值）<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">china      False</div><div class="line">america    False</div><div class="line">test        True</div><div class="line">dtype: bool</div></pre></td></tr></table></figure></p><ul><li>在算术运算中它会<strong>自动对齐</strong>不同索引的数据<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">a = Series([10,20],[&apos;china&apos;,&apos;test&apos;])</div><div class="line">b = Series([10,20],[&apos;test&apos;,&apos;china&apos;])</div><div class="line">print a+b</div></pre></td></tr></table></figure></li></ul><p>输出：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">china    30</div><div class="line">test     30</div><div class="line">dtype: int64</div></pre></td></tr></table></figure></p><ul><li>指定<code>Series</code>对象的<code>name</code>和<code>index</code>的<code>name</code>属性<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">a = Series([10,20],[&apos;china&apos;,&apos;test&apos;])</div><div class="line">a.index.name = &apos;state&apos;</div><div class="line">a.name = &apos;number&apos;</div><div class="line">print a</div></pre></td></tr></table></figure></li></ul><p>输出：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">state</div><div class="line">china    10</div><div class="line">test     20</div><div class="line">Name: number, dtype: int64</div></pre></td></tr></table></figure></p><h4 id="（2）DataFrame"><a href="#（2）DataFrame" class="headerlink" title="（2）DataFrame"></a>（2）DataFrame</h4><ul><li><code>Datarame</code>表示一个表格，类似电子表格的数据结构</li><li>包含一个经过<strong>排序的列表集</strong>（按<code>列名</code>排序）</li><li>每一个都可以有不同的类型值（数字，字符串，布尔等等）</li><li><code>DataFrame</code>在内部把数据存储为一个二维数组的格式，因此你可以采用分层索引以表格格式来表示高维的数据</li><li><p>创建：</p><ul><li>通过字典<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">data = &#123;&apos;state&apos;: [&apos;a&apos;, &apos;b&apos;, &apos;c&apos;, &apos;d&apos;, &apos;d&apos;],</div><div class="line">        &apos;year&apos;: [2000, 2001, 2002, 2001, 2002],</div><div class="line">        &apos;pop&apos;: [1.5, 1.7, 3.6, 2.4, 2.9]&#125;</div><div class="line">frame = DataFrame(data)</div><div class="line">print frame</div></pre></td></tr></table></figure></li></ul><p>输出：(按照<strong>列名排好序</strong>的[若是手动分配列名，会按照你设定的]，并且索引会自动分配)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">    pop state  year</div><div class="line">0  1.5     a  2000</div><div class="line">1  1.7     b  2001</div><div class="line">2  3.6     c  2002</div><div class="line">3  2.4     d  2001</div><div class="line">4  2.9     d  2002</div></pre></td></tr></table></figure></li><li><p>访问</p><ul><li><strong>列</strong>：与<code>Series</code>一样，通过列名访问：<code>frame[&#39;state&#39;]</code>或者<code>frame.state</code></li><li><strong>行</strong>：<code>ix</code> 索引成员（field），<code>frame.ix[2]</code>，返回每一列的第3行数据</li></ul></li><li>赋值：<code>frame2[&#39;debt&#39;] = np.arange(5.)</code>，若没有<code>debt</code>列名，则会新增一列</li><li>删除某一列：<code>del frame2[&#39;eastern&#39;]</code></li><li>像Series一样， <code>values</code> 属性返回一个包含在DataFrame中的数据的二维ndarray</li><li>返回所有的列信息：<code>frame.columns</code></li><li>转置：<code>frame2.T</code></li></ul><h4 id="（3）索引对象"><a href="#（3）索引对象" class="headerlink" title="（3）索引对象"></a>（3）索引对象</h4><ul><li>pandas的索引对象用来保存坐标轴标签和其它元数据（如坐标轴名或名称）</li><li>索引对象是不可变的，因此不能由用户改变</li><li>创建<code>index = pd.Index([1,2,3])</code></li><li>常用操作<ul><li><code>append</code>–&gt;链接额外的索引对象，产生一个新的索引</li><li><code>diff</code>    –&gt;计算索引的差集</li><li><code>intersection</code>    –&gt;计算交集</li><li><code>union</code>    –&gt;计算并集</li><li><code>isin</code>    –&gt;计算出一个布尔数组表示每一个值是否包含在所传递的集合里</li><li><code>delete</code>    –&gt;计算删除位置i的元素的索引</li><li><code>drop</code>    –&gt;计算删除所传递的值后的索引</li><li><code>insert</code>    –&gt;计算在位置i插入元素后的索引</li><li><code>is_monotonic</code>    –&gt;返回True，如果每一个元素都比它前面的元素大或相等</li><li><code>is_unique</code>    –&gt;返回True，如果索引没有重复的值</li><li><code>unique</code>    –&gt;计算索引的唯一值数组</li></ul></li></ul><h3 id="3、重新索引reindex"><a href="#3、重新索引reindex" class="headerlink" title="3、重新索引reindex"></a>3、重新索引<code>reindex</code></h3><h4 id="（1）Series-1"><a href="#（1）Series-1" class="headerlink" title="（1）Series"></a>（1）Series</h4><ul><li><p>（1）重新排列</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">a = Series([2,3,1],index=[&apos;b&apos;,&apos;a&apos;,&apos;c&apos;])</div><div class="line">b = a.reindex([&apos;a&apos;,&apos;b&apos;,&apos;c&apos;])</div><div class="line">print b</div></pre></td></tr></table></figure></li><li><p>（2）重新排列，没有的索引补充为0,<code>b=a.reindex([&#39;a&#39;,&#39;b&#39;,&#39;c&#39;,&#39;d&#39;],fill_value=0)</code></p></li><li>（3）重建索引时对值进行内插或填充<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">a = Series([&apos;a&apos;,&apos;b&apos;,&apos;c&apos;],index=[0,2,4])</div><div class="line">b = a.reindex(range(6),method=&apos;ffill&apos;)</div><div class="line">print b</div></pre></td></tr></table></figure></li></ul><p>输出：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">0    a</div><div class="line">1    a</div><div class="line">2    b</div><div class="line">3    b</div><div class="line">4    c</div><div class="line">5    cdata_link</div><div class="line">dtype: object</div></pre></td></tr></table></figure></p><p><code>method</code>的参数<br>ffill或pad—-&gt;前向（或进位）填充<br>bfill或backfill—-&gt;后向（或进位）填充</p><h4 id="（3）DataFrame"><a href="#（3）DataFrame" class="headerlink" title="（3）DataFrame"></a>（3）DataFrame</h4><ul><li>与Series一样，<code>reindex</code> index</li><li>还可以reindex column列，<code>frame.reindex(columns=[&#39;a&#39;,&#39;b&#39;])</code></li></ul><h3 id="4、从一个坐标轴删除条目"><a href="#4、从一个坐标轴删除条目" class="headerlink" title="4、从一个坐标轴删除条目"></a>4、从一个坐标轴删除条目</h3><h4 id="（1）Series-2"><a href="#（1）Series-2" class="headerlink" title="（1）Series"></a>（1）Series</h4><ul><li><code>a.drop([&#39;a&#39;,&#39;b&#39;])</code> 删除a，b索引项<h4 id="（2）DataFrame-1"><a href="#（2）DataFrame-1" class="headerlink" title="（2）DataFrame"></a>（2）DataFrame</h4></li><li>索引项的删除与<code>Series</code>一样</li><li>删除column—&gt;<code>a.drop([&#39;one&#39;], axis=1)</code> 删除column名为one的一列</li></ul><h3 id="5、索引，挑选和过滤"><a href="#5、索引，挑选和过滤" class="headerlink" title="5、索引，挑选和过滤"></a>5、索引，挑选和过滤</h3><h4 id="（1）Series-3"><a href="#（1）Series-3" class="headerlink" title="（1）Series"></a>（1）Series</h4><ul><li>可以通过index值或者整数值来访问数据，eg：对于<code>a = Series(np.arange(4.), index=[&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;])</code>，<code>a[&#39;b&#39;]</code>和<code>a[1]</code>是一样的</li><li>使用标签来切片和正常的Python切片并不一样，它会把结束点也包括在内<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">a = Series(np.arange(4.), index=[&apos;a&apos;, &apos;b&apos;, &apos;c&apos;, &apos;d&apos;])</div><div class="line">print a[&apos;b&apos;:&apos;c&apos;]</div></pre></td></tr></table></figure></li></ul><p>输出包含c索引对应的值</p><h4 id="（2）DataFrame-2"><a href="#（2）DataFrame-2" class="headerlink" title="（2）DataFrame"></a>（2）DataFrame</h4><ul><li>显示前两行：<code>a[:2]</code></li><li>布尔值访问：<code>a[a[&#39;two&#39;]&gt;5]</code></li><li>索引字段 ix 的使用<ul><li>index为2，column为’one’和’two’—&gt;<code>a.ix[[2],[&#39;one&#39;,&#39;two&#39;]]</code></li><li>index为2的一行：<code>a.ix[2]</code></li></ul></li></ul><h3 id="6、DataFrame和Series运算"><a href="#6、DataFrame和Series运算" class="headerlink" title="6、DataFrame和Series运算"></a>6、DataFrame和Series运算</h3><ul><li>（1）DataFrame每一行都减去一个Series<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">a = pd.DataFrame(np.arange(16).reshape(4,4),index=[0,1,2,3],columns=[&apos;one&apos;,    &apos;two&apos;,&apos;three&apos;,&apos;four&apos;])</div><div class="line">print a</div><div class="line">b = Series([0,1,2,3],index=[&apos;one&apos;,&apos;two&apos;,&apos;three&apos;,&apos;four&apos;])</div><div class="line">print b</div><div class="line">print a-b</div></pre></td></tr></table></figure></li></ul><p>输出：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">   one  two  three  four</div><div class="line">0    0    1      2     3</div><div class="line">1    4    5      6     7</div><div class="line">2    8    9     10    11</div><div class="line">3   12   13     14    15</div><div class="line">one      0</div><div class="line">two      1</div><div class="line">three    2</div><div class="line">four     3</div><div class="line">dtype: int64</div><div class="line">   one  two  three  four</div><div class="line">0    0    0      0     0</div><div class="line">1    4    4      4     4</div><div class="line">2    8    8      8     8</div><div class="line">3   12   12     12    12</div></pre></td></tr></table></figure></p><h3 id="7、读取文件"><a href="#7、读取文件" class="headerlink" title="7、读取文件"></a>7、读取文件</h3><ul><li>（1）<code>csv</code>文件<br><code>pd.read_csv(r&quot;data/train.csv&quot;)</code>，返回的数据类型是<code>DataFrame</code>类型</li></ul><h3 id="8、查看DataFrame的信息"><a href="#8、查看DataFrame的信息" class="headerlink" title="8、查看DataFrame的信息"></a>8、查看DataFrame的信息</h3><ul><li>（1）<code>train_data.describe()</code><br>eg:<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">       PassengerId    Survived      Pclass         Age       SibSp  \</div><div class="line">count   891.000000  891.000000  891.000000  714.000000  891.000000   </div><div class="line">mean    446.000000    0.383838    2.308642   29.699118    0.523008   </div><div class="line">std     257.353842    0.486592    0.836071   14.526497    1.102743   </div><div class="line">min       1.000000    0.000000    1.000000    0.420000    0.000000   </div><div class="line">25%     223.500000    0.000000    2.000000   20.125000    0.000000   </div><div class="line">50%     446.000000    0.000000    3.000000   28.000000    0.000000   </div><div class="line">75%     668.500000    1.000000    3.000000   38.000000    1.000000   </div><div class="line">max     891.000000    1.000000    3.000000   80.000000    8.000000</div></pre></td></tr></table></figure></li></ul><h3 id="9、定位到一列并替换"><a href="#9、定位到一列并替换" class="headerlink" title="9、定位到一列并替换"></a>9、定位到一列并替换</h3><ul><li><code>df.loc[df.Age.isnull(),&#39;Age&#39;] = 23 #&#39;Age&#39;列为空的内容补上数字23</code></li></ul><h3 id="10、将分类变量转化为指示变量get-dummies"><a href="#10、将分类变量转化为指示变量get-dummies" class="headerlink" title="10、将分类变量转化为指示变量get_dummies()"></a>10、将分类变量转化为指示变量<code>get_dummies()</code></h3><ul><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">s = pd.Series(list(&apos;abca&apos;))</div><div class="line">pd.get_dummies(s)</div></pre></td></tr></table></figure></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">   a  b  c</div><div class="line">0  1  0  0</div><div class="line">1  0  1  0</div><div class="line">2  0  0  1</div><div class="line">3  1  0  0</div></pre></td></tr></table></figure><h3 id="11、list和string互相转化"><a href="#11、list和string互相转化" class="headerlink" title="11、list和string互相转化"></a>11、list和string互相转化</h3><ul><li><p>string转list</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; str = &apos;abcde&apos;</div><div class="line">&gt;&gt;&gt; list = list(str)</div><div class="line">&gt;&gt;&gt; list</div><div class="line">[&apos;a&apos;, &apos;b&apos;, &apos;c&apos;, &apos;d&apos;, &apos;e&apos;]</div></pre></td></tr></table></figure></li><li><p>list转string</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; str_convert = &apos;,&apos;.join(list)</div><div class="line">&gt;&gt;&gt; str_convert</div><div class="line">&apos;a,b,c,d,e&apos;</div></pre></td></tr></table></figure></li></ul><h3 id="12、删除原来的索引，重新从0-n索引"><a href="#12、删除原来的索引，重新从0-n索引" class="headerlink" title="12、删除原来的索引，重新从0-n索引"></a>12、删除原来的索引，重新从0-n索引</h3><ul><li><code>x = x.reset_index(drop=True)</code></li></ul><h3 id="13、apply函数"><a href="#13、apply函数" class="headerlink" title="13、apply函数"></a>13、apply函数</h3><ul><li>DataFrame.apply(func, axis=0, broadcast=False, raw=False, reduce=None, …..</li><li>df.apply(numpy.sqrt) # returns DataFrame<ul><li>等价==》df.apply(lambda x : numpy.sqrt(x))==&gt;使用更灵活</li></ul></li><li>df.apply(numpy.sum, axis=0) # equiv to df.sum(0)</li><li>df.apply(numpy.sum, axis=1) # equiv to df.sum(1)</li></ul><h3 id="13、re-search-group-函数"><a href="#13、re-search-group-函数" class="headerlink" title="13、re.search().group()函数"></a>13、re.search().group()函数</h3><ul><li>re.search(pattern, string, flags=0)</li><li>group(num=0)函数返回匹配的字符，默认num=0,可以指定<strong>多个组号</strong>，例如group(0,1)</li></ul><h3 id="14、pandas-cut-函数"><a href="#14、pandas-cut-函数" class="headerlink" title="14、pandas.cut()函数"></a>14、pandas.cut()函数</h3><ul><li>pandas.cut(x, bins, right=True, labels=None, retbins=False, precision=3, include_lowest=False)</li><li>x为以为数组</li><li>bins可以是<strong>int值</strong>或者<strong>序列</strong><ul><li>若是int值就根据x分为bins个数的<strong>区间</strong></li><li>若是序列就是自己指定的区间</li></ul></li><li>right<strong>包含</strong>最右边的区间，默认为True</li><li><p>labels <strong>数组</strong>或者<strong>一个布尔值</strong></p><ul><li>若是数组，需要与对应bins的<strong>结果</strong>一致</li><li>若是布尔值<strong>False</strong>，返回bin中的一个值</li></ul></li><li><p>eg:pd.cut(full[“FamilySize”], bins=[0,1,4,20], labels=[0,1,2])</p></li></ul><h3 id="15、添加一行数据"><a href="#15、添加一行数据" class="headerlink" title="15、添加一行数据"></a>15、添加一行数据</h3><ul><li>定义空的dataframe: <code>data_process = pd.DataFrame(columns=[&#39;route&#39;,&#39;date&#39;,&#39;1&#39;,&#39;2&#39;,&#39;3&#39;,&#39;4&#39;,&#39;5&#39;,&#39;6&#39;,&#39;7&#39;,&#39;8&#39;,&#39;9&#39;,&#39;10&#39;,&#39;11&#39;,&#39;12&#39;])</code></li><li>定义一行新的数据，<code>new = pd.DataFrame(columns=[&#39;route&#39;,&#39;date&#39;,&#39;1&#39;,&#39;2&#39;,&#39;3&#39;,&#39;4&#39;,&#39;5&#39;,&#39;6&#39;,&#39;7&#39;,&#39;8&#39;,&#39;9&#39;,&#39;10&#39;,&#39;11&#39;,&#39;12&#39;],index=[j])</code><ul><li>这里index可以随意设置，若是想指定就指定</li></ul></li><li>添加：<code>data_process = data_process.append(new, ignore_index=True)</code>，<ul><li>注意这里是<code>data_process = data_process.......</code></li></ul></li></ul><h2 id="五、scikit-learn"><a href="#五、scikit-learn" class="headerlink" title="五、scikit-learn"></a>五、scikit-learn</h2><h3 id="1、手写数字识别（SVM）"><a href="#1、手写数字识别（SVM）" class="headerlink" title="1、手写数字识别（SVM）"></a>1、手写数字识别（SVM）</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div></pre></td><td class="code"><pre><div class="line">from sklearn import datasets</div><div class="line">from sklearn import svm</div><div class="line">import numpy as np</div><div class="line">from matplotlib import pyplot as plt</div><div class="line"></div><div class="line">&apos;&apos;&apos;</div><div class="line">使用sciki-learn中的数据集，一般有data,target,DESCR等属性属性</div><div class="line">&apos;&apos;&apos;</div><div class="line"></div><div class="line">digits = datasets.load_digits()                 #加载scikit-learn中的数据集</div><div class="line"></div><div class="line">clf = svm.SVC(gamma=0.001,C=100)                    #使用支持向量机进行分类，gamma为核函数的系数</div><div class="line">clf.fit(digits.data[:-4],digits.target[:-4])        #将除最后4组的数据输入进行训练</div><div class="line"></div><div class="line">predict = clf.predict(digits.data[-4:])         #预测最后4组的数据，[-4:]表示最后4行所有数据，而[-4,:]表示倒数第4行数据</div><div class="line"></div><div class="line">print &quot;预测值为：&quot;,predict</div><div class="line">print &quot;真实值：&quot;,digits.target[-4:]</div><div class="line"></div><div class="line">#显示最后四个图像</div><div class="line">plt.subplot(2,2,1)</div><div class="line">plt.imshow(digits.data[-4,:].reshape(8,8))</div><div class="line">plt.subplot(2,2,2)</div><div class="line">plt.imshow(digits.data[-3,:].reshape(8,8))</div><div class="line">plt.subplot(2,2,3)</div><div class="line">plt.imshow(digits.data[-2,:].reshape(8,8))</div><div class="line">plt.subplot(2,2,4)</div><div class="line">plt.imshow(digits.data[-1,:].reshape(8,8))</div><div class="line">plt.show()</div></pre></td></tr></table></figure><p>svm的参数参数解释：</p><ul><li>（1）<strong>C: 目标函数的惩罚系数C，用来平衡分类间隔margin和错分样本的，default C = 1.0；</strong></li><li>（2）<strong>kernel：参数选择有RBF, Linear, Poly, Sigmoid, 默认的是”RBF”;</strong></li><li>（3）degree：if you choose ‘Poly’ in param 2, this is effective, degree决定了多项式的最高次幂；</li><li>（4）<strong>gamma：核函数的系数(‘Poly’, ‘RBF’ and ‘Sigmoid’), 默认是gamma = 1 / n_features;</strong></li><li>（5）coef0：核函数中的独立项，’RBF’ and ‘Poly’有效；</li><li>（6）probablity: 可能性估计是否使用(true or false)；</li><li>（7）shrinking：是否进行启发式；</li><li>（8）tol（default = 1e - 3）: svm结束标准的精度;</li><li>（9）cache_size: 制定训练所需要的内存（以MB为单位）；</li><li>（10）class_weight:每个类所占据的权重，不同的类设置不同的惩罚参数C,缺省的话自适应；</li><li>（11）verbose: 跟多线程有关，不大明白啥意思具体；</li><li>（12）<strong>max_iter: 最大迭代次数，default = 1000， if max_iter = -1, no limited;</strong></li><li>（13）decision_function_shape ： ‘ovo’ 一对一, ‘ovr’ 多对多  or None 无, default=None</li><li>（14）random_state ：用于概率估计的数据重排时的伪随机数生成器的种子。</li></ul><h3 id="2、保存训练过的模型"><a href="#2、保存训练过的模型" class="headerlink" title="2、保存训练过的模型"></a>2、保存训练过的模型</h3><ul><li><code>from sklearn.externals import joblib</code></li><li><code>joblib.dump(clf, &quot;digits.pkl&quot;)  #将训练的模型保存成digits.pkl文件</code></li><li>加载模型：<br><code>clf = joblib.load(&quot;digits.pkl&quot;)</code><br>其余操作数据即可，预测</li></ul><h3 id="3、鸢尾花分类（svm，分离出测试集）"><a href="#3、鸢尾花分类（svm，分离出测试集）" class="headerlink" title="3、鸢尾花分类（svm，分离出测试集）"></a>3、鸢尾花分类（svm，分离出测试集）</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line">from sklearn import datasets</div><div class="line">from sklearn.cross_validation import train_test_split</div><div class="line">from sklearn.svm import SVC</div><div class="line">import numpy as np</div><div class="line">&apos;&apos;&apos;</div><div class="line">加载scikit-learn中的鸢尾花数据集</div><div class="line">&apos;&apos;&apos;</div><div class="line"></div><div class="line">#加载鸢尾花数据集</div><div class="line">iris = datasets.load_iris()</div><div class="line">iris_data = iris.data;          #相当于X</div><div class="line">iris_target = iris.target;      #对应的label种类，相当于y</div><div class="line"></div><div class="line">x_train,x_test,y_train,y_test =     train_test_split(iris_data,iris_target,test_size=0.2)       #将数据分成训练集x_train和测试集x_test，测试集占总数据的0.2</div><div class="line"></div><div class="line">model = SVC().fit(x_train,y_train);     #使用svm在训练集上拟合</div><div class="line">predict = model.predict(x_test)         #在测试集上预测</div><div class="line">right = sum(predict == y_test)          #求预测正确的个数</div><div class="line"></div><div class="line">print (&apos;测试集准确率：%f%%&apos;%(right*100.0/predict.shape[0]))        #求在测试集上预测的正确率，shape[0]返回第一维的长度，即数据个数</div></pre></td></tr></table></figure><p><strong>[另：留一验证法]：</strong>–&gt;每次取一条数据作为测试集，其余作为训练集<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line">from sklearn import datasets</div><div class="line">from sklearn.svm import SVC</div><div class="line">import numpy as np</div><div class="line"></div><div class="line">def data_svc_test(data,target,index):</div><div class="line">    x_train = np.vstack((data[0:index],data[index+1:-1]))#除第index号之外的    数据为训练集</div><div class="line">    x_test = data[index].reshape(1,-1)                    #第index号数据为测试集，reshape(1,-1)的作用是只有一条数据时，使用reshap    e(1,-1)，否则有个过时方法的警告</div><div class="line">    y_train = np.hstack((target[0:index],target[index+1:-1]))</div><div class="line">    y_test = target[index]</div><div class="line">    model = SVC().fit(x_train,y_train)    #建立SVC模型</div><div class="line">    predict = model.predict(x_test)</div><div class="line">    </div><div class="line">    return predict == y_test        #返回结果是否预测正确</div><div class="line"></div><div class="line">#读取数据</div><div class="line">iris = datasets.load_iris()</div><div class="line">iris_data = iris.data</div><div class="line">iris_target = iris.target</div><div class="line">m = iris_target.shape[0]</div><div class="line"></div><div class="line">right = 0;</div><div class="line">for i in range(0,m):</div><div class="line">    right += data_svc_test(iris_data,iris_target,i)</div><div class="line">print (&quot;%f%%&quot;%(right*100.0/m))</div></pre></td></tr></table></figure></p><h3 id="4、房价预测-SVR–-gt-支持向量回归"><a href="#4、房价预测-SVR–-gt-支持向量回归" class="headerlink" title="4、房价预测(SVR–&gt;支持向量回归)"></a>4、房价预测(SVR–&gt;支持向量回归)</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">from sklearn import datasets</div><div class="line">from sklearn.svm import SVR     #引入支持向量回归所需的SVR模型</div><div class="line">from sklearn.cross_validation import train_test_split</div><div class="line">from sklearn.preprocessing import StandardScaler</div><div class="line">import numpy as np</div><div class="line"></div><div class="line">#加载数据</div><div class="line">house_dataset = datasets.load_boston()</div><div class="line">house_data = house_dataset.data</div><div class="line">house_price = house_dataset.target</div><div class="line">#数据预处理--&gt;归一化</div><div class="line">x_train,x_test,y_train,y_test =     train_test_split(house_data,house_price,test_size=0.2)  </div><div class="line">scaler = StandardScaler()</div><div class="line">scaler.fit(x_train)</div><div class="line">x_train = scaler.transform(x_train) #训练集</div><div class="line">x_test = scaler.transform(x_test)   #测试集</div><div class="line"></div><div class="line">#回归，预测</div><div class="line">model = SVR().fit(x_train,y_train)  #使用SVR回归拟合</div><div class="line">predict = model.predict(x_test)     #预测</div><div class="line">result = np.hstack((y_test.reshape(-1,1),predict.reshape(-1,1))) #reshape(-1,1)所有行转为1列向量</div><div class="line">print(result)</div></pre></td></tr></table></figure><h2 id="六、sk-learn模型总结"><a href="#六、sk-learn模型总结" class="headerlink" title="六、sk-learn模型总结"></a>六、sk-learn模型总结</h2><h3 id="0、数据处理"><a href="#0、数据处理" class="headerlink" title="0、数据处理"></a>0、数据处理</h3><h4 id="（1）均值归一化：from-sklearn-preprocessing-import-StandardScaler"><a href="#（1）均值归一化：from-sklearn-preprocessing-import-StandardScaler" class="headerlink" title="（1）均值归一化：from sklearn.preprocessing import StandardScaler"></a>（1）均值归一化：<code>from sklearn.preprocessing import StandardScaler</code></h4><ul><li>scaler = StandardScaler()</li><li>scaler.fit(X_train)</li><li>X_train = scaler.transform(X_train)</li></ul><h4 id="（2）分割数据：from-sklearn-cross-validation-import-train-test-split"><a href="#（2）分割数据：from-sklearn-cross-validation-import-train-test-split" class="headerlink" title="（2）分割数据：from sklearn.cross_validation import train_test_split"></a>（2）分割数据：<code>from sklearn.cross_validation import train_test_split</code></h4><ul><li><code>x_train,x_test,y_train,y_test =     train_test_split(iris_data,iris_target,test_size=0.2)</code></li></ul><h3 id="1、线性模型from-sklearn-import-linear-model"><a href="#1、线性模型from-sklearn-import-linear-model" class="headerlink" title="1、线性模型from sklearn import linear_model"></a>1、线性模型<code>from sklearn import linear_model</code></h3><h4 id="（1）逻辑回归模型"><a href="#（1）逻辑回归模型" class="headerlink" title="（1）逻辑回归模型"></a>（1）逻辑回归模型</h4><ul><li>linear_model.LogisticRegression()</li><li>重要参数<ul><li>C：正则化作用，默认值<code>1.0</code>，值越小，正则化作用<strong>越强</strong></li><li>max_iter：最大梯度下降执行次数，默认值<code>100</code></li><li>tol：停止执行的容忍度，默认值<code>1e-4</code></li></ul></li><li>重要返回值<ul><li>coef_：对应feature的<strong>系数</strong></li></ul></li></ul><h3 id="2、svm模型from-sklearn-import-svm"><a href="#2、svm模型from-sklearn-import-svm" class="headerlink" title="2、svm模型from sklearn import svm"></a>2、svm模型<code>from sklearn import svm</code></h3><h4 id="（1）分类模型"><a href="#（1）分类模型" class="headerlink" title="（1）分类模型"></a>（1）分类模型</h4><ul><li>svm.SVC()</li><li>重要参数<ul><li>kernel：使用的核函数，默认是<code>rbf</code>径向基函数，还有<code>linear，poly，sigmoid ，precomputed</code>核函数</li><li>C：正则化作用，默认值<code>1.0</code>，值越大，<code>margin</code>越大</li><li>tol：停止执行的容忍度，默认值<code>1e-4</code></li><li>gamma：为核函数的系数，值<strong>越大</strong>拟合的越好，默认是<code>1/feature的个数</code></li><li>degree：对应<code>poly</code>核函数</li></ul></li><li>重要返回值</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;一、Numpy&quot;&gt;&lt;a href=&quot;#一、Numpy&quot; class=&quot;headerlink&quot; title=&quot;一、Numpy&quot;&gt;&lt;/a&gt;一、Numpy&lt;/h2&gt;&lt;h3 id=&quot;1、Numpy特征和导入&quot;&gt;&lt;a href=&quot;#1、Numpy特征和导入&quot; class=&quot;headerlink&quot; title=&quot;1、Numpy特征和导入&quot;&gt;&lt;/a&gt;1、Numpy特征和导入&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;（1）用于多维数组的第三方Python包&lt;/li&gt;
&lt;li&gt;（2）更接近于底层和硬件 (高效)&lt;/li&gt;
&lt;li&gt;（3）专注于科学计算 (方便)&lt;/li&gt;
&lt;li&gt;（4）导入包：&lt;code&gt;import numpy as np&lt;/code&gt;     &lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="Python" scheme="http://lawlite.cn/tags/Python/"/>
    
      <category term="机器学习" scheme="http://lawlite.cn/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>搭建自己的VPN</title>
    <link href="http://lawlite.cn/2016/11/05/%E6%90%AD%E5%BB%BA%E8%87%AA%E5%B7%B1%E7%9A%84VPN/"/>
    <id>http://lawlite.cn/2016/11/05/搭建自己的VPN/</id>
    <published>2016-11-05T09:33:50.000Z</published>
    <updated>2017-06-25T08:50:27.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/assets/blog_images/vpn/vpn_setup_00.png" alt="google" title="vpn_setup_00"></p><h3 id="一、首先租一个服务器"><a href="#一、首先租一个服务器" class="headerlink" title="一、首先租一个服务器"></a>一、首先租一个服务器</h3><ul><li>1、租一个香港的服务器，这里我选的按量付费，如果不使用了释放就可以了，按小时收费的，不过要求你账户上要多于<code>100</code>块钱。<a id="more"></a></li><li>2、操作系统选择的64位<code>CentOS6.5</code>，<code>CentOS7</code>以上下面的命令会有所不同。<br><img src="/assets/blog_images/vpn/vpn_setup_02.png" alt="enter description here" title="vpn_setup_02.png"></li><li>3、创建成功后管理控制台会有公网和私网两个<code>ip</code>地址<br><img src="/assets/blog_images/vpn/vpn_setup_03.png" alt="enter description here" title="vpn_setup_03.png"></li></ul><h3 id="二、配置VPN"><a href="#二、配置VPN" class="headerlink" title="二、配置VPN"></a>二、配置VPN</h3><ul><li><p>1、安装<code>ppp</code>和<code>pptpd</code>:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">yum install ppp pptpd</div></pre></td></tr></table></figure></li><li><p>2、配置<code>DNS</code><br><code>/etc/ppp/options.pptpd</code>文件中<code>的ms-dns</code>配置为：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">ms-dns 8.8.8.8</div><div class="line">ms-dns 8.8.4.4</div></pre></td></tr></table></figure></li><li><p>3、配置<code>IP</code><br><code>/etc/pptpd.conf</code>文件中最后加入：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">localip 192.168.0.1</div><div class="line">remoteip 192.168.0.2-254</div></pre></td></tr></table></figure></li><li><p>4、配置<code>VPN</code>用户名和密码<br><code>/etc/ppp/chap-secrets</code>文件中加入：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">userName  pptpd  password  *</div></pre></td></tr></table></figure></li></ul><p>就是<code>userName</code>位置写上你的用户名<code>，password</code>位置写上你的密码</p><ul><li>5、配置IP转发<br><code>/etc/sysctl.conf</code>文件中<code>net.ipv4.ip_forward = 0</code>改为<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">net.ipv4.ip_forward = 1</div></pre></td></tr></table></figure></li></ul><p>然后执行：<code>sysctl -p</code>使其生效</p><h3 id="三、配置防火墙"><a href="#三、配置防火墙" class="headerlink" title="三、配置防火墙"></a>三、配置防火墙</h3><ul><li><p>1、加入防火墙规则</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">iptables -A INPUT -p TCP -i eth1 --dport  1723  --sport 1024:65534 -j ACCEPT</div><div class="line">iptables -t nat -A POSTROUTING -o eth1 -s 192.168.0.0/24 -j MASQUERADE</div><div class="line">iptables -I FORWARD -p tcp --syn -i ppp+ -j TCPMSS --set-mss 1356</div></pre></td></tr></table></figure><ul><li>注意这里指定的网卡是<code>eth1</code>，其对应外网的网卡，否则能够连上<code>VPN</code>，但是是访问不了外网的。</li><li><code>VPN</code>默认的端口是<code>1723</code> </li></ul></li><li><p>2、保存防火墙配置，启动<code>pptpd</code>，让其开机自启动</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">service iptables save</div><div class="line">service iptables restart</div><div class="line">service pptpd start </div><div class="line">chkconfig pptpd on</div></pre></td></tr></table></figure></li></ul><h3 id="四、测试"><a href="#四、测试" class="headerlink" title="四、测试"></a>四、测试</h3><p>1、<code>window</code>或手机等连接</p><ul><li><p>对应外网<code>IP</code>，设置的用户名和密码 </p></li><li><p>速度是可以的，<br><img src="/assets/blog_images/vpn/vpn_setup_04.png" alt="enter description here" title="vpn_setup_04.png"></p></li><li><p>我也测试了一下国外的服务器，速度非常慢，还不如免费的<code>VPN</code>软件，</p></li></ul><h3 id="五、shell脚本"><a href="#五、shell脚本" class="headerlink" title="五、shell脚本"></a>五、<code>shell</code>脚本</h3><ul><li><p>1、我写了一个简单的<code>shell</code>脚本放在了<code>github</code>上，<code>github</code>地址：<a href="https://github.com/lawlite19/Script" target="_blank" rel="external">https://github.com/lawlite19/Script</a></p></li><li><p>2、运行步骤如下：</p><ul><li>下载脚本：<code>wget https://raw.githubusercontent.com/lawlite19/Script/master/shell/vpn_setup.sh</code></li><li>添加执行权限：<code>chmod +x vpn_setup.sh</code></li><li>执行即可：<code>./vpn_setup.sh</code><br>3、完整代码：<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div></pre></td><td class="code"><pre><div class="line">#!/bin/bash</div><div class="line"></div><div class="line"># Author: Wang Yongzhi(bob)</div><div class="line"># Date:   2016.11.16</div><div class="line">echo -e &quot;-----------------------------------------------&quot;</div><div class="line">echo -e &quot;|                   Setup VPN...              |&quot;</div><div class="line">echo -e &quot;-----------------------------------------------\n&quot;</div><div class="line"></div><div class="line"># Step 1:install ppp and pptpd</div><div class="line"></div><div class="line">yum install -y ppp</div><div class="line">yum install -y pptpd</div><div class="line"></div><div class="line">if [ $? -eq 0 ]</div><div class="line">then</div><div class="line">    echo -e &quot;install ppp and pptpd Success!\n&quot;</div><div class="line">else</div><div class="line">    echo -e &quot;Sorry! install ppp and pptpd Failed!\n&quot;</div><div class="line">    exit 0</div><div class="line">fi</div><div class="line"></div><div class="line"># Step 2:configure pptpd DNS</div><div class="line">sed -i -e &apos;/#ms-dns 10.0.0.1/a\ms-dns 8.8.8.8&apos; /etc/ppp/options.pptpd</div><div class="line">sed -i -e &apos;/#ms-dns 10.0.0.2/a\ms-dns 8.8.4.4&apos; /etc/ppp/options.pptpd</div><div class="line"></div><div class="line">if [ $? -eq 0 ]</div><div class="line">then</div><div class="line">    echo -e &quot;Configure DNS Success!\n&quot;</div><div class="line">else</div><div class="line">    echo -e &quot;Configure DNS Failed!\n&quot;</div><div class="line">    exit 0</div><div class="line">fi</div><div class="line"></div><div class="line"></div><div class="line"># Step 3:configure pptpd IP</div><div class="line"></div><div class="line">echo  localip 192.168.0.1 &gt;&gt; /etc/pptpd.conf</div><div class="line">echo  remoteip 192.168.0.2-254 &gt;&gt; /etc/pptpd.conf</div><div class="line"></div><div class="line">if [ $? -eq 0 ]</div><div class="line">then</div><div class="line">    echo -e &quot;Configure pptpd IP Success!\n&quot;</div><div class="line">else</div><div class="line">    echo -e &quot;Configure pptpd IP Failed!\n&quot;</div><div class="line">    exit 0</div><div class="line">fi</div><div class="line"></div><div class="line"># Step 4: configure VPN userName and password</div><div class="line"></div><div class="line">while true</div><div class="line">do</div><div class="line">    read -p &quot;Please input userName:&quot; userName</div><div class="line">    read -p &quot;Please input passwd:  &quot; Passwd</div><div class="line">    echo $userNamepptpd$Passwd \* &gt;&gt; /etc/ppp/chap-secrets</div><div class="line">    read -p &quot;continue?y/N:         &quot; flag</div><div class="line">    if [ $flag = &quot;n&quot; -o $flag = &quot;N&quot; ]</div><div class="line">    then</div><div class="line">        break</div><div class="line">    fi</div><div class="line">done</div><div class="line"></div><div class="line"></div><div class="line"># Step 5: configure forwarding</div><div class="line"></div><div class="line">sed -i &apos;s/net.ipv4.ip_forward = 0/net.ipv4.ip_forward = 1/g&apos; /etc/sysctl.conf</div><div class="line"></div><div class="line">if [ $? -eq 0 ]</div><div class="line">then</div><div class="line">    echo -e &quot;Configure forwarding Success!\n&quot;</div><div class="line">else</div><div class="line">    echo -e &quot;Configure forwarding Failed\n&quot;</div><div class="line">    exit 0</div><div class="line">fi</div><div class="line"></div><div class="line">sysctl -p</div><div class="line"></div><div class="line"># Step 6: configure iptables</div><div class="line"></div><div class="line">#EXTIF=$(ifconfig | head -n 1 | grep -v lo | cut -d &apos; &apos; -f 1)</div><div class="line">iptables -A INPUT -p TCP -i eth1 --dport  1723  --sport 1024:65534 -j ACCEPT</div><div class="line">iptables -t nat -A POSTROUTING -o eth1 -s 192.168.0.0/24 -j MASQUERADE</div><div class="line">iptables -I FORWARD -p tcp --syn -i ppp+ -j TCPMSS --set-mss 1356</div><div class="line"></div><div class="line"># Step 7: configure when start server to start pptpd and iptables</div><div class="line"></div><div class="line">service iptables save</div><div class="line">service iptables restart</div><div class="line">service pptpd start </div><div class="line">chkconfig pptpd on</div><div class="line"></div><div class="line">echo -e &quot;Complete! Now you can connect the VPN throuth your computer or phone!\n&quot;</div><div class="line"></div><div class="line">echo &quot;                *****         *****&quot;</div><div class="line">echo &quot;              *********     *********&quot;</div><div class="line">echo &quot;            ************* *************&quot;</div><div class="line">echo &quot;           *****************************&quot;</div><div class="line">echo &quot;           *****************************&quot;</div><div class="line">echo &quot;           *****************************&quot;</div><div class="line">echo &quot;            ***************************&quot;</div><div class="line">echo &quot;              ***********************&quot;</div><div class="line">echo &quot;                *******************&quot;</div><div class="line">echo &quot;                  ***************&quot;</div><div class="line">echo &quot;                    ***********&quot;</div><div class="line">echo &quot;                      *******&quot;</div><div class="line">echo &quot;                        ***&quot;</div><div class="line">echo &quot;                         *&quot;</div></pre></td></tr></table></figure></li></ul></li></ul><h3 id="六、总结"><a href="#六、总结" class="headerlink" title="六、总结"></a>六、总结</h3><ul><li>最初是在租了一个国外的服务器测试的，没有问题，但是后来租用香港的服务器就出现的了错误，同样的系统、同样的配置，后来查看内网绑定的是网卡eth0,外网绑定的是网卡<code>eth1</code>，而我防火墙里设置的是内网的网卡<code>eth0</code>。而国外的那个服务器只要一个网卡，所以没有问题。另外练练<code>shell</code>脚本。 </li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;/assets/blog_images/vpn/vpn_setup_00.png&quot; alt=&quot;google&quot; title=&quot;vpn_setup_00&quot;&gt;&lt;/p&gt;
&lt;h3 id=&quot;一、首先租一个服务器&quot;&gt;&lt;a href=&quot;#一、首先租一个服务器&quot; class=&quot;headerlink&quot; title=&quot;一、首先租一个服务器&quot;&gt;&lt;/a&gt;一、首先租一个服务器&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;1、租一个香港的服务器，这里我选的按量付费，如果不使用了释放就可以了，按小时收费的，不过要求你账户上要多于&lt;code&gt;100&lt;/code&gt;块钱。
    
    </summary>
    
    
      <category term="翻墙" scheme="http://lawlite.cn/tags/%E7%BF%BB%E5%A2%99/"/>
    
  </entry>
  
  <entry>
    <title>Scrapy爬虫框架模板</title>
    <link href="http://lawlite.cn/2016/10/09/Python%E7%88%AC%E8%99%AB-Scrapy/"/>
    <id>http://lawlite.cn/2016/10/09/Python爬虫-Scrapy/</id>
    <published>2016-10-09T05:42:56.000Z</published>
    <updated>2017-06-25T08:49:09.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="说明"><a href="#说明" class="headerlink" title="说明"></a>说明</h2><ul><li>github地址：<a href="https://github.com/lawlite19/PythonCrawler-Scrapy-Mysql-File-Template" target="_blank" rel="external">https://github.com/lawlite19/PythonCrawler-Scrapy-Mysql-File-Template</a></li><li><p>使用scrapy爬虫框架将数据保存Mysql数据库和文件中</p><a id="more"></a><h2 id="settings-py"><a href="#settings-py" class="headerlink" title="settings.py"></a>settings.py</h2><ul><li><p>修改Mysql的配置信息</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">#Mysql数据库的配置信息</div><div class="line">MYSQL_HOST = <span class="string">'127.0.0.1'</span></div><div class="line">MYSQL_DBNAME = <span class="string">'testdb'</span>         #数据库名字，请修改</div><div class="line">MYSQL_USER = <span class="string">'root'</span>             #数据库账号，请修改 </div><div class="line">MYSQL_PASSWD = <span class="string">'123456'</span>         #数据库密码，请修改</div><div class="line"></div><div class="line">MYSQL_PORT = <span class="number">3306</span>               #数据库端口，在dbhelper中使用</div></pre></td></tr></table></figure></li><li><p>指定pipelines</p></li></ul></li></ul><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">ITEM_PIPELINES = &#123;</div><div class="line">    <span class="string">'webCrawler_scrapy.pipelines.WebcrawlerScrapyPipeline'</span>: <span class="number">300</span>,#保存到mysql数据库</div><div class="line">    <span class="string">'webCrawler_scrapy.pipelines.JsonWithEncodingPipeline'</span>: <span class="number">300</span>,#保存到文件中</div><div class="line">&#125;</div></pre></td></tr></table></figure><h2 id="items-py"><a href="#items-py" class="headerlink" title="items.py"></a>items.py</h2><ul><li>声明需要格式化处理的字段</li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">class WebcrawlerScrapyItem(scrapy.Item):</div><div class="line">    '''定义需要格式化的内容（或是需要保存到数据库的字段）'''</div><div class="line">    # define the fields for your item here like:</div><div class="line">    # name = scrapy.Field()</div><div class="line">    name = scrapy.Field()   #修改你所需要的字段</div><div class="line">    url = scrapy.Field()</div></pre></td></tr></table></figure><h2 id="pipelines-py"><a href="#pipelines-py" class="headerlink" title="pipelines.py"></a>pipelines.py</h2><h3 id="一、保存到数据库的类WebcrawlerScrapyPipeline（在settings中声明）"><a href="#一、保存到数据库的类WebcrawlerScrapyPipeline（在settings中声明）" class="headerlink" title="一、保存到数据库的类WebcrawlerScrapyPipeline（在settings中声明）"></a>一、保存到数据库的类<code>WebcrawlerScrapyPipeline</code>（在settings中声明）</h3><ul><li>定义一个类方法<code>from_settings</code>，得到settings中的Mysql数据库配置信息，得到数据库连接池dbpool</li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">@classmethod</div><div class="line">def from_settings(cls,settings):</div><div class="line">    '''1、@classmethod声明一个类方法，而对于平常我们见到的则叫做实例方法。 </div><div class="line">       2、类方法的第一个参数cls（class的缩写，指这个类本身），而实例方法的第一个参数是self，表示该类的一个实例</div><div class="line">       3、可以通过类来调用，就像C.f()，相当于java中的静态方法'''</div><div class="line">    dbparams=dict(</div><div class="line">        host=settings['MYSQL_HOST'],#读取settings中的配置</div><div class="line">        db=settings['MYSQL_DBNAME'],</div><div class="line">        user=settings['MYSQL_USER'],</div><div class="line">        passwd=settings['MYSQL_PASSWD'],</div><div class="line">        charset='utf8',#编码要加上，否则可能出现中文乱码问题</div><div class="line">        cursorclass=MySQLdb.cursors.DictCursor,</div><div class="line">        use_unicode=False,</div><div class="line">    )</div><div class="line">    dbpool=adbapi.ConnectionPool('MySQLdb',**dbparams)#**表示将字典扩展为关键字参数,相当于host=xxx,db=yyy....</div><div class="line">    return cls(dbpool)#相当于dbpool付给了这个类，self中可以得到</div></pre></td></tr></table></figure><ul><li><code>__init__</code>中会得到连接池dbpool</li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">def __init__(self,dbpool):</div><div class="line">    self.dbpool=dbpool</div></pre></td></tr></table></figure><ul><li><code>process_item</code>方法是pipeline默认调用的，进行数据库操作</li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">#pipeline默认调用</div><div class="line">def process_item(self, item, spider):</div><div class="line">    query=self.dbpool.runInteraction(self._conditional_insert,item)#调用插入的方法</div><div class="line">    query.addErrback(self._handle_error,item,spider)#调用异常处理方法</div><div class="line">    return item</div></pre></td></tr></table></figure><ul><li>插入数据库方法<code>_conditional_insert</code></li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">#写入数据库中</div><div class="line">def _conditional_insert(self,tx,item):</div><div class="line">    #print item['name']</div><div class="line">    sql="insert into testpictures(name,url) values(%s,%s)"</div><div class="line">    params=(item["name"],item["url"])</div><div class="line">    tx.execute(sql,params)</div></pre></td></tr></table></figure><ul><li>错误处理方法<code>_handle_error</code></li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">#错误处理方法</div><div class="line">def _handle_error(self, failue, item, spider):</div><div class="line">    print failue</div></pre></td></tr></table></figure><h3 id="二、保存到文件中的类JsonWithEncodingPipeline（在settings中声明）"><a href="#二、保存到文件中的类JsonWithEncodingPipeline（在settings中声明）" class="headerlink" title="二、保存到文件中的类JsonWithEncodingPipeline（在settings中声明）"></a>二、保存到文件中的类<code>JsonWithEncodingPipeline</code>（在settings中声明）</h3><ul><li>保存为json格式的文件，比较简单，代码如下</li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">class JsonWithEncodingPipeline(object):</div><div class="line">    '''保存到文件中对应的class</div><div class="line">       1、在settings.py文件中配置</div><div class="line">       2、在自己实现的爬虫类中yield item,会自动执行'''    </div><div class="line">    def __init__(self):</div><div class="line">        self.file = codecs.open('info.json', 'w', encoding='utf-8')#保存为json文件</div><div class="line">    def process_item(self, item, spider):</div><div class="line">        line = json.dumps(dict(item)) + "\n"#转为json的</div><div class="line">        self.file.write(line)#写入文件中</div><div class="line">        return item</div><div class="line">    def spider_closed(self, spider):#爬虫结束时关闭文件</div><div class="line">        self.file.close()</div></pre></td></tr></table></figure><h2 id="dbhelper-py"><a href="#dbhelper-py" class="headerlink" title="dbhelper.py"></a>dbhelper.py</h2><ul><li>自己实现的操作Mysql数据库的类</li><li><strong>init</strong>方法，获取settings配置文件中的信息</li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">def __init__(self):</div><div class="line">    self.settings=get_project_settings() #获取settings配置，设置需要的信息</div><div class="line">    </div><div class="line">    self.host=self.settings['MYSQL_HOST']</div><div class="line">    self.port=self.settings['MYSQL_PORT']</div><div class="line">    self.user=self.settings['MYSQL_USER']</div><div class="line">    self.passwd=self.settings['MYSQL_PASSWD']</div><div class="line">    self.db=self.settings['MYSQL_DBNAME']</div></pre></td></tr></table></figure><ul><li>连接到Mysql</li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">#连接到mysql，不是连接到具体的数据库</div><div class="line">def connectMysql(self):</div><div class="line">    conn=MySQLdb.connect(host=self.host,</div><div class="line">                         port=self.port,</div><div class="line">                         user=self.user,</div><div class="line">                         passwd=self.passwd,</div><div class="line">                         #db=self.db,不指定数据库名</div><div class="line">                         charset='utf8') #要指定编码，否则中文可能乱码</div><div class="line">    return conn</div></pre></td></tr></table></figure><ul><li>连接到settings配置文件中的数据库名（MYSQL_DBNAME）</li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">#连接到具体的数据库（settings中设置的MYSQL_DBNAME）</div><div class="line">def connectDatabase(self):</div><div class="line">    conn=MySQLdb.connect(host=self.host,</div><div class="line">                         port=self.port,</div><div class="line">                         user=self.user,</div><div class="line">                         passwd=self.passwd,</div><div class="line">                         db=self.db,</div><div class="line">                         charset='utf8') #要指定编码，否则中文可能乱码</div><div class="line">    return conn</div></pre></td></tr></table></figure><ul><li>创建数据库（settings文件中配置的数据库名）</li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">#创建数据库</div><div class="line">def createDatabase(self):</div><div class="line">    '''因为创建数据库直接修改settings中的配置MYSQL_DBNAME即可，所以就不要传sql语句了'''</div><div class="line">    conn=self.connectMysql()#连接数据库</div><div class="line">    </div><div class="line">    sql="create database if not exists "+self.db</div><div class="line">    cur=conn.cursor()</div><div class="line">    cur.execute(sql)#执行sql语句</div><div class="line">    cur.close()</div><div class="line">    conn.close()</div></pre></td></tr></table></figure><ul><li>还有一些数据库操作方法传入sql语句和参数即可（具体看代码）</li></ul><h2 id="实现具体的爬虫-py（即模板中的pictureSpider-demo-py文件）"><a href="#实现具体的爬虫-py（即模板中的pictureSpider-demo-py文件）" class="headerlink" title="实现具体的爬虫.py（即模板中的pictureSpider_demo.py文件）"></a>实现具体的爬虫.py（即模板中的<code>pictureSpider_demo.py</code>文件）</h2><ul><li>继承<code>scrapy.spiders.Spider</code> 类</li><li>声明三个属性</li></ul><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">name=<span class="string">"webCrawler_scrapy"</span>    #定义爬虫名，要和settings中的BOT_NAME属性对应的值一致</div><div class="line"></div><div class="line">allowed_domains=[<span class="string">"desk.zol.com.cn"</span>] #搜索的域名范围，也就是爬虫的约束区域，规定爬虫只爬取这个域名下的网页</div><div class="line"></div><div class="line">start_urls=[<span class="string">"http://desk.zol.com.cn/fengjing/1920x1080/1.html"</span>]   #开始爬取的地址</div></pre></td></tr></table></figure><ul><li>实现<code>parse</code>方法，该函数名不能改变，因为Scrapy源码中默认callback函数的函数名就是parse</li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">def parse(self, response):</div></pre></td></tr></table></figure><ul><li>返回item</li></ul><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">item=WebcrawlerScrapyItem()  #实例item（具体定义的item类）,将要保存的值放到事先声明的item属性中</div><div class="line">item[<span class="string">'name'</span>]=file_name </div><div class="line">item[<span class="string">'url'</span>]=realUrl</div><div class="line">print item[<span class="string">"name"</span>],item[<span class="string">"url"</span>]    </div><div class="line">                </div><div class="line">yield item  #返回item,这时会自定解析item</div></pre></td></tr></table></figure><h2 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h2><ul><li>测试DBHelper<br>创建testdb数据库和testtable表</li></ul><p><img src="/assets/blog_images/scrapy_images/testDBHelper.gif" alt="创建testdb数据库和testtable表" title="testDBHelper.gif"></p><ul><li>测试爬虫<ul><li>在D盘建立文件夹pics; 图片自动保存到该文件夹中。</li><li><code>scrapy crawl webCrawler_scrapy</code>运行爬虫后会将爬取得图片保存到本地，并且将name和url保存到数据库中</li></ul></li></ul><p><img src="/assets/blog_images/scrapy_images/testCrawl.gif" alt="测试爬虫" title="testCrawl.gif"></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;说明&quot;&gt;&lt;a href=&quot;#说明&quot; class=&quot;headerlink&quot; title=&quot;说明&quot;&gt;&lt;/a&gt;说明&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;github地址：&lt;a href=&quot;https://github.com/lawlite19/PythonCrawler-Scrapy-Mysql-File-Template&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://github.com/lawlite19/PythonCrawler-Scrapy-Mysql-File-Template&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;使用scrapy爬虫框架将数据保存Mysql数据库和文件中&lt;/p&gt;
    
    </summary>
    
    
      <category term="Python" scheme="http://lawlite.cn/tags/Python/"/>
    
      <category term="爬虫" scheme="http://lawlite.cn/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>算法练习</title>
    <link href="http://lawlite.cn/2016/09/09/%E7%AE%97%E6%B3%95%E7%BB%83%E4%B9%A0/"/>
    <id>http://lawlite.cn/2016/09/09/算法练习/</id>
    <published>2016-09-09T05:35:45.000Z</published>
    <updated>2017-06-25T08:51:00.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="说明"><a href="#说明" class="headerlink" title="说明"></a>说明</h2><ul><li>github地址：<a href="https://github.com/lawlite19/AlgorithmExercises" target="_blank" rel="external">https://github.com/lawlite19/AlgorithmExercises</a><h2 id="一、-排序算法"><a href="#一、-排序算法" class="headerlink" title="一、 排序算法"></a>一、 排序算法</h2><h3 id="1-交换排序"><a href="#1-交换排序" class="headerlink" title="1. 交换排序"></a>1. 交换排序</h3><ul><li><a href="https://github.com/lawlite19/AlgorithmExerises/blob/master/一、排序算法/1.交换排序/冒泡排序.cpp" target="_blank" rel="external">冒泡排序</a>      </li></ul></li><li><a href="https://github.com/lawlite19/AlgorithmExercises/blob/master/一、排序算法/1.交换排序/冒泡排序改进1.cpp" target="_blank" rel="external">冒泡排序改进1</a></li><li><a href="https://github.com/lawlite19/AlgorithmExercises/blob/master/一、排序算法/1.交换排序/冒泡排序改进2.cpp" target="_blank" rel="external">冒泡排序改进2</a></li><li><a href="https://github.com/lawlite19/AlgorithmExercises/blob/master/一、排序算法/1.交换排序/冒泡排序改进3.cpp" target="_blank" rel="external">冒泡排序改进3</a><ul><li><a href="https://github.com/lawlite19/AlgorithmExercises/blob/master/一、排序算法/1.交换排序/快速排序.cpp" target="_blank" rel="external">快速排序</a>     </li></ul></li></ul><a id="more"></a><h3 id="2-插入排序"><a href="#2-插入排序" class="headerlink" title="2. 插入排序"></a>2. 插入排序</h3><ul><li><a href="https://github.com/lawlite19/AlgorithmExercises/blob/master/一、排序算法/2.插入排序/直接插入排序.cpp" target="_blank" rel="external">直接插入排序</a><ul><li><a href="https://github.com/lawlite19/AlgorithmExercises/blob/master/一、排序算法/2.插入排序/直接插入排序递归版.cpp" target="_blank" rel="external">直接插入排序递归版</a></li></ul></li><li><a href="https://github.com/lawlite19/AlgorithmExercises/blob/master/一、排序算法/2.插入排序/希尔排序.cpp" target="_blank" rel="external">希尔排序</a></li></ul><h3 id="3-选择排序"><a href="#3-选择排序" class="headerlink" title="3. 选择排序"></a>3. 选择排序</h3><ul><li><a href="https://github.com/lawlite19/AlgorithmExercises/blob/master/一、排序算法/3.选择排序/简单选择排序.cpp" target="_blank" rel="external">简单选择排序</a><ul><li><a href="https://github.com/lawlite19/AlgorithmExercises/blob/master/一、排序算法/3.选择排序/简单选择排序改进.cpp" target="_blank" rel="external">二元选择排序</a></li></ul></li><li><a href="https://github.com/lawlite19/AlgorithmExercises/blob/master/一、排序算法/3.选择排序/堆排序.cpp" target="_blank" rel="external">堆排序</a></li></ul><h3 id="4-归并排序"><a href="#4-归并排序" class="headerlink" title="4. 归并排序"></a>4. 归并排序</h3><ul><li><a href="https://github.com/lawlite19/AlgorithmExercises/blob/master/一、排序算法/4.归并排序/二路归并排序_递归版.cpp" target="_blank" rel="external">二路归并排序递归版</a></li><li><a href="https://github.com/lawlite19/AlgorithmExercises/blob/master/一、排序算法/4.归并排序/二路归并排序_非递归.cpp" target="_blank" rel="external">二路归并排序非递归版</a></li></ul><h2 id="二、-字符串"><a href="#二、-字符串" class="headerlink" title="二、 字符串"></a>二、 字符串</h2><h3 id="1-字符串旋转"><a href="#1-字符串旋转" class="headerlink" title="1. 字符串旋转"></a>1. 字符串旋转</h3><ul><li><a href="https://github.com/lawlite19/AlgorithmExercises/blob/master/二、字符串/1.字符串旋转/字符串旋转_暴力法.cpp" target="_blank" rel="external">字符串旋转_暴力法</a></li><li><a href="https://github.com/lawlite19/AlgorithmExercises/blob/master/二、字符串/1.字符串旋转/字符串旋转_三步翻转法.cpp" target="_blank" rel="external">字符串旋转_三步翻转法</a></li></ul><h3 id="2-字符串包含"><a href="#2-字符串包含" class="headerlink" title="2. 字符串包含"></a>2. 字符串包含</h3><ul><li><a href="https://github.com/lawlite19/AlgorithmExercises/blob/master/二、字符串/2.字符串包含判断/字符串包含判断_暴力法.cpp" target="_blank" rel="external">字符串包含判断_遍历</a></li><li><a href="https://github.com/lawlite19/AlgorithmExercises/blob/master/二、字符串/2.字符串包含判断/字符串包含_排序法.cpp" target="_blank" rel="external">字符串包含判断_排序</a></li><li><a href="https://github.com/lawlite19/AlgorithmExercises/blob/master/二、字符串/2.字符串包含判断/字符串包含判断_素数.cpp" target="_blank" rel="external">字符串包含判断_素数乘积</a></li><li><a href="https://github.com/lawlite19/AlgorithmExercises/blob/master/二、字符串/2.字符串包含判断/字符串包含判断_哈希.cpp" target="_blank" rel="external">字符串包含判断_哈希</a>   ★★★    </li></ul><h3 id="3-回文"><a href="#3-回文" class="headerlink" title="3. 回文"></a>3. 回文</h3><ul><li><a href="https://github.com/lawlite19/AlgorithmExercises/blob/master/二、字符串/3.回文判断/回文判断.cpp" target="_blank" rel="external">回文判断</a></li></ul><h3 id="4-最长回文子串长度"><a href="#4-最长回文子串长度" class="headerlink" title="4. 最长回文子串长度"></a>4. 最长回文子串长度</h3><ul><li><a href="https://github.com/lawlite19/AlgorithmExercises/blob/master/二、字符串/4.最长回文子串/最长回文子串_一般解法.cpp" target="_blank" rel="external">最长回文子串长度_一般解法</a></li><li><a href="https://github.com/lawlite19/AlgorithmExercises/blob/master/二、字符串/4.最长回文子串/最长回文子串_Manacher.cpp" target="_blank" rel="external">最长回文子串长度_Manacher</a>  ★★★   <ul><li><a href="https://github.com/lawlite19/AlgorithmExercises/blob/master/零、算法说明/Manacher.md" target="_blank" rel="external">算法说明</a>    </li></ul></li></ul><h3 id="5-全排列"><a href="#5-全排列" class="headerlink" title="5. 全排列"></a>5. 全排列</h3><ul><li><a href="https://github.com/lawlite19/AlgorithmExercises/blob/master/二、字符串/5.全排列/全排列_递归.cpp" target="_blank" rel="external">全排列_递归</a> </li><li><a href="https://github.com/lawlite19/AlgorithmExercises/blob/master/二、字符串/5.全排列/全排列_字典序排列.cpp" target="_blank" rel="external">全排列_字典序排列</a></li><li><a href="https://github.com/lawlite19/AlgorithmExercises/blob/master/二、字符串/5.全排列/字典序全排列.cpp" target="_blank" rel="external">字典序全排列</a></li></ul><h3 id="6-变形词"><a href="#6-变形词" class="headerlink" title="6. 变形词"></a>6. 变形词</h3><ul><li><a href="https://github.com/lawlite19/AlgorithmExercises/blob/master/二、字符串/6.变形词/变形词判断.cpp" target="_blank" rel="external">变形词判断</a></li></ul><h3 id="7-字符串中数字串之和"><a href="#7-字符串中数字串之和" class="headerlink" title="7. 字符串中数字串之和"></a>7. 字符串中数字串之和</h3><ul><li><a href="https://github.com/lawlite19/AlgorithmExercises/blob/master/二、字符串/7.字符串中数字串求和/字符串中数字串求和.cpp" target="_blank" rel="external">字符串中数字串之和</a> ★  </li></ul><h3 id="8-去除字符串中连续K个0串"><a href="#8-去除字符串中连续K个0串" class="headerlink" title="8. 去除字符串中连续K个0串"></a>8. 去除字符串中连续K个0串</h3><ul><li><a href="https://github.com/lawlite19/AlgorithmExercises/blob/master/二、字符串/8.去除字符串中连续K个0子串/去除字符串中连续K个0子串.cpp" target="_blank" rel="external">去除字符串中连续K个0串</a></li></ul><h3 id="9-整数字符串转整数值"><a href="#9-整数字符串转整数值" class="headerlink" title="9. 整数字符串转整数值"></a>9. 整数字符串转整数值</h3><ul><li><a href="https://github.com/lawlite19/AlgorithmExercises/blob/master/二、字符串/9.整数字符串转为整数值/整数字符串转为整数值.cpp" target="_blank" rel="external">整数字符串转整数值</a> ★★   </li></ul><h3 id="10-字符串匹配问题"><a href="#10-字符串匹配问题" class="headerlink" title="10. 字符串匹配问题"></a>10. 字符串匹配问题</h3><ul><li><a href="https://github.com/lawlite19/AlgorithmExercises/blob/master/二、字符串/10.字符串匹配问题/字符串匹配_KMP.cpp" target="_blank" rel="external">字符串匹配_KMP</a>  ★★★★★<ul><li><a href="https://github.com/lawlite19/AlgorithmExercises/blob/master/零、算法说明/KMP.md" target="_blank" rel="external">算法说明</a></li></ul></li></ul><h2 id="三、-数组和矩阵"><a href="#三、-数组和矩阵" class="headerlink" title="三、 数组和矩阵"></a>三、 数组和矩阵</h2><h3 id="1-二维数组查找"><a href="#1-二维数组查找" class="headerlink" title="1. 二维数组查找"></a>1. 二维数组查找</h3><ul><li><a href="https://github.com/lawlite19/AlgorithmExercises/blob/master/三、数组和矩阵/1.查找/二维数组的查找.cpp" target="_blank" rel="external">二维数组查找</a></li></ul><h3 id="2-矩阵相关操作"><a href="#2-矩阵相关操作" class="headerlink" title="2. 矩阵相关操作"></a>2. 矩阵相关操作</h3><ul><li><a href="https://github.com/lawlite19/AlgorithmExercises/blob/master/三、数组和矩阵/2.矩阵操作/转圈打印矩阵.cpp" target="_blank" rel="external">转圈打印矩阵</a></li></ul><h3 id="3-最小的k个元素"><a href="#3-最小的k个元素" class="headerlink" title="3. 最小的k个元素"></a>3. 最小的k个元素</h3><ul><li><a href="https://github.com/lawlite19/AlgorithmExercises/blob/master/三、数组和矩阵/3.最小的K个数/最小的K个数_堆.cpp" target="_blank" rel="external">最小的k个元素_堆</a></li><li><a href="https://github.com/lawlite19/AlgorithmExercises/blob/master/三、数组和矩阵/3.最小的K个数/最小的K个数_BFPRT算法.cpp" target="_blank" rel="external">最小的k个元素_BFPRT</a>  ★★★★★<ul><li><a href="https://github.com/lawlite19/AlgorithmExercises/blob/master/零、算法说明/BFPRT.md" target="_blank" rel="external">算法说明</a></li></ul></li></ul><h3 id="4-中间数"><a href="#4-中间数" class="headerlink" title="4.中间数"></a>4.中间数</h3><ul><li><a href="https://github.com/lawlite19/AlgorithmExercises/blob/master/三、数组和矩阵/4.中间数/中间数_辅助数组.cpp" target="_blank" rel="external">中间数_辅助数组</a>  ★ </li></ul><h3 id="5-非负数组和为K的最长子数组"><a href="#5-非负数组和为K的最长子数组" class="headerlink" title="5.非负数组和为K的最长子数组"></a>5.非负数组和为K的最长子数组</h3><ul><li><a href="https://github.com/lawlite19/AlgorithmExercises/blob/master/三、数组和矩阵/5.非负数组和为K的最长子数组/非负数组和为K的最长子数组_双指针.cpp" target="_blank" rel="external">非负数组和为K的最长子数组_双指针</a>  ★★★</li></ul><h3 id="8-次数出现大于N-K的数"><a href="#8-次数出现大于N-K的数" class="headerlink" title="8.次数出现大于N/K的数"></a>8.次数出现大于N/K的数</h3><ul><li><a href="https://github.com/lawlite19/AlgorithmExercises/blob/master/三、数组和矩阵/8.数组中出现次数大于N除以K的数/出现次数大于一半的数.cpp" target="_blank" rel="external">次数出现大于N/2的数</a>  ★ </li><li><a href="https://github.com/lawlite19/AlgorithmExercises/blob/master/三、数组和矩阵/8.数组中出现次数大于N除以K的数/出现次数大于N除以K的数.cpp" target="_blank" rel="external">次数出现大于N/K的数</a>  ★★★</li></ul><h3 id="9-逆序对"><a href="#9-逆序对" class="headerlink" title="9.逆序对"></a>9.逆序对</h3><ul><li><a href="https://github.com/lawlite19/AlgorithmExercises/blob/master/三、数组和矩阵/9.逆序对/逆序对_分治(归并).cpp" target="_blank" rel="external">逆序对数_分治归并</a>★ </li></ul><h3 id="10-两个有序数组的中位数"><a href="#10-两个有序数组的中位数" class="headerlink" title="10.两个有序数组的中位数"></a>10.两个有序数组的中位数</h3><ul><li><a href="https://github.com/lawlite19/AlgorithmExercises/blob/master/三、数组和矩阵/10.两个有序数组的中位数/两个有序数组的中位数_分治.cpp" target="_blank" rel="external">两个有序数组的中位数_分治</a>★★★★<ul><li><a href="https://github.com/lawlite19/AlgorithmExercises/blob/master/零、算法说明/MedianOfTwoSortedArrays.md" target="_blank" rel="external">算法说明</a></li></ul></li></ul><h2 id="四、-递归和动态规划"><a href="#四、-递归和动态规划" class="headerlink" title="四、 递归和动态规划"></a>四、 递归和动态规划</h2><h3 id="1-斐波那契问题"><a href="#1-斐波那契问题" class="headerlink" title="1. 斐波那契问题"></a>1. 斐波那契问题</h3><ul><li><a href="https://github.com/lawlite19/AlgorithmExercises/blob/master/四、递归和动态规划/1.斐波那契问题/矩形覆盖_递归.cpp" target="_blank" rel="external">矩形覆盖_递归</a></li><li><a href="https://github.com/lawlite19/AlgorithmExercises/blob/master/四、递归和动态规划/1.斐波那契问题/矩阵覆盖_dp.cpp" target="_blank" rel="external">矩形覆盖_dp</a>  ★ </li><li><a href="https://github.com/lawlite19/AlgorithmExercises/blob/master/四、递归和动态规划/1.斐波那契问题/矩阵覆盖_矩阵转化_class.cpp" target="_blank" rel="external">矩阵覆盖_矩阵转化_class实现</a>  ★★★</li><li><a href="https://github.com/lawlite19/AlgorithmExercises/blob/master/四、递归和动态规划/1.斐波那契问题/矩阵覆盖_矩阵转化_vector.cpp" target="_blank" rel="external">矩阵覆盖_矩阵转化_vector实现</a>  ★★★<ul><li><a href="https://github.com/lawlite19/AlgorithmExercises/blob/master/零、算法说明/Fibonacci.md" target="_blank" rel="external">算法说明</a></li></ul></li><li><a href="https://github.com/lawlite19/AlgorithmExercises/blob/master/四、递归和动态规划/1.斐波那契问题/爬楼梯问题_递归.cpp" target="_blank" rel="external">爬楼梯_递归</a></li><li><p><a href="https://github.com/lawlite19/AlgorithmExercises/blob/master/四、递归和动态规划/1.斐波那契问题/爬楼梯问题_dp.cpp" target="_blank" rel="external">爬楼梯_dp</a>  ★ </p></li><li><p><a href="https://github.com/lawlite19/AlgorithmExercises/blob/master/四、递归和动态规划/1.斐波那契问题/变态跳台阶_递归.cpp" target="_blank" rel="external">变态跳台阶_递归</a></p></li><li><a href="https://github.com/lawlite19/AlgorithmExercises/blob/master/四、递归和动态规划/1.斐波那契问题/变态跳台阶_计算.cpp" target="_blank" rel="external">变态跳台阶_直接计算</a>  ★ </li></ul><h3 id="2-最大子数组和相关问题"><a href="#2-最大子数组和相关问题" class="headerlink" title="2. 最大子数组和相关问题"></a>2. 最大子数组和相关问题</h3><ul><li><a href="https://github.com/lawlite19/AlgorithmExercises/blob/master/四、递归和动态规划/2.最大子数组和相关问题/最大子数组和.cpp" target="_blank" rel="external">最大子数组和_dp</a>  ★ </li><li><a href="https://github.com/lawlite19/AlgorithmExercises/blob/master/四、递归和动态规划/2.最大子数组和相关问题/两个子数组最大和.cpp" target="_blank" rel="external">两个不相容子数组最大和_辅助数组</a> ★★   </li></ul><h3 id="3-最长递增子序列相关问题"><a href="#3-最长递增子序列相关问题" class="headerlink" title="3. 最长递增子序列相关问题"></a>3. 最长递增子序列相关问题</h3><ul><li><a href="https://github.com/lawlite19/AlgorithmExercises/blob/master/四、递归和动态规划/3.最长递增子序列相关问题/最长递增子序列_一般dp.cpp" target="_blank" rel="external">最长递增子序列_一般dp</a></li><li><a href="https://github.com/lawlite19/AlgorithmExercises/blob/master/四、递归和动态规划/3.最长递增子序列相关问题/最长递增子序列_dp优化.cpp" target="_blank" rel="external">最长递增子序列_dp优化</a> ★★   </li><li><a href="https://github.com/lawlite19/AlgorithmExercises/blob/master/四、递归和动态规划/3.最长递增子序列相关问题/摞数组问题_自定义class.cpp" target="_blank" rel="external">摞数组问题(俄国沙皇问题)_纯代码实现</a> ★★★★</li><li><a href="https://github.com/lawlite19/AlgorithmExercises/blob/master/四、递归和动态规划/3.最长递增子序列相关问题/摞数组问题_借助stl.cpp" target="_blank" rel="external">摞数组问题（俄国沙皇问题）_借助stl</a> ★★★★</li></ul><h2 id="五、-栈和队列"><a href="#五、-栈和队列" class="headerlink" title="五、 栈和队列"></a>五、 栈和队列</h2><h3 id="1-getMin功能栈"><a href="#1-getMin功能栈" class="headerlink" title="1. getMin功能栈"></a>1. getMin功能栈</h3><ul><li><a href="https://github.com/lawlite19/AlgorithmExercises/blob/master/五、栈和队列/1.getMin功能栈/getMin功能栈_方案1.cpp" target="_blank" rel="external">getMin功能栈_方案1</a></li><li><a href="https://github.com/lawlite19/AlgorithmExercises/blob/master/五、栈和队列/1.getMin功能栈/getMin功能栈_方案2.cpp" target="_blank" rel="external">getMin功能栈_方案2</a></li></ul><h3 id="2-两个栈实现队列功能"><a href="#2-两个栈实现队列功能" class="headerlink" title="2. 两个栈实现队列功能"></a>2. 两个栈实现队列功能</h3><ul><li><a href="https://github.com/lawlite19/AlgorithmExercises/blob/master/五、栈和队列/2.两个栈实现队列功能/两个栈实现队列.cpp" target="_blank" rel="external">两个栈实现队列</a></li></ul><h2 id="七、二叉树"><a href="#七、二叉树" class="headerlink" title="七、二叉树"></a>七、二叉树</h2><h3 id="1-遍历"><a href="#1-遍历" class="headerlink" title="1. 遍历"></a>1. 遍历</h3><ul><li><a href="https://github.com/lawlite19/AlgorithmExercises/blob/master/七、二叉树/1.二叉树的遍历/二叉树先中后序遍历_递归.cpp" target="_blank" rel="external">先、中、后序遍历_递归</a></li><li><a href="https://github.com/lawlite19/AlgorithmExercises/blob/master/七、二叉树/1.二叉树的遍历/二叉树先中后序遍历_非递归.cpp" target="_blank" rel="external">先、中、后序遍历_非递归</a> ★★   </li></ul><h2 id="八、位运算"><a href="#八、位运算" class="headerlink" title="八、位运算"></a>八、位运算</h2><ul><li><a href="https://github.com/lawlite19/AlgorithmExercises/blob/master/八、位运算/1.出现奇数次的数/出现奇数次的数.cpp" target="_blank" rel="external">出现奇数次的数</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;说明&quot;&gt;&lt;a href=&quot;#说明&quot; class=&quot;headerlink&quot; title=&quot;说明&quot;&gt;&lt;/a&gt;说明&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;github地址：&lt;a href=&quot;https://github.com/lawlite19/AlgorithmExercises&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://github.com/lawlite19/AlgorithmExercises&lt;/a&gt;&lt;h2 id=&quot;一、-排序算法&quot;&gt;&lt;a href=&quot;#一、-排序算法&quot; class=&quot;headerlink&quot; title=&quot;一、 排序算法&quot;&gt;&lt;/a&gt;一、 排序算法&lt;/h2&gt;&lt;h3 id=&quot;1-交换排序&quot;&gt;&lt;a href=&quot;#1-交换排序&quot; class=&quot;headerlink&quot; title=&quot;1. 交换排序&quot;&gt;&lt;/a&gt;1. 交换排序&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/lawlite19/AlgorithmExerises/blob/master/一、排序算法/1.交换排序/冒泡排序.cpp&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;冒泡排序&lt;/a&gt;      &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/lawlite19/AlgorithmExercises/blob/master/一、排序算法/1.交换排序/冒泡排序改进1.cpp&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;冒泡排序改进1&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/lawlite19/AlgorithmExercises/blob/master/一、排序算法/1.交换排序/冒泡排序改进2.cpp&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;冒泡排序改进2&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/lawlite19/AlgorithmExercises/blob/master/一、排序算法/1.交换排序/冒泡排序改进3.cpp&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;冒泡排序改进3&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/lawlite19/AlgorithmExercises/blob/master/一、排序算法/1.交换排序/快速排序.cpp&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;快速排序&lt;/a&gt;     &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="算法" scheme="http://lawlite.cn/tags/%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>非极大值抑制</title>
    <link href="http://lawlite.cn/1018/11/20/%E9%9D%9E%E6%9E%81%E5%A4%A7%E5%80%BC%E6%8A%91%E5%88%B6/"/>
    <id>http://lawlite.cn/1018/11/20/非极大值抑制/</id>
    <published>1018-11-20T03:19:08.000Z</published>
    <updated>2018-11-27T13:13:19.461Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一、"><a href="#一、" class="headerlink" title="一、"></a>一、</h2>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;一、&quot;&gt;&lt;a href=&quot;#一、&quot; class=&quot;headerlink&quot; title=&quot;一、&quot;&gt;&lt;/a&gt;一、&lt;/h2&gt;
      
    
    </summary>
    
    
      <category term="ObjectDetection" scheme="http://lawlite.cn/tags/ObjectDetection/"/>
    
  </entry>
  
</feed>
